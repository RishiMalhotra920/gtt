{"docid": "TST3-SREX-0000", "doctext": "document : Deep Label Distribution Learning With Label Ambiguity Convolutional Neural Networks ( ConvNets ) have achieved excellent recognition performance in various visual recognition tasks . A large labeled training set is one of the most important factors for its success . However , it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation , head pose estimation , multi - label classification and semantic segmentation . Fortunately , there is ambiguous information among labels , which makes these tasks different from traditional classification . Based on this observation , we convert the label of each image into a discrete label distribution , and learn the label distribution by minimizing a Kullback - Leibler divergence between the predicted and ground - truth label distributions using deep ConvNets . The proposed DLDL ( Deep Label Distribution Learning ) method effectively utilizes the label ambiguity in both feature learning and classifier learning , which help prevent the network from over - fitting even when the training set is small . Experimental results show that the proposed approach produces significantly better results than state - of - the - art methods for age estimation and head pose estimation . At the same time , it also improves recognition performance for multi - label classification and semantic segmentation tasks . Label distribution , deep learning , age estimation , head pose estimation , semantic segmentation . section : Introduction Convolutional Neural Networks ( ConvNets ) have achieved state - of - the - art performance on various visual recognition tasks such as image classification , object detection and semantic segmentation . The availability of a huge set of training images is one of the most important factors for their success . However , it is difficult to collect sufficient training images with unambiguous labels in domains such as age estimation , head pose estimation , multi - label classification and semantic segmentation . Therefore , exploiting deep learning methods with limited samples and ambiguous labels has become an attractive yet challenging topic . Why is it difficult to collect a large and accurately labeled training set ? Firstly , it is difficult ( even for domain experts ) to provide exact labels to some tasks . For example , the pixels close to object boundaries are very difficult to label for annotators in semantic segmentation . In addition , pixel labeling is a time - consuming task that may limit the amount of training samples . Another example is that people \u2019s apparent age and head pose is difficult to describe with an accurate number . Secondly , it is very hard to gather complete and sufficient data . For example , it is difficult to build an age dataset covering people from 1 to 85 years old , and ensure that every age in this range has enough associated images . Similar difficulties arise in head pose estimation , where head poses are usually collected at a small set of angles with a 10 or 15 increment . Thus , the publicly available age , head pose and semantic segmentation datasets are small scale compared to those in image classification tasks . These aforementioned small datasets have a common characteristic , i.e. , label ambiguity , which refers to the uncertainty among the ground - truth labels . On one hand , label ambiguity is unavoidable in some applications . We usually predict another person \u2019s age in a way like \u201c around 25 \u201d , which indicates using not only 25 , but also neighboring ages to describe the face . And , different people may have different guesses towards the same face . Similar situations also hold for other types of tasks . The labels of pixels at object boundaries are difficult to annotate because of the inherent ambiguity of these pixels in semantic segmentation . On the other hand , label ambiguity can also happen if we are not confident in the labels we provide for an image . In the multi - label classification task , some objects are clearly visible but difficult to recognize . This type of objects are annotated as Difficult in the PASCAL Visual Object Classes ( VOC ) classification challenge , e.g. , the chair in the third image of the first row in Fig . [ reference ] . [ Age estimation ] [ Head pose estimation ] [ Multi - label classification ] [ Semantic segmentation ] There are two main types of labeling methods : single - label recognition ( SLR ) and multi - label recognition ( MLR ) . SLR assumes one image or pixel has one label and MLR assumes that one image or pixel may be assigned multiple labels . Both SLR and MLR aim to answer the question of which labels can be used to describe an image or pixel , but they can not describe the label ambiguity associated with it . Label ambiguity will help improve recognition performance if it can be reasonably exploited . In order to utilize label correlation ( which may be considered as a consequence of label ambiguity in some applications ) , Geng et al . proposed a label distribution learning ( LDL ) approach for age estimation and head pose estimation . Recently , some improvements of LDL have been proposed . Xing et al . proposed two algorithms named LDLogitBoost and AOSO - LDLogitBoost to learn general models to relax the maximum entropy model in traditional LDL methods . Furthermore , He et al . generated age label distributions through weighted linear combination of the input image \u2019s label and its context - neighboring samples . However , these methods are suboptimal because they only utilize the correlation of neighboring labels in classifier learning , but not in learning the visual representations . Deep ConvNets have natural advantages in feature learning . Existing ConvNet frameworks can be viewed as classification and regression models based on different optimization objective functions . In many cases , the softmax loss and loss are used in deep ConvNet models for classification and regression problems , respectively . The softmax loss maximizes the estimated probability of the ground - truth class without considering other classes , and the loss minimizes the squared difference between the estimated values of the network and the ground - truth . These methods have achieved satisfactory performance in some domains such as image classification , human pose estimation and object detection . However , existing deep learning methods can not utilize the label ambiguity information . Moreover , a well - known fact is that learning a good ConvNet requires a lot of images . In order to solve the issues mentioned above , we convert both traditional SLR and MLR problems to label distribution learning problems . Every instance is assigned a discrete label distribution according to its ground - truth . The label distribution can naturally describe the ambiguous information among all possible labels . Through deep label distribution learning , the training instances associated with each class label is significantly increased without actually increase the number of the total training examples . Fig . [ reference ] intuitively shows four examples of label distribution for different recognition tasks . Then , we utilize a deep ConvNet to learn the label distribution in both feature learning and classifier learning . Since we learn label distribution with deep ConvNets , we call our method DLDL : Deep Label Distribution Learning . The benefits of DLDL are summarized as follows : DLDL is an end - to - end learning framework which utilizes the label ambiguity in both feature learning and classifier learning ; DLDL not only achieves more robust performance than existing classification and regression methods , but also effectively relaxes the requirement for large amount of training images , e.g. , a training face image with ground - truth label 25 is also useful for predicting faces at age 24 or 26 ; DLDL ( only single model without ensemble ) achieves better performance than the state - of - the - art methods on age and head pose estimation tasks . DLDL also improves the performance for multi - label classification and semantic segmentation . The rest of this paper is organized as follows . We first review the related work in Section [ reference ] . Then , Section [ reference ] proposes the DLDL framework , including the DLDL problem definition , DLDL theory , label distribution construction and training details . After that , the experiments are reported in Section [ reference ] . Finally , Section [ reference ] presents discussions and the conclusion is given in Section [ reference ] . section : Related Work In the past two decades , many efforts have been devoted to visual recognition , including at least image classification , object detection , semantic segmentation , and facial attribute ( apparent age and head pose ) estimation . These works can be divided into two streams . Earlier research was mainly based on hand - crafted features , while more recent ones are usually deep learning methods . In this section , we briefly review these related approaches . Methods based on hand - crafted features usually include two stages . The first stage is feature extraction . The second stage learns models for recognition , detection or estimation using these features . SVM , random forest and neural networks have commonly been used during the learning stage . In addition , Geng et al . proposed the label distribution learning approach to utilize the correlation among adjacent labels , which further improved performance on age estimation and head pose estimation . Although important progresses have been made with these features , the hand - crafted features render them suboptimal for particular tasks such as age or head pose estimation . More recently , learning feature representation has shown great advantages . For example , Lu et al . tried to learn cost - sensitive local binary features for age estimation . Deep learning has substantially improved upon the state - of - the - art in image classification , object detection , semantic segmentation and many other vision tasks . In many cases , the softmax loss is used in deep models for classification . Besides classification , deep ConvNets have also been trained for regression tasks such as head pose estimation and facial landmark detection . In regression problems , the training procedure usually optimizes a squared loss function . Satisfactory performance has also been obtained by using Tukey \u2019s biweight function in human pose estimation . In terms of model architecture , deep ConvNet models which use deeper architecture and smaller convolution filters ( e.g. , VGG - Nets and VGG - Face ) are very powerful . Nevertheless , these deep learning methods do not make use of the presence of label ambiguity in the training set , and usually require a large amount of training data . A latest approach , in Inception - v3 , is based on label smoothing ( LS ) . Instead of only using the ground - truth label , they utilize a mixture of the ground - truth label and a uniform distribution to regularize the classifier . However , LS is limited to the uniform distribution among labels rather than mining labels \u2019 ambiguous information . We believe that label ambiguity is too important to ignore . If we make good use of the ambiguity , we expect the required number of training images for some tasks could be effectively reduced . In this paper , we focus on how to exploit the label ambiguity in deep ConvNets . Age and head pose estimation from still face images are suitable applications of the proposed research . In addition , we also extend our works to multi - label classification and semantic segmentation . section : The Proposed DLDL Approach In this section , we firstly give the definition of the DLDL problem . Then , we present the DLDL theory . Next , we propose the construction methods of label distribution for different recognition tasks . Finally , we briefly introduce the DLDL architecture and training details . subsection : The deep label distribution learning problem Given an input image , we are interested in estimating a category output ( e.g. , age or head pose angles ) . For two input images and with ground - truth labels and , and are supposed to be similar to each other if the correlation of and is strong , and vice versa . For example , the correlation between faces aged 32 and 33 should be stronger than that between faces aged 32 and 64 , in terms of facial details that reflect the age ( e.g. , skin smoothness ) . In other words , we expect high correlation among input images with similar outputs . The label distribution learning approach exploited such correlations in the machine learning phase , but used features that are extracted ignoring these correlations . The proposed DLDL approach , however , is an end - to - end deep learning method which utilizes such correlation information in both feature learning and classifier learning . We will also extend DLDL to handle other types of label ambiguity beyond correlation . To fulfill this goal , instead of outputting a single value for an input , DLDL quantizes the range of possible values into several labels . For example , in age estimation , it is reasonable to assume that , and it is a common practice to estimate integer values for ages . Thus , we can define the set as the ordered label set for age estimation . The task of DLDL is then to predict a label distribution , where is the estimated probability that should be predicted to be years old . By estimating an entire label distribution , the deep learning machine is forced to take care of the ambiguity among labels . Specifically , the input space of our framework is , where , and are the height , width , and number of channels of the input image , respectively . DLDL predicts a label distribution vector , where is the label set defined for a specific task ( e.g. , the above ) . We assume is complete , i.e. , any possible value has a corresponding member in . A training data set with instances is then denoted as . We use boldface lowercase letters like to denote vectors , and the - th element of is denoted as . The goal of DLDL is to directly learn a conditional probability mass function from , where is the parameters in the framework . subsection : Deep label distribution learning Given an instance with label distribution , we assume that is the activation of the last fully connected layer in a deep ConvNet . We use a softmax function to turn these activations into a probability distribution , that is , Given a training data set , the goal of DLDL is to find to generate a distribution that is similar to . There are different criteria to measure the similarity or distance between two distributions . For example , if the Kullback - Leibler ( KL ) divergence is used as the measurement of the similarity between the ground - truth and predicted label distribution , then the best parameter is determined by Thus , we can define the loss function as : Stochastic gradient descent is used to minimize the objective function Eq . [ reference ] . For any and , and the derivative of softmax ( Eq . [ reference ] ) is well known , as where is 1 if , and 0 otherwise . According to the chain rule , for any fixed , we have Thus , the derivative of with respect to is Once is learned , the label distribution of any new instance can be generated by a forward run of the network . If the expected class label is a single one , DLDL outputs , where Prediction with multiple labels is also allowed , which could be a set where is a predefined threshold . If the expected output is a real number , DLDL predicts the expectation of , as where . This indicates that DLDL is suitable for both classification and regression tasks . subsection : Label distribution construction The ground - truth label distribution is not available in most existing datasets , which must be generated under proper assumptions . A desirable label distribution must satisfy some basic principles : ( 1 ) should be a probability distribution . Thus , we have and . ( 2 ) The probability values should have difference among all possible labels associated with an image . In other words , a less ambiguous category must be assigned high probability and those more ambiguous labels must have low probabilities . In this section , we propose the way to construct label distributions for age estimation , head pose estimation , multi - label classification and semantic segmentation . For age estimation , we assume that the probabilities should concentrate around the ground - truth age . Thus , we quantize to get using a normal distribution . For example , the apparent age of a face is labeled by hundreds of users . The ground - truth ( including a mean and a standard deviation ) is calculated from all the votes . For this problem , we find the range of the target ( e.g. , ) , quantize it into a complete and ordered label set , where is the label set size and are all possible predictions for . A label distribution is then , where is the probability that ( i.e. , for ) . Since we use equal step size in quantizing , the normal p.d.f . ( probability density function ) is a natural choice to generate the ground - truth from and : where . Fig . [ reference ] shows a face and its corresponding label distribution . For problems where is unknown , we will show that a reasonably chosen also works well in DLDL . For head pose estimation , we need to jointly estimate pitch and yaw angles . Thus , learning joint distribution is also necessary in DLDL . Suppose the label set is , where is a pair of values . That is , we want to learn the joint distribution of two variables . Then , the label distribution can be represented by an matrix , whose - th element is . For example , when we use two angles ( pitch and yaw ) to describe a head pose , is a pair of pitch and yaw angles . Given an instance with ground - truth mean and covariance matrix , we calculate its label distribution as where . In the above , we assume , that is , the covariance matrix is diagonal . Fig . [ reference ] shows a joint label distribution with head pose and . For multi - label classification , a multi - label image always contains at least one object of the class of interest . There are usually multiple labels for an image . These labels are grouped into three different levels , including Positive , Negative and Difficult in the PASCAL VOC dataset . A label is Positive means an image contains objects from that category , and Negative otherwise . Difficult indicates that an object is clearly visible but difficult to recognize . Existing multi - label methods often view Difficult as Negative , which leads to the loss of useful information . It is not reasonable either if we simply treat Difficult as Positive . Therefore , a nature choice is to use label ambiguity . We define different probabilities for different types of labels , as for Positive , Difficult and Negative labels , respectively . Furthermore , an normalization is applied to ensure : where equals , or if the label is Positive , Difficult or Negative , respectively . The label distribution is shown for a multi - label image in Fig . [ reference ] . For semantic segmentation , we need to label a pixel as belonging to one class if it is a pixel inside an object of that class , or as the background otherwise . Let denote the annotation of the - th pixel , where ( assuming there are categories and 0 for background ) . Fully Convolutional Networks ( FCN ) have been an effective solution to this task . In FCN , a ground - truth label means that and for all . However , it is very difficult to specify ground - truth labels for pixels close to object boundaries , because labels of these pixels are inherently ambiguous . We propose a mechanism to describe the label ambiguity in the boundaries . Considering a Gaussian kernel matrix , we replace the original label distribution with , as where , , is the kernel size , and are padding and stride sizes . In our experiment , we set , and , and the generated label distribution is Fig . [ reference ] gives the semantic label distribution for a bird image which shows that the ambiguity is encoded in the label distributions . subsection : The DLDL architecture and training details We use a deep ConvNet and a training set to learn a as the estimation of . The structure of our network is based on popular deep models such as ZF - Net and VGG - Nets . The ZF - Net consists five convolution layers , followed by three fully connected layers . The VGG - Nets architecture includes 16 or 19 layers . We modify the last fully connected layer \u2019s output based on the task and replace the original softmax loss function with the KL loss function . In addition , we use the parameter ReLU for ZF - Net . In our network , the input is an order three tensor and the output may be a vector ( age estimation and multi - label classification ) , a matrix ( head pose estimation ) or a tensor ( semantic segmentation ) . In this paper , we train the deep models in two ways : Training from scratch . For ZF - Net , the initialization is performed randomly , based on a Gaussian distribution with zero mean and 0.01 standard deviation , and biases are initialized to zero . The coefficient of the parameter ReLU is initialized to 0.25 . The dropout is applied to the last two fully connected layers with rate 0.5 . The coefficient of weight decay is set to . Optimization is done by Stochastic Gradient Descent ( SGD ) using mini - batches of 128 and the momentum coefficient is 0.9 . The initial learning rate is set to 0.01 . The total number of epochs is about 20 . Fine - tuning . Three pre - trained models including VGG - Nets ( 16 - layers and 19 - layers ) and VGG - Face ( 16 - layers ) are used to fine - tune for different tasks . We remove these pre - trained models \u2019 classification layer and loss layer , and put in our label distribution layer which is initialized by the Gaussian distribution and the KL loss layer . The learning rates of the convolutional layers , the first two fully - connected layers and the label distribution layer are initialized as 0.001 , 0.001 and 0.01 , respectively . We fine - tune all layers by back propagation through the whole net using mini - batches of 32 . The total number of epochs is about 10 for age estimation and 20 for multi - label classification . section : Experiments We evaluate DLDL on four tasks , i.e. , age estimation , head pose estimation , multi - label classification and semantic segmentation . Our implementation is based on MatConvNet . All our experiments are carried out on a NVIDIA K40 GPU with 12 GB of onboard memory . subsection : Age estimation Datasets . Two age estimation datasets are used in our experiments . The first is Morph , which is one of the largest publicly available age datasets . There are 55 , 134 face images from more than 13 , 000 subjects . Ages range from 16 to 77 . Since no TRAIN / TEST split is provided , 10 - fold cross - validation is used for Morph . The second dataset is from the apparent age estimation competition , the first competition track of the ICCV ChaLearn LAP 2015 workshop . Compared with Morph , this dataset ( ChaLearn ) consists of images collected in the wild , without any position , illumination or quality restriction . The only condition is that each image contains only one face . The dataset has 4 , 699 images , and is split into 2 , 476 training ( TRAIN ) , 1 , 136 validation ( VAL ) and 1 , 087 testing ( TEST ) images . The apparent age ( i.e. , how old does this person look like ) of each image is labeled by multiple individuals . The age of face images range from 3 to 85 . For each image , its mean age and the corresponding standard deviation are given . Since the ground - truth for TEST images are not published , we train on the TRAIN split and evaluate on the VAL split of ChaLearn images . Baselines . To demonstrate the effectiveness of DLDL , we firstly consider two related methods as baselines : ConvNet + LS ( KL ) and ConvNet + LD ( - div ) . The former uses label smoothing ( LS ) as ground - truth and KL divergence as loss function . The latter uses label distribution ( LD ) as ground - truth and divergence as loss function , which is In addition , we also compare DLDL with the following baseline methods : BFGS - LDL Geng et al . proposed the label distribution learning approach ( IIS - LLD ) for age and head pose estimation . They used traditional image features . To further improve IIS - LLD , Geng et al . proposed a BFGS - LDL algorithm by using the effective quasi - Newton optimization method BFGS . C - ConvNet Classification ConvNets have obtained very competitive performance in various computer vision tasks . ZF - Net and VGG - Net are popular models which use the softmax loss . We replace the ImageNet - specific 1000 - way classification in these modes with the label set . R - ConvNet ConvNets are also successively trained for regression tasks . In R - ConvNet , the ground - truth label ( age and pose angle ) is projected into the range by the mapping , where and are the maximum and minimum values in the training label set . During prediction , the R - ConvNet regression result is reverse mapped to get . To speed up convergence , the last fully connected layer is followed a hyperbolic tangent activation function , which maps to . The squared , and - ins loss functions are used in R - ConvNet . Implementation details . We use the same preprocessing pipeline for all compared methods , including face detection , facial key points detection and face alignment , as shown in Fig [ reference ] . We employ the DPM model to detect the main facial region . Then , the detected face is fed into cascaded convolution networks to get the five facial key points , including the left / right eye centers , nose tip and left / right mouth corners . Finally , based on these facial points , we align the face to the upright pose . Data augmentation are only applied to the training images for ChaLearn . For one color input training image , we generate its gray - scale version , and left - right flip both color and gray - scale versions . Thus , every training image turns into 4 images . [ Input ] [ Detection ] [ Facial points ] [ Alignment ] We define for both datasets . The label distribution of each image is generated using Eq . [ reference ] . The mean is provided in both Morph and ChaLearn . The standard deviation , however , is provided in ChaLearn but not in Morph . We simply set in Morph . Experiments for different methods are conducted under the same data splits . 1 Used 80 % of Morph images for training and 20 % for evaluation ; 2 Used additional external face images ( i.e. , IMDB - WIKI ) ; 3 Used pre - trained model ( i.e. , VGG - Nets or VGG - Face ) . Evaluation criteria . Mean Absolute Error ( MAE ) and Cumulative Score ( CS ) are used to evaluate the performance of age estimation . MAE is the average difference between the predicted and the real age : where and are the estimated and ground - truth age of the - th testing image , respectively . CS is defined as the accuracy rate of correct estimation : where is the number of correct estimation , i.e. , testing images that satisfy . In our experiment , . In addition , a special measurement ( named - error ) is defined by the ChaLearn competition , computed as Results . Table [ reference ] lists results on both datasets . The upper part shows results in the literature . The middle part shows the baseline results . The lower part shows the results of the proposed approach . The first term in the parenthesis behind each method is the loss function corresponding to the method . Max or Exp represent predicting according to Eq . [ reference ] or [ reference ] , respectively . Since cross - validation is used in Morph , we also provide its standard deviations . [ ChaLearn ] [ Morph ] [ AFLW ] [ subfigure ] labelformat = empty [ 40 ] [ 19 ] [ 62 ] [ 23 ] [ 38 ] [ 24 ] [ 26 ] [ 66 ] [ 52 ] [ 22 ] [ red39.69 ] [ red19.29 ] [ red61.61 ] [ red22.94 ] [ red37.87 ] [ red24.27 ] [ red25.40 ] [ blue60.17 ] [ blue35.06 ] [ blue28.55 ] From Table [ reference ] , we can see that DLDL consistently outperforms baselines and other published methods . The difference between DLDL ( KL , Max ) and its competitor C - ConvNet ( softmax , Max ) is 0.51 on Morph . This gap is more than 6 times the sum of their standard deviations ( 0.03 + 0.05 ) , showing statistically significant differences . The advantage of DLDL over R - ConvNet , C - ConvNet and ConvNet + LS suggests that learning label distribution is advantageous in deep end - to - end models . DLDL has much better results than BFGS - LDL , which shows that the learned deep features are more powerful than manually designed ones . Compared to ConvNet + LD ( - div ) , DLDL ( KL ) achieves lower MAE on both datasets . It indicates that KL - divergence is better than - divergence for measuring the similarity of two distributions in this context . We find that C - ConvNet and R - ConvNet are not stable . The R - ConvNet ( ) method , although being the second best method for ChaLearn , is inferior to C - ConvNet ( softmax , Exp ) for Morph . In addition , we also find that Eq . [ reference ] is better than Eq . [ reference ] in many cases , which suggests that Eq . [ reference ] is more suitable than Eq . [ reference ] for age estimation . Fine - tuning DLDL . Instead of training DLDL from scratch , we also fine - tune the network of VGG - Face . On the small scale ChaLearn dataset , the MAE of DLDL is reduced from 5.34 to 3.51 , yielding a significant improvement . The - error of DLDL is reduced from 0.44 to 0.31 , which is close to the best competition result 0.28 on the validation set . In , external training images ( 260 , 282 additional external training images with real age annotation ) were used . DLDL only uses the ChaLearn dataset \u2019s 2 , 476 training images and is the best among ChaLearn teams that do not use external data . In the competition , the best external - data - free - error is 0.48 , which is worse than DLDL \u2019s . However , the idea in to use external data is useful for further reducing DLDL \u2019s estimation error . Fig . [ reference ] and Fig . [ reference ] show the CS curves on ChaLearn and Morph datasets . At every error level , our DLDL fine - tuned VGG - Face always achieves the best accuracy among all methods . It is noteworthy that the CS curves of DLDL ( KL , Max ) and ConvNet ( - div , Max ) are very close to that of the DLDL + VGG - Face ( KL , Max ) on Morph even without lots of external data and very deep model . This observation supports the idea that using DLDL can achieve competitive performance even with limited training samples . In Fig . [ reference ] , we show some examples of face images from the ChaLearn validation set and predicted label distributions by DLDL ( KL , Exp ) . In many cases , our solution is able to accurately predict the apparent age of faces . Failures may come from two causes . The first is the failure to detect or align the face . The second is some extreme conditions of face images such as occlusion , low resolution , heavy makeup and old photos . subsection : Head pose estimation Datasets . We use three datasets in head pose estimation : Pointing\u201904 , BJUT - 3D and Annotated Facial Landmarks in the Wild ( AFLW ) . In them , head pose is determined by two angles : pitch and yaw . Pointing\u201904 discretizes the pitch into 9 angles and the yaw into 13 angles . When the pitch angel is or , the yaw angle is always set to . Thus , there are 93 poses in total . The head images are taken from 15 different human subjects in two different time periods , resulting in images . BJUT - 3D contains 500 3D faces ( 250 male and 250 female people ) , acquired by a CyberWare Laser Scanner in an engineered environment . 9 pitch angles and 13 yaw angles are used . There are in total 93 poses in this dataset , similar to that in Pointing\u201904 . Therefore , face images are obtained . Unlike Pointing\u201904 and BJUT - 3D , the AFLW is a real - world face database . Head pose is coarsely obtained by fitting a mean 3D face with the POSIT algorithm . The dataset contains about 24k faces in real - world images . We select 23 , 409 faces to ensure pitch and yaw angles within . Implementation details . The head region is provided by bounding box annotations in Pointing\u201904 and AFLW . The BJUT - 3D does not contain background regions . Therefore , we will not perform any preprocessing . In DLDL , we set in Pointing\u201904 and in BJUT - 3D for constructing label distributions . For AFLW , ground - truth of head pose angles are given as real numbers . Ground - truth ( pitch and yaw ) angles are divided from to in steps of , so we get ( pitch , yaw ) pair category labels . We set for AFLW . Since the discrete Jeffrey \u2019s divergence is used in LDL , we implement BFGS - LDL with the Kullback - Leibler divergence . All experiments are performed under the same setting , including data splits , input size and network architecture . To validate the effectiveness of DLDL for head pose estimation , we use the same baselines as age estimation . Our experiments show that Eq . [ reference ] has lower accuracy than Eq . [ reference ] . Hence , we use Eq . [ reference ] in this section . Evaluation criteria . Three types of prediction values are evaluated : pitch , yaw , and pitch + yaw , where pitch + yaw jointly estimates the pitch and yaw angles . Two different measurements are used , which is MAE ( Eq . [ reference ] ) and classification accuracy ( Acc ) . When we treat different poses as different classes , Acc measures the pose class classification accuracy . In particular , the MAE of pitch + yaw is calculated as the Euclidean distance between the predicted ( pitch , yaw ) pair and the ground - truth pair ; the Acc of pitch + yaw is calculated by regarding each ( pitch , yaw ) pair as a class . For R - ConvNet , we only report its MAE but not Acc , because its predicted value are continuous real numbers . All methods are tested with 5 - fold cross validation for Pointing\u201904 and BJUT - 3D following . For AFLW , 15 , 561 face images are randomly chosen for training , and the remaining 7 , 848 for evaluation . The setup is similar to the recent literature ( 14 , 000 images for training and the rest 7 , 041 images for testing ) . [ subfigure ] labelformat = empty [ ( + 77 , - 4 ) ] [ ( - 16 , - 1 ) ] [ ( - 1 , - 30 ) ] [ ( + 30 , + 8 ) ] [ ( + 4 , - 4 ) ] [ ( - 36 , + 13 ) ] [ ( - 87 , - 3 ) ] [ ( - 61 , - 58 ) ] [ ( + 63 , + 12 ) ] [ ( + 80 , - 27 ) ] [ red (+ 75 , - 3 ) ] [ red (- 15 , 0 ) ] [ red (- 3 , - 27 ) ] [ red (+ 27 , + 6 ) ] [ red (+ 6 , - 3 ] [ red (- 39 , + 15 ] [ red (- 87 , 0 ) ] [ blue (- 3 , - 12 ) ] [ blue (+ 21 , + 18 ] [ blue (+ 45 , - 15 ) ] Results . Tables [ reference ] , [ reference ] and [ reference ] show results on Pointing\u201904 , BJUT - 3D and AFLW , respectively . Pointing\u201904 is small scale with only 2 , 790 images . We observe that BFGS - LDL ( with hand - crafted features ) has much lower MAE and much higher accuracy than deep learning methods C - ConvNet , R - ConvNet and ConvNet + LS . One reasonable conjecture is that C - ConvNet , R - ConvNet and ConvNet + LS are not well - learned with only small number of training images . DLDL , however , successfully learns the head pose . For example , its accuracy for pitch + yaw is 73.15 ( and C - ConvNet is only 42.97 ) . That is , DLDL is able to perform deep learning with few training images , while C - ConvNet R - ConvNet and ConvNet + LS have failed for this task . On BJUT - 3D and AFLW which have enough training data , we observe that many deep learning methods show higher performance than BFGS - LDL . DLDL achieves the best performance : it has much lower MAE and higher accuracy than other methods . Another observation is also worth mentioning . Although R - ConvNet is better than C - ConvNet when label is dense such as age estimation and head pose estimation on AFLW , it is obviously worse than C - ConvNet on BJUT - 3D and pointing\u201904 for head pose estimation which have sparse labels . In other words , the performance of C - ConvNet and R - ConvNet are not very robust , while the proposed method consistently achieves excellent performance . Fig . [ reference ] shows the pitch + yaw CS curves on the AFLW dataset . There is an obvious gap between DLDL and baseline methods at every error level . Fig . [ reference ] shows the predicted label distributions for different head poses on the AFLW testing set using the DLDL model . Our approach can estimate head pose with low errors but may fail under some extreme conditions . It is noteworthy that DLDL may produce more incorrect estimations when both yaw and pitch are large ( e.g. , ) . The reason might be that there are much fewer training examples for large angles than for other angles . subsection : Multi - label classification Datasets . We evaluate our approach for multi - label classification on the PASCAL VOC dataset : PASCAL VOC2007 and VOC2012 . There are 9 , 963 and 22 , 531 images in them , respectively . Each image is annotated with one or several labels , corresponding to 20 object categories . These images are divided into three subsets including TRAIN , VAL and TEST sets . We train on the TRAINVAL set and evaluate on the TEST set . The evaluation metric is average precision ( AP ) and mean average precision ( mAP ) , complying with the PASCAL challenge protocols . We denote our methods as Images - Fine - tuning - DLDL ( IF - DLDL ) and Proposals - Fine - tuning - DLDL ( PF - DLDL ) when ConvNets are fine - tuned by images and proposals of images , respectively . Details of these two variants are explained later in this section . We compare the proposed approaches with the following methods : VGG + SVM [ ] . This method densely extracted 4 , 096 dimensional ConvNet features at the penultimate layer of VGG - Nets pre - trained on ImageNet . These features from different scales ( smallest image side ) were aggregated by average pooling . Then , these averaged features from two networks ( \u201c Net - D \u201d containing 16 layers and \u201c Net - E \u201d containing 19 layers ) were further fused by stacking . Finally , normalized the resulting image features and used these features to train a linear SVM classifier for multi - label classification . HCP [ ] . HCP proposed to solve the multi - label object recognition task by extracting object proposals from the images . The method used image label and square loss to fine - tune a pre - trained ConvNet . Then , BING or EdgeBoxes was used to extract object proposals , which were used to fine - tune the ConvNet again . Finally , scores of these proposals were max - pooled to obtain the prediction . Fev + Lv [ ] . This approach transformed the multi - label object recognition problem into a multi - class multi - instance learning problem . Two views ( label view and feature view ) were extracted for each proposal of images . Then , these two views were encoded by a Fisher vector for each image . IF - VGG - \u21132 and IF - VGG - KL . We fine - tune the VGG - Nets with square loss and multi - label cross - entropy loss and use them as our IF - DLDL \u2019s baselines . They are trained using the same setting . Implementation details . According to the ground - truth labels , we set different probabilities for all possible labels on PASCAL VOC dataset . In our experiments , , , . Finally , similar to label smoothing , a uniform distribution is added to , where . IF - DLDL . Following , each training image is individually rescaled by randomly sampling in the range [ 256 , 512 ] . We randomly crop patches from these resized images . We also adjust the pooling kernel in the pool5 layer from to . Max - pooling and Avg - pooling are used at pool5 to train two ConvNets . We obtain four ConvNet models thought fine - tuning \u201c Net - D \u201d and \u201c Net - E \u201d . At the prediction stage , the smaller side of each image is scaled to a fixed length . Each scaled image is fed to the fine - tuned ConvNets to obtain the 20 - dim probability outputs . These probability outputs from different scales and different models are averaged to form the final prediction . PF - DLDL . Following , we further fine - tune IF - DLDL models with proposals of images to boost performance . For each training image , we employ EdgeBoxes to produce a set of proposal bounding boxes which are grouped into clusters by the normalized cut algorithm . For each cluster , the top proposals with higher predictive scores generated by EdgeBoxes are resized into square shapes ( i.e. , ) . As a result , we can obtain proposals for an image . Finally , these resized proposals are fed into a fine - tuned IF - DLDL model to obtain prediction scores and these scores are fused by max - pooling to form the prediction distribution of the image . This process can be learned by using an end - to - end way . In our implementation , we set and at the training and the prediction stage , respectively . Similar to IF - DLDL , we also average fuse prediction scores of different models to generate the final prediction . Results . In Table [ reference ] , we compare single model results ( average AP of all classes ) on VOC2007 . Our PF - DLDL defeats all the other methods . Compared with Fev + Lv , 1.7 % improvement can be achieved by PF - DLDL even without using the bounding box annotation . Compared with HCP - VGG , our PF - DLDL can achieve 92.3 % mAP , which is significantly higher than their 90.9 % . This further indicates that it is very important to learn a label distribution . Table [ reference ] and [ reference ] report details of all experimental results on VOC2007 and VOC2012 , respectively . It can be seen that IF - DLDL outperforms IF - VGG - by 1.1 % for VOC2007 and 1.3 % for VOC2012 , which indicates that the KL loss function is more suitable than loss for measuring the similarity of two label distributions . Furthermore , IF - DLDL improves IF - VGG - KL for about 0.2\u20130.3 points in mAP , which suggests that learning a label distribution is beneficial . More importantly , PF - DLDL can achieve 93.4 % for VOC2007 and 92.4 % for VOC2012 in mAP when we average fuse output scores of four PF - DLDL models . Our framework shows good performance especially for scene categories such as \u201c chair \u201d , \u2018 table \u201d and \u201c sofa \u201d . Although PF - DLDL significantly outperforms IF - DLDL in mAP , PF - DLDL has higher computational cost than IF - DLDL on both training and testing stages . Since IF - DLDL does not need region proposals or bounding box information , it may be effectively and efficiently implemented for practical multi - label application such as multi - label image retrieval . It is also possible that by adopting new techniques ( such as the region proposal method using gated unit in , which has higher accuracy that ours on VOC tasks ) , the accuracy of our DLDL methods can be further improved . [ ChaLearn ] [ Morph ] [ Pointing\u201904 ] [ AFLW pitch ] [ AFLW yaw ] BFGS - LDL DLDL subsection : Semantic segmentation Datasets . We employ the PASCAL VOC2011 segmentation dataset and the Semantic Boundaries Dataset ( SBD ) for training the proposed DLDL . There are 2 , 224 images ( 1 , 112 for training and 1 , 112 for testing ) with pixel labels for 20 semantic categories in VOC2011 . SBD contains 11 , 355 annotated images ( 8 , 984 for training and 2 , 371 for testing ) from Hariharan et al . . Following FCN , we train DLDL using the union set ( 8 , 825 images ) of SBD and VOC2011 training images . We evaluate the proposed approach on VOC2011 ( 1 , 112 ) and VOC2012 ( 1 , 456 ) test images . Evaluation criteria . The performance is measured in terms of mean IU ( intersection over union ) , which is the most widely used metric in semantic segmentation . We keep the same settings as FCN including training images and model structure . The main change is that we employ KL divergence as the loss function based on label distribution ( Eq . [ reference ] ) . Note that although we transform the ground - truth to label distribution in the training process , our evaluation rely only on ground - truth label . Recently , Conditional Random Field ( CRF ) has been broadly used in many state - of - the - art semantic segmentation systems . We optionally employ a fully connected CRF to refine the predicted category score maps using the default parameters of . Results . Table [ reference ] gives the performance of DLDL - 8s and DLDL - 8s - CRF on the test images of VOC2011 and VOC2012 and compares it to the well - known FCN - 8s . DLDL - 8s improves the mean IU of FCN - 8s form 62.7 % to 64.9 % on VOC2011 . On VOC2012 , DLDL - 8s leads to an improvement of 2.3 points in mean IU . DLDL achieves better results than FCN , which suggests it is important to improve the segmentation performance using label ambiguity . In addition , the CRF further improve performance of DLDL - 8s , offering a 2.6 % absolute increase in mean IU both on VOC2011 and VOC2012 . Fig . [ reference ] shows four semantic segmentation examples from the VOC2011 validation images using FCN - 8s , DLDL - 8s and DLDL - 8s - CRF . We can see that DLDL - 8s can successfully segment some small objects ( e.g. , car and bicycle ) and particularly improve the segmentation of object boundaries ( e.g. , horse \u2019s leg and plant \u2019s leaves ) , but FCN - 8s does not . DLDL - 8s may fail , e.g. , it sees a flowerpot as a potted plant in the fourth row in Fig . [ reference ] . Furthermore , compared to DLDL - 8s , DLDL - 8s - CRF is able to refine coarse pixel - level label predictions to produce sharp boundaries and fine - grained segmentations ( e.g. , plant \u2019s leaves ) . [ subfloat ] labelformat = empty position = top [ Image ] [ FCN - 8s ] [ DLDL - 8s ] [ DLDL - 8s + CRF ] [ Ground - truth ] [ ChaLearn ] [ Morph ] [ BJUT - 3D ] [ AFLW ] section : Discussions In this section , we try to understand the generalization performance of DLDL through feature visualization , and to analyze why DLDL can achieve high accuracy with limited training data . In addition , a study of the hyper - parameter is also provided . Feature visualization . We visualize the model features in a low - dimensional space . Early layers learn low - level features ( e.g. , edge and corner ) and latter layers learn high level features ( e.g. , shapes and objects ) in a deep ConvNet . Hence , we extract the penultimate layer features ( 4 , 096 - dimensional ) on Morph , ChaLearn , Pointing\u201904 and AFLW validation sets . To obtain the 2 - dimensional embeddings of the extracted high dimensional features , we employ a popular dimension reduction algorithm t - SNE . The low - dimensional embeddings of validation images from the above four datasets are shown in Fig . [ reference ] . The first row shows the 2 - dim embeddings of hand - crafted features ( BIF for Morph and Chalearn , HOG for Pointing\u201904 and AFLW ) and the second row shows that of the DLDL features . These figures are colored by their semantic category . It can be observed that clear semantic clusterings ( old or young for age datasets , left or right , up or down for head pose datasets ) appear in deep features but do not in hand - crafted features . Reduce over - fitting . DLDL can effectively reduce over - fitting when the training set is small . This effect can be explained by the label ambiguity . Considering an input sample with one single label . In traditional deep ConvNet , and for all . In DLDL , the label distribution contains many non zeros elements . The diversity of labels helps reduce over - fitting . Moreover , the objective function ( Eq . [ reference ] ) of DLDL can be rewritten as In Eq . [ reference ] , the first term is the tradition ConvNet loss function . The second term maximize the log - likelihood of the ambiguous labels . Unlike existing data augmentation techniques such as random cropping on the images , DLDL augments data on the label side . In Fig . [ reference ] , MAE is shown as a function of the number of epochs on two age datasets ( ChaLearn and Morph ) and two head pose datasets ( BJUT - 3D and AFLW ) . On ChaLearn and AFLW , C - ConveNet ( softmax ) achieves the lowest training MAE , but produces the highest validation MAE . In particular , the validation MAE increases after the 8th epoch on ChaLearn . Similar phenomenon is observed on AFLW . This fact shows that over - fitting happens in C - ConvNet when the number of training images is small . Although there are 15 , 561 training images in AFLW , each category contains on averagely 4 training images since there are 3 , 721 categories . Accelerate convergence . We further analyze the convergence performance of DLDL , C - ConvNet and R - ConvNet . We can observe that the training MAE is reduced very slowly at the beginning of training using C - ConvNet and R - ConveNet in many cases as shown in Fig . [ reference ] . On the contrary , the MAE of DLDL reduces quickly . Robust performance . One notable observation is that C - ConvNet and R - ConveNet is unstable . Fig . [ reference ] shows the MAE for pitch + yaw , a complicated estimation of the joint distribution . This is a very sparse label set because the interval of adjacent class ( pitch or yaw ) is . R - ConvNet has difficulty in estimating this output , yielding errors that are roughly 20 times higher than DLDL and C - ConvNet . On the other hand , C - ConvNet easily fall into over - fitting when there are not enough training data ( e.g , Fig . [ reference ] and Fig . [ reference ] ) . The proposed DLDL is more amenable to small datasets or sparse labels than C - ConvNet and R - ConvNet . Analyze the hyper - parameter . DLDL \u2019s performance may be affected by the label distribution . Here , we take age estimation ( Morph ) and head pose estimation ( Pointing\u201904 ) for examples . is a common hyper - parameter in these tasks if it is not provided in the ground - truth . We have empirically set in Morph , and in Pointing\u201904 in our experiments . In order to study the impact of , we test DLDL with different values , changing from 0 to 3 with 0.5 interval . Fig . [ reference ] shows the MAE performance on Morph and Pointing\u201904 with different . We can see that a proper is important for low MAE . But generally speaking , a value that is close to the interval between neighboring labels is a good choice . Because the shape of all curves are V - shape like , it is also very convenient to find an optimal value using the cross - validation strategy . section : Conclusion We observe that current deep ConvNets can not successfully learn good models when there are not enough training data and / or the labels are ambiguous . We propose DLDL , a deep label distribution learning framework to solve this issue by exploiting label ambiguity . In DLDL , each image is labeled by a label distribution , which can utilize label ambiguity in both feature learning and classifier learning . DLDL consistently improves the network training process in our experiments , by preventing it from over - fitting when the training set is small . We empirically showed that DLDL produces robust and competitive performances than traditional classification or regression deep models on several popular visual recognition tasks . However , constructing a reasonable label distribution is still challenging due to the diversity of label space for different recognition tasks . It is an interesting direction to extend DLDL to more recognition problems by constructing different label distributions . bibliography : References [ ] Bin - Bin Gao received the B.S. and M.S. degrees in applied mathematics in 2010 and 2013 , respectively . He is currently pursuing the Ph.D. degree in the Department of Computer Science and Technology , Nanjing University , China . His research interests include computer vision and machine learning . { IEEEbiography} [] Chao Xing received the B.S. degree in software engineering from Southeast University , China , in 2014 . He is currently a postgraduate student in the School of Computer Science and Engineering at Southeast University , China . His research interests include pattern recognition , machine learning , and data mining . { IEEEbiography} [] Chen - Wei Xie received his B.S. degree from Southeast University , China , in 2015 . He is currently a postgraduate student in the Department of Computer Science and Technology , Nanjing University , China . His research interests include computer vision and machine learning . { IEEEbiography} [] Jianxin Wu ( M\u201909 ) received the B.S. and M.S. degrees in computer science from Nanjing University , and the Ph.D. degree in computer science from the Georgia Institute of Technology . He was an Assistant Professor with the Nanyang Technological University , Singapore . He is currently a Professor with the Department of Computer Science and Technology , Nanjing University , China , and is associated with the National Key Laboratory for Novel Software Technology , China . His current research interests include computer vision and machine learning . He has served as an Area Chair for CVPR 2017 and ICCV 2015 , a Senior PC Member for AAAI 2017 and AAAI 2016 , and an Associate Editor of Pattern Recognition Journal . { IEEEbiography} [] Xin Geng ( M\u201913 ) received the B.S. and M.S. degrees in computer science from Nanjing University , China , in 2001 and 2004 , respectively , and the Ph . D degree from Deakin University , Australia in 2008 . He joined the School of Computer Science and Engineering at Southeast University , China , in 2008 , and is currently a professor and vice dean of the school . He has authored over 50 refereed papers , and he holds five patents in these areas . His research interests include pattern recognition , machine learning , and computer vision .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["ChaLearn_2015"]]], "Method": [[["DLDL_VGG-Face"]]], "Metric": [[["MAE"]]], "Task": [[["Age_Estimation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["MORPH_Album2"]]], "Method": [[["DLDL_VGG-Face"]]], "Metric": [[["MAE"]]], "Task": [[["Age_Estimation"]]]}]}
{"docid": "TST3-SREX-0001", "doctext": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling , a task central to language understanding . We extend current models to deal with two key challenges present in this task : corpora and vocabulary sizes , and complex , long term structure of language . We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long - Short Term Memory , on the One Billion Word Benchmark . Our best single model significantly improves state - of - the - art perplexity from 51.3 down to 30.0 ( whilst reducing the number of parameters by a factor of 20 ) , while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7 . We also release these models for the NLP and ML community to study and improve upon . ExploringtheLimitsofLanguageModeling section : Introduction Language Modeling ( LM ) is a task central to Natural Language Processing ( NLP ) and Language Understanding . Models which can accurately place distributions over sentences not only encode complexities of language such as grammatical structure , but also distill a fair amount of information about the knowledge that a corpora may contain . Indeed , models that are able to assign a low probability to sentences that are grammatically correct but unlikely may help other tasks in fundamental language understanding like question answering , machine translation , or text summarization . LMs have played a key role in traditional NLP tasks such as speech recognition , machine translation , or text summarization . Often ( although not always ) , training better language models improves the underlying metrics of the downstream task ( such as word error rate for speech recognition , or BLEU score for translation ) , which makes the task of training better LMs valuable by itself . Further , when trained on vast amounts of data , language models compactly extract knowledge encoded in the training data . For example , when trained on movie subtitles , these language models are able to generate basic answers to questions about object colors , facts about people , etc . Lastly , recently proposed sequence - to - sequence models employ conditional language models as their key component to solve diverse tasks like machine translation or video generation . Deep Learning and Recurrent Neural Networks ( RNNs ) have fueled language modeling research in the past years as it allowed researchers to explore many tasks for which the strong conditional independence assumptions are unrealistic . Despite the fact that simpler models , such as N - grams , only use a short history of previous words to predict the next word , they are still a key component to high quality , low perplexity LMs . Indeed , most recent work on large scale LM has shown that RNNs are great in combination with N - grams , as they may have different strengths that complement N - gram models , but worse when considered in isolation . We believe that , despite much work being devoted to small data sets like the Penn Tree Bank ( PTB ) , research on larger tasks is very relevant as overfitting is not the main limitation in current language modeling , but is the main characteristic of the PTB task . Results on larger corpora usually show better what matters as many ideas work well on small data sets but fail to improve on larger data sets . Further , given current hardware trends and vast amounts of text available on the Web , it is much more straightforward to tackle large scale modeling than it used to be . Thus , we hope that our work will help and motivate researchers to work on traditional LM beyond PTB \u2013 for this purpose , we will open - source our models and training recipes . We focused on a well known , large scale LM benchmark : the One Billion Word Benchmark data set . This data set is much larger than PTB ( one thousand fold , 800k word vocabulary and 1B words training data ) and far more challenging . Similar to Imagenet , which helped advance computer vision , we believe that releasing and working on large data sets and models with clear benchmarks will help advance Language Modeling . The contributions of our work are as follows : We explored , extended and tried to unify some of the current research on large scale LM . Specifically , we designed a Softmax loss which is based on character level CNNs , is efficient to train , and is as precise as a full Softmax which has orders of magnitude more parameters . Our study yielded significant improvements to the state - of - the - art on a well known , large scale LM task : from 51.3 down to 30.0 perplexity for single models whilst reducing the number of parameters by a factor of 20 . We show that an ensemble of a number of different models can bring down perplexity on this task to 23.7 , a large improvement compared to current state - of - art . We share the model and recipes in order to help and motivate further research in this area . In Section [ reference ] we review important concepts and previous work on language modeling . Section [ reference ] presents our contributions to the field of neural language modeling , emphasizing large scale recurrent neural network training . Sections [ reference ] and [ reference ] aim at exhaustively describing our experience and understanding throughout the project , as well as emplacing our work relative to other known approaches . section : Related Work In this section we describe previous work relevant to the approaches discussed in this paper . A more detailed discussion on language modeling research is provided in . subsection : Language Models Language Modeling ( LM ) has been a central task in NLP . The goal of LM is to learn a probability distribution over sequences of symbols pertaining to a language . Much work has been done on both parametric ( e.g. , log - linear models ) and non - parametric approaches ( e.g. , count - based LMs ) . Count - based approaches ( based on statistics of N - grams ) typically add smoothing which account for unseen ( yet possible ) sequences , and have been quite successful . To this extent , Kneser - Ney smoothed 5 - gram models are a fairly strong baseline which , for large amounts of training data , have challenged other parametric approaches based on Neural Networks . Most of our work is based on Recurrent Neural Networks ( RNN ) models which retain long term dependencies . To this extent , we used the Long - Short Term Memory model which uses a gating mechanism to ensure proper propagation of information through many time steps . Much work has been done on small and large scale RNN - based LMs . The architectures that we considered in this paper are represented in Figure [ reference ] . In our work , we train models on the popular One Billion Word Benchmark , which can be considered to be a medium - sized data set for count - based LMs but a very large data set for NN - based LMs . This regime is most interesting to us as we believe learning a very good model of human language is a complex task which will require large models , and thus large amounts of data . Further advances in data availability and computational resources helped our study . We argue this leap in scale enabled tremendous advances in deep learning . A clear example found in computer vision is Imagenet , which enabled learning complex vision models from large amounts of data . A crucial aspect which we discuss in detail in later sections is the size of our models . Despite the large number of parameters , we try to minimize computation as much as possible by adopting a strategy proposed in of projecting a relatively big recurrent state space down so that the matrices involved remain relatively small , yet the model has large memory capacity . subsection : Convolutional Embedding Models There is an increased interest in incorporating character - level inputs to build word embeddings for various NLP problems , including part - of - speech tagging , parsing and language modeling . The additional character information has been shown useful on relatively small benchmark data sets . The approach proposed in builds word embeddings using bidirectional LSTMs over the characters . The recurrent networks process sequences of characters from both sides and their final state vectors are concatenated . The resulting representation is then fed to a Neural Network . This model achieved very good results on a part - of - speech tagging task . In , the words characters are processed by a 1 - d CNN with max - pooling across the sequence for each convolutional feature . The resulting features are fed to a 2 - layer highway network , which allows the embedding to learn semantic representations . The model was evaluated on small - scale language modeling experiments for various languages and matched the best results on the PTB data set despite having 60 % fewer parameters . subsection : Softmax Over Large Vocabularies Assigning probability distributions over large vocabularies is computationally challenging . For modeling language , maximizing log - likelihood of a given word sequence leads to optimizing cross - entropy between the target probability distribution ( e.g. , the target word we should be predicting ) , and our model predictions . Generally , predictions come from a linear layer followed by a Softmax non - linearity : where is the logit corresponding to a word . The logit is generally computed as an inner product where is a context vector and is a \u201c word embedding \u201d for . The main challenge when is very large ( in the order of one million in this paper ) is the fact that computing all inner products between and all embeddings becomes prohibitively slow during training ( even when exploiting matrix - matrix multiplications and modern GPUs ) . Several approaches have been proposed to cope with the scaling issue : importance sampling , Noise Contrastive Estimation ( NCE ) , self normalizing partition functions or Hierarchical Softmax \u2013 they all offer good solutions to this problem . We found importance sampling to be quite effective on this task , and explain the connection between it and NCE in the following section , as they are closely related . section : Language Modeling Improvements Recurrent Neural Networks based LMs employ the chain rule to model joint probabilities over word sequences : where the context of all previous words is encoded with an LSTM , and the probability over words uses a Softmax ( see Figure [ reference ] ( a ) ) . subsection : Relationship between Noise Contrastive Estimation and Importance Sampling As discussed in Section [ reference ] , a large scale Softmax is necessary for training good LMs because of the vocabulary size . A Hierarchical Softmax employs a tree in which the probability distribution over words is decomposed into a product of two probabilities for each word , greatly reducing training and inference time as only the path specified by the hierarchy needs to be computed and updated . Choosing a good hierarchy is important for obtaining good results and we did not explore this approach further for this paper as sampling methods worked well for our setup . Sampling approaches are only useful during training , as they propose an approximation to the loss which is cheap to compute ( also in a distributed setting ) \u2013 however , at inference time one still has to compute the normalization term over all words . Noise Contrastive Estimation ( NCE ) proposes to consider a surrogate binary classification task in which a classifier is trained to discriminate between true data , or samples coming from some arbitrary distribution . If both the noise and data distributions were known , the optimal classifier would be : where is the binary random variable indicating whether comes from the true data distribution , is the number of negative samples per positive word , and and are the data and noise distribution respectively ( we dropped any dependency on previous words for notational simplicity ) . It is easy to show that if we train a logistic classifier where is the logistic function , then , is a good approximation of ( is a logit which e.g. an LSTM LM computes ) . The other technique , which is based on importance sampling ( IS ) , proposes to directly approximate the partition function ( which comprises a sum over all words ) with an estimate of it through importance sampling . Though the methods look superficially similar , we will derive a similar surrogate classification task akin to NCE which arrives at IS , showing a strong connection between the two . Suppose that , instead of having a binary task to decide if a word comes from the data or from the noise distribution , we want to identify the words coming from the true data distribution in a set , comprised of noise samples and one data distribution sample . Thus , we can train a multiclass loss over a multinomial random variable which maximizes , assuming w.l.o.g . that is always the word coming from true data . By Bayes rule , and ignoring terms that are constant with respect to , we can write : and , following a similar argument than for NCE , if we define then is a good approximation of . Note that the only difference between NCE and IS is that , in NCE , we define a binary classification task between true or noise words with a logistic loss , whereas in IS we define a multiclass classification problem with a Softmax and cross entropy loss . We hope that our derivation helps clarify the similarities and differences between the two . In particular , we observe that IS , as it optimizes a multiclass classification task ( in contrast to solving a binary task ) , may be a better choice . Indeed , the updates to the logits with IS are tied whereas in NCE they are independent . subsection : CNN Softmax The character - level features allow for a smoother and compact parametrization of the word embeddings . Recent efforts on small scale language modeling have used CNN character embeddings for the input embeddings . Although not as straightforward , we propose an extension to this idea to also reduce the number of parameters of the Softmax layer . Recall from Section [ reference ] that the Softmax computes a logit as where is a context vector and the word embedding . Instead of building a matrix of ( whose rows correspond to ) , we produce with a CNN over the characters of as \u2013 we call this a CNN Softmax . We used the same network architecture to dynamically generate the Softmax word embeddings without sharing the parameters with the input word - embedding sub - network . For inference , the vectors can be precomputed , so there is no computational complexity increase w.r.t . the regular Softmax . We note that , when using an importance sampling loss such as the one described in Section [ reference ] , only a few logits have non - zero gradient ( those corresponding to the true and sampled words ) . With a Softmax where are independently learned word embeddings , this is not a problem . But we observed that , when using a CNN , all the logits become tied as the function mapping from to is quite smooth . As a result , a much smaller learning rate had to be used . Even with this , the model lacks capacity to differentiate between words that have very different meanings but that are spelled similarly . Thus , a reasonable compromise was to add a small correction factor which is learned per word , such that : where is a matrix projecting a low - dimensional embedding vector back up to the dimensionality of the projected LSTM hidden state of . This amounts to adding a bottleneck linear layer , and brings the CNN Softmax much closer to our best result , as can be seen in Table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the CNN Softmax . Aside from a big reduction in the number of parameters and incorporating morphological knowledge from words , the other benefit of this approach is that out - of - vocabulary ( OOV ) words can easily be scored . This may be useful for other problems such as Machine Translation where handling out - of - vocabulary words is very important . This approach also allows parallel training over various data sets since the model is no longer explicitly parametrized by the vocabulary size \u2013 or the language . This has shown to help when using byte - level input embeddings for named entity recognition , and we hope it will enable similar gains when used to map onto words . subsection : Char LSTM Predictions The CNN Softmax layer can handle arbitrary words and is much more efficient in terms of number of parameters than the full Softmax matrix . It is , though , still considerably slow , as to evaluate perplexities we need to compute the partition function . A class of models that solve this problem more efficiently are character - level LSTMs . They make predictions one character at a time , thus allowing to compute probabilities over a much smaller vocabulary . On the other hand , these models are more difficult to train and seem to perform worse even in small tasks like PTB . Most likely this is due to the sequences becoming much longer on average as the LSTM reads the input character by character instead of word by word . Thus , we combine the word and character - level models by feeding a word - level LSTM hidden state into a small LSTM that predicts the target word one character at a time ( see Figure [ reference ] ( c ) ) . In order to make the whole process reasonably efficient , we train the standard LSTM model until convergence , freeze its weights , and replace the standard word - level Softmax layer with the aforementioned character - level LSTM . The resulting model scales independently of vocabulary size \u2013 both for training and inference . However , it does seem to be worse than regular and CNN Softmax \u2013 we are hopeful that further research will enable these models to replace fixed vocabulary models whilst being computationally attractive . section : Experiments All experiments were run using the TensorFlow system , with the exception of some older models which were used in the ensemble . subsection : Data Set The experiments are performed on the 1B Word Benchmark data set introduced by , which is a publicly available benchmark for measuring progress of statistical language modeling . The data set contains about 0.8B words with a vocabulary of 793471 words , including sentence boundary markers . All the sentences are shuffled and the duplicates are removed . The words that are out of vocabulary ( OOV ) are marked with a special UNK token ( there are approximately 0.3 % such words ) . subsection : Model Setup The typical measure used for reporting progress in language modeling is perplexity , which is the average per - word log - probability on the holdout data set : . We follow the standard procedure and sum over all the words ( including the end of sentence symbol ) . We used the 1B Word Benchmark data set without any pre - processing . Given the shuffled sentences , they are input to the network as a batch of independent streams of words . Whenever a sentence ends , a new one starts without any padding ( thus maximizing the occupancy per batch ) . For the models that consume characters as inputs or as targets , each word is fed to the model as a sequence of character IDs of preespecified length ( see Figure [ reference ] ( b ) ) . The words were processed to include special begin and end of word tokens and were padded to reach the expected length . I.e. if the maximum word length was 10 , the word \u201c cat \u201d would be transformed to \u201c $ cat^ \u201d due to the CNN model . In our experiments we found that limiting the maximum word length in training to 50 was sufficient to reach very good results while 32 was clearly insufficient . We used 256 characters in our vocabulary and the non - ascii symbols were represented as a sequence of bytes . subsection : Model Architecture We evaluated many variations of RNN LM architectures . These include the dimensionalities of the embedding layers , the state , projection sizes , and number of LSTM layers to use . Exhaustively trying all combinations would be extremely time consuming for such a large data set , but our findings suggest that LSTMs with a projection layer ( i.e. , a bottleneck between hidden states as in ) trained with truncated BPTT for 20 steps performed well . Following we use dropout before and after every LSTM layer . The biases of LSTM forget gate were initialized to 1.0 . The size of the models will be described in more detail in the following sections , and the choices of hyper - parameters will be released as open source upon publication . For any model using character embedding CNNs , we closely follow the architecture from . The only important difference is that we use a larger number of convolutional features of 4096 to give enough capacity to the model . The resulting embedding is then linearly transformed to match the LSTM projection sizes . This allows it to match the performance of regular word embeddings but only uses a small fraction of parameters . subsection : Training Procedure The models were trained until convergence with an AdaGrad optimizer using a learning rate of 0.2 . In all the experiments the RNNs were unrolled for 20 steps without ever resetting the LSTM states . We used a batch size of 128 . We clip the gradients of the LSTM weights such that their norm is bounded by 1.0 . Using these hyper - parameters we found large LSTMs to be relatively easy to train . The same learning rate was used in almost all of the experiments . In a few cases we had to reduce it by an order of magnitude . Unless otherwise stated , the experiments were performed with 32 GPU workers and asynchronous gradient updates . Further details will be fully specified with the code upon publication . Training a model for such large target vocabulary ( 793471 words ) required to be careful with some details about the approximation to full Softmax using importance sampling . We used a large number of negative ( or noise ) samples : 8192 such samples were drawn per step , but were shared across all the target words in the batch ( 2560 total , i.e. 128 times 20 unrolled steps ) . This results in multiplying ( 2560 x 1024 ) times ( 1024 x ( 8192 + 1 ) ) ( instead of ( 2560 x 1024 ) times ( 1024 x 793471 ) ) , i.e. about 100 - fold less computation . section : Results and Analysis In this section we summarize the results of our experiments and do an in - depth analysis . Table [ reference ] contains all results for our models compared to previously published work . Table [ reference ] shows previous and our own work on ensembles of models . We hope that our encouraging results , which improved the best perplexity of a single model from 51.3 to 30.0 ( whilst reducing the model size considerably ) , and set a new record with ensembles at 23.7 , will enable rapid research and progress to advance Language Modeling . For this purpose , we will release the model weights and recipes upon publication . subsection : Size Matters Unsurprisingly , size matters : when training on a very large and complex data set , fitting the training data with an LSTM is fairly challenging . Thus , the size of the LSTM layer is a very important factor that influences the results , as seen in Table [ reference ] . The best models are the largest we were able to fit into a GPU memory . Our largest model was a 2 - layer LSTM with 8192 + 1024 dimensional recurrent state in each of the layers . Increasing the embedding and projection size also helps but causes a large increase in the number of parameters , which is less desirable . Lastly , training an RNN instead of an LSTM yields poorer results ( about 5 perplexity worse ) for a comparable model size . subsection : Regularization Importance As shown in Table [ reference ] , using dropout improves the results . To our surprise , even relatively small models ( e.g. , single layer LSTM with 2048 units projected to 512 dimensional outputs ) can over - fit the training set if trained long enough , eventually yielding holdout set degradation . Using dropout on non - recurrent connections largely mitigates these issues . While over - fitting still occurs , there is no more need for early stopping . For models that had 4096 or less units in the LSTM layer , we used 10 % dropout probability . For larger models , 25 % was significantly better . Even with such regularization , perplexities on the training set can be as much as 6 points below test . In one experiment we tried to use a smaller vocabulary comprising of the 100 , 000 most frequent words and found the difference between train and test to be smaller \u2013 which suggests that too much capacity is given to rare words . This is less of an issue with character CNN embedding models as the embeddings are shared across all words . subsection : Importance Sampling is Data Efficient Table [ reference ] shows the test perplexities of NCE vs IS loss after a few epochs of 2048 unit LSTM with 512 projection . The IS objective significantly improves the speed and the overall performance of the model when compared to NCE . subsection : Word Embeddings vs Character CNN Replacing the embedding layer with a parametrized neural network that process characters of a given word allows the model to consume arbitrary words and is not restricted to a fixed vocabulary . This property is useful for data sets with conversational or informal text as well as for morphologically rich languages . Our experiments show that using character - level embeddings is feasible and does not degrade performance \u2013 in fact , our best single model uses a Character CNN embedding . An additional advantage is that the number of parameters of the input layer is reduced by a factor of 11 ( though training speed is slightly worse ) . For inference , the embeddings can be precomputed so there is no speed penalty . Overall , the embedding of the best model is parametrized by 72 M weights ( down from 820 M weights ) . Table [ reference ] shows a few examples of nearest neighbor embeddings for some out - of - vocabulary words when character CNNs are used . subsection : Smaller Models with CNN Softmax Even with character - level embeddings , the model is still fairly large ( though much smaller than the best competing models from previous work ) . Most of the parameters are in the linear layer before the Softmax : 820 M versus a total of 1.04B parameters . In one of the experiments we froze the word - LSTM after convergence and replaced the Softmax layer with the CNN Softmax sub - network . Without any fine - tuning that model was able to reach 39.8 perplexity with only 293 M weights ( as seen in Table [ reference ] ) . As described in Section [ reference ] , adding a \u201c correction \u201d word embedding term alleviates the gap between regular and CNN Softmax . Indeed , we can trade - off model size versus perplexity . For instance , by adding 100 M weights ( through a 128 dimensional bottleneck embedding ) we achieve 35.8 perplexity ( see Table [ reference ] ) . To contrast with the CNN Softmax , we also evaluated a model that replaces the Softmax layer with a smaller LSTM that predicts one character at a time ( see Section [ reference ] ) . Such a model does not have to learn long dependencies because the base LSTM still operates at the word - level ( see Figure [ reference ] ( c ) ) . With a single - layer LSTM of 1024 units we reached 49.0 test perplexity , far below the best model . In order to make the comparisons more fair , we performed a very expensive marginalization over the words in the vocabulary ( to rule out words not in the dictionary which the character LSTM would assign some probability ) . When doing this marginalization , the perplexity improved a bit down to 47.9 . subsection : Training Speed We used 32 Tesla K40 GPUs to train our models . The smaller version of the LSTM model with 2048 units and 512 projections needs less than 10 hours to reach below 45 perplexity and after only 2 hours of training the model beats previous state - of - the art on this data set . The best model needs about 5 days to get to 35 perplexity and 10 days to 32.5 . The best results were achieved after 3 weeks of training . See Table [ reference ] for more details . subsection : Ensembles We averaged several of our best models and we were able to reach 23.7 test perplexity ( more details and results can be seen in Table [ reference ] ) , which is more than 40 % improvement over previous work . Interestingly , including the best N - gram model reduces the perplexity by 1.2 point even though the model is rather weak on its own ( 67.6 perplexity ) . Most previous work had to either ensemble with the best N - gram model ( as their RNN only used a limited output vocabulary of a few thousand words ) , or use N - gram features as additional input to the RNN . Our results , on the contrary , suggest that N - grams are of limited benefit , and suggest that a carefully trained LSTM LM is the most competitive model . subsection : LSTMs are best on the tail words Figure [ reference ] shows the difference in log probabilities between our best model ( at 30.0 perplexity ) and the KN - 5 . As can be seen from the plot , the LSTM is better across all the buckets and significantly outperforms KN - 5 on the rare words . This is encouraging as it seems to suggest that LSTM LMs may fare even better for languages or data sets where the number of rare words is larger than traditional N - gram models . subsection : Samples from the model To qualitatively evaluate the model , we sampled many sentences . We discarded short and politically incorrect ones , but the sample shown below is otherwise \u201c raw \u201d ( i.e. , not hand picked ) . The samples are of high quality \u2013 which is not a surprise , given the perplexities attained \u2013 but there are still some occasional mistakes . Sentences generated by the ensemble ( about 26 perplexity ) : With even more new technologies coming onto the market quickly during the past three years , an increasing number of companies now must tackle the ever - changing and ever - changing environmental challenges online . < S > Check back for updates on this breaking news story . < S > About 800 people gathered at Hever Castle on Long Beach from noon to 2 pm , three to four times that of the funeral cort\u00e8ge . < S > We are aware of written instructions from the copyright holder not to , in any way , mention Rosenberg \u2019s negative comments if they are relevant as indicated in the documents , \u201d eBay said in a statement . < S > It is now known that coffee and cacao products can do no harm on the body . < S > Yuri Zhirkov was in attendance at the Stamford Bridge at the start of the second half but neither Drogba nor Malouda was able to push on through the Barcelona defence . section : Discussion and Conclusions In this paper we have shown that RNN LMs can be trained on large amounts of data , and outperform competing models including carefully tuned N - grams . The reduction in perplexity from 51.3 to 30.0 is due to several key components which we studied in this paper . Thus , a large , regularized LSTM LM , with projection layers and trained with an approximation to the true Softmax with importance sampling performs much better than N - grams . Unlike previous work , we do not require to interpolate both the RNN LM and the N - gram , and the gains of doing so are rather marginal . By exploring recent advances in model architectures ( e.g. LSTMs ) , exploiting small character CNNs , and by sharing our findings in this paper and accompanying code and models ( to be released upon publication ) , we hope to inspire research on large scale Language Modeling , a problem we consider crucial towards language understanding . We hope for future research to focus on reasonably sized datasets taking inspiration from recent advances seen in the computer vision community thanks to efforts such as Imagenet . section : Acknowledgements We thank Ciprian Chelba , Ilya Sutskever , and the Google Brain Team for their help and discussions . We also thank Koray Kavukcuoglu for his help with the manuscript . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["LSTM-8192-1024"]]], "Metric": [[["Number_of_params"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["LSTM-8192-1024___CNN_Input"]]], "Metric": [[["Number_of_params"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["LSTM-8192-1024"]]], "Metric": [[["PPL"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["LSTM-8192-1024___CNN_Input"]]], "Metric": [[["PPL"]]], "Task": [[["Language_Modelling"]]]}]}
{"docid": "TST3-SREX-0002", "doctext": "Bridging Saliency Detection to Weakly Supervised Object Detection Based on Self - paced Curriculum Learning section : Abstract Weakly - supervised object detection ( WOD ) is a challenging problems in computer vision . The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors , given only the training images with weak image - level labels . Intuitively , by simulating the selective attention mechanism of human visual system , saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD . However , the way to adopt saliency detection in WOD is not trivial since the detected saliency region might be possibly highly ambiguous in complex cases . To this end , this paper first comprehensively analyzes the challenges in applying saliency detection to WOD . Then , we make one of the earliest efforts to bridge saliency detection to WOD via the self - paced curriculum learning , which can guide the learning procedure to gradually achieve faithful knowledge of multi - class objects from easy to hard . The experimental results demonstrate that the proposed approach can successfully bridge saliency detection and WOD tasks and achieve the state - of - the - art object detection results under the weak supervision . section : Introduction 1 Object detection is one of the most fundamental yetchallenging problems in computer vision community . The most recent breakthrough was achieved by Girshick et al . , who trained the Convolutional Neural Network ( CNN ) by using large amount of human labelled bounding boxes to learn the powerful feature representations and object classifiers . Despite their success , the problem of object detection is still under - addressed in practice due to the heavy burden of labeling the training samples . Essentially , in this big data era , humans more desire intelligent machines which are capable of automatically discovering the intrinsic patterns from the cheaply and massively collected weakly * The corresponding author labeled images . Thus weakly supervised object detection ( WOD ) systems have been gaining more interests recently . The key problem in WOD is how to extract the exact object localizations and train the corresponding object detectors from the weakly labelled training images . In such chicken - egg problem , most methods ( including the proposed one ) usually use the alternative learning strategy that first provides some coarse estimation to initialize the potential object locations and then gradually train the object detectors and update object locations jointly . In this paper , we leverage saliency detection to initialize the potential object locations due to the following reasons : 1 ) Saliency detection ; [ reference ][ reference ] aims at simulating the selective attention mechanism of human visual system to automatically select sub - regions ( usually the regions containing objects of interest ) in image scenes . Thus , it can be readily utilized to provide useful priors to estimate the potential object localizations and fit well to the investigated task . 2 ) Some recent saliency detection methods such as [ reference ] and [ reference ] can process much faster than the priors , e.g. , intra - class similarity [ reference ] , inter - class variance [ reference ] , and distance mapping relation [ reference ] , adopted in the existing WOD systems . 3 ) Several existing works , e.g. , , have attempted to apply saliency detection techniques to WOD . However they still have not sufficiently explore the intrinsic bridge between these two tasks , which motivates us to clarify the insightful relationship between these two tasks and further develop powerful learning regime to bridge them . Essentially , although it sounds reasonable to apply saliency detection to WOD , the way to bridge these two tasks is not trivial . The main problem is that saliency detection is formulated as category - free models which only distinguish attractive regions from the image background while irrelevant to the concrete object category . Thus , as shown in Fig . 1 , in the images only containing one category of objects ( considered as \" easy \" images ) , the objects can be captured by saliency detection methods easily and associated with the corresponding image label properly . Whereas in the images weakly labelled as containing multiple categories of objects ( considered as \" hard \" images ) , objects in all of categories will have the probabilities to attract the human attention and their corresponding locations are also hard for saliency models to identify , which largely increases the ambiguity when considering to apply the obtained salient detection results to initializing the training samples for WOD . Thus , it is unreliable to directly apply saliency detection to WOD . To alleviate this problem , we propose to bridge saliency detection to WOD via a self - paced curriculum learning ( SPCL ) regime . SPCL was proposed in as a general learning framework including both the curriculum learning ( CL ) and self - paced learning ( SPL ) components . To the best of our knowledge , both of these two learning components are critical in successfully bridging saliency detection to WOD , whereas none of the existing literature has explored them before . Specifically , CL was proposed by [ reference ] , which is usually learned based on the learning priorities derived by predetermined heuristics for particular problems . SPL was proposed by [ reference ] , where the learning pace is dynamically generated by the learner itself , according to which the learner has already learned from the data . Thus , the CL and SPL components in SPCL can be correspondingly used to solve the training sample initialization and object detector updating problems in the proposed saliency - guided WOD . To implement SPCL for our task , we first design a task - specific curriculum to assign the \" easy \" images with larger priority than the \" hard \" images during the learning procedure , which indicates that only the salient object hypotheses in the \" easy \" images are selected as the initial training samples , while the object hypotheses in the \" hard \" images will be gradually involved in the subsequent learning iterations . To guide the learner to gradually learn faithful knowledge of multi - class objects from the \" easy \" ( high - confidence ) images to the \" hard \" ( high - ambiguity ) ones , a novel self - paced learning regularizer is proposed to enforce the learner to select confident and diverse training hypotheses in each iteration and learn the object detectors of multiple categories simultaneously . Finally , the proposed SPCL regime can fit well to solve the problems in this paper . Compared with the SPCL model in , the learning regime proposed in this paper mainly has three differences : 1 ) We design a task - specific learning curriculum for bridging saliency detection and WOD effectively . 2 ) We introduce an additional term , the sample diversity term , in the self - paced regularizer to prevent the selected training hypotheses from drifting to a small collection of training images . 3 ) Considering the latent relationship among the multiple categories of co - occurring objects , we further generalize the SPL regime into multi - class formulation , which facilitates the learning system to penalize indiscriminative object hypotheses predicted as belonging to multiple object categories at the same time . To sum up , there are three - fold contributions in this paper : \uf0b7 We comprehensively analyze the prospect and challenges in the idea of bridging saliency detection to WOD and propose an effective way to alleviate the problem , which achieves the state - of - the - art detection performance under the weak supervision . \uf0b7 We establish a novel SPCL regime containing both the task - specific learning curriculum and the data - driven self - learning pace . The regime is well formulated as a concise optimization model . \uf0b7 We incorporate SPCL with a sample diversity term and further generalize it to work in multi - class scenario . section : Related Works Saliency detection : Most saliency detection methods highlight the attractive image regions by exploring some bottom - up cues . As one frequently explored cue , local contrast [ reference ][ reference ] is usually used in the saliency detection models to highlight the image regions appearing differently with their spatially neighbor regions . Another widely used bottom - up cue is the global contrast . Being different from local contrast , global contrast [ reference ][ reference ] is used to discover image regions which are unique in the entire image context . More recently , background prior becomes another important cue for saliency detection . This kind of methods , e.g. , , assume that regions near image boundaries are probably backgrounds and detect salient regions as Figure 1 : This figure illustrates the main idea of this paper . As can be seen , in the training images with weak labels , some of them ( in blue frame ) are labelled only containing one object category , which are considered as \" easy \" images for the saliency detection methods . While others ( in pink frame ) labelled as containing multiple object categories are considered as \" hard \" images . Due to the category free property of saliency detection , the objects in \" easy \" images have larger confidence to be extracted correctly by the saliency detection methods , whereas the objects in \" hard \" images can not be extracted successfully . To this end , we develop a novel self - paced curriculum learning paradigm to guide the learner to gradually achieve the faithful knowledge of the multiple object categories from easy ( confident ) to hard ( ambiguous ) . calculating the contrast to these image boundary regions . As can be seen , the bottom - up cues explored by saliency detection models are highly potential to provide helpful priors to the object localizations in each image . Weakly - supervised object detection : Two key issues in WOD are 1 ) predict the potential object localizations and 2 ) learn the object detectors . Some early WOD methods [ reference ] held the view that a better initial estimation of the object localizations is critical to this task as they can largely impact the subsequent learning process . Thus , they explored different ways , e.g. intra - class similarity [ reference ] , inter - class variance [ reference ] , and distance mapping relation [ reference ] , to initialize the training object hypotheses . Later on , some recent WOD methods started to pay more attention to the optimization procedure designed for better training object detectors under the weak supervision . For example , [ reference ][ reference ] proposed to smooth the object formulation to better obtain the optimal solutions . [ reference ] proposed to incorporate convex clustering in the learning procedure , which enforces the local similarity of the selected hypotheses during optimization . Essentially , both of the above mentioned problems are critical in WOD task . To this end , this paper proposes a novel SPCL model which explicitly encode both the former problem ( with the designed curriculum ) and the later ( with the self - paced regularizer ) into a unified formulation and handle both problems in a theoretically sound manner . section : Self - paced ( curriculum ) learning : Inspired by the learning process of humans / animals , the theory of self - paced ( or curriculum ) learning [ reference ][ reference ] is proposed lately . The idea is to learn the model iteratively from easy to complex samples in a self - paced fashion . By virtue of its generality , the SPL theory has been widely applied to various tasks , such as multi - view clustering [ reference ] , multi - label propagation [ reference ] , multimedia event detection , and co - saliency detection . More recently , introduced the pre - defined learning curriculum to the conventional self - paced learning regime which can take into account both the helpful prior knowledge known before training and the self - learning progress during training . Inspired by this work , we design a task - specific learning curriculum and construct a unified SPCL model specifically for both the saliency detection and WOD tasks , through which both can be naturally related . section : The Proposed Approach section : Algorithm Overview Given K training images with weak labels consisting of C categories , we first extract the bounding box object hypotheses and their corresponding feature representations from each image . Denote the features of each hypothesis in the k th image as { , where , ( ) \u2208 [ 0 , 1 ] , indicating the labels and the real - valued importance weights of each hypothesis , respectively , which are unknown at the beginning and will be optimized during the proposed learning regime . The aim of the proposed approach is to learn the object detectors { W , b } , where = { } , = { } , of C object categories from the weakly - labeled training images , and then use them to detect objects in the test images . Specifically , we first design a simple yet effective curriculum to select the salient hypotheses in \" easy \" images as initialization . Then , the object detectors are trained and updated gradually under the guidance of the proposed self - paced learning strategy . Finally , the obtained object detectors are used to detect the corresponding objects in the test images . The overall algorithm is shown in Algorithm 1 . section : Problem Formulation Given object hypotheses from the training images , we propose a simple yet effective curriculum to initialize the learning procedure . Specifically , we first obtain the \" easy \" images based on the number of weak labels of each image , i.e. , images weakly labelled as only containing one object category are considered as \" easy \" . Then , for each \" easy \" image , we adopt an unsupervised saliency detection method , i.e. , RC [ reference ] in this paper due to its efficiency , to generate the corresponding saliency estimation . Finally , the important weights v are initialized as the intersection - over - union ( IOU ) score between each hypothesis and the salient region . The hypotheses with weights larger than 0 are selected as the initial training hypotheses and their labels in y are set according to the label of the images containing them . Afterwards , in order to gradually adapt the learner from the \" easy \" domain to the \" hard \" domain and finally capture the faithful knowledge of the objects of interest , a novel self - paced learning regularizer is proposed as follows : + 1 \u2264 2 , enforces that each hypothesis should belong to only one object category , or no class , i.e. , the background category . This constraint inherently penalizes the indiscriminative object hypotheses , i.e. the hypotheses predicted to belong to multiple object categories , when calculating their importance weight in ( 2 ) . The third one , i.e. \u2211 , * ( ) + 1 \u2265 2 , means that for all object hypotheses located in the k th image , at least one should belong to the class which the image has been weakly annotated . This will make the learned result finely comply with the prior knowledge . In the proposed SPCL regime , the self - paced capability is followed by the involvement of the SPL regularizer ( ) ; , with the following form : where , are the class - specific parameters imposed on the easiness term and the diversity term , respectively . The negative l 1 - norm term is inherited from the conventional SPL [ reference ] , which favors selecting easy over complex hypotheses . If we omit the diversity term , i.e. let = 0 , the regularizer degenerates to the traditional hard SPL function proposed in [ reference ] , which conducts either 1 or 0 ( i.e. selected in training or not ) for the weight , ( ) imposed on hypothesis ( ) , by judging whether its loss value is smaller than the pace parameter or not . That is , a sample with smaller loss is taken as an easy sample and thus should be learned preferentially and vice versa . Another regularization term favors selecting diverse hypotheses residing in more images . This can be easily understood by seeing that its negative leads to the group - wise sparse representation of v. Contrariwise , this diversity term should have a counter - effect to group - wise sparsity . That is , minimizing this diversity term tends to disperse non - zero elements of v over more images , and thus favors selecting more diverse hypotheses . Consequently , this anti - group - sparsity representation is expected to realize the desired diversity . Different from the commonly utilized l 2 , 1 norm , our utilized group - sparsity term is concave , leading to the convexity of its negative . This on one side simplifies the designation of the solving strategy , and on the other hand well fits the previous axiomatic definition for the SPL regularizer . section : Optimization Method The solution of ( 1 ) can be approximately attained by alternatively optimizing the involved parameters { W , b } , y and v as described in Algorithm 1 . The optimization mainly contains following steps : Object detectors updating : Optimize object detector parameters { W , b } via one - vs - all SVM under fixed y and v. In this case , ( 1 ) degenerates to the following form : ; , , which can be equivalently reformulated as solving the following sub - optimization problems for each c=1 , 2 , \u2026 , C : , . This is a standard one - vs - all ( weighted ) SVM model [ reference ] . section : Hypotheses labelling : Optimize under fixed { W , b } and v : The goal of this step is to learn the pseudo - labels of training hypotheses from the current object detectors . The model in this case can be reformulated as : This problem can be equivalently decomposed into sub - problems with respect to each = 1 , \u22ef , , i.e. for each image , where c * is the weak labels of the k th image : [ reference ] indicates the labels of the hypotheses in the k th image . Its global optimum can be attained by Algorithm 2 , which can be derived from the theorem in . Hypotheses re - weighting : Optimize v under fixed { W , b } and y : After updating the pseudo - labels , we aim to renew the weights on all hypotheses to reflect their different importance to learning of the current decision surface . In this case , ( 1 ) degenerates to the following form : which is equivalent to independently solving the following sub - optimization problem for each = 1 , \u22ef , and = 1 , \u22ef , via : We can easily simplify the above optimization problem as : This model is convex and according to , we can apdopt an effective algorithm , i.e. , the Algorithm 3 , for extracting the global optimum to it . section : Experimental Results section : Experimental Settings We evaluate our method on the Pascal VOC 2007 dataset [ reference ] which is widely used by the previous works . In our experiments , we follow the previous works [ reference ][ reference ][ reference ][ reference ] to discard any images that only contain object instances marked as \" difficult \" or \" truncated \" during the training phase , while all the images in the VOC07 - Test are used during the test phase . For fair comparison , we follow the standard VOC procedure [ reference ] and report average precision ( AP ) on the Pascal VOC 2007 test split . Being consistent with the recently proposed WOD methods [ reference ][ reference ][ reference ] , we apply Selective Search [ reference ] to generate around 1500 bounding box object hypotheses in each image and adopt the CNN features [ reference ] pre - trained on the ImageNet 2012 to represent each of the extracted object hypotheses . Before training , and in ( 2 ) need to be set in advance . As suggested in , we set according to the number of the selected training hypotheses which is set to be 2 % of the total bounding box windows extracted from the images weakly labelled as containing the c th object category . Then , is set to be equal to empirically . section : Comparison to the State - of - the - arts In this section , we evaluate the object detection performance of our framework by comparing it with 6 state - of - the - art WOD approaches which are PR [ reference ] , CC [ reference ] , MDD [ reference ] , LLO [ reference ] , VPC [ reference ] , and MfMIL [ reference ] . For quantitative comparison , we report the evaluation results in terms of the AP score in Fig . 2 . As can be seen , the proposed approach obtains the highest score of 29.96 on average . According to our analysis , the proposed approach can obtain significantly better results than MDD and MfMIL mainly due to the better feature representation , stronger saliency prior , and more powerful learning scheme . Compared with PR , CC , LLO , and VPC , the performance gain of the proposed approach mainly comes from the core insight of this paper , i.e. , developing property way to bridge saliency detection to WOD , as we used the same feature representation with these methods . More specifically , compared with PR and CC , the performance gain of the proposed approach comes mainly from the idea to bridge saliency detection to WOD because they only adopted weak priors in their initialization . Compared with LLO and VPC , the performance gain of the proposed approach mainly comes from the proposed SPCL regime as these two methods also explored strong prior information for initializing the training hypotheses in their frameworks . Some examples of the detection results are also shown in Fig . 2 , which includes some successful cases , i.e. , the examples in the bus and cat categories , as well as some failure cases , i.e. , examples in the plant and chair categories . The successful cases subjectively demonstrate the effectiveness of the proposed approach . For the failure cases , the main problem is that very limited number of images only contains the objects like plant and chair , leading to the insufficient training hypotheses in the initialization stage . This problem can be solved by designing more proper learning curriculum for WOD in future works . section : Model Analysis To further analyze the proposed framework in this paper , we make more comprehensive evaluations in this section by comparing with five baseline models as described in Table 1 . The experimental results are shown in Fig . 3 , from which we can see : 1 ) The performance gap between Sal + SVM and OURS demonstrates the importance to develop proper ways to bridge saliency detection and WOD . 2 ) The experimental results of Sal + SPL , Sal + SPCL , and OURS demonstrate the better performance of the proposed learning regime as compared with some existing self - paced ( curriculum ) learning regimes . 3 ) The performance gap between OURS and LLO + SPCL * demonstrates the saliency prior can provide more helpful information than the prior designed in LLO . 4 ) The performance gap between LLO + SPCL * and LLO indicates the better capability of the proposed learning regime as compared with the learning model in one state - of - the - art WOD framework . According to the above analysis , the key insight of this paper , i.e. , developing powerful learning regime , i.e. , the proposed SPCL , can better bridge saliency detection to WOD and help the learner to capture the faithful knowledge of the object categories under weak supervision , has been demonstrated comprehensively . section : Conclusion In this paper , in order to address the challenging WOD problem , we proposed an effective framework to bridge saliency detection to the investigated task based on a novel SPCL regime . The insight of this paper is that by developing powerful learning regime which contains both the task - specific learning curriculum and the data - driven self - learning pace , saliency detection technique can be better leveraged to provide beneficial information for helping the learner to capture the faithful knowledge of the object categories under weak supervision . Experiments including comparisons to other state - of - the - arts and comprehensive analysis of the proposed framework on the benchmark dataset have demonstrated the effectiveness of our approach . For the future work , inspired by [ reference ] , we plane to enable the proposed method to transfer the knowledge that has be captured to new concepts via novel regularizers . . LLO Baseline WOD framework [ reference ] . LLO + SPCL * Replace the learning model , i.e. , SLSVM , in LLO with the proposed SPCL regime . section : section : Acknowledgments : This work was supported in part by the National Science Foundation of China under Grants 61522207 and 61473231 , the Doctorate Foundation , and the Excellent Doctorate Foundation of Northwestern Polytechnical University . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["Self-paced_curriculum_learning"]]], "Metric": [[["MAP"]]], "Task": [[["Weakly_Supervised_Object_Detection"]]]}]}
{"docid": "TST3-SREX-0003", "doctext": "document : The IBM 2016 English Conversational Telephone Speech Recognition System We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6 % on the Switchboard subset of the Hub5 2000 evaluation testset . On the acoustic side , we use a score fusion of three strong models : recurrent nets with maxout activations , very deep convolutional nets with 3x3 kernels , and bidirectional long short - term memory nets which operate on FMLLR and i - vector features . On the language modeling side , we use an updated model \u201c M \u201d and hierarchical neural network LMs . GeorgeSaon , TomSercu , StevenRennieandHong - KwangJ.Kuo IBMT.J.WatsonResearchCenter , YorktownHeights , NY , 10598 gsaon@us.ibm.com \u00a9 IEEE2004 SubmittedtoInterspeech2016 , pleasedonotredistribute Index Terms : recurrent neural networks , convolutional neural networks , conversational speech recognition section : Introduction The landscape of neural network acoustic modeling is rapidly evolving . Spurred by the success of deep feed - forward neural nets for LVCSR in and inspired by other research areas like image classification and natural language processing , many speech groups have looked at more sophisticated architectures such as deep convolutional nets , deep recurrent nets , time - delay neural nets , and long - short term memory nets . The trend is to remove a lot of the complexity and human knowledge that was necessary in the past to build good ASR systems ( e.g. speaker adaptation , phonetic context modeling , discriminative feature processing , etc . ) and to replace them with a powerful neural network architecture that can be trained agnostically on a lot of data . With the advent of numerous neural network toolkits which can implement these sophisticated models out - of - the - box and powerful hardware based on GPUs , the barrier of entry for building high performing ASR systems has been lowered considerably . First case in point : front - end processing has been simplified considerably with the use of CNNs which treat the log - mel spectral representation as an image and do n\u2019t require extra processing steps such as PLP cepstra , LDA , FMLLR , fMPE transforms , etc . Second case in point : end - to - end ASR systems such as bypass the need of having phonetic context decision trees and HMMs altogether and directly map the sequence of acoustic features to a sequence of characters or context independent phones . Third case in point : training algorithms such as connectionist temporal classification do n\u2019t require an initial alignment of the training data which is typically done with a GMM - based baseline model . The above points beg the question whether , in this age of readily available NN toolkits , speech recognition expertise is still necessary or whether one can simply point a neural net to the audio and transcripts , let it train , and obtain a good acoustic model . While it is true that , as the amount of training data increases , the need for human ASR expertise is lessened , at the moment the performance of end - to - end systems ultimately remains inferior to that of more traditional , i.e. HMM and decision tree - based , approaches . Since the goal of this work is to obtain the lowest possible WER on the Switchboard dataset regardless of other practical considerations such as speed and / or simplicity , we have focused on the latter approaches . The paper is organized as follows : in section [ reference ] we discuss acoustic and language modeling improvements and in section [ reference ] we summarize our findings . section : System improvements In this section we describe three different acoustic models that were trained on 2000 hours of English conversational telephone speech : recurrent nets with maxout activations and annealed dropout , very deep convolutional nets with 3 3 kernels , and bidirectional long short - term memory nets operating on FMLLR and i - vector features . All models are used in a hybrid HMM decoding scenario by subtracting the logarithm of the HMM state priors from the log of the softmax output scores . The training and test data , frontend processing , speaker adaptation are identical to and their description will be omitted . At the end of the section , we also provide an update on our vocabulary and language modeling experiments . subsection : Recurrent nets with maxout activations We remind the reader that maxout nets generalize ReLu units by employing non - linearities of the form where the subsets of neurons are typically disjoint . In we have shown that maxout DNNs and CNNs trained with annealed dropout outperform their sigmoid - based counterparts on both 300 hours and 2000 hours training regimes . What was missing there was a comparison between maxout and sigmoid for unfolded RNNs . The architecture of the maxout RNNs comprises one recurrent layer with 2828 units projected to 1414 units via non - overlapping maxout operations . This layer is followed by 4 non - recurrent layers with 2828 units ( also projected to 1414 ) followed by a bottleneck with 1024 512 units and an output layer with 32000 neurons corresponding to as many context - dependent HMM states . The number of neurons for the maxout layers have been chosen such that the weight matrices have roughly the same number of parameters as the baseline sigmoid network which has 2048 units per hidden layer . The recurrent layer is unfolded backwards in time for 6 time steps and has 340 - dimensional inputs consisting of 6 spliced right context 40 - dimensional FMLLR frames ( ) to which we append a 100 - dimensional speaker - based ivector . The unfolded maxout RNN architecture is depicted in Figure [ reference ] . The network is trained one hidden layer at a time with discriminative pretraining followed by 12 epochs of SGD CE training on randomized minibatches of 250 samples . The model is refined with Hessian - free sequence discriminative training using the state - based MBR criterion for 10 iterations . In Table [ reference ] we report the error rates for sigmoid and maxout RNNs on the Switchboard and CallHome subsets of Hub5\u201900 . The decodings are done with a small vocabulary of 30 K words and a small 4 - gram language model with 4 M n - grams . Note that the sigmoid RNNs have better error rates than what was reported in because they have been retrained after the data has been realigned with the best joint RNN / CNN model . We observe that the maxout RNNs are consistently better and that , by themselves , they achieve a similar WER as our previous best model which was the joint RNN / CNN with sigmoid activations . subsection : Very deep convolutional networks Very deep CNNs with small kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN - HMM speech recognition systems . Results were provided after cross - entropy training on the 300 hours switchboard - 1 dataset in , and results from sequence training on both switchboard - 1 and the 2000 hours switchboard + Fisher dataset are in . The very deep convolutional networks are inspired by the \u201c VGG Net \u201d architecture introduced in for the 2014 ImageNet classification challenge , with the central idea to replace large convolutional kernels by small kernels . By stacking many of these convolutional layers with ReLU nonlinearities before pooling layers , the same receptive field is created with less parameters and more nonlinearity . Figure [ reference ] shows the design of the networks . Note that as we go deeper in the network , the time and frequency resolution is reduced through pooling only , while the convolutions are zero - padded as to not reduce the size of the feature maps . We increase the number of feature maps gradually from 64 to 512 ( indicated by the different colors ) . We pool right before the layer that increases the number of feature maps . Note that the indication of feature map size on the right only applies to the rightmost 2 designs . In contrast , the classical CNN architecture has only two layers , goes to 512 feature maps directly , and uses a large kernel on the first layer . Our 10 - layer CNN has about the same number of parameters as the classical CNN , converges in 5 times fewer epochs , but is computationally more expensive . Results for 3 variations of the 10 - layer CNN are in table [ reference ] . For model combination , we use the version with pooling , which is the exact same model without modifications from the original paper . Our implementation was done in Torch . We adopt the balanced sampling from , by sampling from context dependent state with probability . We keep throughout the experiments during cross - entropy training . During CE training , we optimize with simple SGD or NAG , during ST we found NAG to be superior to SGD . We regularize the stochastic sequence training by adding the gradient of cross - entropy loss , as proposed in . subsection : Bidirectional LSTMs Given the recent popularity of LSTMs for acoustic modeling , we have experimented with such models on the Switchboard task using the Torch toolkit . We have looked at the effect of the input features on LSTM performance , the number of layers and whether start states for the recurrent layers should be reset or carried over . We use bidirectional LSTMs that are trained on non - overlapping subsequences of 20 frames . The subsequences coming from the same utterance are contiguous so that the left - to - right final states for the current subsequence can be copied to the left - to - right start states for the next subsequence ( i.e. carried over ) . For processing speed and in order to get good gradient estimates , we group subsequences from multiple utterances into minibatches of size 256 . Regardless of the number of LSTM layers , all models use a linear bottleneck of size 256 before the softmax output layer ( of size 32000 ) . In one experiment , we compare the effect of input features on model performance . The baseline models are trained on 40 - dimensional FMLLR + 100 - dimensional ivector frames and have 1024 ( or 512 ) LSTM units per layer and per direction ( left - to - right and right - to - left ) . The forward and backward activations from the previous LSTM layer are concatenated and fed into the next LSTM layer . The contrast model is a single layer bidirectional LSTM trained on 128 - dim features obtained by performing PCA on 512 - dimensional bottleneck features . The features are obtained from a 6 - layer DNN cross entropy trained on blocks of 11 consecutive FMLLR frames and 100 - dimensional i - vectors . In Table [ reference ] , we report recognition results on Hub5\u201900 for these four models trained with 15 passes of cross - entropy SGD on the 300 hour ( SWB - 1 ) subset . Due to a bug that affected our earlier multi - layer LSTM results , we decided to go ahead with single layer bidirectional LSTMs on bottleneck features on the full 2000 hour training set . We also experimented with how to deal with the start states at the beginning of the left - to - right pass . One option is to carry them over from the previous subsequence and the other one is to reset the start states at the beginning of each subsequence . In Figure [ reference ] we compare the cross - entropy loss on held - out data between these two models . As can be seen , the LSTM model with carried over start states is much better at predicting the correct HMM state . However , when comparing word error rates in Table [ reference ] , the LSTM with start states that are reset has a better performance . We surmise that this is because the increased memory of the LSTM with carried over start states is in conflict with the state sequence constraints imposed by the HMM topology and the language model . Additionally , we show the WERs of the DNN used for the bottleneck features and of a 4 - layer 512 unit LSTM . We observe that the 4 layer LSTM is significantly better than the DNN and the two single layer LSTMs trained on bottleneck features . subsection : Model combination In Table [ reference ] we report the performance of the individual models ( RNN , VGG and 4 - layer LSTM ) described in the previous subsections as well as the results after frame - level score fusion . All decodings are done with a 30 K word vocabulary and a small 4 - gram language model with 4 M n - grams . We note that RNNs and VGG nets exhibit similar performance and have a strong complementarity which improves the WER by 0.6 % and 0.9 % on SWB and CH , respectively . subsection : Language modeling experiments Our language modeling strategy largely parallels that described in . For completeness , we will repeat some of the details here . The main difference is an increase in the vocabulary size from 30 K words to 85 K words . When comparing acoustic models in previous sections , we used a relatively small legacy language model used in previous publications : a 4 M n - gram ( n=4 ) language model with a vocabulary of 30.5 K words . We wanted to increase the language model coverage in a manner that others can replicate . To this end , we increased the vocabulary size from 30.5 K words to 85 K words by adding the vocabulary of the publicly available Broadcast News task . We also added to the LM publicly available text data from LDC , including Switchboard , Fisher , Gigaword , and Broadcast News and Conversations . The most relevant data are the transcripts of the 1975 hour audio data used to train the acoustic model , consisting of about 24 M words . For each corpus we trained a 4 - gram model with modified Kneser - Ney smoothing . The component LMs are linearly interpolated with weights chosen to optimize perplexity on a held - out set . Entropy pruning was applied , resulting in a single 4 - gram LM consisting of 36 M n - grams . This new n - gram LM was used together with our best acoustic model to decode and generate word lattices for LM rescoring experiments . The first two lines of Table [ reference ] show the improvement using this larger n - gram LM with larger vocabulary trained on more data . The WER improved by 1.0 % for SWB . Part of this improvement ( 0.1 - 0.2 % ) was due to also using a larger beam for decoding and a change in vocabulary tokenization . We used two types of LMs for LM rescoring : model M , a class - based exponential model and feed - forward neural network LM ( NNLM ) . We built a model M LM on each corpus and interpolated the models , together with the 36 M n - gram LM . As shown in Table [ reference ] , using model M results in an improvement of 0.6 % on SWB . We built two NNLMs for interpolation . One was trained on just the most relevant data : the 24 M word corpus ( Switchboard / Fisher / CallHome acoustic transcripts ) . Another was trained on a 560 M word subset of the LM training data : in order to speed up training for this larger set , we employed a hierarchical NNLM approximation . Table [ reference ] shows that the NNLMs provided an additional 0.4 % improvement over the model M result on SWB . Compared with the n - gram LM baseline , LM rescoring yielded a total improvement of 1.0 % on SWB ( 7.6 % to 6.6 % ) and 1.5 % on CH ( 13.7 % to 12.2 % ) . section : Conclusion In our previous Switchboard system paper we have observed a good complementarity between recurrent nets and convolutional nets and their combination led to significant accuracy gains . In this paper we have presented an improved unfolded RNN ( with maxout instead of sigmoid activations ) and a stronger CNN obtained by adding more convolutional layers with smaller kernels and ReLu nonlinearities . These improved models still have good complementarity and their frame - level score combination in conjunction with a multi - layer LSTM leads to a 0.4% - 0.7 % decrease in WER over the LSTM . Multi - layer LSTMs were the strongest performing model followed closely by the RNN and VGG nets . We also believe that LSTMs have more potential for direct sequence - to - sequence modeling and we are actively exploring this area of research . On the language modeling side , we have increased our vocabulary from 30 K to 85 K words and updated our component LMs . At the moment , we are less than 3 % away from achieving human performance on the Switchboard data ( estimated to be around 4 % ) . Unfortunately , it looks like future improvements on this task will be considerably harder to get and will probably require a breakthrough in direct sequence - to - sequence modeling and a significant increase in training data . section : Acknowledgment The authors wish to thank E. Marcheret , J. Cui and M. Nussbaum - Thom for useful suggestions about LSTMs . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Switchboard___Hub500"]]], "Method": [[["IBM_2016"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Switchboard___Hub500"]]], "Method": [[["RNN___VGG___LSTM_acoustic_model_trained_on_SWB_Fisher_CH__N-gram____model_M____NNLM_language_model"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}, {"incident_type": "SciREX_incident", "Material": [[["swb_hub_500_WER_fullSWBCH"]]], "Method": [[["RNN___VGG___LSTM_acoustic_model_trained_on_SWB_Fisher_CH__N-gram____model_M____NNLM_language_model"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}]}
{"docid": "TST3-SREX-0004", "doctext": "document : Rethinking the Inception Architecture for Computer Vision Convolutional networks are at the core of most state - of - the - art computer vision solutions for a wide variety of tasks . Since 2014 very deep convolutional networks started to become mainstream , yielding substantial gains in various benchmarks . Although increased model size and computational cost tend to translate to immediate quality gains for most tasks ( as long as enough labeled data is provided for training ) , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big - data scenarios . Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization . We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art : top - and top - error for single frame evaluation using a network with a computational cost of billion multiply - adds per inference and with using less than 25 million parameters . With an ensemble of models and multi - crop evaluation , we report top - error and top - error . section : Introduction Since the 2012 ImageNet competition winning entry by Krizhevsky et al , their network \u201c AlexNet \u201d has been successfully applied to a larger variety of computer vision tasks , for example to object - detection , segmentation , human pose estimation , video classification , object tracking , and superresolution . These successes spurred a new line of research that focused on finding higher performing convolutional neural networks . Starting in 2014 , the quality of network architectures significantly improved by utilizing deeper and wider networks . VGGNet and GoogLeNet yielded similarly high performance in the 2014 ILSVRC classification challenge . One interesting observation was that gains in the classification performance tend to transfer to significant quality gains in a wide variety of application domains . This means that architectural improvements in deep convolutional architecture can be utilized for improving performance for most other computer vision tasks that are increasingly reliant on high quality , learned visual features . Also , improvements in the network quality resulted in new application domains for convolutional networks in cases where AlexNet features could not compete with hand engineered , crafted solutions , e.g. proposal generation in detection . Although VGGNet has the compelling feature of architectural simplicity , this comes at a high cost : evaluating the network requires a lot of computation . On the other hand , the Inception architecture of GoogLeNet was also designed to perform well even under strict constraints on memory and computational budget . For example , GoogleNet employed only 5 million parameters , which represented a reduction with respect to its predecessor AlexNet , which used million parameters . Furthermore , VGGNet employed about 3x more parameters than AlexNet . The computational cost of Inception is also much lower than VGGNet or its higher performing successors . This has made it feasible to utilize Inception networks in big - data scenarios , , where huge amount of data needed to be processed at reasonable cost or scenarios where memory or computational capacity is inherently limited , for example in mobile vision settings . It is certainly possible to mitigate parts of these issues by applying specialized solutions to target memory use , or by optimizing the execution of certain operations via computational tricks . However , these methods add extra complexity . Furthermore , these methods could be applied to optimize the Inception architecture as well , widening the efficiency gap again . Still , the complexity of the Inception architecture makes it more difficult to make changes to the network . If the architecture is scaled up naively , large parts of the computational gains can be immediately lost . Also , does not provide a clear description about the contributing factors that lead to the various design decisions of the GoogLeNet architecture . This makes it much harder to adapt it to new use - cases while maintaining its efficiency . For example , if it is deemed necessary to increase the capacity of some Inception - style model , the simple transformation of just doubling the number of all filter bank sizes will lead to a 4x increase in both computational cost and number of parameters . This might prove prohibitive or unreasonable in a lot of practical scenarios , especially if the associated gains are modest . In this paper , we start with describing a few general principles and optimization ideas that that proved to be useful for scaling up convolution networks in efficient ways . Although our principles are not limited to Inception - type networks , they are easier to observe in that context as the generic structure of the Inception style building blocks is flexible enough to incorporate those constraints naturally . This is enabled by the generous use of dimensional reduction and parallel structures of the Inception modules which allows for mitigating the impact of structural changes on nearby components . Still , one needs to be cautious about doing so , as some guiding principles should be observed to maintain high quality of the models . section : General Design Principles Here we will describe a few design principles based on large - scale experimentation with various architectural choices with convolutional networks . At this point , the utility of the principles below are speculative and additional future experimental evidence will be necessary to assess their accuracy and domain of validity . Still , grave deviations from these principles tended to result in deterioration in the quality of the networks and fixing situations where those deviations were detected resulted in improved architectures in general . Avoid representational bottlenecks , especially early in the network . Feed - forward networks can be represented by an acyclic graph from the input layer ( s ) to the classifier or regressor . This defines a clear direction for the information flow . For any cut separating the inputs from the outputs , one can access the amount of information passing though the cut . One should avoid bottlenecks with extreme compression . In general the representation size should gently decrease from the inputs to the outputs before reaching the final representation used for the task at hand . Theoretically , information content can not be assessed merely by the dimensionality of the representation as it discards important factors like correlation structure ; the dimensionality merely provides a rough estimate of information content . Higher dimensional representations are easier to process locally within a network . Increasing the activations per tile in a convolutional network allows for more disentangled features . The resulting networks will train faster . Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power . For example , before performing a more spread out ( e.g. ) convolution , one can reduce the dimension of the input representation before the spatial aggregation without expecting serious adverse effects . We hypothesize that the reason for that is the strong correlation between adjacent unit results in much less loss of information during dimension reduction , if the outputs are used in a spatial aggregation context . Given that these signals should be easily compressible , the dimension reduction even promotes faster learning . Balance the width and depth of the network . Optimal performance of the network can be reached by balancing the number of filters per stage and the depth of the network . Increasing both the width and the depth of the network can contribute to higher quality networks . However , the optimal improvement for a constant amount of computation can be reached if both are increased in parallel . The computational budget should therefore be distributed in a balanced way between the depth and width of the network . Although these principles might make sense , it is not straightforward to use them to improve the quality of networks out of box . The idea is to use them judiciously in ambiguous situations only . section : Factorizing Convolutions with Large Filter Size Much of the original gains of the GoogLeNet network arise from a very generous use of dimension reduction . This can be viewed as a special case of factorizing convolutions in a computationally efficient manner . Consider for example the case of a convolutional layer followed by a convolutional layer . In a vision network , it is expected that the outputs of near - by activations are highly correlated . Therefore , we can expect that their activations can be reduced before aggregation and that this should result in similarly expressive local representations . Here we explore other ways of factorizing convolutions in various settings , especially in order to increase the computational efficiency of the solution . Since Inception networks are fully convolutional , each weight corresponds to one multiplication per activation . Therefore , any reduction in computational cost results in reduced number of parameters . This means that with suitable factorization , we can end up with more disentangled parameters and therefore with faster training . Also , we can use the computational and memory savings to increase the filter - bank sizes of our network while maintaining our ability to train each model replica on a single computer . subsection : Factorization into smaller convolutions Convolutions with larger spatial filters ( e.g. or ) tend to be disproportionally expensive in terms of computation . For example , a convolution with filters over a grid with filters is 25 / 9 = 2.78 times more computationally expensive than a convolution with the same number of filters . Of course , a filter can capture dependencies between signals between activations of units further away in the earlier layers , so a reduction of the geometric size of the filters comes at a large cost of expressiveness . However , we can ask whether a convolution could be replaced by a multi - layer network with less parameters with the same input size and output depth . If we zoom into the computation graph of the convolution , we see that each output looks like a small fully - connected network sliding over tiles over its input ( see Figure [ reference ] ) . Since we are constructing a vision network , it seems natural to exploit translation invariance again and replace the fully connected component by a two layer convolutional architecture : the first layer is a convolution , the second is a fully connected layer on top of the output grid of the first layer ( see Figure [ reference ] ) . Sliding this small network over the input activation grid boils down to replacing the convolution with two layers of convolution ( compare Figure [ reference ] with [ reference ] ) . This setup clearly reduces the parameter count by sharing the weights between adjacent tiles . To analyze the expected computational cost savings , we will make a few simplifying assumptions that apply for the typical situations : We can assume that , that is that we want to change the number of activations / unit by a constant alpha factor . Since the convolution is aggregating , is typically slightly larger than one ( around 1.5 in the case of GoogLeNet ) . Having a two layer replacement for the layer , it seems reasonable to reach this expansion in two steps : increasing the number of filters by in both steps . In order to simplify our estimate by choosing ( no expansion ) , If we would naivly slide a network without reusing the computation between neighboring grid tiles , we would increase the computational cost . sliding this network can be represented by two convolutional layers which reuses the activations between adjacent tiles . This way , we end up with a net reduction of computation , resulting in a relative gain of by this factorization . The exact same saving holds for the parameter count as each parameter is used exactly once in the computation of the activation of each unit . Still , this setup raises two general questions : Does this replacement result in any loss of expressiveness ? If our main goal is to factorize the linear part of the computation , would it not suggest to keep linear activations in the first layer ? We have ran several control experiments ( for example see figure [ reference ] ) and using linear activation was always inferior to using rectified linear units in all stages of the factorization . We attribute this gain to the enhanced space of variations that the network can learn especially if we batch - normalize the output activations . One can see similar effects when using linear activations for the dimension reduction components . subsection : Spatial Factorization into Asymmetric Convolutions The above results suggest that convolutions with filters larger a might not be generally useful as they can always be reduced into a sequence of convolutional layers . Still we can ask the question whether one should factorize them into smaller , for example convolutions . However , it turns out that one can do even better than by using asymmetric convolutions , e.g. . For example using a convolution followed by a convolution is equivalent to sliding a two layer network with the same receptive field as in a convolution ( see figure [ reference ] ) . Still the two - layer solution is cheaper for the same number of output filters , if the number of input and output filters is equal . By comparison , factorizing a convolution into a two convolution represents only a saving of computation . . In theory , we could go even further and argue that one can replace any convolution by a convolution followed by a convolution and the computational cost saving increases dramatically as grows ( see figure 6 ) . In practice , we have found that employing this factorization does not work well on early layers , but it gives very good results on medium grid - sizes ( On feature maps , where ranges between and ) . On that level , very good results can be achieved by using convolutions followed by convolutions . section : Utility of Auxiliary Classifiers has introduced the notion of auxiliary classifiers to improve the convergence of very deep networks . The original motivation was to push useful gradients to the lower layers to make them immediately useful and improve the convergence during training by combating the vanishing gradient problem in very deep networks . Also Lee et al argues that auxiliary classifiers promote more stable learning and better convergence . Interestingly , we found that auxiliary classifiers did not result in improved convergence early in the training : the training progression of network with and without side head looks virtually identical before both models reach high accuracy . Near the end of training , the network with the auxiliary branches starts to overtake the accuracy of the network without any auxiliary branch and reaches a slightly higher plateau . Also used two side - heads at different stages in the network . The removal of the lower auxiliary branch did not have any adverse effect on the final quality of the network . Together with the earlier observation in the previous paragraph , this means that original the hypothesis of that these branches help evolving the low - level features is most likely misplaced . Instead , we argue that the auxiliary classifiers act as regularizer . This is supported by the fact that the main classifier of the network performs better if the side branch is batch - normalized or has a dropout layer . This also gives a weak supporting evidence for the conjecture that batch normalization acts as a regularizer . section : Efficient Grid Size Reduction Traditionally , convolutional networks used some pooling operation to decrease the grid size of the feature maps . In order to avoid a representational bottleneck , before applying maximum or average pooling the activation dimension of the network filters is expanded . For example , starting a grid with filters , if we would like to arrive at a grid with filters , we first need to compute a stride - 1 convolution with filters and then apply an additional pooling step . This means that the overall computational cost is dominated by the expensive convolution on the larger grid using operations . One possibility would be to switch to pooling with convolution and therefore resulting in reducing the computational cost by a quarter . However , this creates a representational bottlenecks as the overall dimensionality of the representation drops to resulting in less expressive networks ( see Figure [ reference ] ) . Instead of doing so , we suggest another variant the reduces the computational cost even further while removing the representational bottleneck . ( see Figure [ reference ] ) . We can use two parallel stride 2 blocks : and . is a pooling layer ( either average or maximum pooling ) the activation , both of them are stride the filter banks of which are concatenated as in figure [ reference ] . section : Inception - v2 Here we are connecting the dots from above and propose a new architecture with improved performance on the ILSVRC 2012 classification benchmark . The layout of our network is given in table [ reference ] . Note that we have factorized the traditional convolution into three convolutions based on the same ideas as described in section [ reference ] . For the Inception part of the network , we have traditional inception modules at the with filters each . This is reduced to a grid with filters using the grid reduction technique described in section [ reference ] . This is is followed by instances of the factorized inception modules as depicted in figure [ reference ] . This is reduced to a grid with the grid reduction technique depicted in figure [ reference ] . At the coarsest level , we have two Inception modules as depicted in figure [ reference ] , with a concatenated output filter bank size of 2048 for each tile . The detailed structure of the network , including the sizes of filter banks inside the Inception modules , is given in the supplementary material , given in the model.txt that is in the tar - file of this submission . However , we have observed that the quality of the network is relatively stable to variations as long as the principles from Section [ reference ] are observed . Although our network is layers deep , our computation cost is only about higher than that of GoogLeNet and it is still much more efficient than VGGNet . section : Model Regularization via Label Smoothing Here we propose a mechanism to regularize the classifier layer by estimating the marginalized effect of label - dropout during training . For each training example , our model computes the probability of each label : . Here , are the logits or unnormalized log - probabilities . Consider the ground - truth distribution over labels for this training example , normalized so that . For brevity , let us omit the dependence of and on example . We define the loss for the example as the cross entropy : . Minimizing this is equivalent to maximizing the expected log - likelihood of a label , where the label is selected according to its ground - truth distribution . Cross - entropy loss is differentiable with respect to the logits and thus can be used for gradient training of deep models . The gradient has a rather simple form : , which is bounded between and . Consider the case of a single ground - truth label , so that and for all . In this case , minimizing the cross entropy is equivalent to maximizing the log - likelihood of the correct label . For a particular example with label , the log - likelihood is maximized for , where is Dirac delta , which equals for and otherwise . This maximum is not achievable for finite but is approached if for all \u2013 that is , if the logit corresponding to the ground - truth label is much great than all other logits . This , however , can cause two problems . First , it may result in over - fitting : if the model learns to assign full probability to the ground - truth label for each training example , it is not guaranteed to generalize . Second , it encourages the differences between the largest logit and all others to become large , and this , combined with the bounded gradient , reduces the ability of the model to adapt . Intuitively , this happens because the model becomes too confident about its predictions . We propose a mechanism for encouraging the model to be less confident . While this may not be desired if the goal is to maximize the log - likelihood of training labels , it does regularize the model and makes it more adaptable . The method is very simple . Consider a distribution over labels , independent of the training example x , and a smoothing parameter . For a training example with ground - truth label , we replace the label distribution with which is a mixture of the original ground - truth distribution and the fixed distribution , with weights and , respectively . This can be seen as the distribution of the label obtained as follows : first , set it to the ground - truth label ; then , with probability , replace with a sample drawn from the distribution . We propose to use the prior distribution over labels as . In our experiments , we used the uniform distribution , so that We refer to this change in ground - truth label distribution as label - smoothing regularization , or LSR . Note that LSR achieves the desired goal of preventing the largest logit from becoming much larger than all others . Indeed , if this were to happen , then a single would approach while all others would approach . This would result in a large cross - entropy with because , unlike , all have a positive lower bound . Another interpretation of LSR can be obtained by considering the cross entropy : Thus , LSR is equivalent to replacing a single cross - entropy loss with a pair of such losses and . The second loss penalizes the deviation of predicted label distribution from the prior , with the relative weight . Note that this deviation could be equivalently captured by the KL divergence , since and is fixed . When is the uniform distribution , is a measure of how dissimilar the predicted distribution is to uniform , which could also be measured ( but not equivalently ) by negative entropy ; we have not experimented with this approach . In our ImageNet experiments with classes , we used and . For ILSVRC 2012 , we have found a consistent improvement of about absolute both for top - error and the top - error ( cf . Table [ reference ] ) . section : Training Methodology We have trained our networks with stochastic gradient utilizing the TensorFlow distributed machine learning system using replicas running each on a NVidia Kepler GPU with batch size for epochs . Our earlier experiments used momentum with a decay of , while our best models were achieved using RMSProp with decay of and . We used a learning rate of , decayed every two epoch using an exponential rate of . In addition , gradient clipping with threshold was found to be useful to stabilize the training . Model evaluations are performed using a running average of the parameters computed over time . section : Performance on Lower Resolution Input A typical use - case of vision networks is for the the post - classification of detection , for example in the Multibox context . This includes the analysis of a relative small patch of the image containing a single object with some context . The tasks is to decide whether the center part of the patch corresponds to some object and determine the class of the object if it does . The challenge is that objects tend to be relatively small and low - resolution . This raises the question of how to properly deal with lower resolution input . The common wisdom is that models employing higher resolution receptive fields tend to result in significantly improved recognition performance . However it is important to distinguish between the effect of the increased resolution of the first layer receptive field and the effects of larger model capacitance and computation . If we just change the resolution of the input without further adjustment to the model , then we end up using computationally much cheaper models to solve more difficult tasks . Of course , it is natural , that these solutions loose out already because of the reduced computational effort . In order to make an accurate assessment , the model needs to analyze vague hints in order to be able to \u201c hallucinate \u201d the fine details . This is computationally costly . The question remains therefore : how much does higher input resolution helps if the computational effort is kept constant . One simple way to ensure constant effort is to reduce the strides of the first two layer in the case of lower resolution input , or by simply removing the first pooling layer of the network . For this purpose we have performed the following three experiments : receptive field with stride and maximum pooling after the first layer . receptive field with stride and maximum pooling after the first layer . receptive field with stride and without pooling after the first layer . All three networks have almost identical computational cost . Although the third network is slightly cheaper , the cost of the pooling layer is marginal and ( within of the total cost of the ) network . In each case , the networks were trained until convergence and their quality was measured on the validation set of the ImageNet ILSVRC 2012 classification benchmark . The results can be seen in table [ reference ] . Although the lower - resolution networks take longer to train , the quality of the final result is quite close to that of their higher resolution counterparts . However , if one would just naively reduce the network size according to the input resolution , then network would perform much more poorly . However this would an unfair comparison as we would are comparing a 16 times cheaper model on a more difficult task . Also these results of table [ reference ] suggest , one might consider using dedicated high - cost low resolution networks for smaller objects in the R - CNN context . section : Experimental Results and Comparisons Inception - v2 RMSProp Inception - v2 Label Smoothing Inception - v2 Factorized Table [ reference ] shows the experimental results about the recognition performance of our proposed architecture ( Inception - v2 ) as described in Section [ reference ] . Each Inception - v2 line shows the result of the cumulative changes including the highlighted new modification plus all the earlier ones . Label Smoothing refers to method described in Section [ reference ] . Factorized includes a change that factorizes the first convolutional layer into a sequence of convolutional layers . BN - auxiliary refers to the version in which the fully connected layer of the auxiliary classifier is also batch - normalized , not just the convolutions . We are referring to the model in last row of Table [ reference ] as Inception - v3 and evaluate its performance in the multi - crop and ensemble settings . All our evaluations are done on the 48238 non - blacklisted examples on the ILSVRC - 2012 validation set , as suggested by . We have evaluated all the 50000 examples as well and the results were roughly 0.1 % worse in top - 5 error and around 0.2 % in top - 1 error . In the upcoming version of this paper , we will verify our ensemble result on the test set , but at the time of our last evaluation of BN - Inception in spring indicates that the test and validation set error tends to correlate very well . section : Conclusions We have provided several design principles to scale up convolutional networks and studied them in the context of the Inception architecture . This guidance can lead to high performance vision networks that have a relatively modest computation cost compared to simpler , more monolithic architectures . Our highest quality version of Inception - v3 reaches , top - and top - 5 error for single crop evaluation on the ILSVR 2012 classification , setting a new state of the art . This is achieved with relatively modest ( ) increase in computational cost compared to the network described in Ioffe et al . Still our solution uses much less computation than the best published results based on denser networks : our model outperforms the results of He et al \u2013 cutting the top - ( top - ) error by ( ) relative , respectively \u2013 while being six times cheaper computationally and using at least five times less parameters ( estimated ) . Our ensemble of four Inception - v3 models reaches with multi - crop evaluation reaches top - error which represents an over reduction to the best published results and is almost half of the error of ILSVRC 2014 winining GoogLeNet ensemble . We have also demonstrated that high quality results can be reached with receptive field resolution as low as . This might prove to be helpful in systems for detecting relatively small objects . We have studied how factorizing convolutions and aggressive dimension reductions inside neural network can result in networks with relatively low computational cost while maintaining high quality . The combination of lower parameter count and additional regularization with batch - normalized auxiliary classifiers and label - smoothing allows for training high quality networks on relatively modest sized training sets . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["ImageNet"]]], "Method": [[["Inception_V3"]]], "Metric": [[["Top_1_Accuracy"]]], "Task": [[["Image_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ImageNet"]]], "Method": [[["Inception_V3"]]], "Metric": [[["Top_5_Accuracy"]]], "Task": [[["Image_Classification"]]]}]}
{"docid": "TST3-SREX-0005", "doctext": "document : ProNet : Learning to Propose Object - specific Boxes for Cascaded Neural Networks This paper aims to classify and locate objects accurately and efficiently , without using bounding box annotations . It is challenging as objects in the wild could appear at arbitrary locations and in different scales . In this paper , we propose a novel classification architecture ProNet based on convolutional neural networks . It uses computationally efficient neural networks to propose image regions that are likely to contain objects , and applies more powerful but slower networks on the proposed regions . The basic building block is a multi - scale fully - convolutional network which assigns object confidence scores to boxes at different locations and scales . We show that such networks can be trained effectively using image - level annotations , and can be connected into cascades or trees for efficient object classification . ProNet outperforms previous state - of - the - art significantly on PASCAL VOC 2012 and MS COCO datasets for object classification and point - based localization . section : Introduction We address the problem of object classification and localization in natural images . As objects could be small and appear at arbitrary locations , several frameworks rely on bounding boxes to train object - centric classifiers , and apply the classifiers by searching over different locations of the images . However , the annotation process for object bounding boxes is usually resource intensive and difficult to scale up . In light of this , we aim to simultaneously classify and locate objects given only image - level annotations for training . To cope with the lack of object - level annotations , several methods extract feature activations from convolutional neural networks ( CNN ) by scanning over different image regions . They then aggregate the extracted features into image - level representations for classification purpose . Under this scheme , regions that belong to the background are considered as important as regions that contain objects . Such global approaches tend to be sensitive to background , and can not be used directly for localization . We choose to use the fully - convolutional network ( FCN ) architecture for simultaneous object classification and localization . It replaces the fully - connected layers of a standard CNN ( e.g. AlexNet ) with convolutional layers . This enables an FCN to take images of arbitrary sizes , and generate classification score maps efficiently . Each element in a score map corresponds to a rectangular box ( receptive field ) in the original image . The score maps can then be used for classification and localization . The sampling strides and box sizes are determined by the FCN \u2019s network architecture . As box sizes are fixed , FCN might face difficulty dealing with objects of different scales . We address this problem by using a multi - stream multi - scale architecture . All streams share the same parameters , but take input images of different scales . To train the multi - scale FCN without object - level annotations , we generate image - level scores by pooling the score maps over multiple - scales , and compute the losses with image - level labels for back - propagation . Once a multi - scale FCN is trained , it can be used for classification and localization directly . From another perspective , it also proposes a set of promising boxes that are likely to contain objects . We can then build a cascade architecture by zooming onto those promising boxes , and train new classifiers to verify them . The cascade allows the system to balance accuracy and speed : each stage filters out parts of image regions that are unlikely to contain objects . We name this propose and zoom pipeline as ProNet . Figure [ reference ] provides the high - level intuition behind ProNet : three boxes are proposed for bird , potted plant and cat categories . The boxes are cropped out and verified further , until a certain decision is made . To train the later classifiers in ProNet , we sample hard negatives based on image - level labels . For positives , as no object - level annotations are available , it is impossible to tell objects from background . To avoid over - fitting , we randomly sample positive boxes above a relative low threshold . Different positive boxes from the same image can be sampled at different iterations of the stochastic gradient descent training process . At test time , only a small subset of boxes ( 10 to 20 per image ) with highest object confidence scores are fed to the later classifiers . This allows us to utilize CNNs that have stronger representation power with little computational overhead . ProNet is highly configurable : for example , one could set a list of important object categories , and only verify the proposed boxes for those categories . Moreover , apart from a traditional chain - structured cascade , we show that it is also possible to build tree - structured cascades , where each branch handles categories from a particular domain ( set of vehicles or animals ) . In summary , our paper makes the following contributions : We propose ProNet , a cascaded neural network framework that zooms onto promising object - specific boxes for efficient object classification and localization . We introduce strategies to train ProNet with image - level annotations effectively ; and demonstrate the implementations of chain - and tree - structured cascades . We show that ProNet outperforms previous state - of - the - art significantly on the object classification and point - based localization tasks of the PASCAL VOC 2012 dataset and the recently released MS COCO dataset . section : Related Work Object classification is a fundamental problem in Computer Vision . Earlier work focused on classification from object - centric images . They usually extract hand - crafted low - level features and aggregate the features into image - level feature vectors . More challenging datasets have since been collected . They are of larger scale , and contain smaller objects which could be partially occluded . Recently , deep convolutional neural networks ( CNN ) have achieved state - of - the - art performance on a wide range of visual recognition tasks , including object classification and detection . Although CNNs require large amount of data for training , it has been shown that they are able to learn representations that generalize to other tasks . Such representations can be adapted to image classification by fine - tuning , or extracted as holistic features for classification with linear SVMs . When used as generic feature extractors , feature aggregation techniques designed for hand - crafted features can also work with CNN embeddings and achieve competitive performance . An alternative approach for object classification is via detection . Among those utilizing bounding box annotations , RCNN achieves competitive performance by directly representing image boxes with CNN features and learning classifiers on top of the features . Object proposal techniques are used to sample the image patches for classification . A recent framework , fast RCNN , uses fully - convolutional networks ( FCN ) to generate box - level features in batch , and is thus more computational efficient . Object localization with image - level annotations is a weakly - supervised problem . It can be formulated as a multiple instance learning problem , and has been addressed to learn concept detectors from Internet data . It has also been studied for object detection and segmentation . For object classification , Wei et al . treat images as bags of patches , where the patches are selected using objectness criteria . They then use max pooling to fine - tune CNNs based on image - level annotations . Oquab et al . follow a similar approach , but make the training process end - to - end by converting CNNs into FCNs . The proposal generation network in ProNet is also based on FCN , but uses a multi - stream architecture and cross - scale LSE pooling to achieve scale - awareness . Cascaded classifiers are a well - studied technique in Computer Vision . Cascades with CNNs have been explored for facial point detection , face detection and pose estimation . However , such methods require fully annotated training examples . ProNet adopts the cascade philosophy to balance speed and accuracy , but does not require object bounding boxes for training . Since ProNet is a general object classifier , it can also be extended to have tree structure , where each leaf is a domain expert . section : ProNet Framework ProNet has two basic components : an object - specific box proposal unit , and a verification unit . For each image , for each object category , the box proposal unit generates a list of confidence scores of the presence of the object instances , and the coordinates indicating the locations of the objects . ProNet then zooms onto image boxes with higher scores to further verify if they are positive or hard negatives . The verification units can either take all boxes , which forms a chain structure ; or a subset of boxes corresponding to certain domains ( animal ) , which forms a tree structure . We implement these two units with convolutional neural networks . Figure [ reference ] illustrates the overall ProNet framework . subsection : Proposal Generation The first stage in our framework is to generate object - specific box proposals with CNNs . For an input image and object category , we want to learn a proposal scoring function where corresponds to the location of a rectangular image region denoted by its top left and bottom right corners . A typical CNN architecture for image classification task ( e.g. AlexNet ) involves a hierarchy of convolutional layers and fully connected layers . The convolutional layers operate on local image patches to extract feature representations . For a color image with 3 channels , the convolutional layers generate a feature map of elements , where is the output feature dimension . and correspond to the width and height of the feature map , they are controlled by input image size , as well as the kernel size , sampling step and padding size of the convolutional layers . The fully connected layers serve as classifiers which take fixed - size inputs , thus require the width and height of input images to be fixed . Therefore , one possible way to compute is to enumerate locations and scales in a sliding window fashion or with bounding box proposals , and feed such image regions to CNNs . We take an alternative approach based on fully convolutional networks ( e.g. OverFeat ) . Fully convolutional networks ( FCN ) do not contain fully - connected layers . Rather , they use only the convolutional layers , which allows them to process images of arbitrary sizes . The outputs of FCNs are in the form of feature maps , where is the number of categories . Each element in a feature map corresponds to the activation response for a particular category over a certain region . Such regions are called receptive fields for the activations . Compared with region sampling with sliding windows or bounding box proposals , FCNs offer a seamless solution for end - to - end training under the CNN framework , and also naturally allow the sharing of intermediate features over overlapping image regions . Scale adaptation with multi - stream FCNs . One issue in use of FCNs is that the sizes of receptive fields are typically fixed , while the object scales may vary a lot . We address this problem by using a multi - stream architecture . Assume an FCN has been trained with inputs where objects have been resized to the same scale . We expand the network into streams , where every stream shares the same parameters as the pre - trained one . Given an image , we scale it to different sizes and feed to the - stream FCN . The output feature map of each stream corresponds to a different scale in the original image . Training with image - level annotations . When object bounding boxes are available , training FCNs is straight - forward : one could either crop images with the bounding boxes , or use a loss function which operates directly on feature maps and takes the object locations into account . As such supervision is absent , we need to aggregate local responses into global ones so that image - level labels can be used for training . We use the log - sum - exp ( LSE ) pooling function applied by for semantic segmentation : where is the category , corresponds to the - th stream of FCN , correspond to location in the feature map , is the total number of such elements and is a hyper parameter . The function \u2019s output is close to average when is small and maximum when is large . Setting larger makes the aggregation focus on a smaller subset of image boxes , and has the potential to handle smaller objects better . LSE pooling function can be implemented as a layer in a neural network . As illustrated in Figure [ reference ] , it is connected to the final layers of all - stream FCNs and produces a dimensional vector for each image . We then compute the loss for each category and back - propagate the error gradients to the earlier layers . Computing proposal scores . Once the FCNs have been trained , we compute proposal scores from the feature maps . Specifically , for every neuron in the final layer of single - stream FCN , we compute its receptive field and use it as the location ; the corresponding activation of the neuron is used as proposal score . Although the exact receptive field may vary due to different padding strategies , we use a simple estimation which has been reported to work well in practice . Denote the sampling stride of a spatial convolutional layer as and the kernel size of a max pooling layer as , the overall sampling stride is given by where is the collection of all convolutional layers and is the collection of all max pooling layers . Implementation . Our - stream FCNs are implemented with Torch . For each stream , we use the CNN - M 2048 architecture proposed in . It has 5 convolutional layers and 3 fully - connected layers . It achieves higher accuracy on ImageNet than AlexNet , while being faster and less memory consuming than very deep CNNs . We use the model parameters released by the authors , which were pre - trained from ImageNet dataset with 1 , 000 categories . We convert the model into an FCN by replacing the three fully - connected layers with convolutional layers . The first convolutional layer has 512 input planes , 4096 output planes and kernel size of 6 . The second has 4096 input planes , 2048 output planes and kernel size of 1 . Since the final layer is task - specific , it is initialized from scratch with 2048 input planes , output planes and kernel size of 1 . To adapt the model parameters for object classification on different datasets , we only fine - tune the final two layers and freeze the model parameters from previous layers . The sampling stride of feature maps is 32 pixels , and the window size is 223 pixels . We set the number of streams to be 3 . During training , all three streams share the same set of parameters . To facilitate training with mini - batches , every image is rescaled to , and pixels . As the aspect ratios of images could be different , we rescale the longer edge to 300 , 500 and 700 respectively , and fill the empty pixels by mirroring the images . Traditional cross entropy loss for multi - class classification introduces competition between different classes , thus it is not suitable for images with multiple labels . We compute the loss with binary cross entropy criteria for each class separately , and sum up the error gradients from losses of all classes for back - propagation . subsection : Cascade - style Proposal Verification By setting thresholds on proposal scores , a small subset of image boxes which might contain objects are selected . Similar to object detection frameworks , we run CNN classifiers on the selected boxes . The proposal step also serves as a filter whose goal is to preserve the object boxes with high recall rate , while removing the easy negatives . The verification classifiers then address a more focused problem on a smaller set of instances . Connecting the two steps is essentially the same as training a cascade of classifiers . Verification network architecture . As a later classifier in the cascade , accuracy is more important than speed . We choose the VGG - 16 network architecture . Compared with AlexNet variants , it offers better accuracy for most visual recognition tasks , but is also slower and more memory demanding . We use the VGG - 16 model parameters released by the authors , which was trained on 1 , 000 ImageNet categories . We use the same binary cross entropy criterion to compute losses . To make the training process faster , we only fine - tune the final two fully - connected layers and freeze all previous layers . InputInput OutputOutput Training images with proposal scores , batch size , threshold stopping criteria not met Randomly select images from ; Initialize mini - batch ; has proposal with score Randomly sample a proposal where ; Set the sample \u2019s active class to ; Add proposed region to ; Resize and add full image to ; Set all classes as active ; Forward pass with ; Compute loss for the active class of each sample ; Update model parameters . Mini - batch sampling algorithm for training cascade classifier with stochastic gradient descent . Training strategy for the cascade . Ideally , we want the verification network to handle hard examples from both positive and negative data . When a proposed region from an image not containing a given label has a high score of that class , we know it is a hard negative . However , it is impossible to tell a hard positive from background without using bounding box annotations . We attempt to avoid using background by selecting only the top scoring image region for each positive class . This results in significant over - fitting and poor generalizability for the trained verification net . The main problem with the above sampling strategy is that for positive instances , only easy examples which have been learned well are preserved . To fix this , we use a random sampling strategy as described in Algorithm [ reference ] . For each image , we randomly select an image box whose proposal score is higher than threshold for class . In practice , the threshold is set to a relative low value ( ) . If is labeled as positive for the image , we treat the box as a positive instance ( though it might belong to background ) , and otherwise negative . Note that the sampled box could be easy negatives for classes beyond . To avoid oversampling the easy negatives , we set as the active class during back - propagation and only compute the loss for the active class . Inference with cascade . During inference , an image is passed to the proposal generation FCN to compute proposal scores . A small subset of proposed boxes with high scores are then passed to the verification network . For each class , we select the top scoring proposals if the scores are higher than threshold . We then use the following equation to combine the outputs from both networks : where is the set of selected proposals for class , is the score of class from the proposal network after LSE pooling , and is the verification network \u2019s output for class on region . When no proposal is selected , we preserve scores from the proposal network without calibration as they are typically low . Discussion . Decomposing classification into cascade of proposal and verification networks allows the system to achieve high accuracy while maintaining a reasonable computational cost . It is also a flexible framework for different design choices . For example , one could decide to verify a subset of object classes which require higher accuracy . With the cascade training algorithm , we can build tree - structured cascaded neural networks , where each branch focuses on a subset of categories . We can also extend the cascade to have more stages , and train the new stages with newly annotated training data . Figure [ reference ] illustrates these structures . section : Experiments Experimental setup . We work with the PASCAL VOC 2012 dataset and the MS COCO dataset . VOC 2012 has 5 , 000 images for training , 5 , 000 for validation and 10 , 000 for testing . There are 20 object classes in total . COCO has 80 , 000 images for training and 40 , 000 images for validation . It has 80 object classes in 12 super - categories . We evaluated ProNet on object classification and point - based object localization tasks . For object classification , we use the average precision metric . We used VOC \u2019s result server to compute average precisions on the VOC 2012 dataset . For point - based object localization , we use the criteria introduced in . For every image and every class , we output a location with maximum response for that class . The location is deemed correct if it falls into any bounding box associated with that class , with a tolerance of 18 pixels as used in . This information is then used to compute average precision . Although object extent is not evaluated , the metric remains challenging as shown by . To generate localization coordinates for evaluation , we kept track of the image boxes which give highest responses at each stage , and used the center point of the selected boxes . We tried different values of hyper - parameter for LSE pooling , and found that generally gave good performance . We fixed in all the following experiments . We used the stochastic gradient descent algorithm for training . To train proposal network , the learning rate was set to 0.01 ; to train verification network , the learning rate was set to 0.001 . We set the filtering threshold for cascade to 0.1 . Which pooling method is better ? We compare maximum pooling , average pooling and LSE pooling methods to train proposal network with image - level supervision . Table [ reference ] lists the classification and localization performance of the three different pooling methods . We can see that LSE achieves the best classification mAP . Average pooling is 3.7 % worse than LSE , which we believe is because it assigns equal importance to foreground and background . Max pooling is 1.4 % worse ; compared with LSE pooling , it only uses a single patch to generate image - level score , thus is more sensitive to noise and model initialization during training . We also generated visualizations to study the impact of pooling method on trained models . Figure [ reference ] shows heat maps of the class train when different models are applied to the same image . We can see that the model trained by average pooling has high activations not only on the train but also on part of the background . For max pooling , only the wheel of the train has high response , presumably because it is the most discriminative for the train . Model trained by LSE pooling has high response on the train , but not on the background . Does cascade help ? We study the impact of adding cascaded classifiers on classification and localization performance . We first use a single level of cascade with one multi - scale FCN and one verification network . For each image and each class , we selected the top 3 regions per scale if their scores are higher than 0.1 . The average number of regions to be verified is 24 per image . In Table [ reference ] , we can see that on PASCAL VOC 2012 , using a cascade helps improve classification mAP by 3.3 % and localization mAP by 2.9 % . Is a longer cascade better ? We are interested in observing how the performance changes with more levels of cascade . For this purpose , we first trained another set of proposal and verification networks using PASCAL VOC data alone , but found that the network overfitted easily . Since the training set of VOC 2012 has only 5 , 000 images , we found that the first set of proposal and verification networks \u201c perfectly solved \u201d this training set , leaving little room to improve its generalizability . In light of this , we used the 80 , 000 images from COCO training set as complementary data source . It covers the 20 categories used in VOC but also has 60 other categories . Rather than re - training all the networks by combining VOC and COCO data , we take that the previous CNNs in the cascade have already been trained and fixed , and only train new CNNs with the extra data . Note that our cascade architecture offers a natural way to select the challenging instances from such incoming images . The final row in Table [ reference ] shows the mAPs after adding a new set of cascades trained from COCO images . We can see that it offers another 1 % improvement over the previous cascade , which indicates that it is desirable to train a longer cascade when more training data becomes available . Expanding cascades into trees . We also investigated the effect of building tree - structured cascades . COCO dataset is used for evaluation as it has 3 times more categories than VOC . We trained 12 verification networks corresponding to the 12 super - categories of COCO . Each network focuses on a single super - category , and processes the sampled boxes whose active classes belong to that super - category . At test time , each proposed box only goes through a single root to leaf path in the tree . The final row of Table [ reference ] shows its classification and localization performance . We can see that compared with the chain structured cascade , tree - structured cascade achieves better performance , probably because it trains the neural networks to be focused on a small subset of similar categories . Comparison with detection based approaches . We compare our proposed framework with two recent state - of - the - art object detection methods : RCNN and Fast RCNN . Unlike our framework , they require bounding box annotations for training . Both methods use selective search to generate object proposals and CNNs for classification . RCNN uses AlexNet pre - trained from ImageNet , while fast RCNN uses VGG - 16 pre - trained from ImageNet . To generate classification and localization results , for each class we select the detection output with maximum confidence score , and use the center of the detected bounding box for localization evaluation . We first fix the number of window proposals to 1000 for RCNN and fast RCNN . Table [ reference ] shows the performance comparison . We can see that for classification , our proposed framework outperforms both RCNN and fast RCNN . For localization , our proposed framework outperforms RCNN , but is 4 % worse than fast RCNN . We also study the impact of number of proposed boxes on our system \u2019s performance . For this purpose , we let the proposal network select top regions per scale for each class , and compute the average number of proposed boxes per image . For comparison , we ask fast RCNN to use up to 10 , 50 , 500 and 1000 selective search proposals per image . Table [ reference ] shows the classification and localization performances respectively . We can see that ProNet is quite robust to the number of proposed boxes , and achieves reasonably good performance with only 9 boxes on average . This confirms that ProNet offers better accuracy with relatively small computational overhead . Meanwhile , fast RCNN requires many more proposals to reach peak performance , presumably because the selective search proposals are for general objectness and not optimized for object classification in cascade fashion . Comparison with other weakly - supervised methods . We compare ProNet with several state - of - the - art object classification frameworks . Classification and localization performance on PASCAL VOC 2012 are shown in Table [ reference ] and Table [ reference ] respectively . Table [ reference ] and Figure [ reference ] show results and localization examples on COCO dataset . Among the compared systems , Oquab et al . and NUS - HCP use CNNs pre - trained on the expanded ImageNet data with more than 1500 categories , which has been shown to be useful for classification . Since ProNet uses cascades or trees of CNNs , it can apply a more powerful CNN model VGG - 16 with small computational overhead . This helps our system outperform most of the previous state - of - the - art systems significantly on both datasets . ProNet is also slightly better than Simonyan et al . which extracts VGG - 16 features at three different scales over full images . Their system is 3x to 6x slower than our cascade at test time . Limitation . We evaluate ProNet using the standard IOU metric , which considers object extent as well as location . Since the boxes generated by our proposal CNN have fixed aspect ratios , we follow to aggregate the heat maps over 1000 bounding box proposals generated by selective search per image . No bounding box regression is conducted . Cascade CNN is then used to verify the high - scoring proposals . On PASCAL VOC 2012 validation set , our proposal CNN has an mAP of 13.0 % when overlap threshold is 0.5 . The cascade CNN improves the mAP to 15.5 % . Although both results are higher than 11.7 % as reported by , there is still a huge gap between the state - of - the - art object detection pipelines . Our proposal network tends to select the most discriminative / confusing parts of objects , which is good for cascade classification but bad for getting full object extents . Separating and counting multiple objects are also challenging issues . section : Conclusion We proposed ProNet , a cascaded neural network for object classification and localization . ProNet learns to propose object - specific boxes by multi - scale FCNs trained from image - level annotations . It then sends a small subset of promising boxes to latter CNNs for verification . Detailed experimental evaluations have shown the effectiveness of ProNet on the challenging PASCAL VOC 2012 dataset and MS COCO dataset . Acknowledgement : We would like to thank Sergey Zagoruyko for help with fast RCNN experiments ; Pedro O. Pinheiro , Bolei Zhou , Maxime Oquab , Jo\u00ebl Legrand , Yuandong Tian , L\u00e9on Bottou and Florent Perronnin for valuable discussions . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["COCO"]]], "Method": [[["ProNet"]]], "Metric": [[["MAP"]]], "Task": [[["Weakly_Supervised_Object_Detection"]]]}]}
{"docid": "TST3-SREX-0006", "doctext": "document : Real - Time Video Super - Resolution with Spatio - Temporal Networks and Motion Compensation Convolutional neural networks have enabled accurate image super - resolution in real - time . However , recent attempts to benefit from temporal correlations in video super - resolution have been limited to naive or inefficient architectures . In this paper , we introduce spatio - temporal sub - pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real - time speed . Specifically , we discuss the use of early fusion , slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames . We also propose a novel joint motion compensation and video super - resolution algorithm that is orders of magnitude more efficient than competing methods , relying on a fast multi - resolution spatial transformer module that is end - to - end trainable . These contributions provide both higher accuracy and temporally more consistent videos , which we confirm qualitatively and quantitatively . Relative to single - frame models , spatio - temporal networks can either reduce the computational cost by 30 % whilst maintaining the same quality or provide a 0.2dB gain for a similar computational cost . Results on publicly available datasets demonstrate that the proposed algorithms surpass current state - of - the - art performance in both accuracy and efficiency . SRSRsuper - resolution LRLRlow - resolution HRHRhigh - resolution CNNCNNconvolutionalneuralnetwork TVTVtotalvariation HDHDhighdefinition MSEMSEmeansquarederror PSNRPSNRpeaksignal - to - noiseratio SSIMSSIMstructuralsimilarity section : Introduction Image and video SR are long - standing challenges of signal processing . SR aims at recovering a HR image or video from its LR version , and finds direct applications ranging from medical imaging to satellite imaging , as well as facilitating tasks such as face recognition . The reconstruction of HR data from a LR input is however a highly ill - posed problem that requires additional constraints to be solved . While those constraints are often application - dependent , they usually rely on data redundancy . In single image SR , where only one LR image is provided , methods exploit inherent image redundancy in the form of local correlations to recover lost high - frequency details by imposing sparsity constraints or assuming other types of image statistics such as multi - scale patch recurrence . In multi - image SR it is assumed that different observations of the same scene are available , hence the shared explicit redundancy can be used to constrain the problem and attempt to invert the downscaling process directly . Transitioning from images to videos implies an additional data dimension ( time ) with a high degree of correlation that can also be exploited to improve performance in terms of accuracy as well as efficiency . subsection : Related work Video SR methods have mainly emerged as adaptations of image SR techniques . Kernel regression methods have been shown to be applicable to videos using 3D kernels instead of 2D ones . Dictionary learning approaches , which define LR images as a sparse linear combination of dictionary atoms coupled to a HR dictionary , have also been adapted from images to videos . Another approach is example - based patch recurrence , which assumes patches in a single image or video obey multi - scale relationships , and therefore missing high - frequency content at a given scale can be inferred from coarser scale patches . This was successfully presented by Glasner et al . for image SR and has later been extended to videos . When adapting a method from images to videos it is usually beneficial to incorporate the prior knowledge that frames of the same scene of a video can be approximated by a single image and a motion pattern . Estimating and compensating motion is a powerful mechanism to further constrain the problem and expose temporal correlations . It is therefore very common to find video SR methods that explicitly model motion through frames . A natural choice has been to preprocess input frames by compensating inter - frame motion using displacement fields obtained from off - the - shelf optical flow algorithms . This nevertheless requires frame preprocessing and is usually expensive . Alternatively , motion compensation can also be performed jointly with the SR task , as done in the Bayesian approach of Liu et al . by iteratively estimating motion as part of its wider modeling of the downscaling process . The advent of neural network techniques that can be trained from data to approximate complex nonlinear functions has set new performance standards in many applications including SR . Dong et al . proposed to use a CNN architecture for single image SR that was later extended by Kappeler et al . in a video SR network ( VSRnet ) which jointly processes multiple input frames . Additionally , compensating the motion of input images with a TV - based optical flow algorithm showed an improved accuracy . Joint motion compensation for SR with neural networks has also been studied through recurrent bidirectional networks . The common paradigm for CNN based approaches has been to upscale the LR image with bicubic interpolation before attempting to solve the SR problem . However , increasing input image size through interpolation considerably impacts the computational burden for CNN processing . A solution was proposed by Shi et al . with an efficient sub - pixel convolution network ( ESPCN ) , where an upscaling operation directly mapping from LR to HR space is learnt by the network . This technique reduces runtime by an order of magnitude and enables real - time video SR by independently processing frames with a single frame model . Similar solutions to improve efficiency have also been proposed based on transposed convolutions . subsection : Motivation and contributions [ b ] 0.32 [ b ] 0.32 [ b ] 0.32 Existing solutions for HD video SR have not been able to effectively exploit temporal correlations while performing in real - time . On the one hand , ESPCN leverages sub - pixel convolution for a very efficient operation , but its naive extension to videos treating frames independently fails to exploit inter - frame redundancies and does not enforce a temporally consistent result . VSRnet , on the other hand , can improve reconstruction quality by jointly processing multiple input frames . However , the preprocessing of LR images with bicubic upscaling and the use of an inefficient motion compensation mechanism slows runtime to about frames per second even on videos smaller than standard definition resolution . Spatial transformer networks provide a means to infer parameters for a spatial mapping between two images . These are differentiable networks that can be seamlessly combined and jointly trained with networks targeting other objectives to enhance their performance . For instance , spatial transformer networks were initially shown to facilitate image classification by transforming images onto the same frame of reference . Recently , it has been shown how spatial transformers can encode optical flow features with unsupervised training , but they have nevertheless not yet been investigated for video motion compensation . Related approaches have emerged for view synthesis assuming rigid transformations . In this paper , we combine the efficiency of sub - pixel convolution with the performance of spatio - temporal networks and motion compensation to obtain a fast and accurate video SR algorithm . We study different treatments of the temporal dimension with early fusion , slow fusion and 3D convolutions , which have been previously suggested to extend classification from images to videos . Additionally , we build a motion compensation scheme based on spatial transformers , which is combined with spatio - temporal models to lead to a very efficient solution for video SR with motion compensation that is end - to - end trainable . A high - level diagram of the proposed approach is show in fig : network . The main contributions of this paper are : Presenting a real - time approach for video SR based on sub - pixel convolution and spatio - temporal networks that improves accuracy and temporal consistency . Comparing early fusion , slow fusion and 3D convolutions as alternative architectures for discovering spatio - temporal correlations . Proposing an efficient method for dense inter - frame motion compensation based on a multi - scale spatial transformer network . Combining the proposed motion compensation technique with spatio - temporal models to provide an efficient , end - to - end trainable motion compensated video SR algorithm . section : Methods Our starting point is the real - time image SR method ESPCN . We restrict our analysis to standard architectural choices and do not further investigate potentially beneficial extensions such as recurrence , residual connections or training networks based on perceptual loss functions . Throughout the paper we assume all image processing is performed on the y - channel in colour space , and thus we represent all images as 2D matrices . subsection : Sub - pixel convolution SR For a given LR image which is assumed to be the result of low - pass filtering and downscaling by a factor the HR image , the CNN super - resolved solution can be expressed as Here , are model parameters and represents the mapping function from LR to HR . A convolutional network models this function as a concatenation of layers defined by sets of weights and biases , each followed by non - linearities , with . Formally , the output of each layer is written as with . We assume the shape of filtering weights to be , where and represent the number and size of filters in layer , with the single frame input meaning . Model parameters are optimised minimising a loss given a set of LR and HR example image pairs , commonly MSE : Methods preprocessing with bicubic upsampling before mapping from LR to HR impose that the output number of filters is . Using sub - pixel convolution allows to process directly in the LR space and then use output filters to obtain an HR output tensor with shape that can be reordered to obtain . This implies that if there exists an upscaling operation that is better suited for the problem than bicubic upsampling , the network can learn it . Moreover , and most importantly , all convolutional processing is performed in LR space , making this approach very efficient . subsection : Spatio - temporal networks Spatio - temporal networks assume input data to be a block of spatio - temporal information , such that instead of a single input frame , a sequence of consecutive frames is considered . This can be represented in the network by introducing an additional dimension for temporal depth , with the input depth representing an odd number of consecutive input frames . If we denote the temporal radius of a spatio - temporal block to be , we define the group of input frames centered at time as , and the problem in eq : image - sr becomes The shape of weighting filters is also extended by their temporal size , and their tensor shape becomes . We note that it is possible to consider solutions that aim to jointly reconstruct more than a single output frame , which could have advantages at least in terms of computational efficiency . However , in this work we focus on the reconstruction of only a single output frame . subsubsection : Early fusion One of the most straightforward approaches for a CNN to process videos is to match the temporal depth of the input layer to the number of frames . This will collapse all temporal information in the first layer and the remaining operations are identical to those in a single image SR network , meaning . An illustration of early fusion is shown in fig : early - fusion for , where the temporal dimension has been colour coded and the output mapping to 2D space is omitted . This design has been studied for video classification and action recognition , and was also one of the architectures proposed in VSRnet . However , VSRnet requires bicubic upsampling as opposed to sub - pixel convolution , making the framework computationally much less efficient in comparison . subsubsection : Slow fusion Another option is to partially merge temporal information in a hierarchical structure , so it is slowly fused as information progresses through the network . In this case , the temporal depth of network layers is configured to be , and therefore some layers also have a temporal extent until all information has been merged and the depth of the network reduces to . This architecture , termed slow fusion , has shown better performance than early fusion for video classification . In fig : slow - fusion we show a slow fusion network where and the rate of fusion is defined by for or otherwise , meaning that at each layer only two consecutive frames or filter activations are merged until the network \u2019s temporal depth shrinks to . Note that early fusion is an special case of slow fusion . subsubsection : 3D convolutions Another variation of slow fusion is to force layer weights to be shared across the temporal dimension , which has computational advantages . Assuming an online processing of frames , when a new frame becomes available the result of some layers for the previous frame can be reused . For instance , refering to the diagram in fig : slow - fusion and assuming the bottom frame to be the latest frame received , all activations above the dashed line are readily available because they were required for processing the previous frame . This architecture is equivalent to using 3D convolutions , initially proposed as an effective tool to learn spatio - temporal features that can help for video action recognition . An illustration of this design from a 3D convolution perspective is shown in fig:3dconv , where the arrangement of the temporal and filter features is swapped relative to fig : slow - fusion . subsection : Spatial transformer motion compensation We propose the use of an efficient spatial transformer network to compensate the motion between frames fed to the SR network . It has been shown how spatial transformers can effectively encode optical flow to describe motion , and are therefore suitable for motion compensation . We will compensate blocks of three consecutive frames to combine the compensation module with the SR network as shown in fig : network , but for simplicity we first introduce motion compensation between two frames . Notice that the data used contains inherent motion blur and ( dis ) occlusions , and even though an explicit modelling for these effects is not used it could potentially improve results . The task is to find the best optical flow representation relating a new frame with a reference current frame . The flow is assumed pixel - wise dense , allowing to displace each pixel to a new position , and the resulting pixel arrangement requires interpolation back onto a regular grid . We use bilinear interpolation as it is much more efficient than the thin - plate spline interpolation originally proposed in . Optical flow is a function of parameters and is represented with two feature maps corresponding to displacements for the and dimensions , thus a compensated image can be expressed as , or more concisely We adopt a multi - scale design to represent the flow , which has been shown to be effective in classical methods and also in more recently proposed spatial transformer techniques . A schematic of the design is shown in fig : transformer and flow estimation modules are detailed in tab : transformer . First , a coarse estimate of the flow is obtained by early fusing the two input frames and downscaling spatial dimensions with strided convolutions . The estimated flow is upscaled with sub - pixel convolution and the result is applied to warp the target frame producing . The warped image is then processed together with the coarse flow and the original images through a fine flow estimation module . This uses a single strided convolution with stride and a final upscaling stage to obtain a finer flow map . The final motion compensated frame is obtained by warping the target frame with the total flow . Output activations use tanh to represent pixel displacement in normalised space , such that a displacement of means maximum displacement from the center to the border of the image . To train the spatial transformer to perform motion compensation we optimise its parameters to minimise the MSE between the transformed frame and the reference frame . Similary to classical optical flow methods , we found that it is generally helpful to constrain the flow to behave smoothly in space , and so we penalise the Huber loss of the flow map gradients , namely In practice we approximate the Huber loss with , where . This function has a smooth behaviour near the origin and is sparsity promoting far from it . The spatial transformer module is advantageous relative to other motion compensation mechanisms as it is straightforward to combine with a SR network to perform joint motion compensation and video SR . Referring to fig : network , the same parameters can be used to model motion of the outer two frames relative to the central frame . The spatial transformer and SR modules are both differentiable and therefore end - to - end trainable . As a result , they can be jointly optimised to minimise a composite loss combining the accuracy of the reconstruction in eq : image - sr - objective with the fidelity of motion compensation in eq : motion - compensation - objective , namely section : Experiments and results In this section , we first analyse spatio - temporal networks for video SR in isolation and later evaluate the benefits of introducing motion compensation . We restrict our experiments to tackle and upscaling of full HD video resolution ( ) , and no compression is applied . To ensure a fair comparison of methods , the number of network parameters need to be comparable so that gains in performance can be attributed to specific choices of network resource allocation and not to a trivial increase in capacity . For a layer , the number of floating - point operations to reconstruct a frame is approximated by In measuring the complexity of slow fusion networks with weight sharing we look at steady - state operation where the output of some layers is reused from one frame to the following . We note that the analysis of VSRnet variants in does not take into account model complexity . subsection : Experimental setup subsubsection : Data We use the CDVL database , which contains uncompressed full HD videos excluding repeated videos , and choose a subset of videos for training . The videos are downscaled and random samples are extracted from each HR - LR video pair to obtain training samples , of which are used for validation . Depending on the network architecture , we refer to a sample as a single input - output frame pair for single frame networks , or as a block of consecutive LR input frames and the corresponding central HR frame for spatio - temporal networks . The remaining videos are used for testing . Although the total number of training frames is large , we foresee that the methods presented could benefit from a richer , more diverse set of videos . Additionally , we present a benchmark against various SR methods on publicly available videos that are recurrently used in the literature and we refer to as Vid4 . subsubsection : Network training and parameters All SR models are trained following the same protocol and share similar hyperparameters . Filter sizes are set to , and all non - linearities are rectified linear units except for the output layer , which uses a linear activation . Biases are initialised to and weights use orthogonal initialisation with gain following recommendations in . All hidden layers are set to have the same number of features . Video samples are broken into non - overlapping sub - samples of spatial dimensions , which are randomly grouped in batches for stochastic optimisation . We employ Adam with a learning rate and an initial batch size . Every epochs the batch size is doubled until it reaches a maximum size of . We choose for layers where the network temporal depth is ( layers in gray in fig : early - fusion , fig : slow - fusion , fig:3dconv ) , and to maintain comparable network sizes we choose . This ensures that the number of features per hidden layer in early and slow fusion networks is always the same . For instance , the network shown in fig : slow - fusion , for which and for , the number of features in a layer network for SR would be 6 , 8 , 12 , 24 , 24 , . subsection : Spatio - temporal video SR subsubsection : Single vs multi frame early fusion First , we investigate the impact of the number of input frames on complexity and accuracy without motion compensation . We compare single frame models ( SF ) against early fusion spatio - temporal models using 3 , 5 and 7 input frames ( E3 , E5 and E7 ) . PSNR results on the CDVL dataset for networks of 6 to 11 layers are plotted in fig : single - vs - multi - frame . Exploiting spatio - temporal correlations provides a more accurate result relative to an independent processing of frames . The increase in complexity from early fusion is marginal because only the first layer contributes to an increase of operations . Although the accuracy of spatio - temporal models is relatively similar , we find that E7 slightly underperforms . It is likely that temporal dependencies beyond 5 frames become too complex for networks to learn useful information and act as noise degrading their performance . Notice also that , whereas the performance increase from network depth is minimal after 8 layers for single frame networks , this increase is more consistent for spatio - temporal models . subsubsection : Early vs slow fusion Here we compare the different treatments of the temporal dimension discussed in ssec : st - networks . We assume networks with an input of frames and slow fusion models with filter temporal depths as in fig : st - networks . Using SF , E5 , S5 , and S5 - SW to refer to single frame networks and 5 frame input networks using early fusion , slow fusion , and slow fusion with shared weights , we show in tab : early - vs - slow - fusion results for 7 and 9 layer networks . As seen previously , early fusion networks attain a higher accuracy at a marginal 3 % increase in operations relative to the single frame models , and as expected , slow fusion architectures provide efficiency advantages . Slow fusion is faster than early fusion because it uses fewer features in the initial layers . Referring to eq : operations , slow fusion uses in the first layers and , which results in fewer operations than , as used in early fusion . While the 7 layer network sees a considerable decrease in accuracy using slow fusion relative to early fusion , the 9 layer network can benefit from the same accuracy while reducing its complexity with slow fusion by about 30 % . This suggests that in shallow networks the best use of network resources is to utilise the full network capacity to jointly process all temporal information as done by early fusion , but that in deeper networks slowly fusing the temporal dimension is beneficial , which is in line with the results presented by for video classification . Additionally , weight sharing decreases accuracy because of the reduction in network parameters , but the reusability of network features means fewer operations are needed per frame . For instance , the 7 layer S5 - SW network shows a reduction of almost 30 % of operations with a minimal decrease in accuracy relative to SF . Using 7 layers with E5 nevertheless shows better performance and faster operation than S5 - SW with 9 layers , and in all cases we found that early or slow fusion consistently outperformed slow fusion with shared weights in this performance and efficiency trade - off . Convolutions in spatio - temporal domain were shown in to work well for video action recognition , but with larger capacity and many more frames processed jointly . We speculate this could be the reason why the conclusions drawn from this high - level vision task do not extrapolate to the SR problem . subsection : Motion compensated video SR [ b ] 0.32 [ b ] 0.32 [ b ] 0.32 In this section , the proposed frame motion compensation is combined with an early fusion network of temporal depth . First , the motion compensation module is trained independently using eq : video - sr - memc , where the first term is ignored and , . This results in a network that will compensate the motion of three consecutive frames by estimating the flow maps of outer frames relative to the middle frame . An example of a flow map obtained for one frame is shown in fig : memc , where we also show the effect the motion compensation module has on three consecutive frames . The early fusion motion compensated SR network ( E3 - MC ) is initialised with a compensation and a SR network pretrained separately , and the full model is then jointly optimised with eq : video - sr - memc ( , ) . Results for SR on CDVL are compared in tab : motion - compensated - video - sr against a single frame ( SF ) model and early fusion without motion compensation ( E3 ) . E3 - MC results in a PSNR that is sometimes almost twice the improvement of E3 relative to SF , which we attribute to the fact that the network adapts the SR input to maximise temporal redundancy . In fig : mcsr - x3 we show how this improvement is reflected in better structure preservation . subsection : Comparison to state - of - the - art We show in tab : set4 the performance on Vid4 for SRCNN , ESPCN , VSRnet and the proposed method , which we refer to as video ESPCN ( VESPCN ) . To demonstrate its benefits in efficiency and quality we evaluate two early fusion models : a 5 layer 3 frame network ( 5L - E3 ) and a 9 layer 3 frame network with motion compensation ( 9L - E3 - MC ) . The metrics compared are PSNR , SSIM and MOVIE indices . The MOVIE index was designed as a metric measuring video quality that correlates with human perception and incorporates a notion of temporal consistency . We also directly compare the number of operations per frame of all CNN - based approaches for upscaling a generic p frame . Reconstructions for SRCNN , ESPCN and VSRnet use models provided by the authors . SRCNN , ESPCN and VESPCN were tested on Theano and Lasagne , and for VSRnet we used available Caffe Matlab code . We crop spatial borders as well as initial and final frames on all reconstructions for fair comparison against VSRnet . subsubsection : Quality comparison An example of visual differences is shown in fig : set4 - visualisation against the motion compensated network . From the close - up images , we see how the structural detail of the original video is better recovered by the proposed VESPCN method . This is reflected in tab : set4 , where it surpasses any other method in PSNR and SSIM by a large margin . fig : set4 - visualisation also shows temporal profiles on the row highlighted by a dashed line through 25 consecutive frames , demonstrating a better temporal coherence of the reconstruction proposed . The great temporal coherence of VESPCN also explains the significant reduction in the MOVIE index . subsubsection : Efficiency comparison The complexity of methods in tab : set4 is determined by network and input image sizes . SRCNN and VSRnet upsample LR images before attempting to super - resolve them , which considerably increases the required number of operations . VSRnet is particularly expensive because it processes input frames in and feature layers , whereas sub - pixel convolution greatly reduces the number of operations required in ESPCN and VESPCN . As a reference , ESPCN runs at ms per frame on a K2 GPU . The enhanced capabilities of spatio - temporal networks allow to reduce the network operations of VESPCN relative to ESPCN while still matching its accuracy . As an example we show VESPCN with 5L - E3 , which reduces the number of operations by about 20 % relative to ESPCN while maintaining a similar performance in all evaluated quality metrics . The operations for motion compensation in VESPCN with 9L - E3 - MC , included in tab : set4 results , amount to and GOps for and upscaling , applied twice for each input frame requiring motion compensation . This makes the proposed motion compensated video SR very efficient relative to other approaches . For example , motion compensation in VSRnet is said to require 55 seconds per frame and is the computational bottleneck . This is not accounted for in tab : set4 but is slower than VESPCN with 9L - E3 - MC , which can run in the order of seconds . The optical flow method in VSRnet was originally shown to run at ms on GPU for each frame of dimensions , but this is still considerably slower than the proposed solution considering motion compensation is required for more than a single frame of HD dimensions . section : Conclusion In this paper we combine the efficiency advantages of sub - pixel convolutions with temporal fusion strategies to present real - time spatio - temporal models for video SR . The spatio - temporal models used are shown to facilitate an improvement in reconstruction accuracy and temporal consistency or reduce computational complexity relative to independent single frame processing . The models investigated are extended with a motion compensation mechanism based on spatial transformer networks that is efficient and jointly trainable for video SR . Results obtained with approaches that incorporate explicit motion compensation are demonstrated to be superior in terms of PSNR and temporal consistency compared to spatio - temporal models alone , and outperform the current state of the art in video SR . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Vid4_-_4x_upscaling"]]], "Method": [[["VESPCN"]]], "Metric": [[["MOVIE"]]], "Task": [[["Video_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Vid4_-_4x_upscaling"]]], "Method": [[["bicubic"]]], "Metric": [[["MOVIE"]]], "Task": [[["Video_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Vid4_-_4x_upscaling"]]], "Method": [[["VESPCN"]]], "Metric": [[["PSNR"]]], "Task": [[["Video_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Vid4_-_4x_upscaling"]]], "Method": [[["bicubic"]]], "Metric": [[["PSNR"]]], "Task": [[["Video_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Vid4_-_4x_upscaling"]]], "Method": [[["VESPCN"]]], "Metric": [[["SSIM"]]], "Task": [[["Video_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Vid4_-_4x_upscaling"]]], "Method": [[["bicubic"]]], "Metric": [[["SSIM"]]], "Task": [[["Video_Super-Resolution"]]]}]}
{"docid": "TST3-SREX-0007", "doctext": "document : LINE : Large - scale Information Network Embedding This paper studies the problem of embedding very large information networks into low - dimensional vector spaces , which is useful in many tasks such as visualization , node classification , and link prediction . Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes . In this paper , we propose a novel network embedding method called the \u201c LINE , \u201d which is suitable for arbitrary types of information networks : undirected , directed , and / or weighted . The method optimizes a carefully designed objective function that preserves both the local and global network structures . An edge - sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference . Empirical experiments prove the effectiveness of the LINE on a variety of real - world information networks , including language networks , social networks , and citation networks . The algorithm is very efficient , which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine . The source code of the LINE is available online . CopyrightisheldbytheInternationalWorldWideWebConferenceCommittee ( IW3C2 ) .IW3C2reservestherighttoprovideahyperlinktotheauthor\u2019ssiteiftheMaterialisusedinelectronicmedia . WWW2015 , May18\u201322 , 2015 , Florence , Italy . ACM 978 - 1 - 4503 - 3469 - 3 / 15 / 05 . http: // dx.doi.org / 10.1145 / 2736277.2741093 I.2.6Artificial IntelligenceLearning Algorithms , Experimentation section : Introduction Information networks are ubiquitous in the real world with examples such as airline networks , publication networks , social and communication networks , and the World Wide Web . The size of these information networks ranges from hundreds of nodes to millions and billions of nodes . Analyzing large information networks has been attracting increasing attention in both academia and industry . This paper studies the problem of embedding information networks into low - dimensional spaces , in which every vertex is represented as a low - dimensional vector . Such a low - dimensional embedding is very useful in a variety of applications such as visualization , node classification , link prediction , and recommendation . Various methods of graph embedding have been proposed in the machine learning literature ( e.g. , ) . They generally perform well on smaller networks . The problem becomes much more challenging when a real world information network is concerned , which typically contains millions of nodes and billions of edges . For example , the Twitter followee - follower network contains 175 million active users and around twenty billion edges in 2012 . Most existing graph embedding algorithms do not scale for networks of this size . For example , the time complexity of classical graph embedding algorithms such as MDS , IsoMap , Laplacian eigenmap are at least quadratic to the number of vertices , which is too expensive for networks with millions of nodes . Although a few very recent studies approach the embedding of large - scale networks , these methods either use an indirect approach that is not designed for networks ( e.g. , ) or lack a clear objective function tailored for network embedding ( e.g. , ) . We anticipate that a new model with a carefully designed objective function that preserves properties of the graph and an efficient optimization technique should effectively find the embedding of millions of nodes . In this paper , we propose such a network embedding model called the \u201c LINE , \u201d which is able to scale to very large , arbitrary types of networks : undirected , directed and / or weighted . The model optimizes an objective which preserves both the local and global network structures . Naturally , the local structures are represented by the observed links in the networks , which capture the first - order proximity between the vertices . Most existing graph embedding algorithms are designed to preserve this first - order proximity , e.g. , IsoMap and Laplacian eigenmap , even if they do not scale . We observe that in a real - world network many ( if not the majority of ) legitimate links are actually not observed . In other words , the observed first - order proximity in the real world data is not sufficient for preserving the global network structures . As a complement , we explore the second - order proximity between the vertices , which is not determined through the observed tie strength but through the shared neighborhood structures of the vertices . The general notion of the second - order proximity can be interpreted as nodes with shared neighbors being likely to be similar . Such an intuition can be found in the theories of sociology and linguistics . For example , \u201c the degree of overlap of two people \u2019s friendship networks correlates with the strength of ties between them , \u201d in a social network ; and \u201c You shall know a word by the company it keeps \u201d ( Firth , J. R. 1957:11 ) in text corpora . Indeed , people who share many common friends are likely to share the same interest and become friends , and words that are used together with many similar words are likely to have similar meanings . Fig . [ reference ] presents an illustrative example . As the weight of the edge between vertex 6 and 7 is large , i.e. , 6 and 7 have a high first - order proximity , they should be represented closely to each other in the embedded space . On the other hand , though there is no link between vertex 5 and 6 , they share many common neighbors , i.e. , they have a high second - order proximity and therefore should also be represented closely to each other . We expect that the consideration of the second - order proximity effectively complements the sparsity of the first - order proximity and better preserves the global structure of the network . In this paper , we will present carefully designed objectives that preserve the first - order and the second - order proximities . Even if a sound objective is found , optimizing it for a very large network is challenging . One approach that attracts attention in recent years is using the stochastic gradient descent for the optimization . However , we show that directly deploying the stochastic gradient descent is problematic for real world information networks . This is because in many networks , edges are weighted and the weights usually present a high variance . Consider a word co - occurrence network , in which the weights ( co - occurrences ) of word pairs may range from one to hundreds of thousands . These weights of the edges will be multiplied into the gradients , resulting in the explosion of the gradients and thus compromise the performance . To address this , we propose a novel edge - sampling method , which improves both the effectiveness and efficiency of the inference . We sample the edges with the probabilities proportional to their weights , and then treat the sampled edges as binary edges for model updating . With this sampling process , the objective function remains the same and the weights of the edges no longer affect the gradients . The LINE is very general , which works well for directed or undirected , weighted or unweighted graphs . We evaluate the performance of the LINE with various real - world information networks , including language networks , social networks , and citation networks . The effectiveness of the learned embeddings is evaluated within multiple data mining tasks , including word analogy , text classification , and node classification . The results suggest that the LINE model outperforms other competitive baselines in terms of both effectiveness and efficiency . It is able to learn the embedding of a network with millions of nodes and billions of edges in a few hours on a single machine . To summarize , we make the following contributions : We propose a novel network embedding model called the \u201c LINE , \u201d which suits arbitrary types of information networks and easily scales to millions of nodes . It has a carefully designed objective function that preserves both the first - order and second - order proximities . We propose an edge - sampling algorithm for optimizing the objective . The algorithm tackles the limitation of the classical stochastic gradient decent and improves the effectiveness and efficiency of the inference . We conduct extensive experiments on real - world information networks . Experimental results prove the effectiveness and efficiency of the proposed LINE model . Organization . The rest of this paper is organized as follows . Section [ reference ] summarizes the related work . Section [ reference ] formally defines the problem of large - scale information network embedding . Section [ reference ] introduces the LINE model in details . Section [ reference ] presents the experimental results . Finally we conclude in Section [ reference ] . section : Related Work Our work is related to classical methods of graph embedding or dimension reduction in general , such as multidimensional scaling ( MDS ) , IsoMap , LLE and Laplacian Eigenmap . These approaches typically first construct the affinity graph using the feature vectors of the data points , e.g. , the K - nearest neighbor graph of data , and then embed the affinity graph into a low dimensional space . However , these algorithms usually rely on solving the leading eigenvectors of the affinity matrices , the complexity of which is at least quadratic to the number of nodes , making them inefficient to handle large - scale networks . Among the most recent literature is a technique called graph factorization . It finds the low - dimensional embedding of a large graph through matrix factorization , which is optimized using stochastic gradient descent . This is possible because a graph can be represented as an affinity matrix . However , the objective of matrix factorization is not designed for networks , therefore does not necessarily preserve the global network structure . Intuitively , graph factorization expects nodes with higher first - order proximity are represented closely . Instead , the LINE model uses an objective that is particularly designed for networks , which preserves both the first - order and the second - order proximities . Practically , the graph factorization method only applies to undirected graphs while the proposed model is applicable for both undirected and directed graphs . The most recent work related with ours is DeepWalk , which deploys a truncated random walk for social network embedding . Although empirically effective , the DeepWalk does not provide a clear objective that articulates what network properties are preserved . Intuitively , DeepWalk expects nodes with higher second - order proximity yield similar low - dimensional representations , while the LINE preserves both first - order and second - order proximities . DeepWalk uses random walks to expand the neighborhood of a vertex , which is analogical to a depth - first search . We use a breadth - first search strategy , which is a more reasonable approach to the second - order proximity . Practically , DeepWalk only applies to unweighted networks , while our model is applicable for networks with both weighted and unweighted edges . In Section [ reference ] , we empirically compare the proposed model with these methods using various real world networks . section : Problem Definition We formally define the problem of large - scale information network embedding using first - order and second - order proximities . We first define an information network as follows : theorem : ( Information Network ) An information network is defined as = G ( V , E ) , where V is the set of vertices , each representing a data object and E is the set of edges between the vertices , each representing a relationship between two data objects . Each edge \u2208eE is an ordered pair = e ( u , v ) and is associated with a weight > w\u2062uv0 , which indicates the strength of the relation . If G is undirected , we have \u2261 ( u , v )( v , u ) and \u2261w\u2062uvw\u2062vu ; if G is directed , we have \u2262 ( u , v )( v , u ) and \u2262w\u2062uvw\u2062vu . In practice , information networks can be either directed ( e.g. , citation networks ) or undirected ( e.g. , social network of users in Facebook ) . The weights of the edges can be either binary or take any real value . Note that while negative edge weights are possible , in this study we only consider non - negative weights . For example , in citation networks and social networks , takes binary values ; in co - occurrence networks between different objects , can take any non - negative value . The weights of the edges in some networks may diverge as some objects co - occur many times while others may just co - occur a few times . Embedding an information network into a low - dimensional space is useful in a variety of applications . To conduct the embedding , the network structures must be preserved . The first intuition is that the local network structure , i.e. , the local pairwise proximity between the vertices , must be preserved . We define the local network structures as the first - order proximity between the vertices : theorem : ( First - order Proximity ) The first - order proximity in a network is the local pairwise proximity between two vertices . For each pair of vertices linked by an edge ( u , v ) , the weight on that edge , w\u2062uv , indicates the first - order proximity between u and v. If no edge is observed between u and v , their first - order proximity is 0 . The first - order proximity usually implies the similarity of two nodes in a real - world network . For example , people who are friends with each other in a social network tend to share similar interests ; pages linking to each other in World Wide Web tend to talk about similar topics . Because of this importance , many existing graph embedding algorithms such as IsoMap , LLE , Laplacian eigenmap , and graph factorization have the objective to preserve the first - order proximity . However , in a real world information network , the links observed are only a small proportion , with many others missing . A pair of nodes on a missing link has a zero first - order proximity , even though they are intrinsically very similar to each other . Therefore , first - order proximity alone is not sufficient for preserving the network structures , and it is important to seek an alternative notion of proximity that addresses the problem of sparsity . A natural intuition is that vertices that share similar neighbors tend to be similar to each other . For example , in social networks , people who share similar friends tend to have similar interests and thus become friends ; in word co - occurrence networks , words that always co - occur with the same set of words tend to have similar meanings . We therefore define the second - order proximity , which complements the first - order proximity and preserves the network structure . theorem : ( Second - order Proximity ) The second - order proximity between a pair of vertices ( u , v ) in a network is the similarity between their neighborhood network structures . Mathematically , let = pu ( wu , 1 , \u2026 , wu , |V| ) denote the first - order proximity of u with all the other vertices , then the second - order proximity between u and v is determined by the similarity between pu and pv . If no vertex is linked from / to both u and v , the second - order proximity between u and v is 0 . We investigate both first - order and second - order proximity for network embedding , which is defined as follows . theorem : ( Large - scale Information Network Embedding ) Given a large network = G ( V , E ) , the problem of Large - scale Information Network Embedding aims to represent each vertex \u2208vV into a low - dimensional space Rd , i.e. , learning a function : fG\u2192VRd , where \u226ad|V|. In the space Rd , both the first - order proximity and the second - order proximity between the vertices are preserved . Next , we introduce a large - scale network embedding model that preserves both first - and second - order proximities . section : LINE : Large - scale Information Network Embedding A desirable embedding model for real world information networks must satisfy several requirements : first , it must be able to preserve both the first - order proximity and the second - order proximity between the vertices ; second , it must scale for very large networks , say millions of vertices and billions of edges ; third , it can deal with networks with arbitrary types of edges : directed , undirected and / or weighted . In this section , we present a novel network embedding model called the \u201c LINE , \u201d which satisfies all the three requirements . subsection : Model Description We describe the LINE model to preserve the first - order proximity and second - order proximity separately , and then introduce a simple way to combine the two proximity . subsubsection : LINE with First - order Proximity The first - order proximity refers to the local pairwise proximity between the vertices in the network . To model the first - order proximity , for each undirected edge , we define the joint probability between vertex and as follows : where is the low - dimensional vector representation of vertex . Eqn . ( [ reference ] ) defines a distribution over the space , and its empirical probability can be defined as , where . To preserve the first - order proximity , a straightforward way is to minimize the following objective function : where is the distance between two distributions . We choose to minimize the KL - divergence of two probability distributions . Replacing with KL - divergence and omitting some constants , we have : Note that the first - order proximity is only applicable for undirected graphs , not for directed graphs . By finding the that minimize the objective in Eqn . ( [ reference ] ) , we can represent every vertex in the d - dimensional space . subsubsection : LINE with Second - order Proximity The second - order proximity is applicable for both directed and undirected graphs . Given a network , without loss of generality , we assume it is directed ( an undirected edge can be considered as two directed edges with opposite directions and equal weights ) . The second - order proximity assumes that vertices sharing many connections to other vertices are similar to each other . In this case , each vertex is also treated as a specific \u201c context \u201d and vertices with similar distributions over the \u201c contexts \u201d are assumed to be similar . Therefore , each vertex plays two roles : the vertex itself and a specific \u201c context \u201d of other vertices . We introduce two vectors and , where is the representation of when it is treated as a vertex while is the representation of when it is treated as a specific \u201c context \u201d . For each directed edge , we first define the probability of \u201c context \u201d generated by vertex as : where is the number of vertices or \u201c contexts . \u201d For each vertex , Eqn . ( [ reference ] ) actually defines a conditional distribution over the contexts , i.e. , the entire set of vertices in the network . As mentioned above , the second - order proximity assumes that vertices with similar distributions over the contexts are similar to each other . To preserve the second - order proximity , we should make the conditional distribution of the contexts specified by the low - dimensional representation be close to the empirical distribution . Therefore , we minimize the following objective function : where is the distance between two distributions . As the importance of the vertices in the network may be different , we introduce in the objective function to represent the prestige of vertex in the network , which can be measured by the degree or estimated through algorithms such as PageRank . The empirical distribution is defined as , where is the weight of the edge and is the out - degree of vertex , i.e. , where is the set of out - neighbors of . In this paper , for simplicity we set as the degree of vertex , i.e. , , and here we also adopt KL - divergence as the distance function . Replacing with KL - divergence , setting and omitting some constants , we have : By learning and that minimize this objective , we are able to represent every vertex with a d - dimensional vector . subsubsection : Combining first - order and second - order proximities To embed the networks by preserving both the first - order and second - order proximity , a simple and effective way we find in practice is to train the LINE model which preserves the first - order proximity and second - order proximity separately and then concatenate the embeddings trained by the two methods for each vertex . A more principled way to combine the two proximity is to jointly train the objective function ( [ reference ] ) and ( [ reference ] ) , which we leave as future work . subsection : Model Optimization Optimizing objective ( [ reference ] ) is computationally expensive , which requires the summation over the entire set of vertices when calculating the conditional probability . To address this problem , we adopt the approach of negative sampling proposed in , which samples multiple negative edges according to some noisy distribution for each edge . More specifically , it specifies the following objective function for each edge : where is the sigmoid function . The first term models the observed edges , the second term models the negative edges drawn from the noise distribution and is the number of negative edges . We set as proposed in , where is the out - degree of vertex . For the objective function ( [ reference ] ) , there exists a trivial solution : , for i= and . To avoid the trivial solution , we can still utilize the negative sampling approach ( [ reference ] ) by just changing to . We adopt the asynchronous stochastic gradient algorithm ( ASGD ) for optimizing Eqn . ( [ reference ] ) . In each step , the ASGD algorithm samples a mini - batch of edges and then updates the model parameters . If an edge is sampled , the gradient w.r.t . the embedding vector of vertex will be calculated as : Note that the gradient will be multiplied by the weight of the edge . This will become problematic when the weights of edges have a high variance . For example , in a word co - occurrence network , some words co - occur many times ( e.g. , tens of thousands ) while some words co - occur only a few times . In such networks , the scales of the gradients diverge and it is very hard to find a good learning rate . If we select a large learning rate according to the edges with small weights , the gradients on edges with large weights will explode while the gradients will become too small if we select the learning rate according to the edges with large weights . subsubsection : Optimization via Edge Sampling The intuition in solving the above problem is that if the weights of all the edges are equal ( e.g. , network with binary edges ) , then there will be no problem of choosing an appropriate learning rate . A simple treatment is thus to unfold a weighted edge into multiple binary edges , e.g. , an edge with weight is unfolded into binary edges . This will solve the problem but will significantly increase the memory requirement , especially when the weights of the edges are very large . To resolve this , one can sample from the original edges and treat the sampled edges as binary edges , with the sampling probabilities proportional to the original edge weights . With this edge - sampling treatment , the overall objective function remains the same . The problem boils down to how to sample the edges according to their weights . Let denote the sequence of the weights of the edges . One can simply calculate the sum of the weights first , and then to sample a random value within the range of to see which interval [ the random value falls into . This approach takes time to draw a sample , which is costly when the number of edges is large . We use the alias table method to draw a sample according to the weights of the edges , which takes only time when repeatedly drawing samples from the same discrete distribution . Sampling an edge from the alias table takes constant time , , and optimization with negative sampling takes time , where is the number of negative samples . Therefore , overall each step takes time . In practice , we find that the number of steps used for optimization is usually proportional to the number of edges . Therefore , the overall time complexity of the LINE is , which is linear to the number of edges , and does not depend on the number of vertices . The edge sampling treatment improves the effectiveness of the stochastic gradient descent without compromising the efficiency . subsection : Discussion We discuss several practical issues of the LINE model . Low degree vertices . One practical issue is how to accurately embed vertices with small degrees . As the number of neighbors of such a node is very small , it is very hard to accurately infer its representation , especially with the second - order proximity based methods which heavily rely on the number of \u201c contexts . \u201d An intuitive solution to this is expanding the neighbors of those vertices by adding higher order neighbors , such as neighbors of neighbors . In this paper , we only consider adding second - order neighbors , i.e. , neighbors of neighbors , to each vertex . The weight between vertex and its second - order neighbor is measured as In practice , one can only add a subset of vertices which have the largest proximity with the low degree vertex . New vertices . Another practical issue is how to find the representation of newly arrived vertices . For a new vertex , if its connections to the existing vertices are known , we can obtain the empirical distribution and over existing vertices . To obtain the embedding of the new vertex , according to the objective function Eqn . ( [ reference ] ) or Eqn . ( [ reference ] ) , a straightforward way is to minimize either one of the following objective functions by updating the embedding of the new vertex and keeping the embeddings of existing vertices . If no connections between the new vertex and existing vertices are observed , we must resort to other information , such as the textual information of the vertices , and we leave it as our future work . section : Experiments We empirically evaluated the effectiveness and efficiency of the LINE . We applied the method to several large - scale real - world networks of different types , including a language network , two social networks , and two citation networks . subsection : Experiment Setup paragraph : Data Sets ( 1 ) Language network . We constructed a word co - occurrence network from the entire set of English Wikipedia pages . Words within every 5 - word sliding window are considered to be co - occurring with each other . Words with frequency smaller than 5 are filtered out . ( 2 ) Social networks . We use two social networks : Flickr and YoutubeAvailable at http: // socialnetworks.mpi - sws.org / data - imc2007.html . The Flickr network is denser than the Youtube network ( the same network as used in DeepWalk ) . ( 3 ) Citation Networks . Two types of citation networks are used : an author citation network and a paper citation network . We use the DBLP data set to construct the citation networks between authors and between papers . The author citation network records the number of papers written by one author and cited by another author . The detailed statistics of these networks are summarized into Table [ reference ] . They represent a variety of information networks : directed and undirected , binary and weighted . Each network contains at least half a million nodes and millions of edges , with the largest network containing around two million nodes and a billion edges . paragraph : Compared Algorithms We compare the LINE model with several existing graph embedding methods that are able to scale up to very large networks . We do not compare with some classical graph embedding algorithms such as MDS , IsoMap , and Laplacian eigenmap , as they can not handle networks of this scale . Graph factorization ( GF ) . We compare with the matrix factorization techniques for graph factorization . An information network can be represented as an affinity matrix , and is able to represent each vertex with a low - dimensional vector through matrix factorization . Graph factorization is optimized through stochastic gradient descent and is able to handle large networks . It only applies to undirected networks . DeepWalk . DeepWalk is an approach recently proposed for social network embedding , which is only applicable for networks with binary edges . For each vertex , truncated random walks starting from the vertex are used to obtain the contextual information , and therefore only second - order proximity is utilized . LINE - SGD . This is the LINE model introduced in Section [ reference ] that optimizes the objective Eqn . ( [ reference ] ) or Eqn . ( [ reference ] ) directly with stochastic gradient descent . With this approach , the weights of the edges are directly multiplied into the gradients when the edges are sampled for model updating . There are two variants of this approach : LINE - SGD ( 1st ) and LINE - SGD ( 2nd ) , which use first - and second - order proximity respectively . LINE . This is the LINE model optimized through the edge - sampling treatment introduced in Section [ reference ] . In each stochastic gradient step , an edge is sampled with the probability proportional to its weight and then treated as binary for model updating . There are also two variants : LINE ( 1st ) and LINE ( 2nd ) . Like the graph factorization , both LINE ( 1st ) and LINE - SGD ( 1st ) only apply to undirected graphs . LINE ( 2nd ) and LINE - SGD ( 2nd ) apply to both undirected and directed graphs . LINE ( 1st + 2nd ) : To utilize both first - order and second - order proximity , a simple and effective way is to concatenate the vector representations learned by LINE ( 1st ) and LINE ( 2nd ) into a longer vector . After concatenation , the dimensions should be re - weighted to balance the two representations . In a supervised learning task , the weighting of dimensions can be automatically found based on the training data . In an unsupervised task , however , it is more difficult to set the weights . Therefore we only apply LINE ( 1st + 2nd ) to the scenario of supervised tasks . paragraph : Parameter Settings The mini - batch size of the stochastic gradient descent is set as 1 for all the methods . Similar to , the learning rate is set with the starting value and , where is the total number of mini - batches or edge samples . For fair comparisons , the dimensionality of the embeddings of the language network is set to 200 , as used in word embedding . For other networks , the dimension is set as 128 by default , as used in . Other default settings include : the number of negative samples for LINE and LINE - SGD ; the total number of samples billion for LINE ( 1st ) and LINE ( 2nd ) , billion for GF ; window size , walk length , walks per vertex for DeepWalk . All the embedding vectors are finally normalized by setting . subsection : Quantitative Results subsubsection : Language Network We start with the results on the language network , which contains two million nodes and a billion edges . Two applications are used to evaluate the effectiveness of the learned embeddings : word analogy and document classification . Word Analogy . This task is introduced by Mikolov et al . . Given a word pair and a word , the task aims to find a word , such that the relation between and is similar to the relation between and , or denoted as : . For instance , given a word pair ( \u201c China\u201d , \u201cBeijing \u201d ) and a word \u201c France , \u201d the right answer should be \u201c Paris \u201d because \u201c Beijing \u201d is the capital of \u201c China \u201d just as \u201c Paris \u201d is the capital of \u201c France . \u201d Given the word embeddings , this task is solved by finding the word whose embedding is closest to the vector in terms of cosine proximity , i.e. , . Two categories of word analogy are used in this task : semantic and syntactic . Significantly outperforms GF at the : * * 0.01 and * 0.05 level , paired t - test . Table [ reference ] reports the results of word analogy using the embeddings of words learned on the Wikipedia corpora ( SkipGram ) or the Wikipedia word network ( all other methods ) . For graph factorization , the weight between each pair of words is defined as the logarithm of the number of co - occurrences , which leads to better performance than the original value of co - occurrences . For DeepWalk , different cutoff thresholds are tried to convert the language network into a binary network , and the best performance is achieved when all the edges are kept in the network . We also compare with the state - of - the - art word embedding model SkipGram , which learns the word embeddings directly from the original Wikipedia pages and is also implicitly a matrix factorization approach . The window size is set as 5 , the same as used for constructing the language network . We can see that LINE ( 2nd ) outperforms all other methods , including the graph embedding methods and the SkipGram . This indicates that the second - order proximity better captures the word semantics compared to the first - order proximity . This is not surprising , as a high second - order proximity implies that two words can be replaced in the same context , which is a stronger indicator of similar semantics than first - order co - occurrences . It is intriguing that the LINE ( 2nd ) outperforms the state - of - the - art word embedding model trained on the original corpus . The reason may be that a language network better captures the global structure of word co - occurrences than the original word sequences . Among other methods , both graph factorization and LINE ( 1st ) significantly outperform DeepWalk even if DeepWalk explores second - order proximity . This is because DeepWalk has to ignore the weights ( i.e. , co - occurrences ) of the edges , which is very important in a language network . The performance by the LINE models directly optimized with SGD is much worse , because the weights of the edges in the language network diverge , which range from a single digit to tens of thousands , making the learning process suffer . The LINE optimized by the edge - sampling treatment effectively addresses this problem , and performs very well using either first - order or second - order proximity . All the models are run on a single machine with 1 T memory , 40 CPU cores at 2.0GHZ using 16 threads . Both the LINE ( 1st ) and LINE ( 2nd ) are quite efficient , which take less than 3 hours to process such a network with 2 million nodes and a billion edges . Both are at least 10 % faster than graph factorization , and much more efficient than DeepWalk ( five times slower ) . The reason that LINE - SGDs are slightly slower is that a threshold - cutting technique has to be applied to prevent the gradients from exploding . Document Classification . Another way to evaluate the quality of the word embeddings is to use the word vectors to compute document representation , which can be evaluated with document classification tasks . To obtain document vectors , we choose a very simple approach , taking the average of the word vector representations in that document . This is because we aim to compare the word embeddings with different approaches instead of finding the best method for document embeddings . The readers can find advanced document embedding approaches in . We download the abstracts of Wikipedia pages from and the categories of these pages from . We choose 7 diverse categories for classification including \u201c Arts , \u201d \u201c History , \u201d \u201c Human , \u201d \u201c Mathematics , \u201d \u201c Nature , \u201d \u201c Technology , \u201d and \u201c Sports . \u201d For each category , we randomly select 10 , 000 articles , and articles belonging to multiple categories are discarded . We randomly sample different percentages of the labeled documents for training and use the rest for evaluation . All document vectors are used to train a one - vs - rest logistic regression classifier using the LibLinear package . We report the classification metrics Micro - F1 and Macro - F1 . The results are averaged over 10 different runs by sampling different training data . Table [ reference ] reports the results of Wikipedia page classification . Similar conclusion can be made as in the word analogy task . The graph factorization outperforms DeepWalk as DeepWalk ignores the weights of the edges . The LINE - SGDs perform worse due to the divergence of the weights of the edges . The LINE optimized by the edge - sampling treatment performs much better than directly deploying SGD . The LINE ( 2nd ) outperforms LINE ( 1st ) and is slightly better than the graph factorization . Note that with the supervised task , it is feasible to concatenate the embeddings learned with LINE ( 1st ) and LINE ( 2nd ) . As a result , the LINE ( 1st + 2nd ) method performs significantly better than all other methods . This indicates that the first - order and second - order proximities are complementary to each other . To provide the readers more insight about the first - order and second - order proximities , Table [ reference ] compares the most similar words to a given word using first - order and second - order proximity . We can see that by using the contextual proximity , the most similar words returned by the second - order proximity are all semantically related words . The most similar words returned by the first - order proximity are a mixture of syntactically and semantically related words . subsubsection : Social Network Significantly outperforms DeepWalk at the : * * 0.01 and * 0.05 level , paired t - test . Compared with the language networks , the social networks are much sparser , especially the Youtube network . We evaluate the vertex embeddings through a multi - label classification task that assigns every node into one or more communities . Different percentages of the vertices are randomly sampled for training and the rest are used for evaluation . The results are averaged over 10 different runs . Flickr Network . Let us first take a look at the results on the Flickr network . We choose the most popular 5 communities as the categories of the vertices for multi - label classification . Table [ reference ] reports the results . Again , LINE ( 1st + 2nd ) significantly outperforms all other methods . LINE ( 1st ) is slightly better than LINE ( 2nd ) , which is opposite to the results on the language network . The reasons are two fold : ( 1 ) first - order proximity is still more important than second - order proximity in social network , which indicates strong ties ; ( 2 ) when the network is too sparse and the average number of neighbors of a node is too small , the second - order proximity may become inaccurate . We will further investigate this issue in Section [ reference ] . LINE ( 1st ) outperforms graph factorization , indicating a better capability of modeling the first - order proximity . LINE ( 2nd ) outperforms DeepWalk , indicating a better capability of modeling the second - order proximity . By concatenating the representations learned by LINE ( 1st ) and LINE ( 2nd ) , the performance further improves , confirming that the two proximities are complementary to each other . Significantly outperforms DeepWalk at the : * * 0.01 and * 0.05 level , paired t - test . Significantly outperforms DeepWalk at the : * * 0.01 and * 0.05 level , paired t - test . Significantly outperforms DeepWalk at the : * * 0.01 and * 0.05 level , paired t - test . Youtube Network . Table [ reference ] reports the results on Youtube network , which is extremely sparse and the average degree is as low as 5 . In most cases with different percentages of training data , LINE ( 1st ) outperforms LINE ( 2nd ) , consistent with the results on the Flickr network . Due to the extreme sparsity , the performance of LINE ( 2nd ) is even inferior to DeepWalk . By combining the representations learned by the LINE with both the first - and second - order proximity , the performance of LINE outperforms DeepWalk with either 128 or 256 dimension , showing that the two proximities are complementary to each other and able to address the problem of network sparsity . It is interesting to observe how DeepWalk tackles the network sparsity through truncated random walks , which enrich the neighbors or contexts of each vertex . The random walk approach acts like a depth - first search . Such an approach may quickly alleviate the sparsity of the neighborhood of nodes by bringing in indirect neighbors , but it may also introduce nodes that are long range away . A more reasonable way is to expand the neighborhood of each vertex using a breadth - first search strategy , i.e. , recursively adding neighbors of neighbors . To verify this , we expand the neighborhood of the vertices whose degree are less than 1 , 000 by adding the neighbors of neighbors until the size of the extended neighborhood reaches 1 , 000 nodes . We find that adding more than 1 , 000 vertices does not further increase the performance . The results in the brackets in Table [ reference ] are obtained on this reconstructed network . The performance of GF , LINE ( 1st ) and LINE ( 2nd ) all improves , especially LINE ( 2nd ) . In the reconstructed network , the LINE ( 2nd ) outperforms DeepWalk in most cases . We can also see that the performance of LINE ( 1st + 2nd ) on the reconstructed network does not improve too much compared with those on the original network . This implies that the combination of first - order and second - order proximity on the original network has already captured most information and LINE ( 1st + 2nd ) approach is a quite effective and efficient way for network embedding , suitable for both dense and sparse networks . subsubsection : Citation Network We present the results on two citation networks , both of which are directed networks . Both the GF and LINE methods , which use first - order proximity , are not applicable for directed networks , and hence we only compare DeepWalk and LINE ( 2nd ) . We also evaluate the vertex embeddings through a multi - label classification task . We choose 7 popular conferences including AAAI , CIKM , ICML , KDD , NIPS , SIGIR , and WWW as the classification categories . Authors publishing in the conferences or papers published in the conferences are assumed to belong to the categories corresponding to the conferences . DBLP ( AuthorCitation ) Network . Table [ reference ] reports the results on the DBLP ( AuthorCitation ) network . As this network is also very sparse , DeepWalk outperforms LINE ( 2nd ) . However , by reconstructing the network through recursively adding neighbors of neighbors for vertices with small degrees ( smaller than 500 ) , the performance of LINE ( 2nd ) significantly increases and outperforms DeepWalk . The LINE model directly optimized by stochastic gradient descent , LINE ( 2nd ) , does not perform well as expected . DBLP ( PaperCitation ) Network . Table [ reference ] reports the results on the DBLP ( PaperCitation ) network . The LINE ( 2nd ) significantly outperforms DeepWalk . This is because the random walk on the paper citation network can only reach papers along the citing path ( i.e. , older papers ) and can not reach other references . Instead , the LINE ( 2nd ) represents each paper with its references , which is obviously more reasonable . The performance of LINE ( 2nd ) is further improved when the network is reconstructed by enriching the neighbors of vertices with small degrees ( smaller than 200 ) . subsection : Network Layouts An important application of network embedding is to create meaningful visualizations that layout a network on a two dimensional space . We visualize a co - author network extracted from the DBLP data . We select some conferences from three different research fields : WWW , KDD from \u201c data mining , \u201d NIPS , ICML from \u201c machine learning , \u201d and CVPR , ICCV from \u201c computer vision . \u201d The co - author network is built from the papers published in these conferences . Authors with degree less than 3 are filtered out , and finally the network contains 18 , 561 authors and 207 , 074 edges . Laying out this co - author network is very challenging as the three research fields are very close to each other . We first map the co - author network into a low - dimensional space with different embedding approaches and then further map the low - dimensional vectors of the vertices to a 2 - D space with the t - SNE package . Fig . [ reference ] compares the visualization results with different embedding approaches . The visualization using graph factorization is not very meaningful , in which the authors belonging to the same communities are not clustered together . The result of DeepWalk is much better . However , many authors belonging to different communities are clustered tightly into the center area , most of which are high degree vertices . This is because DeepWalk uses a random walk based approach to enrich the neighbors of the vertices , which brings in a lot of noise due to the randomness , especially for vertices with higher degrees . The LINE ( 2nd ) performs quite well and generates meaningful layout of the network ( nodes with same colors are distributed closer ) . subsection : Performance w.r.t . Network Sparsity In this subsection , we formally analyze the performance of the above models w.r.t . the sparsity of networks . We use the social networks as examples . We first investigate how the sparsity of the networks affects the LINE ( 1st ) and LINE ( 2nd ) . Fig . [ reference ] shows the results w.r.t . the percentage of links on the Flickr network . We choose Flickr network as it is much denser than the Youtube network . We randomly select different percentages of links from the original network to construct networks with different levels of sparsity . We can see that in the beginning , when the network is very sparse , the LINE ( 1st ) outperforms LINE ( 2nd ) . As we gradually increase the percentage of links , the LINE ( 2nd ) begins to outperform the LINE ( 1st ) . This shows that the second - order proximity suffers when the network is extremely sparse , and it outperforms first - order proximity when there are sufficient nodes in the neighborhood of a node . Fig . [ reference ] shows the performance w.r.t . the degrees of the vertices on both the original and reconstructed Youtube networks . We categorize the vertices into different groups according to their degrees including , and then evaluate the performance of vertices in different groups . Overall , the performance of different models increases when the degrees of the vertices increase . In the original network , the LINE ( 2nd ) outperforms LINE ( 1st ) except for the first group , which confirms that the second - order proximity does not work well for nodes with a low degree . In the reconstructed dense network , the performance of the LINE ( 1st ) or LINE ( 2nd ) improves , especially the LINE ( 2nd ) that preserves the second - order proximity . We can also see that the LINE ( 2nd ) model on the reconstructed network outperforms DeepWalk in all the groups . subsection : Parameter Sensitivity Next , we investigate the performance w.r.t . the parameter dimension and the converging performance of different models w.r.t the number of samples on the reconstructed Youtube network . Fig . [ reference ] reports the performance of the LINE model w.r.t . the dimension . We can see that the performance of the LINE ( 1st ) or LINE ( 2nd ) drops when the dimension becomes too large . Fig . [ reference ] shows the results of the LINE and DeepWalk w.r.t . the number of samples during the optimization . The LINE ( 2nd ) consistently outperforms LINE ( 1st ) and DeepWalk , and both the LINE ( 1st ) and LINE ( 2nd ) converge much faster than DeepWalk . subsection : Scalability Finally , we investigate the scalability of the LINE model optimized by the edge - sampling treatment and asynchronous stochastic gradient descent , which deploys multiple threads for optimization . Fig . [ reference ] shows the speed up w.r.t . the number of threads on the Youtube data set . The speed up is quite close to linear . Fig . [ reference ] shows that the classification performance remains stable when using multiple threads for model updating . The two figures together show that the inference algorithm of the LINE model is quite scalable . section : Conclusion This paper presented a novel network embedding model called the \u201c LINE , \u201d which can easily scale up to networks with millions of vertices and billions of edges . It has carefully designed objective functions that preserve both the first - order and second - order proximities , which are complementary to each other . An efficient and effective edge - sampling method is proposed for model inference , which solved the limitation of stochastic gradient descent on weighted edges without compromising the efficiency . Experimental results on various real - world networks prove the efficiency and effectiveness of LINE . In the future , we plan to investigate higher - order proximity beyond the first - order and second - order proximities in the network . Besides , we also plan to investigate the embedding of heterogeneous information networks , e.g. , vertices with multiple types . section : Acknowledgments The authors thank the three anonymous reviewers for the helpful comments . The co - author Ming Zhang is supported by the National Natural Science Foundation of China ( NSFC Grant No . 61472006 ) ; Qiaozhu Mei is supported by the National Science Foundation under grant numbers IIS - 1054199 and CCF - 1048168 . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["BlogCatalog"]]], "Method": [[["LINE"]]], "Metric": [[["Accuracy"]]], "Task": [[["Node_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["BlogCatalog"]]], "Method": [[["LINE"]]], "Metric": [[["Macro-F1"]]], "Task": [[["Node_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Wikipedia"]]], "Method": [[["LINE"]]], "Metric": [[["Accuracy"]]], "Task": [[["Node_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Wikipedia"]]], "Method": [[["LINE"]]], "Metric": [[["Macro-F1"]]], "Task": [[["Node_Classification"]]]}]}
{"docid": "TST3-SREX-0008", "doctext": "document : Compositional Sequence Labeling Models for Error Detection in Learner Writing In this paper , we present the first experiments using neural network models for the task of error detection in learner writing . We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs . Experiments on the CoNLL - 14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing . Finally , the model is integrated with a publicly deployed self - assessment system , leading to performance comparable to human annotators . section : Introduction Automated systems for detecting errors in learner writing are valuable tools for second language learning and assessment . Most work in recent years has focussed on error correction , with error detection performance measured as a byproduct of the correction output . However , this assumes that systems are able to propose a correction for every detected error , and accurate systems for correction might not be optimal for detection . While closed - class errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach , content - content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks . Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold - standard corrections . Therefore , we treat error detection in learner writing as an independent task and propose a system for labeling each token as being correct or incorrect in context . Common approaches to similar sequence labeling tasks involve learning weights or probabilities for context n - grams of varying sizes , or relying on previously extracted high - confidence context patterns . Both of these methods can suffer from data sparsity , as they treat words as independent units and miss out on potentially related patterns . In addition , they need to specify a fixed context size and are therefore often limited to using a small window near the target . Neural network models aim to address these weaknesses and have achieved success in various NLP tasks such as language modeling and speech recognition . Recent developments in machine translation have also shown that text of varying length can be represented as a fixed - size vector using convolutional networks or recurrent neural networks . In this paper , we present the first experiments using neural network models for the task of error detection in learner writing . We perform a systematic comparison of alternative compositional structures for constructing informative context representations . Based on the findings , we propose a novel framework for performing error detection in learner writing , which achieves state - of - the - art results on two datasets of error - annotated learner essays . The sequence labeling model creates a single variable - size network over the whole sentence , conditions each label on all the words , and predicts all labels together . The effects of different datasets on the overall performance are investigated by incrementally providing additional training data to the model . Finally , we integrate the error detection framework with a publicly deployed self - assessment system , leading to performance comparable to human annotators . section : Background and Related Work The field of automatically detecting errors in learner text has a long and rich history . Most work has focussed on tackling specific types of errors , such as usage of incorrect prepositions , articles , verb forms , and adjective - noun pairs . However , there has been limited work on more general error detection systems that could handle all types of errors in learner text . Chodorow1998 proposed a method based on mutual information and the chi - square statistic to detect sequences of part - of - speech tags and function words that are likely to be ungrammatical in English . Gamon2011 used Maximum Entropy Markov Models with a range of features , such as POS tags , string features , and outputs from a constituency parser . The pilot Helping Our Own shared task also evaluated grammatical error detection of a number of different error types , though most systems were error - type specific and the best approach was heavily skewed towards article and preposition errors . We extend this line of research , working towards general error detection systems , and investigate the use of neural compositional models on this task . The related area of grammatical error correction has also gained considerable momentum in the past years , with four recent shared tasks highlighting several emerging directions . The current state - of - the - art approaches can broadly be separated into two categories : Phrase - based statistical machine translation techniques , essentially translating the incorrect source text into the corrected version Averaged Perceptrons and Naive Bayes classifiers making use of native - language error correction priors . Error correction systems require very specialised models , as they need to generate an improved version of the input text , whereas a wider range of tagging and classification models can be deployed on error detection . In addition , automated writing feedback systems that indicate the presence and location of errors may be better from a pedagogic point of view , rather than providing a panacea and correcting all errors in learner text . In Section [ reference ] we evaluate a neural sequence tagging model on the latest shared task test data , and compare it to the top participating systems on the task of error detection . section : Sequence Labeling Architectures We construct a neural network sequence labeling framework for the task of error detection in learner writing . The model receives only a series of tokens as input , and outputs the probability of each token in the sentence being correct or incorrect in a given context . The architectures start with the vector representations of individual words , , where is the length of the sentence . Different composition functions are then used to calculate a hidden vector representation of each token in context , . These representations are passed through a softmax layer , producing a probability distribution over the possible labels for every token in context : where is the weight matrix between the hidden vector and the output layer . We investigate six alternative neural network architectures for the task of error detection : convolutional , bidirectional recurrent , bidirectional LSTM , and multi - layer variants of each of them . In the convolutional neural network ( CNN , Figure [ reference ] a ) for token labeling , the hidden vector is calculated based on a fixed - size context window . The convolution acts as a feedforward network , using surrounding context words as input , and therefore it will learn to detect the presence of different types of n - grams . The assumption behind the convolutional architecture is that memorising erroneous token sequences from the training data is sufficient for performing error detection . The convolution uses tokens on either side of the target token , and the vectors for these tokens are concatenated , preserving the ordering : where is used as notation for vector concatenation of and . The combined vector is then passed through a non - linear layer to produce the hidden representation : The deep convolutional network ( Figure [ reference ] b ) adds an extra convolutional layer to the architecture , using the first layer as input . It creates convolutions of convolutions , thereby capturing more complex higher - order features from the dataset . In a recurrent neural network ( RNN ) , each hidden representation is calculated based on the current token embedding and the hidden vector at the previous time step : where is a nonlinear function , such as the sigmoid function . Instead of a fixed context window , information is passed through the sentence using a recursive function and the network is able to learn which patterns to disregard or pass forward . This recurrent network structure is referred to as an Elman - type network , after Elman1990 . The bidirectional RNN ( Figure [ reference ] c ) consists of two recurrent components , moving in opposite directions through the sentence . While the unidirectional version takes into account only context on the left of the target token , the bidirectional version recursively builds separate context representations from either side of the target token . The left and right context are then concatenated and used as the hidden representation : Recurrent networks have been shown to perform well on the task of language modeling , where they learn an incremental composition function for predicting the next token in the sequence . However , while language models can estimate the probability of each token , they are unable to differentiate between infrequent and incorrect token sequences . For error detection , the composition function needs to learn to identify semantic anomalies or ungrammatical combinations , independent of their frequency . The bidirectional model provides extra information , as it allows the network to use context on both sides of the target token . Irsoy2014a created an extension of this architecture by connecting together multiple layers of bidirectional Elman - type recurrent network modules . This deep bidirectional RNN ( Figure [ reference ] d ) calculates a context - dependent representation for each token using a bidirectional RNN , and then uses this as input to another bidirectional RNN . The multi - layer structure allows the model to learn more complex higher - level features and effectively perform multiple recurrent passes through the sentence . The long - short term memory ( LSTM ) is an advanced alternative to the Elman - type networks that has recently become increasingly popular . It uses two separate hidden vectors to pass information between different time steps , and includes gating mechanisms for modulating its own output . LSTMs have been successfully applied to various tasks , such as speech recognition , machine translation , and natural language generation . Two sets of gating values ( referred to as the input and forget gates ) are first calculated based on the previous states of the network : where is the current input , is the previous hidden state , and are biases , is the previous internal state ( referred to as the cell ) , and is the logistic function . The new internal state is calculated based on the current input and the previous hidden state , and then interpolated with the previous internal state using and as weights : where is element - wise multiplication . Finally , the hidden state is calculated by passing the internal state through a nonlinearity , and weighting it with . The values of are conditioned on the new internal state ( ) , as opposed to the previous one ( ) : Because of the linear combination in equation ( [ reference ] ) , the LSTM is less susceptible to vanishing gradients over time , thereby being able to make use of longer context when making predictions . In addition , the network learns to modulate itself , effectively using the gates to predict which operation is required at each time step , thereby incorporating higher - level features . In order to use this architecture for error detection , we create a bidirectional LSTM , making use of the advanced features of LSTM and incorporating context on both sides of the target token . In addition , we experiment with a deep bidirectional LSTM , which includes two consecutive layers of bidirectional LSTMs , modeling even more complex features and performing multiple passes through the sentence . For comparison with non - neural models , we also report results using CRFs , which are a popular choice for sequence labeling tasks . We trained the CRF ++ implementation on the same dataset , using as features unigrams , bigrams and trigrams in a 7 - word window surrouding the target word ( 3 words before and after ) . The predicted label is also conditioned on the previous label in the sequence . section : Experiments We evaluate the alternative network structures on the publicly released First Certificate in English dataset ( FCE - public , Yannakoudakis2011 ) . The dataset contains short texts , written by learners of English as an additional language in response to exam prompts eliciting free - text answers and assessing mastery of the upper - intermediate proficiency level . The texts have been manually error - annotated using a taxonomy of 77 error types . We use the released test set for evaluation , containing 2 , 720 sentences , leaving 30 , 953 sentences for training . We further separate 2 , 222 sentences from the training set for development and hyper - parameter tuning . The dataset contains manually annotated error spans of various types of errors , together with their suggested corrections . We convert this to a token - level error detection task by labeling each token inside the error span as being incorrect . In order to capture errors involving missing words , the error label is assigned to the token immediately after the incorrect gap \u2013 this is motivated by the intuition that while this token is correct when considered in isolation , it is incorrect in the current context , as another token should have preceeded it . As the main evaluation measure for error detection we use , which was also the measure adopted in the CoNLL - 14 shared task on error correction . It combines both precision and recall , while assigning twice as much weight to precision , since accurate feedback is often more important than coverage in error detection applications . Following Chodorow2012 , we also report raw counts for predicted and correct tokens . Related evaluation measures , such as the - scorer and the I - measure , require the system to propose a correction and are therefore not directly applicable on the task of error detection . During the experiments , the input text was lowercased and all tokens that occurred less than twice in the training data were represented as a single unk token . Word embeddings were set to size and initialised using the publicly released pretrained Word2Vec vectors . The convolutional networks use window size on either side of the target token and produce a 300 - dimensional context - dependent vector . The recurrent networks use hidden layers of size 200 in either direction . We also added an extra hidden layer of size between each of the composition functions and the output layer \u2013 this allows the network to learn a separate non - linear transformation and reduces the dimensionality of the compositional vectors . The parameters were optimised using gradient descent with initial learning rate , the ADAM algorithm for dynamically adapting the learning rate , and batch size of 64 sentences . on the development set was evaluated at each epoch , and the best model was used for final evaluations . section : Results Table [ reference ] contains results for experiments comparing different composition architectures on the task of error detection . The CRF has the lowest score compared to any of the neural models . It memorises frequent error sequences with high precision , but does not generalise sufficiently , resulting in low recall . The ability to condition on the previous label also does not provide much help on this task \u2013 there are only two possible labels and the errors are relatively sparse . The architecture using convolutional networks performs well and achieves the second - highest result on the test set . It is designed to detect error patterns from a fixed window of 7 words , which is large enough to not require the use of more advanced composition functions . In contrast , the performance of the bidirectional recurrent network ( Bi - RNN ) is somewhat lower , especially on the test set . In Elman - type recurrent networks , the context signal from distant words decreases fairly rapidly due to the sigmoid activation function and diminishing gradients . This is likely why the Bi - RNN achieves the highest precision of all systems \u2013 the predicted label is mostly influenced by the target token and its immediate neighbours , allowing the network to only detect short high - confidence error patterns . The convolutional network , which uses 7 context words with equal attention , is able to outperform the Bi - RNN despite the fixed - size context window . The best overall result and highest is achieved by the bidirectional LSTM composition model ( Bi - LSTM ) . This architecture makes use of the full sentence for building context vectors on both sides of the target token , but improves on Bi - RNN by utilising a more advanced composition function . Through the application of a linear update for the internal cell representation , the LSTM is able to capture dependencies over longer distances . In addition , the gating functions allow it to adaptively decide which information to include in the hidden representations or output for error detection . We found that using multiple layers of compositional functions in a deeper network gave comparable or slightly lower results for all the composition architectures . This is in contrast to Irsoy2014a , who experimented with Elman - type networks and found some improvements using multiple layers of Bi - RNNs . The differences can be explained by their task benefiting from alternative features : the evaluation was performed on opinion mining where most target sequences are longer phrases that need to be identified based on their semantics , whereas many errors in learner writing are short and can only be identified by a contextual mismatch . In addition , our networks contain an extra hidden layer before the output , which allows the models to learn higher - level representations without adding complexity through an extra compositional layer . section : Additional Training Data There are essentially infinitely many ways of committing errors in text and introducing additional training data should alleviate some of the problems with data sparsity . We experimented with incrementally adding different error - tagged corpora into the training set and measured the resulting performance . This allows us to provide some context to the results obtained by using each of the datasets , and gives us an estimate of how much annotated data is required for optimal performance on error detection . The datasets we consider are as follows : FCE - public \u2013 the publicly released subset of FCE , as described in Section [ reference ] . NUCLE \u2013 the NUS Corpus of Learner English , used as the main training set for CoNLL shared tasks on error correction . IELTS \u2013 a subset of the IELTS examination dataset extracted from the Cambridge Learner Corpus ( CLC , Nicholls2003 ) , containing 68 , 505 sentences from all proficiency levels , also used by Felice2014 . FCE \u2013 a larger selection of FCE texts from the CLC , containing 323 , 192 sentences . CPE \u2013 essays from the proficient examination level in the CLC , containing 210 , 678 sentences . CAE \u2013 essays from the advanced examination level in the CLC , containing 219 , 953 sentences . Table [ reference ] contains results obtained by incrementally adding training data to the Bi - LSTM model . We found that incorporating the NUCLE dataset does not improve performance over using only the FCE - public dataset , which is likely due to the two corpora containing texts with different domains and writing styles . The texts in FCE are written by young intermediate students , in response to prompts eliciting letters , emails and reviews , whereas NUCLE contains mostly argumentative essays written by advanced adult learners . The differences in the datasets offset the benefits from additional training data , and the performance remains roughly the same . In contrast , substantial improvements are obtained when introducing the IELTS and FCE datasets , with each of them increasing the score by roughly . The IELTS dataset contains essays from all proficiency levels , and FCE from mid - level English learners , which provides the model with a distribution of \u2018 average \u2019 errors to learn from . Adding even more training data from high - proficiency essays in CPE and CAE only provides minor further improvements . Figure [ reference ] also shows on the FCE - public test set as a function of the total number of tokens in the training data . The optimal trade - off between performance and data size is obtained at around 8 million tokens , after introducing the FCE dataset . section : CoNLL - 14 Shared Task The CoNLL - 14 shared task focussed on automatically correcting errors in learner writing . The NUCLE dataset was provided as the main training dataset , but participants were allowed to include other annotated corpora and external resources . For evaluation , 25 students were recruited to each write two new essays , which were then annotated by two experts . We used the same methods from Section [ reference ] for converting the shared task annotation to a token - level labeling task in order to evaluate the models on error detection . In addition , the correction outputs of all the participating systems were made available online , therefore we are able to report their performance on this task . In order to convert their output to error detection labels , the corrected sentences were aligned with the original input using Levenshtein distance , and any changes proposed by the system resulted in the corresponding source words being labeled as errors . The results on the two annotations of the shared task test data can be seen in Table [ reference ] . We first evaluated each of the human annotators with respect to the other , in order to estimate the upper bound on this task . The average of roughly 50 % shows that the task is difficult and even human experts have a rather low agreement . It has been shown before that correcting grammatical errors is highly subjective , but these results indicate that trained annotators can disagree even on the number and location of errors . In the same table , we provide error detection results for the top 3 participants in the shared task : CAMB , CUUI , and AMU . They each preserve their relative ranking also in the error detection evaluation . The CAMB system has a lower precision but the highest recall , also resulting in the highest . CUUI and AMU are close in performance , with AMU having slightly higher precision . After the official shared task , Susanto2014 published a system which combines several alternative models and outperforms the shared task participants when evaluated on error correction . However , on error detection it receives lower results , ranking 3rd and 4th when evaluated on ( P1 + P2 + S1 + S2 in Table [ reference ] ) . The system has detected a small number of errors with high precision , and does not reach the highest . Finally , we present results for the Bi - LSTM sequence labeling system for error detection . Using only FCE - public for training , the overall performance is rather low as the training set is very small and contains texts from a different domain . However , these results show that the model behaves as expected \u2013 since it has not seen similar language during training , it labels a very large portion of tokens as errors . This indicates that the network is trying to learn correct language constructions from the limited data and classifies unseen structures as errors , as opposed to simply memorising error sequences from the training data . When trained on all the datasets from Section [ reference ] , the model achieves the highest of all systems on both of the CoNLL - 14 shared task test annotations , with an absolute improvement of over the previous best result . It is worth noting that the full Bi - LSTM has been trained on more data than the other CoNLL contestants . However , as the shared task systems were not restricted to the NUCLE training set , all the submissions also used differing amounts of training data from various sources . In addition , the CoNLL systems are mostly combinations of many alternative models : the CAMB system is a hybrid of machine translation , a rule - based system , and a language model re - ranker ; CUUI consists of different classifiers for each individual error type ; and P1 + P2 + S1 + S2 is a combination of four different error correction systems . In contrast , the Bi - LSTM is a single model for detecting all error types , and therefore represents a more scalable data - driven approach . section : Essay Scoring In this section , we perform an extrinsic evaluation of the efficacy of the error detection system and examine the extent to which it generalises at higher levels of granularity on the task of automated essay scoring . More specifically , we replicate experiments using the text - level model described by Andersen2013 , which is currently deployed in a self - assessment and tutoring system ( SAT ) , an online automated writing feedback tool actively used by language learners . The SAT system predicts an overall score for a given text , which provides a holistic assessment of linguistic competence and language proficiency . The authors trained a supervised ranking perceptron model on the FCE - public dataset , using features such as error - rate estimates from a language model and various lexical and grammatical properties of text ( e.g. , word n - grams , part - of - speech n - grams and phrase - structure rules ) . We replicate this experiment and add the average probability of each token in the essay being correct , according to the error detection model , as an additional feature for the scoring framework . The system was then retrained on FCE - public and evaluated on correctly predicting the assigned essay score . Table [ reference ] presents the experimental results . The human performance on the test set is calculated as the average inter - annotator correlation on the same data , and the existing SAT system has demonstrated levels of performance that are very close to that of human assessors . Nevertheless , the Bi - LSTM model trained only on FCE - public complements the existing features , and the combined model achieves an absolute improvement of around 1 % percent , corresponding to 20 - 31 % relative error reduction with respect to the human performance . Even though the Bi - LSTM is trained on the same dataset and the SAT system already includes various linguistic features for capturing errors , our error detection model manages to further improve its performance . When the Bi - LSTM is trained on all the available data from Section [ reference ] , the combination achieves further substantial improvements . The relative error reduction on Pearson \u2019s correlation is 64 % , and the system actually outperforms human annotators on Spearman \u2019s correlation . section : Conclusions In this paper , we presented the first experiments using neural network models for the task of error detection in learner writing . Six alternative compositional network architectures for modeling context were evaluated . Based on the findings , we propose a novel error detection framework using token - level embeddings , bidirectional LSTMs for context representation , and a multi - layer architecture for learning more complex features . This structure allows the model to classify each token as being correct or incorrect , using the full sentence as context . The self - modulation architecture of LSTMs was also shown to be beneficial , as it allows the network to learn more advanced composition rules and remember dependencies over longer distances . Substantial performance improvements were achieved by training the best model on additional datasets . We found that the largest benefit was obtained from training on 8 million tokens of text from learners with varying levels of language proficiency . In contrast , including even more data from higher - proficiency learners gave marginal further improvements . As part of future work , it would be beneficial to investigate the effect of automatically generated training data for error detection ( e.g. , Rozovskaya2010 ) . We evaluated the performance of existing error correction systems from CoNLL - 14 on the task of error detection . The experiments showed that success on error correction does not necessarily mean success on error detection , as the current best correction system ( P1 + P2 + S1 + S2 ) is not the same as the best shared task detection system ( CAMB ) . In addition , the neural sequence tagging model , specialised for error detection , was able to outperform all other participating systems . Finally , we performed an extrinsic evaluation by incorporating probabilities from the error detection system as features in an essay scoring model . Even without any additional data , the combination further improved performance which is already close to the results from human annotators . In addition , when the error detection model was trained on a larger training set , the essay scorer was able to exceed human - level performance . section : Acknowledgments We would like to thank Prof Ted Briscoe and the reviewers for providing useful feedback . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["CoNLL-2014_A1"]]], "Method": [[["Bi-LSTM__trained_on_FCE_"]]], "Metric": [[["F0_5"]]], "Task": [[["Grammatical_Error_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CoNLL-2014_A1"]]], "Method": [[["Bi-LSTM__unrestricted_data_"]]], "Metric": [[["F0_5"]]], "Task": [[["Grammatical_Error_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CoNLL-2014_A2"]]], "Method": [[["Bi-LSTM__trained_on_FCE_"]]], "Metric": [[["F0_5"]]], "Task": [[["Grammatical_Error_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CoNLL-2014_A2"]]], "Method": [[["Bi-LSTM__unrestricted_data_"]]], "Metric": [[["F0_5"]]], "Task": [[["Grammatical_Error_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["FCE"]]], "Method": [[["Bi-LSTM"]]], "Metric": [[["F0_5"]]], "Task": [[["Grammatical_Error_Detection"]]]}]}
{"docid": "TST3-SREX-0009", "doctext": "document : Filtered Channel Features for Pedestrian Detection This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low - level features in combination with a boosted decision forest . Based on this observation we propose a unifying framework and experimentally explore different filter families . We report extensive results enabling a systematic analysis . Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets , while using only HOG + LUV as low - level features . When adding optical flow features we further improve detection quality and report the best known results on the Caltech dataset , reaching 93 % recall at 1 FPPI . = - 1 section : Introduction Pedestrian detection is an active research area , with 1000 + papers published in the last decade , and well established benchmark datasets . It is considered a canonical case of object detection , and has served as playground to explore ideas that might be effective for generic object detection . Although many different ideas have been explored , and detection quality has been steadily improving , arguably it is still unclear what are the key ingredients for good pedestrian detection ; e.g. it remains unclear how effective parts , components , and features learning are for this task . Current top performing pedestrian detection methods all point to an intermediate layer ( such as max - pooling or filtering ) between the low - level feature maps and the classification layer . In this paper we explore the simplest of such intermediary : a linear transformation implemented as convolution with a filter bank . We propose a framework for filtered channel features ( see figure [ reference ] ) that unifies multiple top performing methods , and that enables a systematic exploration of different filter banks . With our experiments we show that , with the proper filter bank , filtered channel features reach top detection quality . It has been shown that using extra information at test time ( such as context , stereo images , optical flow , etc . ) can boost detection quality . In this paper we focus on the \u2018 \u2018 core \u2019 \u2019 sliding window algorithm using solely HOG + LUV features ( i.e. oriented gradient magnitude and colour features ) . We consider context information and optical flow as add - ons , included in the experiments section for the sake of completeness and comparison with existing methods . Using only HOG + LUV features we already reach top performance on the challenging Caltech and KITTI datasets , matching results using optical flow and significantly more features ( such as LBP and covariance ) . subsection : Related work Recent survey papers discuss the diverse set of ideas explored for pedestrian detection . The most recent survey indicates that the classifier choice ( e.g. linear / non - linear SVM versus decision forest ) is not a clear differentiator regarding quality ; rather the features used seem more important . Creativity regarding different types of features has not been lacking . HOG ) The classic HOG descriptor is based on local image differences ( plus pooling and normalization steps ) , and has been used directly , as input for a deformable parts model , or as features to be boosted . The integral channel features detector uses a simpler HOG variant with sum pooling and no normalizations . Many extensions of HOG have been proposed ( e.g. ) . LBP ) Instead of using the magnitude of local pixel differences , LBP uses the difference sign only as signal . Colour ) Although the appearance of pedestrians is diverse , the background and skin areas do exhibit a colour bias . Colour has shown to be an effective feature for pedestrian detection and hence multiple colour spaces have been explored ( both hand - crafted and learned ) . Local structure ) Instead of simple pixel values , some approaches try to encode a larger local structure based on colour similarities ( soft - cue ) , segmentation methods ( hard - decision ) , or by estimating local boundaries . Covariance ) Another popular way to encode richer information is to compute the covariance amongst features ( commonly colour , gradient , and oriented gradient ) . Etc . ) Other features include bag - of - words over colour , HOG , or LBP features ; learning sparse dictionary encoders ; and training features via a convolutional neural network . Additional features specific for stereo depth or optical flow have been proposed , however we consider these beyond the focus of this paper . For our flow experiments we will use difference of frames from weakly stabilized videos ( SDt ) . All the feature types listed above can be used in the integral channel features detector framework . This family of detectors is an extension of the old ideas from Viola & Jones . Sums of rectangular regions are used as input to decision trees trained via Adaboost . Both the regions to pool from and the thresholds in the decision trees are selected during training . The crucial difference from the pioneer work is that the sums are done over feature channels other than simple image luminance . Current top performing pedestrian detection methods ( dominating INRIA , Caltech and KITTI datasets ) are all extensions of the basic integral channel features detector ( named ChnFtrs in , which uses only HOG + LUV features ) . SquaresChnFtrs , InformedHaar , and LDCF , are discussed in detail in section [ reference ] . Katamari exploits context and optical flow for improved performance . SpatialPooling (+ ) adds max - pooling on top of sum - pooling , and uses additional features such as covariance , LBP , and optical flow . Similarly , Regionlets also uses extended features and max - pooling , together with stronger weak classifiers and training a cascade of classifiers . Out of these , Regionlets is the only method that has also shown good performance on general classes datasets such as Pascal VOC and ImageNet . In this paper we will show that vanilla HOG + LUV features have not yet saturated , and that , when properly used , they can reach top performance for pedestrian detection . subsection : Contributions We point out the link between ACF , ( Squares ) ChnFtrs , InformedHaar , and LDCF . See section [ reference ] . We provide extensive experiments to enable a systematic analysis of the filtered integral channels , covering aspects not explored by related work . We report the summary of trained models ( corresponding days of single machine computation ) . See sections [ reference ] , [ reference ] and [ reference ] . We show that top detection performance can be reached on Caltech and KITTI using HOG + LUV features only . We additionally report the best known results on Caltech . See section [ reference ] . section : Filtered channel features Before entering the experimental section , let us describe our general architecture . Methods such as ChnFtrs , SquaresChnFtrs and ACF all use the basic architecture depicted in figure [ reference ] top part ( best viewed in colours ) . The input image is transformed into a set of feature channels ( also called feature maps ) , the feature vector is constructed by sum - pooling over a ( large ) set of rectangular regions . This feature vector is fed into a decision forest learned via Adaboost . The split nodes in the trees are a simple comparison between a feature value and a learned threshold . Commonly only a subset of the feature vector is used by the learned decision forest . Adaboost serves both for feature selection and for learning the thresholds in the split nodes . A key observation , illustrated in figure [ reference ] ( bottom ) , is that such sum - pooling can be re - written as convolution with a filter bank ( one filter per rectangular shape ) followed by reading a single value of the convolution \u2019s response map . This \u2018 \u2018 filter + pick \u2019 \u2019 view generalizes the integral channel features detectors by allowing to use any filter bank ( instead of only rectangular shapes ) . We name this generalization \u2018 \u2018 filtered channel features detectors \u2019 \u2019 . In our framework , ACF has a single filter in its bank , corresponding to a uniform pooling region . ChnFtrs was a very large ( tens of thousands ) filter bank comprised of random rectangular shapes . SquaresChnFtrs , on the other hand , was only filters , each with a square - shaped uniform pooling region of different sizes . See figure [ reference ] for an illustration of the SquaresChnFtrs filters , the upper - left filter corresponds to ACF \u2019s one . The InformedHaar method can also be seen as a filtered channel features detector , where the filter bank ( and read locations ) are based on a human shape template ( thus the \u2018 \u2018 informed \u2019 \u2019 naming ) . LDCF is also a particular instance of this framework , where the filter bank consists of PCA bases of patches from the training dataset . In sections [ reference ] and [ reference ] we provide experiments revisiting some of the design decisions of these methods . Note that all the methods mentioned above ( and in the majority of experiments below ) use only HOG + LUV feature channels ( 10 channels total ) . Using linear filters and decision trees on top of these does not allow to reconstruct the decision functions obtained when using LBP or covariance features ( used by SpatialPooling and Regionlets ) . We thus consider the approach considered here orthogonal to adding such types of features . subsection : Evaluation protocol For our experiments we use the Caltech and KITTI datasets . The popular INRIA dataset is considered too small and too close to saturation to provide interesting results . All Caltech results are evaluated using the provided toolbox , and summarised by log - average miss - rate ( MR , lower is better ) in the range for the \u2018 \u2018 reasonable \u2019 \u2019 setup . KITTI results are evaluated via the online evaluation portal , and summarised as average precision ( AP , higher is better ) for the \u2018 \u2018 moderate \u2019 \u2019 setup . paragraph : Caltech10x The raw Caltech dataset consists of videos ( acquired at ) with every frame annotated . The standard training and evaluation considers one out of each frames ( pedestrians over frames in training , pedestrians over frames in testing ) . In our experiments of section [ reference ] we will also consider a increased training set where every rd frame is used ( linear growth in pedestrians and images ) . We name this extended training set \u2018 \u2018 Caltech10x \u2019 \u2019 . LDCF uses a similar extended set for training its model ( every th frame ) . paragraph : Flow Methods using optical flow do not only use additional neighbour frames during training ( depending on the method ) , but they also do so at test time . Because they have access to additional information at test time , we consider them as a separate group in our results section . paragraph : Validation set In order to explore the design space of our pedestrian detector we setup a Caltech validation set by splitting the six training videos into five for training and one for testing ( one of the splits suggested in ) . Most of our experiments use this validation setup . We also report ( a posteriori ) our key results on the standard test set for comparison to the state of the art . For the KITTI experiments we also validate some design choices ( such as search range and number of scales ) before submission on the evaluation server . There we use a validation setup . subsection : Baselines paragraph : ACF Our experiments are based on the open source release of ACF . Our first baseline is vanilla ACF re - trained on the standard Caltech set ( not Caltech10x ) . On the Caltech test set it obtains ( on validation set ) . Note that this baseline already improves over more than previously published methods on this dataset . There is also a large gap between ACF - Ours ( ) and the original number from ACF - Caltech ( ) . The improvement is mainly due to the change towards a larger model size ( from to ) . All parameter details are described in section [ reference ] , and kept identical across experiments unless explicitly stated . paragraph : InformedHaar Our second baseline is a re - implementation of InformedHaar . Here again we observe an important gain from using a larger model size ( same change as for ACF ) . While the original InformedHaar paper reports , InformedHaar - Ours reaches on the Caltech test set ( on validation set ) . For both our baselines we use exactly the same training set as the original papers . Note that the InformedHaar - Ours baseline ( ) is right away the best known result for a method trained on the standard Caltech training set . In section [ reference ] we will discuss our re - implementation of LDCF . subsection : Model parameters Unless otherwise specified we train all our models using the following parameters . Feature channels are HOG + LUV only . The final classifier includes level - 2 decision trees ( L2 , 3 stumps per tree ) , trained via vanilla discrete Adaboost . Each tree is built by doing exhaustive greedy search for each node ( no randomization ) . The model has size , and is built via four rounds of hard negative mining ( starting from a model with trees , and then , , , trees ) . Each round adds additional negatives to the training set . The sliding window stride is ( both during hard negative mining and at test time ) . Compared to the default ACF parameters , we use a bigger model , more trees , more negative samples , and more boosting rounds . But we do use the same code - base and the same training set . Starting from section [ reference ] we will also consider results with the Caltech10x data , there we use level - 4 decision trees ( L4 ) , and Realboost instead of discrete Adaboost . All other parameters are left unchanged . section : Filter bank families Given the general architecture and the baselines described in section [ reference ] , we now proceed to explore different types of filter banks . Some of them are designed using prior knowledge and they do not change when applied across datasets , others exploit data - driven techniques for learning their filters . Sections [ reference ] and [ reference ] will compare their detection quality . paragraph : InformedFilters Starting from the InformedHaar baseline we use the same \u2018 \u2018 informed \u2019 \u2019 filters but let free the positions where they are applied ( instead of fixed in InformedHaar ) ; these are selected during the boosting learning . Our initial experiments show that removing the position constraint has a small ( positive ) effect . Additionally we observe that the original InformedHaar filters do not include simple square pooling regions ( \u00e0 la SquaresChnFtrs ) , we thus add these too . We end up with filters in total , to be applied over each of the feature channels . This is equivalent to training decision trees over ( non filtered ) channel features . As illustrated in figure [ reference ] the InformedFilters have different sizes , from to cells ( ) , and each cell takes a value in . These filters are applied with a step size of . For a model of this results in features per channel , features in total . In practice considering border effects ( large filters are not applied on the border of the model to avoid reading outside it ) we end up with features . When training level - 2 decision trees , at most features will be used , that is of the total . In this scenario ( and all others considered in this paper ) Adaboost has a strong role of feature selection . paragraph : Checkerboards As seen in section [ reference ] InformedHaar is a strong baseline . It is however unclear how much the \u2018 \u2018 informed \u2019 \u2019 design of the filters is effective compared to other possible choices . Checkerboards is a na\u00efve set of filters that covers the same sizes ( in number of cells ) as InformedHaar / InformedFilters and for each size defines ( see figure [ reference ] ) : a uniform square , all horizontal and vertical gradient detectors ( values ) , and all possible checkerboard patterns . These configurations are comparable to InformedFilters but do not use the human shape as prior . The total number of filters is a direct function of the maximum size selected . For up to cells we end up with filters , up to cells filters , up to cells filters , and up to cells filters . paragraph : RandomFilters Our next step towards removing a hand - crafted design is simply using random filters ( see figure [ reference ] ) . Given a desired number of filters and a maximum filter size ( in cells ) , we sample the filter size with uniform distribution , and set its cell values to with uniform probability . We also experimented with values and observed a ( small ) quality decrease compared to the binary option ) . The design of the filters considered above completely ignores the available training data . In the following , we consider additional filters learned from data . paragraph : LDCF The work on PCANet showed that applying arbitrary non - linearities on top of PCA projections of image patches can be surprisingly effective for image classification . Following this intuition LDCF uses learned PCA eigenvectors as filters ( see figure [ reference ] ) . We present a re - implementation of based on ACF \u2019s source code . We try to follow the original description as closely as possible . We use the same top filters of , selected per feature channel based on their eigenvalues ( filters total ) . We do change some parameters to be consistent amongst all experiments , see sections [ reference ] and [ reference ] . The main changes are the training set ( we use Caltech10x , sampled every 3 frames , instead of every 4 frames in ) , and the model size ( pixels instead of ) . As will be shown in section [ reference ] , our implementation ( LDCF - Ours ) clearly improves over the previously published numbers , showing the potential of the method . For comparison with PcaForeground we also consider training LDCF8 where the top filters are selected per channel ( filters total ) . paragraph : PcaForeground In LDCF the filters are learned using all of the training data available . In practice this means that the learned filters will be dominated by background information , and will have minimal information about the pedestrians . Put differently , learning filters from all the data assumes that the decision boundary is defined by a single distribution ( like in Linear Discriminant Analysis ) , while we might want to define it based on the relation between the background distribution and the foreground distribution ( like Fisher \u2019s Discriminant Analysis ) . In PcaForeground we train filters per feature channel , learned from background image patches , and learned from patches extracted over pedestrians ( see figure [ reference ] ) . Compared to LDCF8 the obtained filters are similar but not identical , all other parameters are kept identical . Other than via PcaForeground / LDCF8 , it is not clear how to further increase the number of filters used in LDCF . Past filters per channel , the eigenvalues decrease to negligible values and the eigenvectors become essentially random ( similar to RandomFilters ) . To keep the filtered channel features setup close to InformedHaar , the filters are applied with a step of However , to stay close to the original LDCF , the LDCF / PcaForeground filters are evaluated every . Although ( for example ) LDCF8 uses only of the number of filters per channel compared to Checkerboards4x4 , due to the step size increase , the obtained feature vector size is . section : How many filters ? Given a fixed set of channel features , a larger filter bank provides a richer view over the data compared to a smaller one . With enough training data one would expect larger filter banks to perform best . We want thus to analyze the trade - off between number of filters and detection quality , as well as which filter bank family performs best . Figure [ reference ] presents the results of our initial experiments on the Caltech validation set . It shows detection quality versus number of filters per channel . This figure densely summarizes trained models . paragraph : InformedFilters The first aspect to notice is that there is a meaningful gap between InformedHaar - Ours and InformedFilters despite having a similar number of filters ( versus ) . This validates the importance of letting Adaboost choose the pooling locations instead of hand - crafting them . Keep in mind that InformedHaar - Ours is a top performing baseline ( see \u00a7 [ reference ] ) . Secondly , we observe that ( for the fixed training data available ) filters is better than . Below filters the performance degrades for all methods ( as expected ) . To change the number of filters in InformedFilters we train a full model ( filters ) , pick the most frequently used filters ( selected from node splitting in the decision forest ) , and use these to train the desired reduced model . We can select the most frequent filters across channels or per channel ( marked as Inf . FiltersPerChannel ) . We observe that per channel selection is slightly worse than across channels , thus we stick to the latter . Using the most frequently used filters for selection is clearly a crude strategy since frequent usage does not guarantee discriminative power , and it ignores relation amongst filters . We find this strategy good enough to convey the main points of this work . paragraph : Checkerboards also reaches best results in the filters region . Here the number of filters is varied by changing the maximum filter size ( in number of cells ) . Regarding the lowest miss - rate there is no large gap between the \u2018 \u2018 informed \u2019 \u2019 filters and this na\u00efve baseline . paragraph : RandomFilters The hexagonal dots and their deviation bars indicate the mean , maximum and minimum miss - rate obtained out of five random runs . When using a larger number of filters ( ) we observe a lower ( better ) mean but a larger variance compared to when using fewer filters ( ) . Here again the gap between the best random run and the best result of other methods is not large . Given a set of five models , we select the most frequently used filters and train new reduced models ; these are shown in the RandomFilters line . Overall the random filters are surprisingly close to the other filter families . This indicates that expanding the feature channels via filtering is the key step for improving detection quality , while selecting the \u2018 \u2018 perfect \u2019 \u2019 filters is a secondary concern . paragraph : LDCF / PcaForeground In contrast to the other filter bank families , LDCF under - performs when increasing the number of filters ( from to ) while using the standard Caltech training set ( consistent with the observations in ) . PcaForeground improves marginally over LDCF8 . paragraph : Takeaways From figure [ reference ] we observe two overall trends . First , the more filters the merrier , with filters as sweet spot for Caltech training data . Second , there is no flagrant difference between the different filter types . section : Additional training data One caveat of the previous experiments is that as we increase the number of filters used , so does the number of features Adaboost must pick from . Since we increased the model capacity ( compared to ACF which uses a single filter ) , we consider using the Caltech10x dataset ( \u00a7 [ reference ] ) to verify that our models are not starving for data . Similar to the experiments in , we also reconsider the decision tree depth , since additional training data enables bigger models . Results for two representative methods are collected in table [ reference ] . First we observe that already with the original training data , deeper trees do provide significant improvement over level - 2 ( which was selected when tuning over INRIA data ) . Second , we notice that increasing the training data volume does provide the expected improvement only when the decision trees are deep enough . For our following experiments we choose to use level - 4 decision trees ( ) as a good balance between increased detection quality and reasonable training times . paragraph : Realboost Although previous papers on ChnFtrs detectors reported that different boosting variants all obtain equal results on this task , the recent indicated that Realboost has an edge over discrete Adaboost when additional training data is used . We observe the same behaviour in our Caltech10x setup . As summarized in table [ reference ] using filtered channels , deeper trees , additional training data , and Realboost does provide a significant detection quality boost . For the rest of the paper our models trained on Caltech10x all use level - 4 trees and RealBoost , instead of level - 2 and discrete Adaboost for the Caltech1x models . paragraph : Timing When using Caltech data ACF takes about one hour for training and one for testing . Checkerboards4x4 takes about and hours respectively . When using Caltech10x the training times for these methods augment to 2 and 29 hours , respectively . The training time does not increase proportionally with the training data volume because the hard negative mining reads a variable amount of images to attain the desired quota of negative samples . This amount increases when a detector has less false positive mistakes . subsection : Validation set experiments Based on the results in table [ reference ] we proceed to evaluate on Caltech10x the most promising configurations ( filter type and number ) from section [ reference ] . The results over the Caltech validation set are collected in table [ reference ] . We observe a clear overall gain from increasing the training data . Interestingly with enough RandomFilters we can outperform the strong performance of LDCF - Ours . We also notice that the na\u00efve Checkerboards outperforms the manual design of InformedFilters . section : Add - ons Before presenting the final test set results of our \u2018 \u2018 core \u2019 \u2019 method ( section [ reference ] ) , we also consider some possible \u2018 \u2018 add - ons \u2019 \u2019 based on the suggestions from . For the sake of evaluating complementarity , comparison with existing method , and reporting the best possible detection quality , we consider extending our detector with context and optical flow information . paragraph : Context Context is modelled via the 2Ped re - scoring method of . It is a post - processing step that merges our detection scores with the results of a two person DPM trained on the INRIA dataset ( with extended annotations ) . In the authors reported an improvement of ( percent points ) on the Caltech set , across different methods . In an improvement of is reported over their strong detector ( SquaresChnFtrs + DCT + SDt ) . In our experiments however we obtain a gain inferior to . We have also investigated fusing the 2Ped detection results via a different , more principled , fusion method . We observe consistent results : as the strength of the starting point increases , the gain from 2Ped decreases . When reaching our Checkerboards results , all gains have evaporated . We believe that the 2Ped approach is a promising one , but our experiments indicate that the used DPM template is simply too weak in comparison to our filtered channels . paragraph : Optical flow Optical flow is fed to our detector as an additional set of channels ( not filtered ) . We use the implementation from SDt which uses differences of weakly stabilized video frames . On Caltech , the authors of reported a gain over ACF ( ) , while reported a percent points improvement over their strong baseline ( SquaresChnFtrs + DCT + 2Ped \u2062%27.4MR ) . When using + SDt our results are directly comparable to Katamari and SpatialPooling + which both use optical flow too . Using our stronger Checkerboards results SDt provides a gain . Here again we observe an erosion as the starting point improves ( for confirmation , reproduced the ACF + SDt results , ) . We name our Checkerboards + SDt detector All - in - one . Our filtered channel features results are strong enough to erode existing context and flow features . Although these remain complementary cues , more sophisticated ways of extracting this information will be required to further progress in detection quality . It should be noted that despite our best efforts we could not reproduce the results from neither 2Ped nor SDt on the KITTI dataset ( in spite of its apparent similarity to Caltech ) . Effective methods for context and optical flow across datasets have yet to be shown . Our main contribution remains on the core detector ( only HOG + LUV features over local sliding window pixels in a single frame ) . section : Test set results Having done our exploration of the parameters space on the validation set , we now evaluate the most promising methods on the Caltech and KITTI test sets . paragraph : Caltech test set Figures [ reference ] and [ reference ] present our key results on the Caltech test set . For proper comparison , only methods using the same training set should be compared ( see for a similar table comparing previous methods ) . We include for comparison the baselines mentioned in section [ reference ] , Roerei [ ] the best known method trained without any Caltech images , MT - DPM [ ] the best known method based on DPM , and SDN [ ] the best known method using convolutional neural networks . We also include the top performers Katamari and SpatialPooling + . We mark as \u2018 \u2018 \u2019\u2019 both the Caltech10x training set and the one used in LDCF ( see section [ reference ] ) . paragraph : KITTI test set Figure [ reference ] presents the results on the KITTI test set ( \u2018 \u2018 moderate \u2019 \u2019 setup ) , together with all other reported methods using only monocular image content ( no stereo or LIDAR data ) . The KITTI evaluation server only recently has started receiving submissions ( for this task , in the last year ) , and thus is less prone to dataset over - fitting . We train our model on the KITTI training set using almost identical parameters as for Caltech . The only change is a subtle pre - processing step in the HOG + LUV computation . On KITTI the input image is smoothed ( radius ) before the feature channels are computed , while on Caltech we do not . This subtle change provided a ( percent points ) improvement on the KITTI validation set . subsection : Analysis With a ( percent points ) gap between ACF / InformedHaar and ACF / InformedHaar - Ours ( see figure [ reference ] ) , the results of our baselines show the importance of proper validation of training parameters ( large enough model size and negative samples ) . InformedHaar - Ours is the best reported result when training with Caltech1x . When considering methods trained on Caltech10x , we obtain a clear gap with the previous best results ( LDCF Checkerboards ) . Using our architecture and the adequate number of filters one can obtain strong results using only HOG + LUV features . The exact type of filters seems not critical , in our experiments Checkerboards4x3 gets best performance given the available training data . RandomFilters reaches the same result , but requires training and merging multiple models . Our results cut by half miss - rate of the best known convnet for pedestrian detection ( SDN [ ] ) , which in principle could learn similar low - level features and their filtering . When adding optical flow we further push the state of the art and reach , a comfortable improvement over the previous best optical flow method ( SpatialPooling + ) . This is the best reported result on this challenging dataset . The results on the KITTI dataset confirm the strength of our approach , reaching , just below the best known result on this dataset . Competing methods ( Regionlets [ ] and SpatialPooling ) both use HOG together with additional LBP and covariance features . Adding these remains a possibility for our system . Note that our results also improve over methods using LIDAR + Image , such as Fusion - DPM ( , not included in figure [ reference ] for clarity ) . section : Conclusion Through this paper we have shown that the seemingly disconnected methods ACF , ( Squares ) ChnFtrs , InformedHaar , and LDCF can be all put under the filtered channel features detectors umbrella . We have systematically explored different filter banks for such architecture and shown that they provide means for important improvements for pedestrian detection . Our results indicate that HOG + LUV features have not yet saturated , and that competitive results ( over Caltech and KITTI datasets ) can be obtained using only them . When optical flow information is added we set the new state of art for the Caltech dataset , reaching ( recall at false positive per image ) . In future work we plan to explore how the insights of this work can be exploited into a more general detection architecture such as convolutional neural networks . paragraph : Acknowledgements We thank Jan Hosang for the help provided setting up some of the experiments . We also thank Seong Joon Oh and Sabrina Hoppe for their useful comments . bibliography : References appendix : Learned model In figures [ reference ] and [ reference ] we present some qualitative aspects of the final learned models Checkerboards4x3 and RandomFilters ( see results section of main paper ) , not included in the main submission due to space limitations . In figure [ reference ] we compare the spatial distribution of our models versus a significantly weaker model ( Roerei , trained on INRIA , see figure 5 of main paper ) . We observe that our strong models focalize in similar areas than the weak Roerei model . This indicates that using filtered channels does not change which areas of the pedestrian are informative , but rather that at the same locations filtered channels are able to extract more discriminative information . In all three models we observe that diagonal oriented channels focus on left and right shoulders . The U colour channel is mainly used around the face , while L ( luminance ) and gradient magnitude ( ) channels are used all over the body . Overall head , feet , and upper torso areas provide most clues for detection . In figure [ reference ] we observe that the filters usage distribution is similar across different filter bank families . [ Filters from Roerei ( scale ) model . Copied from ] [ Final Checkerboards4x3 model ] [ Final RandomFilters model ] [ Filters used in our final Checkerboards4x3 model ] [ Filters used in our final RandomFilters model ]", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Caltech"]]], "Method": [[["Checkerboards_"]]], "Metric": [[["Reasonable_Miss_Rate"]]], "Task": [[["Pedestrian_Detection"]]]}]}
{"docid": "TST3-SREX-0010", "doctext": "document : Semi - Supervised Sequence Modeling with Cross - View Training kevclark@cs.stanford.edu , thangluong@google.com , manning@cs.stanford.edu , qvl@google.com Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text . However , the supervised models only learn from task - specific labeled data during the main training phase . We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data . On labeled examples , standard supervised learning is used . On unlabeled examples , CVT teaches auxiliary prediction modules that see restricted views of the input ( e.g. , only part of a sentence ) to match the predictions of the full model seeing the whole input . Since the auxiliary modules and the full model share intermediate representations , this in turn improves the full model . Moreover , we show that CVT is particularly effective when combined with multi - task learning . We evaluate CVT on five sequence tagging tasks , machine translation , and dependency parsing , achieving state - of - the - art results . section : Introduction Deep learning models work best when trained on large amounts of labeled data . However , acquiring labels is costly , motivating the need for effective semi - supervised learning techniques that leverage unlabeled examples . A widely successful semi - supervised learning strategy for neural NLP is pre - training word vectors Mikolov2013DistributedRO . More recent work trains a Bi - LSTM sentence encoder to do language modeling and then incorporates its context - sensitive representations into supervised models dai2015semi , peters2018deep . Such pre - training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training . A key disadvantage of pre - training is that the first representation learning phase does not take advantage of labeled data \u2013 the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task . Older semi - supervised learning algorithms like self - training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data . Self - training has historically been effective for NLP yarowsky1995unsupervised , mcclosky2006effective , but is less commonly used with neural models . This paper presents Cross - View Training ( CVT ) , a new self - training algorithm that works well for neural sequence models . In self - training , the model learns as normal on labeled examples . On unlabeled examples , the model acts as both a \u00e2\u0080\u009cteacher\u00e2\u0080\u009d that makes predictions about the examples and a \u00e2\u0080\u009cstudent\u00e2\u0080\u009d that is trained on those predictions . Although this process has shown value for some tasks , it is somewhat tautological : the model already produces the predictions it is being trained on . Recent research on computer vision addresses this by adding noise to the student \u2019s input , training the model so it is robust to input perturbations sajjadi2016regularization , wei2018improving . However , applying noise is difficult for discrete inputs like text . As a solution , we take inspiration from multi - view learning blum1998combining , Xu2013ASO and train the model to produce consistent predictions across different views of the input . Instead of only training the full model as a student , CVT adds auxiliary prediction modules \u2013 neural networks that transform vector representations into predictions \u2013 to the model and also trains them as students . The input to each student prediction module is a subset of the model \u2019s intermediate representations corresponding to a restricted view of the input example . For example , one auxiliary prediction module for sequence tagging is attached to only the \u201c forward \u201d LSTM in the model \u2019s first Bi - LSTM layer , so it makes predictions without seeing any tokens to the right of the current one . CVT works by improving the model \u2019s representation learning . The auxiliary prediction modules can learn from the full model \u2019s predictions because the full model has a better , unrestricted view of the input . As the auxiliary modules learn to make accurate predictions despite their restricted views of the input , they improve the quality of the representations they are built on top of . This in turn improves the full model , which uses the same shared representations . In short , our method combines the idea of representation learning on unlabeled data with classic self - training . CVT can be applied to a variety of tasks and neural architectures , but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi - LSTM encoder . We propose auxiliary prediction modules that work well for sequence taggers , graph - based dependency parsers , and sequence - to - sequence models . We evaluate our approach on English dependency parsing , combinatory categorial grammar supertagging , named entity recognition , part - of - speech tagging , and text chunking , as well as English to Vietnamese machine translation . CVT improves over previously published results on all these tasks . Furthermore , CVT can easily and effectively be combined with multi - task learning : we just add additional prediction modules for the different tasks on top of the shared Bi - LSTM encoder . Training a unified model to jointly perform all of the tasks except machine translation improves results ( outperforming a multi - task ELMo model ) while decreasing the total training time . section : Cross - View Training We first present Cross - View Training and describe how it can be combined effectively with multi - task learning . See Figure [ reference ] for an overview of the training method . subsection : Method Let represent a labeled dataset and represent an unlabeled dataset We use to denote the output distribution over classes produced by the model with parameters on input . During CVT , the model alternates learning on a minibatch of labeled examples and learning on a minibatch of unlabeled examples . For labeled examples , CVT uses standard cross - entropy loss : CVT adds auxiliary prediction modules to the model , which are used when learning on unlabeled examples . A prediction module is usually a small neural network ( e.g. , a hidden layer followed by a softmax layer ) . Each one takes as input an intermediate representation produced by the model ( e.g. , the outputs of one of the LSTMs in a Bi - LSTM model ) . It outputs a distribution over labels . Each is chosen such that it only uses a part of the input ; the particular choice can depend on the task and model architecture . We propose variants for several tasks in Section [ reference ] . The auxiliary prediction modules are only used during training ; the test - time prediction come from the primary prediction module that produces . On an unlabeled example , the model first produces soft targets by performing inference . CVT trains the auxiliary prediction modules to match the primary prediction module on the unlabeled data by minimizing where is a distance function between probability distributions ( we use KL divergence ) . We hold the primary module \u2019s prediction fixed during training ( i.e. , we do not back - propagate through it ) so the auxiliary modules learn to imitate the primary one , but not vice versa . CVT works by enhancing the model \u2019s representation learning . As the auxiliary modules train , the representations they take as input improve so they are useful for making predictions even when some of the model \u2019s inputs are not available . This in turn improves the primary prediction module , which is built on top of the same shared representations . We combine the supervised and CVT losses into the total loss , , and minimize it with stochastic gradient descent . In particular , we alternate minimizing over a minibatch of labeled examples and minimizing over a minibatch of unlabeled examples . For most neural networks , adding a few additional prediction modules is computationally cheap compared to the portion of the model building up representations ( such as an RNN or CNN ) . Therefore our method contributes little overhead to training time over other self - training approaches for most tasks . CVT does not change inference time or the number of parameters in the fully - trained model because the auxiliary prediction modules are only used during training . subsection : Combining CVT with Multi - Task Learning CVT can easily be combined with multi - task learning by adding additional prediction modules for the other tasks on top of the shared Bi - LSTM encoder . During supervised learning , we randomly select a task and then update using a minibatch of labeled data for that task . When learning on the unlabeled data , we optimize jointly across all tasks at once , first running inference with all the primary prediction modules and then learning from the predictions with all the auxiliary prediction modules . As before , the model alternates training on minibatches of labeled and unlabeled examples . Examples labeled across many tasks are useful for multi - task systems to learn from , but most datasets are only labeled with one task . A benefit of multi - task CVT is that the model creates ( artificial ) all - tasks - labeled examples from unlabeled data . This significantly improves the model \u2019s data efficiency and training time . Since running prediction modules is computationally cheap , computing is not much slower for many tasks than it is for a single one . However , we find the all - tasks - labeled examples substantially speed up model convergence . For example , our model trained on six tasks takes about three times as long to converge as the average model trained on one task , a 50 % decrease in total training time . section : Cross - View Training Models CVT relies on auxiliary prediction modules that have restricted views of the input . In this section , we describe specific constructions of the auxiliary prediction modules that are effective for sequence tagging , dependency parsing , and sequence - to - sequence learning . subsection : Bi - LSTM Sentence Encoder All of our models use a two - layer CNN - BiLSTM chiu2015named , ma2016end sentence encoder . It takes as input a sequence of words . First , each word is represented as the sum of an embedding vector and the output of a character - level Convolutional Neural Network , resulting in a sequence of vectors . The encoder applies a two - layer bidirectional LSTM graves2005framewise to these representations . The first layer runs a Long Short - Term Memory unit hochreiter1997long in the forward direction ( taking as input at each step ) and the backward direction ( taking at each step ) to produce vector sequences and . The output of the Bi - LSTM is the concatenation of these vectors : The second Bi - LSTM layer works the same , producing outputs , except it takes as input instead of . subsection : CVT for Sequence Tagging In sequence tagging , each token has a corresponding label . The primary prediction module for sequence tagging produces a probability distribution over classes for the label using a one - hidden - layer neural network applied to the corresponding encoder outputs : The auxiliary prediction modules take and , the outputs of the forward and backward LSTMs in the first Bi - LSTM layer , as inputs . We add the following four auxiliary prediction modules to the model ( see Figure [ reference ] ) : The \u201c forward \u201d module makes each prediction without seeing the right context of the current token . The \u201c future \u201d module makes each prediction without the right context or the current token itself . Therefore it works like a neural language model that , instead of predicting which token comes next in the sequence , predicts which class of token comes next . The \u201c backward \u201d and \u201c past \u201d modules are analogous . subsection : CVT for Dependency Parsing In a dependency parse , words in a sentence are treated as nodes in a graph . Typed directed edges connect the words , forming a tree structure describing the syntactic structure of the sentence . In particular , each word in a sentence receives exactly one in - going edge going from word ( called the \u201c head \u201d ) to it ( the \u201c dependent \u201d ) of type ( the \u201c relation \u201d ) . We use a graph - based dependency parser similar to the one from Dozat2017Deep . This treats dependency parsing as a classification task where the goal is to predict which in - going edge connects to each word . First , the representations produced by the encoder for the candidate head and dependent are passed through separate hidden layers . A bilinear classifier applied to these representations produces a score for each candidate edge . Lastly , these scores are passed through a softmax layer to produce probabilities . Mathematically , the probability of an edge is given as : where is the scoring function : The bilinear classifier uses a weight matrix specific to the candidate relation as well as a weight matrix shared across all relations . Note that unlike in most prior work , our dependency parser only takes words as inputs , not words and part - of - speech tags . We add four auxiliary prediction modules to our model for cross - view training : Each one has some missing context ( not seeing either the preceding or following words ) for the candidate head and candidate dependent . subsection : CVT for Sequence - to - Sequence Learning We use an encoder - decoder sequence - to - sequence model with attention sutskever2014sequence , bahdanau2014neural . Each example consists of an input ( source ) sequence and output ( target ) sequence . The encoder \u2019s representations are passed into an LSTM decoder using a bilinear attention mechanism Luong2015EffectiveAT . In particular , at each time step the decoder computes an attention distribution over source sequence hidden states as where is the decoder \u2019s current hidden state . The source hidden states weighted by the attention distribution form a context vector : . Next , the context vector and current hidden state are combined into an attention vector . Lastly , a softmax layer predicts the next token in the output sequence : . We add two auxiliary decoders when applying CVT . The auxiliary decoders share embedding and LSTM parameters with the primary decoder , but have different parameters for the attention mechanisms and softmax layers . For the first one , we restrict its view of the input by applying attention dropout , randomly zeroing out a fraction of its attention weights . The second one is trained to predict the next word in the target sequence rather than the current one : . Since there is no target sequence for unlabeled examples , we can not apply teacher forcing to get an output distribution over the vocabulary from the primary decoder at each time step . Instead , we produce hard targets for the auxiliary modules by running the primary decoder with beam search on the input sequence . This idea has previously been applied to sequence - level knowledge distillation by Kim2016SequenceLevelKD and makes the training procedure similar to back - translation Sennrich2016ImprovingNM . section : Experiments We compare Cross - View Training against several strong baselines on seven tasks : Combinatory Categorial Grammar ( CCG ) Supertagging : We use data from CCGBank hockenmaier2007ccgbank . Text Chunking : We use the CoNLL - 2000 data tjong2000introduction . Named Entity Recognition ( NER ) : We use the CoNLL - 2003 data tjong2003introduction . Fine - Grained NER ( FGN ) : We use the OntoNotes hovy2006ontonotes dataset . Part - of - Speech ( POS ) Tagging : We use the Wall Street Journal portion of the Penn Treebank marcus1993building . Dependency Parsing : We use the Penn Treebank converted to Stanford Dependencies version 3.3.0 . Machine Translation : We use the English - Vietnamese translation dataset from IWSLT 2015 iwslt15 . We report ( tokenized ) BLEU scores on the tst2013 test set . We use the 1 Billion Word Language Model Benchmark chelba2013one as a pool of unlabeled sentences for semi - supervised learning . subsection : Model Details and Baselines We apply dropout during training , but not when running the primary prediction module to produce soft targets on unlabeled examples . In addition to the auxiliary prediction modules listed in Section [ reference ] , we find it slightly improves results to add another one that sees the whole input rather than a subset ( but unlike the primary prediction module , does have dropout applied to its representations ) . Unless indicated otherwise , our models have LSTMs with 1024 - sized hidden states and 512 - sized projection layers . See the appendix for full training details and hyperparameters . We compare CVT with the following other semi - supervised learning algorithms : Word Dropout . In this method , we only train the primary prediction module . When acting as a teacher it is run as normal , but when acting as a student , we randomly replace some of the input words with a REMOVED token . This is similar to CVT in that it exposes the model to a restricted view of the input . However , it is less data efficient . By carefully designing the auxiliary prediction modules , it is possible to train the auxiliary prediction modules to match the primary one across many different views of the input a once , rather than just one view at a time . Virtual Adversarial Training ( VAT ) . VAT miyato2015distributional works like word dropout , but adds noise to the word embeddings of the student instead of dropping out words . Notably , the noise is chosen adversarially so it most changes the model \u2019s prediction . This method was applied successfully to semi - supervised text classification by miyato2016adversarial . ELMo . ELMo incorporates the representations from a large separately - trained language model into a task - specific model . Our implementaiton follows peters2018deep . When combining ELMo with multi - task learning , we allow each task to learn its own weights for the ELMo embeddings going into each prediction module . We found applying dropout to the ELMo embeddings was crucial for achieving good performance . subsection : Results Results are shown in Table [ reference ] . CVT on its own outperforms or is comparable to the best previously published results on all tasks . Figure [ reference ] shows an example win for CVT over supervised learning . . Of the prior results listed in Table [ reference ] , only TagLM and ELMo are semi - supervised . These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier . Our base models use 1024 hidden units in their LSTMs ( compared to 4096 in ELMo ) , require fewer training steps ( around one pass over the billion - word benchmark rather than many passes ) , and do not require a pipelined training procedure . Therefore , although they perform on par with ELMo , they are faster and simpler to train . Increasing the size of our CVT + Multi - task model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo + Multi - task ones . We suspect there could be further gains from combining our method with language model pre - training , which we leave for future work . CVT + Multi - Task . We train a single shared - encoder CVT model to perform all of the tasks except machine translation ( as it is quite different and requires more training time than the other ones ) . Multi - task learning improves results on all of the tasks except fine - grained NER , sometimes by large margins . Prior work on many - task NLP such as hashimoto2016joint uses complicated architectures and training algorithms . Our result shows that simple parameter sharing can be enough for effective many - task learning when the model is big and trained on a large amount of data . Interestingly , multi - task learning works better in conjunction with CVT than with ELMo . We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors , which perhaps hinders the model from learning effective representations that transfer across tasks . We also believe CVT alleviates the danger of the model \u201c forgetting \u201d one task while training on the other ones , a well - known problem in many - task learning Kirkpatrick2017OvercomingCF . During multi - task CVT , the model makes predictions about unlabeled examples across all tasks , creating ( artificial ) all - tasks - labeled examples , so the model does not only see one task at a time . In fact , multi - task learning plus self training is similar to the Learning without Forgetting algorithm Li2016LearningWF , which trains the model to keep its predictions on an old task unchanged when learning a new task . To test the value of all - tasks - labeled examples , we trained a multi - task CVT model that only computes on one task at a time ( chosen randomly for each unlabeled minibatch ) instead of for all tasks in parallel . The one - at - a - time model performs substantially worse ( see Table [ reference ] ) . Model Generalization . In order to evaluate how our models generalize to the dev set from the train set , we plot the dev vs. train accuracy for our different methods as they learn ( see Figure [ reference ] ) . Both CVT and multi - task learning improve model generalization : for the same train accuracy , the models get better dev accuracy than purely supervised learning . Interestingly , CVT continues to improve in dev set accuracy while close to 100 % train accuracy for CCG , Chunking , and NER , perhaps because the model is still learning from unlabeled data even when it has completely fit to the train set . We also show results for a smaller multi - task + CVT model . Although it generalizes at least as well as the larger one , it halts making progress on the train set earlier . This suggests it is important to use sufficiently large neural networks for multi - task learning : otherwise the model does not have the capacity to fit to all the training data . Auxiliary Prediction Module Ablation . We briefly explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table [ reference ] . We find that both kinds of auxiliary prediction modules improve performance , but that the future and past modules improve results more than the forward and backward ones , perhaps because they see a more restricted and challenging view of the input . Training Models on Small Datasets . We explore how CVT scales with dataset size by varying the amount of training data the model has access to . Unsurprisingly , the improvement of CVT over purely supervised learning grows larger as the amount of labeled data decreases ( see Figure [ reference ] , left ) . Using only 25 % of the labeled data , our approach already performs as well or better than a fully supervised model using 100 % of the training data , demonstrating that CVT is particularly useful on small datasets . Training Larger Models . Most sequence taggers and dependency parsers in prior work use small LSTMs ( hidden state sizes of around 300 ) because larger models yield little to no gains in performance reimers2017reporting . We found our own supervised approaches also do not benefit greatly from increasing the model size . In contrast , when using CVT accuracy scales better with model size ( see Figure [ reference ] , right ) . This finding suggests the appropriate semi - supervised learning methods may enable the development of larger , more sophisticated models for NLP tasks with limited amounts of labeled data . Generalizable Representations . Lastly , we explore training the CVT + multi - task model on five tasks , freezing the encoder , and then only training a prediction module on the sixth task . This tests whether the encoder \u2019s representations generalize to a new task not seen during its training . Only training the prediction module is very fast because ( 1 ) the encoder ( which is by far the slowest part of the model ) has to be run over each example only once and ( 2 ) we do not back - propagate into the encoder . Results are shown in Table [ reference ] . Training only a prediction module on top of multi - task representations works remarkably well , outperforming ELMo embeddings and sometimes even a vanilla supervised model , showing the multi - task model is building up effective representations for language . In particular , the representations could be used like skip - thought vectors kiros2015skip to quickly train models on new tasks without slow representation learning . section : Related Work Unsupervised Representation Learning . Early approaches to deep semi - supervised learning pre - train neural models on unlabeled data , which has been successful for applications in computer vision jarrett2009best , lecun2010convolutional and NLP . Particularly noteworthy for NLP are algorithms for learning effective word embeddings and language model pretraining dai2015semi , ramachandran2016unsupervised , peters2018deep , howard2018universal , radford2018improving . Pre - training on other tasks such as machine translation has also been studied McCann2017LearnedIT . Other approaches train \u201c thought vectors \u201d representing sentences through unsupervised or supervised learning . Self - Training . One of the earliest approaches to semi - supervised learning is self - training scudder1965probability , which has been successfully applied to NLP tasks such as word - sense disambiguation yarowsky1995unsupervised and parsing mcclosky2006effective . In each round of training , the classifier , acting as a \u201c teacher , \u201d labels some of the unlabeled data and adds it to the training set . Then , acting as a \u201c student , \u201d it is retrained on the new training set . Many recent approaches ( including the consistentency regularization methods discussed below and our own method ) train the student with soft targets from the teacher \u2019s output distribution rather than a hard label , making the procedure more akin to knowledge distillation hinton2015distilling . It is also possible to use multiple models or prediction modules for the teacher , such as in tri - training zhou2005tri , ruder2018strong . Consistency Regularization . Recent works add noise ( e.g. , drawn from a Gaussian distribution ) or apply stochastic transformations ( e.g. , horizontally flipping an image ) to the student \u2019s inputs . This trains the model to give consistent predictions to nearby data points , encouraging distributional smoothness in the model . Consistency regularization has been very successful for computer vision applications bachman2014learning , laine2016temporal , tarvainen2017weight . However , stochastic input alterations are more difficult to apply to discrete data like text , making consistency regularization less used for natural language processing . One solution is to add noise to the model \u2019s word embeddings miyato2016adversarial ; we compare against this approach in our experiments . CVT is easily applicable to text because it does not require changing the student \u2019s inputs . Multi - View Learning . Multi - view learning on data where features can be separated into distinct subsets has been well studied Xu2013ASO . Particularly relevant are co - training blum1998combining and co - regularization Sindhwani2005ACA , which trains two models with disjoint views of the input . On unlabeled data , each one acts as a \u201c teacher \u201d for the other model . In contrast to these methods , our approach trains a single unified model where auxiliary prediction modules see different , but not necessarily independent views of the input . Self Supervision . Self - supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human - provided labels . Recent work has jointly trained image classifiers with tasks like relative position and colorization doersch2017multi , sequence taggers with language modeling rei2017semi , and reinforcement learning agents with predicting changes in the environment jaderberg2016reinforcement . Unlike these approaches , our auxiliary losses are based on self - labeling , not labels deterministically constructed from the input . Multi - Task Learning . There has been extensive prior work on multi - task learning Caruana1997MultitaskL , Ruder2017AnOO . For NLP , most work has focused on a small number of closely related tasks Luong2015MultitaskST , zhang2016stack , Sgaard2016DeepML , Peng2017DeepML . Many - task systems are less commonly developed . Collobert2008AUA propose a many - task system sharing word embeddings between the tasks , hashimoto2016joint train a many - task model where the tasks are arranged hierarchically according to their linguistic level , and subramanian2018learning train a shared - encoder many - task model for the purpose of learning better sentence representations for use in downstream tasks , not for improving results on the original tasks . section : Conclusion We propose Cross - View Training , a new method for semi - supervised learning . Our approach allows models to effectively leverage their own predictions on unlabeled data , training them to produce effective representations that yield accurate predictions even when some of the input is not available . We achieve excellent results across seven NLP tasks , especially when CVT is combined with multi - task learning . section : Acknowledgements We thank Abi See , Christopher Clark , He He , Peng Qi , Reid Pryzant , Yuaho Zhang , and the anonymous reviewers for their thoughtful comments and suggestions . We thank Takeru Miyato for help with his virtual adversarial training code and Emma Strubell for answering our questions about OntoNotes NER . Kevin is supported by a Google PhD Fellowship . bibliography : References appendix : Detailed Results We provide a more detailed version of the test set results in the paper , adding two decimals of precision , standard deviations of the 5 runs for each model , and more prior work , in Table [ reference ] . appendix : Model Details Our models use two layer CNN - BiLSTM encoders chiu2015named , ma2016end , lample2016neural and task - specific prediction modules . See Section [ reference ] of the paper for details . We provide a few minor details not covered there below . Sequence Tagging . For Chunking and Named Entity Recognition , we use a BIOES tagging scheme . We apply label smoothing szegedy2016rethinking , pereyra2017regularizing with a rate of 0.1 to the target labels when training on the labeled data . Dependency Parsing . We omit punctuation from evaluation , which is standard practice for the PTB - SD 3.3.0 dataset . ROOT is represented with a fixed vector instead of using a vector from the encoder , but otherwise dependencies coming from ROOT are scored the same way as the other dependencies . Machine Translation . We apply dropout to the output of each LSTM layer in the decoder . Our implementation is heavily based off of the Google NMT Tutorial luong17 . We attribute our significantly better results to using pre - trained word embeddings , a character - level CNN , a larger model , stronger regularization , and better hyperparameter tuning . Target words occurring 5 or fewer times in the train set are replaced with a UNK token ( but not during evaluation ) . We use a beam size of 10 when performing beam search . We found it slightly beneficial to apply label smoothing with a rate of 0.1 to the teacher \u2019s predictions ( unlike our other tasks , the teacher only provides hard targets to the students for translation ) . Multi - Task Learning . Several of our datasets are constructed from the Penn Treebank . However , we treat them as separate rather than providing examples labeled across multiple tasks to our model during supervised training . Furthermore , the Penn Treebank tasks do not all use the same train / dev / test splits . We ensure the training split of one task never overlaps the evaluation split of another by discarding the overlapping examples from the train sets . Other Details . We apply dropout hinton2012improving to the word embeddings and outputs of each Bi - LSTM . We use an exponential - moving - average ( EMA ) of the model weights from training for the final model ; we found this to slightly improve accuracy and significantly reduce the variance in accuracy between models trained with different random initializations . The model is trained using SGD with momentum polyak1964some , sutskever2013importance . Word embeddings are initialized with GloVe vectors pennington2014glove and fine - tuned during training . The full set of model hyperparameters are listed in Table [ reference ] . Baselines . Baselines were run with the same architecture and hyperparameters as the CVT model . For the \u201c word dropout \u201d model , we randomly replace words in the input sentence with a REMOVED token with probability 0.1 ( this value worked well on the dev sets ) . For Virtual Adversarial Training , we set the norm of the perturbation to be 1.5 for CCG , 1.0 for Dependency Parsing , and 0.5 for the other tasks ( these values worked best on the dev sets ) . Otherwise , the implementation is as described in miyato2016adversarial ; we based our implementation off of their code . We were unable to successfully apply VAT to machine translation , perhaps because the student is provided hard targets for that task . For ELMo , we applied dropout to the ELMo embeddings before they are incorporated into the rest of the model . When training the multi - task ELMo model , each prediction module has its own set of softmax - normalized weights ( in peters2018deep ) for the ELMo emeddings going into the task - specific prediction modules . All tasks share the same weights for the ELMo embeddings going into the shared Bi - LSTM encoder . appendix : CVT for Image Recognition Although the focus of our work is on NLP , we also applied CVT to image recognition and found it performs competitively with existing methods . Most of the semi - supervised image recognition approaches we compare against rely on the inputs being continuous , so they would be difficult to apply to text . More specifically , consistency regularization methods sajjadi2016regularization , laine2016temporal , miyato2017virtual rely on adding continuous noise and applying image - specific transformations like cropping to inputs , GANs salimans2016improved , wei2018improving are very difficult to train on text due to its discrete nature , and mixup zhang2017mixup , verma2018manifold requires a way of smoothly interpolating between different inputs . Approach . Our image recognition models are based on Convolutional Neural Networks , which produce a set of features from an image . The first two dimensions of index into the spatial coordinates of feature vectors and is the size of the feature vectors . For shallower CNNs , a particular feature vector corresponds to a region of the input image . For example , would be a - dimensional vector of features extracted from the upper left corner . For deeper CNNs , a particular feature vector would be extracted from the whole image , but still only use a \u201c region \u201d of the representations from an earlier layer . The CNNs in our experiments are all in the first category . The primary prediction layers of our CNNs take as input the mean of over the first two dimensions , which results in a - dimensional vector that is fed into a softmax layer : We add auxiliary prediction layers to the top of the CNN . The th layer takes a single feature vector as input : Data . We evaluated our models on the CIFAR - 10 krizhevsky2009learning dataset . Following previous work , we make the datasets semi - supervised by only using the provided labels for a subset of the examples in the training set ; the rest are treated as unlabeled examples . Model . We use the convolutional neural network from miyato2017virtual , adapting their TensorFlow implementation . Their model contains 9 convolutional layers and 2 max pooling layers . See Appendix D of Miyato et al . \u2019s paper for more details . We add 36 auxiliary softmax layers to the collection of feature vectors produced by the CNN . Each auxiliary layer sees a patch of the image ranging in size from pixels ( the corner ) to pixels ( the center ) of the pixel images . For some experiments , we combine CVT with standard consistency regularization by adding a perturbation ( e.g. , a small random vector ) to the student \u2019s inputs when computing . Results . The results are shown in Table [ reference ] . Unsurprisingly , adding continuous noise to the inputs works much better with images , where the inputs are naturally continuous , than with language . Therefore we see much better results from VAT on semi - supervised CIFAR - 10 compared to on our NLP tasks . However , we still find incorporating CVT improves over models without CVT . Our CVT + VAT models are competitive with current start - of - the - art approaches . We found the gains from CVT are larger when no data augmentation is applied , perhaps because random translations of the input expose the model to different \u201c views \u201d in a similar manner as with CVT . appendix : Negative Results We briefly describe a few ideas we implemented that did not seem to be effective in initial experiments . Note these findings are from early one - off experiments . We did not pursue them further after our first attempts did not pan out , so it is possible that some of these approaches could be effective with the proper adjustments and tuning . Hard vs soft targets : Classic self - training algorithms train the student model with one - hot \u201c hard \u201d targets corresponding to the teacher \u2019s highest probability prediction . In our experiments , this decreased performance compared to using soft targets . This finding is consistent with research on knowledge distillation hinton2015distilling , furlanello2018born where soft targets also work notably better than hard targets . Confidence thresholding : Classic self - training often only trains the student on a subset of the unlabeled examples on which the teacher has confident predictions ( i.e. , the output distribution has low entropy ) . We tried both \u201c hard \u201d ( where the student ignores low - confidence examples ) and \u201c soft \u201d ( where examples are weighted according to the teacher \u2019s confidence ) versions of this for training our models , but they did not seem to improve performance . Mean Teacher : The Mean Teacher method tarvainen2017weight tracks an exponential moving average ( EMA ) of model weights , which are used to produce targets for the students . The idea is that these targets may be better quality due to a self - ensembling effect . However , we found this approach to have little to no benefit in our experiments , although using EMA model weights at test time did improve results slightly . Purely supervised CVT : Lastly , we explored adding cross - view losses to purely supervised classifiers . We hoped that adding auxiliary softmax layers with different views of the input would act as a regularizer on the model . However , we found little to no benefit from this approach . This negative result suggests that the gains from CVT are from the improved semi - supervised learning mechanism , not the additional prediction layers regularizing the model .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["CCGBank"]]], "Method": [[["Clark_et_al_"]]], "Metric": [[["Accuracy"]]], "Task": [[["CCG_Supertagging"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CoNLL_2003__English_"]]], "Method": [[["CVT___Multi-Task"]]], "Metric": [[["F1"]]], "Task": [[["Named_Entity_Recognition__NER_"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Ontonotes_v5__English_"]]], "Method": [[["CVT___Multi-Task"]]], "Metric": [[["F1"]]], "Task": [[["Named_Entity_Recognition__NER_"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["CVT___Multi-Task"]]], "Metric": [[["LAS"]]], "Task": [[["Dependency_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["CVT___Multi-Task"]]], "Metric": [[["POS"]]], "Task": [[["Dependency_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["CVT___Multi-Task"]]], "Metric": [[["UAS"]]], "Task": [[["Dependency_Parsing"]]]}]}
{"docid": "TST3-SREX-0011", "doctext": "Skip - gram Language Modeling Using Sparse Non - negative Matrix Probability Estimation section : Abstract We present a novel family of language model ( LM ) estimation techniques named Sparse Non - negative Matrix ( SNM ) estimation . A first set of experiments empirically evaluating it on the One Billion Word Benchmark [ reference ] shows that SNM n - gram LMs perform almost as well as the well - established Kneser - Ney ( KN ) models . When using skip - gram features the models are able to match the state - of - the - art recurrent neural network ( RNN ) LMs ; combining the two modeling techniques yields the best known result on the benchmark . The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength , promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n - gram LMs do . section : Introduction A statistical language model estimates probability values P ( W ) for strings of words W in a vocabulary V whose size is in the tens , hundreds of thousands and sometimes even millions . Typically the string W is broken into sentences , or other segments such as utterances in automatic speech recognition , which are often assumed to be conditionally independent ; we will assume that W is such a segment , or sentence . Estimating full sentence language models is computationally hard if one seeks a properly normalized probability model 1 over strings of words of finite length in [ reference ] We note that in some practical systems the constraint on using a properly normalized language V * . A simple and sufficient way to ensure proper normalization of the model is to decompose the sentence probability according to the chain rule and make sure that the end - of - sentence symbol < / s > is predicted with non - zero probability in any context . With W = w 1 , w 2 , . . . , w n we get : P ( w i |w 1 , w 2 , . . . , w i\u22121 ) Since the parameter space of P ( w k |w 1 , w 2 , . . . , w k\u22121 ) is too large , the language model is forced to put the context W k\u22121 = w 1 , w 2 , . . . , w k\u22121 into an equivalence class determined by a function \u03a6 ( W k\u22121 ) . As a result , Research in language modeling consists of finding appropriate equivalence classifiers \u03a6 and methods to estimate P ( w k |\u03a6 ( W k\u22121 ) ) . The most successful paradigm in language modeling uses the ( n \u2212 1 )- gram equivalence classification , that is , defines \u03a6 ( W k\u22121 ) . = w k\u2212n + 1 , w k\u2212n + 2 , . . . , w k\u22121 Once the form \u03a6 ( W k\u22121 ) is specified , only the problem of estimating P ( w k |\u03a6 ( W k\u22121 ) ) from training data remains . section : Perplexity as a Measure of Language Model Quality A statistical language model can be evaluated by how well it predicts a string of symbols W t - commonly referred to as test data - generated by the source to be modeled . A commonly used quality measure for a given model M is related to the entropy of the underlying source and was introduced under the name of perplexity ( PPL ) : For an excellent discussion on the use of perplexity in statistical language modeling , as well as various estimates for the entropy of English the reader is referred to [ reference ] , Section 8.4 , pages 141 - 142 and the additional reading suggested in Section 8.5 of the same book . model is side - stepped at a gain in modeling power and simplicity . Very likely , not all words in the test string W t are part of the language model vocabulary . It is common practice to map all words that are out - of - vocabulary to a distinguished unknown word symbol , and report the out - of - vocabulary ( OOV ) rate on test data - the rate at which one encounters OOV words in the test string W tas yet another language model performance metric besides perplexity . Usually the unknown word is assumed to be part of the language model vocabulary - open vocabulary language models - and its occurrences are counted in the language model perplexity calculation , Eq . ( 3 ) . A situation less common in practice is that of closed vocabulary language models where all words in the test data will always be part of the vocabulary V. section : Skip - gram Language Modeling Recently , neural network ( NN ) smoothing [ reference ] , [ reference ] , [ reference ] , and in particular recurrent neural networks [ reference ] ( RNN ) have shown excellent performance in language modeling [ reference ] . Their excellent performance is attributed to a combination of leveraging long - distance context , and training a vector representation for words . Another simple way of leveraging long distance context is to use skip - grams . In our approach , a skip - gram feature extracted from the context W k\u22121 is characterized by the tuple ( r , s , a ) where : \u2022 r denotes number of remote context words \u2022 s denotes the number of skipped words \u2022 a denotes the number of adjacent context words relative to the target word w k being predicted . For example , in the sentence , < S > The quick brown fox jumps over the lazy dog < / S > a ( 1 , 2 , 3 ) skip - gram feature for the target word dog is : [ brown skip - 2 over the lazy ] For performance reasons , it is recommended to limit s and to limit either ( r + a ) or limit both r and s ; not setting any limits will result in events containing a set of skip - gram features whose total representation size is quintic in the length of the sentence . We configure the skip - gram feature extractor to produce all features f , defined by the equivalence class \u03a6 ( W k\u22121 ) , that meet constraints on the minimum and maximum values for : \u2022 the number of context words used r + a ; \u2022 the number of remote words r ; \u2022 the number of adjacent words a ; \u2022 the skip length s. We also allow the option of not including the exact value of s in the feature representation ; this may help with smoothing by sharing counts for various skip features . Tied skip - gram features will look like : [ curiousity skip - * the cat ] In order to build a good probability estimate for the target word w k in a context W k\u22121 we need a way of combining an arbitrary number of skip - gram features f k\u22121 , which do not fall into a simple hierarchy like regular n - gram features . The following section describes a simple , yet novel approach for combining such predictors in a way that is computationally easy , scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view . section : Sparse Non - negative Matrix Modeling section : Model definition In the Sparse Non - negative Matrix ( SNM ) paradigm , we represent the training data as a sequence of events E = e 1 , e 2 , ... where each event e \u2208 E consists of a sparse non - negative feature vector f and a sparse non - negative target word vector t. Both vectors are binary - valued , indicating the presence or absence of a feature or target words , respectively . Hence , the training data consists of |E||P os ( f ) | positive and |E||P os ( f ) | ( |V| \u2212 1 ) negative training examples , where P os ( f ) denotes the number of positive elements in the vector f . A language model is represented by a non - negative matrix M that , when applied to a given feature vector f , produces a dense prediction vector y : Upon evaluation , we normalize y such that we end up with a conditional probability distribution P M ( t|f ) for a model M. For each word w \u2208 V that corresponds to index j in t , and its feature vector f that is defined by the equivalence class \u03a6 applied to the history h ( w ) of that word in a text , the conditional probability P M ( w|\u03a6 ( h ( w ) ) ) then becomes : For convenience , we will write P ( t j |f ) instead of P M ( t j |f ) in the rest of the paper . As required by the denominator in Eq . ( 5 ) , this computation involves summing over all of the present features for the entire vocabulary . However , if we precompute the row sums |V| u=1 M iu and store them together with the model , the evaluation can be done very efficiently in only |P os ( f ) | time . Moreover , only the positive entries in M i need to be considered , making the range of the sum sparse . section : Adjustment function and metafeatures We let the entries of M be a slightly modified version of the relative frequencies : where C is a feature - target count matrix , computed over the entire training corpus and A ( i , j ) is a real - valued function , dubbed adjustment function . For each featuretarget pair ( f i , t j ) , the adjustment function extracts k new features \u03b1 k , called metafeatures , which are hashed as keys to store corresponding weights \u03b8 ( hash ( \u03b1 k ) ) in a huge hash table . To limit memory usage , we use a flat hash table and allow collisions , although this has the potentially undesirable effect of tying together the weights of different metafeatures . Computing the adjustment function for any ( f i , t j ) then amounts to summing the weights that correspond to its metafeatures : From the given input features , such as regular n - grams and skip n - grams , we construct our metafeatures as conjunctions of any or all of the following elementary metafeatures : \u2022 feature identity , e.g. [ brown skip - 2 over the lazy ] \u2022 feature type , e.g. ( 1 , 2 , 3 ) skip - grams \u2022 feature count C i * \u2022 target identity , e.g. dog where we reused the example from Section 2 . Note that the seemingly absent feature - target identity is represented by the conjunction of the feature identity and the target identity . Since the metafeatures may involve the feature count and feature - target count , in the rest of the paper we will write \u03b1 k ( i , j , C i * , C ij ) . This will become important later when we discuss leave - one - out training . Each elementary metafeature is joined with the others to form more complex metafeatures which in turn are joined with all the other elementary and complex metafeatures , ultimately ending up with all 2 5 \u2212 1 possible combinations of metafeatures . Before they are joined , count metafeatures are bucketed together according to their ( floored ) log 2 value . As this effectively puts the lowest count values , of which there are many , into a different bucket , we optionally introduce a second ( ceiled ) bucket to assure smoother transitions . Both buckets are then weighted according to the log 2 fraction lost by the corresponding rounding operation . Note that if we apply double bucketing to both the feature and feature - target count , the amount of metafeatures per input feature becomes 2 7 \u2212 1 . We will come back to these metafeatures in Section 4.4 where we examine their individual effect on the model . section : Loss function Estimating a model M corresponds to finding optimal weights \u03b8 k for all the metafeatures for all events in such a way that the average loss over all events between the target vector t and the prediction vector y is minimized , according to some loss function L. The most natural choice of loss function is one that is based on the multinomial distribution . That is , we consider t to be multinomially distributed with |V| possible outcomes . The loss function L multi then is : Another possibility is the loss function based on the Poisson distribution 2 : we consider each t j in t to be Poisson distributed with parameter y j . The conditional probability of P P oisson ( t|f ) then is : and the corresponding Poisson loss function is : where we dropped the last term , since t j is binary - valued 3 . Although this choice is not obvious in the context of language modeling , it is well suited to gradient - based optimization and , as we will see , the experimental results are in fact excellent . section : Model Estimation The adjustment function is learned by applying stochastic gradient descent on the loss function . That is , for each feature - target pair ( f i , t j ) in each event we need to update the parameters of the metafeatures by calculating the gradient with respect to the adjustment function . For the multinomial loss , this gradient is : The problem with this update rule is that we need to sum over the entire vocabulary V in the denominator . For most features f i , this is not a big deal as C iu = 0 , but some features occur with many if not all targets e.g. the empty feature for unigrams . Although we might be able to get away with this by re - using these sums and applying them to many / all events in a mini batch , we chose to work with the Poisson loss in our first implementation . If we calculate the gradient of the Poisson loss , we get the following : If we were to apply this gradient to each ( positive and negative ) training example , it would be computationally too expensive , because even though the second term is zero for all the negative training examples , the first term needs to be computed for all |E||P os ( f ) ||V| training examples . However , since the first term does not depend on y j , we are able to distribute the updates for the negative examples over the positive ones by adding in gradients for a fraction of the events where f i = 1 , but t j = 0 . In particular , instead of adding the term f i M ij , we add f i t j which lets us update the gradient only on positive examples . We note that this update is only strictly correct for batch training , and not for online training since M ij changes after each update . Nonetheless , we found this to yield good results as well as seriously reducing the computational cost . The online gradient applied to each training example then becomes : which is non - zero only for positive training examples , hence speeding up computation by a factor of |V|. These aggregated gradients however do not allow us to use additional data to train the adjustment function , since they tie the update computation to the relative frequencies . Instead , we have to resort to leave - one - out training to prevent the model from overfitting the training data . We do this by excluding the event , generating the gradients , from the counts used to compute those gradients . So , for each positive example ( f i , t j ) of each event e = ( f , t ) , we compute the gradient , excluding f i from C i * and f i t j from C ij . For the gradients of the negative examples on the other hand we only exclude f i from C i * and we leave C ij untouched , since here we did not observe t j . In order to keep the aggregate computation of the gradients for the negative examples , we distribute them uniformly over all the positive examples with the same feature ; each of the C ij positive examples will then compute the gradient of negative examples . To summarize , when we do leave - one - out training we apply the following gradient update rule on all positive training examples : where y \u2032 j is the product of leaving one out for all the relevant features i.e. section : Experiments section : Corpus : One Billion Benchmark Our experimental setup used the One Billion Word Benchmark corpus 4 made available by [ reference ] . For completeness , here is a short description of the corpus , containing only monolingual English data : \u2022 Total number of training tokens is about 0.8 billion \u2022 The vocabulary provided consists of 793471 words including sentence boundary markers < S > , < \\S > , and was constructed by discarding all words with count below 3 \u2022 Words outside of the vocabulary were mapped to < UNK > token , also part of the vocabulary \u2022 Sentence order was randomized \u2022 The test data consisted of 159658 words ( without counting the sentence beginning marker < S > which is never predicted by the language model ) \u2022 The out - of - vocabulary ( OoV ) rate on the test set was 0.28 % . section : SNM for n - gram LMs When trained using solely n - gram features , SNM comes very close to the stateof - the - art Kneser - Ney [ reference ] ( KN ) models . Table 1 shows that Katz [ reference ] performs considerably worse than both SNM and KN which only differ by about 5 % . When we interpolate these two models linearly , the added gain is only about 1 % , suggesting that they are approximately modeling the same things . The difference between KN and SNM becomes smaller when we increase the size of the context , going from 5 % for 5 - grams to 3 % for 8 - grams , which indicates that SNM is better suited to a large number of features . section : Sparse Non - negative Modeling for Skip n - grams When we incorporate skip - gram features , we can either build a ' pure ' skip - gram SNM that contains no regular n - gram features , except for unigrams , and interpolate this model with KN , or we can build a single SNM that has both the regular ngram features and the skip - gram features . We compared the two approaches by choosing skip - gram features that can be considered the skip - equivalent of 5 - grams i.e. they contain at most 4 words . In particular , we used skip - gram features where the remote span is limited to at most 3 words for skips of length between 1 and 3 ( r = [ 1 . section : ] ) . We then built a model that uses both these features and regular 5 - grams ( SNM5 - skip ) , as well as one that only uses the skip - gram features ( SNM5 - skip ( no n - grams ) ) . section : Model Num . Params PPL SNM5 - skip ( no n - grams ) 61 B 69.8 SNM5 - skip 62 B 54.2 KN5 + SNM5 - skip ( no n - grams ) 56.5 KN5 + SNM5 - skip 53.6 Table 2 : Number of parameters ( in billions ) and perplexity results for SNM5 - skip models with and without n - grams , as well as perplexity results for the interpolation with KN5 . As it turns out and as can be seen from Table 2 , it is better to incorporate all the features into one single SNM model than to interpolate with a KN 5 - gram model ( KN5 ) . Interpolating the all - in - one SNM5 - skip with KN5 yields almost no additional gain . The best SNM results so far ( SNM10 - skip ) were achieved using 10 - grams , together with untied skip features of at most 5 words with a skip of exactly 1 word ( s = 1 , r + a = [ 1 . section : ] ) . This mixture of rich short - distance and shallow long - distance features enables the model to achieve state - of - the - art results , as can be seen in Table 3 . When we compare the perplexity of this model with the state - of - the art RNN results in [ reference ] , the difference is only 3 % . Moreover , although our model has more parameters than the RNN ( 33 vs 20 billion ) , training takes about a tenth of the time ( 24 hours vs 240 hours ) . Interestingly , when we interpolate the two models , we have an additional gain of 20 % , and as far as we know , the perplexity of 41.3 is already the best ever reported on this database , beating the previous best by 6 % [ reference ] . Finally , when we optimize interpolation weights over all models in [ reference ] , including SNM5 - skip and SNM10 - skip , the contribution of the other models as well as the perplexity reduction is negligible , as can be seen in Table 3 , which also summarizes the perplexity results for each of the individual models . section : Ablation Experiments To find out how much , if anything at all , each metafeature contributes to the adjustment function , we ran a series of ablation experiments in which we ablated one metafeature at a time . When we experimented on SNM5 , we found , unsurprisingly , that the most important metafeature is the feature - target count . At first glance , it does not seem to matter much whether the counts are stored in 1 or 2 buckets , but the second bucket really starts to pay off for models with a large number of singleton features e.g. SNM10 - skip 5 . This is not the case for the feature counts , where having a single bucket is always better , although in general the feature counts do not contribute much . In any case , feature counts are definitely the least important for the model . The remaining metafeatures all contribute more or less equally , all of which can be seen in Table 4 . section : Related Work SNM estimation is closely related to all n - gram LM smoothing techniques that rely on mixing relative frequencies at various orders . Unlike most of those , it combines the predictors at various orders without relying on a hierarchical nesting of the contexts , setting it closer to the family of maximum entropy ( ME ) [ reference ] , or exponential models . We are not the first ones to highlight the effectiveness of skip n - grams at capturing dependencies across longer contexts , similar to RNN LMs ; previous such results were reported in [ reference ] . [ reference ] attempts to capture long range dependencies in language where the skip n - grams are identified using a left - to - right syntactic parser . Approaches such as [ reference ] leverage latent semantic information , whereas [ reference ] integrates both syntactic and topic - based modeling in a unified approach . The speed - ups to ME , and RNN LM training provided by hierarchically predicting words at the output layer [ reference ] , and subsampling [ reference ] still require updates that are linear in the vocabulary size times the number of words in the training data , whereas the SNM updates in Eq . ( 15 ) for the much smaller adjustment function eliminate the dependency on the vocabulary size . Scaling up RNN LM training is described in [ reference ] and [ reference ] . The computational advantages of SNM over both Maximum Entropy and RNN LM estimation are probably its main strength , promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n - gram LMs do . section : section : Conclusions and Future Work We have presented SNM , a new family of LM estimation techniques . A first empirical evaluation on the One Billion Word Benchmark [ reference ] shows that SNM n - gram LMs perform almost as well as the well - established KN models . When using skip - gram features the models are able to match the stat - of - the - art RNN LMs ; combining the two modeling techniques yields the best known result on the benchmark . Future work items include model pruning , exploring richer features similar to [ reference ] , as well as richer metafeatures in the adjustment model , mixing SNM models trained on various data sources such that they perform best on a given development set , and estimation techniques that are more flexible in this respect . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["Sparse_Non-Negative"]]], "Metric": [[["Number_of_params"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["Sparse_Non-Negative"]]], "Metric": [[["PPL"]]], "Task": [[["Language_Modelling"]]]}]}
{"docid": "TST3-SREX-0012", "doctext": "Domain - Adversarial Training of Neural Networks section : Abstract We introduce a new representation learning approach for domain adaptation , in which data at training and test time come from similar but different distributions . Our approach is directly inspired by the theory on domain adaptation suggesting that , for effective domain transfer to be achieved , predictions must be made based on features that can not discriminate between the training ( source ) and test ( target ) domains . The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain ( no labeled target - domain data is necessary ) . As the training progresses , the approach promotes the emergence of features that are ( i ) discriminative for the main learning task on the source domain and ( ii ) indiscriminate with respect to the shift between the domains . We show that this adaptation behaviour can be achieved in almost any feed - forward model by augmenting it with few standard layers and a new gradient reversal layer . The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent , and can thus be implemented with little effort using any of the deep learning packages . We demonstrate the success of our approach for two distinct classification problems ( document sentiment analysis and image classification ) , where state - of - the - art domain adaptation performance on standard benchmarks is achieved . We also validate the approach for descriptor learning task in the context of person re - identification application . section : Introduction The cost of generating labeled data for a new machine learning task is often an obstacle for applying machine learning methods . In particular , this is a limiting factor for the further progress of deep neural network architectures , that have already brought impressive advances to the state - of - the - art across a wide variety of machine - learning tasks and applications . For problems lacking labeled data , it may be still possible to obtain training sets that are big enough for training large - scale deep models , but that suffer from the shift in data distribution from the actual data encountered at \" test time \" . One important example is training an image classifier on synthetic or semi - synthetic images , which may come in abundance and be fully labeled , but which inevitably have a distribution that is different from real images [ reference ][ reference ][ reference ][ reference ] . Another example is in the context of sentiment analysis in written reviews , where one might have labeled data for reviews of one type of product ( e.g. , movies ) , while having the need to classify reviews of other products ( e.g. , books ) . Learning a discriminative classifier or other predictor in the presence of a shift between training and test distributions is known as domain adaptation ( DA ) . The proposed approaches build mappings between the source ( training - time ) and the target ( test - time ) domains , so that the classifier learned for the source domain can also be applied to the target domain , when composed with the learned mapping between domains . The appeal of the domain adaptation approaches is the ability to learn a mapping between domains in the situation when the target domain data are either fully unlabeled ( unsupervised domain annotation ) or have few labeled samples ( semi - supervised domain adaptation ) . Below , we focus on the harder unsupervised case , although the proposed approach ( domain - adversarial learning ) can be generalized to the semi - supervised case rather straightforwardly . Unlike many previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and deep feature learning within one training process . Our goal is to embed domain adaptation into the process of learning representation , so that the final classification decisions are made based on features that are both discriminative and invariant to the change of domains , i.e. , have the same or very similar distributions in the source and the target domains . In this way , the obtained feed - forward network can be applicable to the target domain without being hindered by the shift between the two domains . Our approach is motivated by the theory on domain adaptation [ reference ] , that suggests that a good representation for cross - domain transfer is one for which an algorithm can not learn to identify the domain of origin of the input observation . We thus focus on learning features that combine ( i ) discriminativeness and ( ii ) domaininvariance . This is achieved by jointly optimizing the underlying features as well as two discriminative classifiers operating on these features : ( i ) the label predictor that predicts class labels and is used both during training and at test time and ( ii ) the domain classifier that discriminates between the source and the target domains during training . While the parameters of the classifiers are optimized in order to minimize their error on the training set , the parameters of the underlying deep feature mapping are optimized in order to minimize the loss of the label classifier and to maximize the loss of the domain classifier . The latter update thus works adversarially to the domain classifier , and it encourages domain - invariant features to emerge in the course of the optimization . Crucially , we show that all three training processes can be embedded into an appropriately composed deep feed - forward network , called domain - adversarial neural network ( DANN ) ( illustrated by Figure 1 , page 12 ) that uses standard layers and loss functions , and can be trained using standard backpropagation algorithms based on stochastic gradient descent or its modifications ( e.g. , SGD with momentum ) . The approach is generic as a DANN version can be created for almost any existing feed - forward architecture that is trainable by backpropagation . In practice , the only non - standard component of the proposed architecture is a rather trivial gradient reversal layer that leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during the backpropagation . We provide an experimental evaluation of the proposed domain - adversarial learning idea over a range of deep architectures and applications . We first consider the simplest DANN architecture where the three parts ( label predictor , domain classifier and feature extractor ) are linear , and demonstrate the success of domain - adversarial learning for such architecture . The evaluation is performed for synthetic data as well as for the sentiment analysis problem in natural language processing , where DANN improves the state - of - the - art marginalized Stacked Autoencoders ( mSDA ) of [ reference ] on the common Amazon reviews benchmark . We further evaluate the approach extensively for an image classification task , and present results on traditional deep learning image data sets - such as MNIST [ reference ] and SVHN [ reference ])- as well as on Office benchmarks [ reference ] , where domain - adversarial learning allows obtaining a deep architecture that considerably improves over previous state - of - the - art accuracy . Finally , we evaluate domain - adversarial descriptor learning in the context of person re - identification application [ reference ] , where the task is to obtain good pedestrian image descriptors that are suitable for retrieval and verification . We apply domainadversarial learning , as we consider a descriptor predictor trained with a Siamese - like loss instead of the label predictor trained with a classification loss . In a series of experiments , we demonstrate that domain - adversarial learning can improve cross - data - set re - identification considerably . section : Related work The general approach of achieving domain adaptation explored under many facets . Over the years , a large part of the literature has focused mainly on linear hypothesis ( see for instance [ reference ][ reference ][ reference ][ reference ] . More recently , non - linear representations have become increasingly studied , including neural network representations [ reference ] and most notably the state - of - the - art mSDA [ reference ] . That literature has mostly focused on exploiting the principle of robust representations , based on the denoising autoencoder paradigm [ reference ] . Concurrently , multiple methods of matching the feature distributions in the source and the target domains have been proposed for unsupervised domain adaptation . Some ap - proaches perform this by reweighing or selecting samples from the source domain [ reference ] , while others seek an explicit feature space transformation that would map source distribution into the target one [ reference ][ reference ][ reference ] ) . An important aspect of the distribution matching approach is the way the ( dis ) similarity between distributions is measured . Here , one popular choice is matching the distribution means in the kernelreproducing Hilbert space [ reference ] , whereas [ reference ] and [ reference ] map the principal axes associated with each of the distributions . Our approach also attempts to match feature space distributions , however this is accomplished by modifying the feature representation itself rather than by reweighing or geometric transformation . Also , our method uses a rather different way to measure the disparity between distributions based on their separability by a deep discriminatively - trained classifier . Note also that several approaches perform transition from the source to the target domain [ reference ][ reference ] by changing gradually the training distribution . Among these methods , [ reference ] does this in a \" deep \" way by the layerwise training of a sequence of deep autoencoders , while gradually replacing source - domain samples with target - domain samples . This improves over a similar approach of [ reference ] that simply trains a single deep autoencoder for both domains . In both approaches , the actual classifier / predictor is learned in a separate step using the feature representation learned by autoencoder ( s ) . In contrast to [ reference ][ reference ] , our approach performs feature learning , domain adaptation and classifier learning jointly , in a unified architecture , and using a single learning algorithm ( backpropagation ) . We therefore argue that our approach is simpler ( both conceptually and in terms of its implementation ) . Our method also achieves considerably better results on the popular Office benchmark . While the above approaches perform unsupervised domain adaptation , there are approaches that perform supervised domain adaptation by exploiting labeled data from the target domain . In the context of deep feed - forward architectures , such data can be used to \" fine - tune \" the network trained on the source domain [ reference ][ reference ][ reference ] . Our approach does not require labeled target - domain data . At the same time , it can easily incorporate such data when they are available . An idea related to ours is described in [ reference ] . While their goal is quite different ( building generative deep networks that can synthesize samples ) , the way they measure and minimize the discrepancy between the distribution of the training data and the distribution of the synthesized data is very similar to the way our architecture measures and minimizes the discrepancy between feature distributions for the two domains . Moreover , the authors mention the problem of saturating sigmoids which may arise at the early stages of training due to the significant dissimilarity of the domains . The technique they use to circumvent this issue ( the \" adversarial \" part of the gradient is replaced by a gradient computed with respect to a suitable cost ) is directly applicable to our method . Also , recent and concurrent reports by [ reference ] focus on domain adaptation in feed - forward networks . Their set of techniques measures and minimizes the distance between the data distribution means across domains ( potentially , after embedding distributions into RKHS ) . Their approach is thus different from our idea of matching distributions by making them indistinguishable for a discriminative classifier . Below , we compare our approach to ; [ reference ] on the Office benchmark . Another approach to deep domain adaptation , which is arguably more different from ours , has been developed in parallel by [ reference ] . From a theoretical standpoint , our approach is directly derived from the seminal theoretical works of [ reference ] . Indeed , DANN directly optimizes the notion of H - divergence . We do note the work of [ reference ] , in which HMM representations are learned for word tagging using a posterior regularizer that is also inspired by Ben - David et al . 's work . In addition to the tasks being different - Huang and Yates ( 2012 ) focus on word tagging problems - , we would argue that DANN learning objective more closely optimizes the H - divergence , with Huang and Yates ( 2012 ) relying on cruder approximations for efficiency reasons . A part of this paper has been published as a conference paper [ reference ] . This version extends [ reference ] very considerably by incorporating the report [ reference ] ( presented as part of the Second Workshop on Transfer and Multi - Task Learning ) , which brings in new terminology , in - depth theoretical analysis and justification of the approach , extensive experiments with the shallow DANN case on synthetic data as well as on a natural language processing task ( sentiment analysis ) . Furthermore , in this version we go beyond classification and evaluate domain - adversarial learning for descriptor learning setting within the person re - identification application . section : Domain Adaptation We consider classification tasks where X is the input space and Y = { 0 , 1 , . . . , L\u22121 } is the set of L possible labels . Moreover , we have two different distributions over X\u00d7Y , called the source domain D S and the target domain D T . An unsupervised domain adaptation learning algorithm is then provided with a labeled source sample S drawn i.i.d . from D S , and an unlabeled target sample with N = n + n being the total number of samples . The goal of the learning algorithm is to build a classifier \u03b7 : X \u2192 Y with a low target risk while having no information about the labels of D T . section : Domain Divergence To tackle the challenging domain adaptation task , many approaches bound the target error by the sum of the source error and a notion of distance between the source and the target distributions . These methods are intuitively justified by a simple assumption : the source risk is expected to be a good indicator of the target risk when both distributions are similar . Several notions of distance have been proposed for domain adaptation [ reference ][ reference ][ reference ] . In this paper , we focus on the H - divergence used by [ reference ] , and based on the earlier work of [ reference ] . Note that we assume in definition 1 below that the hypothesis class H is a ( discrete or continuous ) set of binary classifiers \u03b7 : X \u2192 { 0 , 1}. 1 Definition 1 [ reference ][ reference ] That is , the H - divergence relies on the capacity of the hypothesis class H to distinguish between examples generated by D X S from examples generated by D X T . [ reference ] proved that , for a symmetric hypothesis class H , one can compute the empirical where I [ a ] is the indicator function which is 1 if predicate a is true , and 0 otherwise . section : Proxy Distance Ben - David et al . [ reference ] suggested that , even if it is generally hard to computed H ( S , T ) exactly ( e.g. , when H is the space of linear classifiers on X ) , we can easily approximate it by running a learning algorithm on the problem of discriminating between source and target examples . To do so , we construct a new data set where the examples of the source sample are labeled 0 and the examples of the target sample are labeled 1 . Then , the risk of the classifier trained on the new data set U approximates the \" min \" part of Equation ( 1 ) . Given a generalization error on the problem of discriminating between source and target examples , the H - divergence is then approximated b\u0177 In [ reference ] , the valued A is called the , where A is a subset of X. Note that , by choosing A = { A \u03b7 |\u03b7 \u2208 H } , with A \u03b7 the set represented by the characteristic function \u03b7 , the A - distance and the H - divergence of Definition 1 are identical . In the experiments section of this paper , we compute the PAD value following the approach of [ reference ][ reference ] , i.e. , we train either a linear SVM or a deeper MLP classifier on a subset of U ( Equation 2 ) , and we use the obtained classifier error on the other subset as the value of in Equation ( 3 ) . More details and illustrations of the linear SVM case are provided in Section 5.1.5 . section : Generalization Bound on the Target Risk The work of [ reference ] is upper bounded by its empirical estimated H ( S , T ) plus a constant complexity term that depends on the VC dimension of H and the size of samples S and T . By combining this result with a similar bound on the source risk , the following theorem is obtained . , and is the empirical source risk . The previous result tells us that R D T ( \u03b7 ) can be low only when the \u03b2 term is low , i.e. , only when there exists a classifier that can achieve a low risk on both distributions . It also tells us that , to find a classifier with a small R D T ( \u03b7 ) in a given class of fixed VC dimension , the learning algorithm should minimize ( in that class ) a trade - off between the source risk R S ( \u03b7 ) and the empirical H - divergenced H ( S , T ) . As pointed - out by [ reference ] , a strategy to control the H - divergence is to find a representation of the examples where both the source and the target domain are as indistinguishable as possible . Under such a representation , a hypothesis with a low source risk will , according to Theorem 2 , perform well on the target data . In this paper , we present an algorithm that directly exploits this idea . section : Domain - Adversarial Neural Networks ( DANN ) An original aspect of our approach is to explicitly implement the idea exhibited by Theorem 2 into a neural network classifier . That is , to learn a model that can generalize well from one domain to another , we ensure that the internal representation of the neural network contains no discriminative information about the origin of the input ( source or target ) , while preserving a low risk on the source ( labeled ) examples . In this section , we detail the proposed approach for incorporating a \" domain adaptation component \" to neural networks . In Subsection 4.1 , we start by developing the idea for the simplest possible case , i.e. , a single hidden layer , fully connected neural network . We then describe how to generalize the approach to arbitrary ( deep ) network architectures . section : Example Case with a Shallow Neural Network Let us first consider a standard neural network ( NN ) architecture with a single hidden layer . For simplicity , we suppose that the input space is formed by m - dimensional real vectors . Thus , X = R m . The hidden layer G f learns a function G f : X \u2192 R D that maps an example into a new D - dimensional representation 2 , and is parameterized by a matrix - vector pair ( W , b ) \u2208 R D\u00d7m \u00d7 R D : with sigm ( a ) = Similarly , the prediction layer G y learns a function Here we have L = |Y |. By using the softmax function , each component of vector G y ( G f ( x ) ) denotes the conditional probability that the neural network assigns x to the class in Y represented by that component . Given a source example ( x i , y i ) , the natural classification loss to use is the negative log - probability of the correct label : Training the neural network then leads to the following optimization problem on the source domain : where , y i is a shorthand notation for the prediction loss on the i - th example , and R ( W , b ) is an optional regularizer that is weighted by hyper - parameter \u03bb . The heart of our approach is to design a domain regularizer directly derived from the H - divergence of Definition 1 . To this end , we view the output of the hidden layer G f ( \u00b7 ) ( Equation 4 ) as the internal representation of the neural network . Thus , we denote the source sample representations as Similarly , given an unlabeled sample from the target domain we denote the corresponding representations Based on Equation ( 1 ) , the empirical H - divergence of a symmetric hypothesis class H between samples S ( G f ) and Let us consider H as the class of hyperplanes in the representation space . Inspired by the Proxy A - distance ( see Section 3.2 ) , we suggest estimating the \" min \" part of Equation ( 6 ) by a domain classification layer G d that learns a logistic regressor parameterized by a vector - scalar pair ( u , z ) \u2208 R D \u00d7 R , that models the probability that a given input is from the source domain D Hence , the function G d ( \u00b7 ) is a domain regressor . We define its loss by where d i denotes the binary variable ( domain label ) for the i - th example , which indicates whether x i come from the source distribution ( Recall that for the examples from the source distribution ( d i = 0 ) , the corresponding labels y i \u2208 Y are known at training time . For the examples from the target domains , we do not know the labels at training time , and we want to predict such labels at test time . This enables us to add a domain adaptation term to the objective of Equation ( 5 ) , giving the following regularizer : where This regularizer seeks to approximate the H - divergence of Equation ( 6 ) , as 2 ( 1\u2212R ( W , b ) ) is a surrogate ford H S ( G f ) , T ( G f ) . In line with Theorem 2 , the optimization problem given by Equations ( 5 ) and ( 8 ) implements a trade - off between the minimization of the source risk R S ( \u00b7 ) and the divergenced H ( \u00b7 , \u00b7 ) . The hyper - parameter \u03bb is then used to tune the trade - off between these two quantities during the learning process . For learning , we first note that we can rewrite the complete optimization objective of Equation ( 5 ) as follows : where we are seeking the parameters\u0174 , V , b , \u0109 , \u00fb , \u1e91 that deliver a saddle point given by Thus , the optimization problem involves a minimization with respect to some parameters , as well as a maximization with respect to the others . Algorithm 1 Shallow DANN - Stochastic training update while stopping criterion is not met do 6 : for i from 1 to n do 7 : # Forward propagation 8 : 32 : # Update neural network parameters 33 : end for 41 : end while Note : In this pseudo - code , e ( y ) refers to a \" one - hot \" vector , consisting of all 0s except for a 1 at position y , and is the element - wise product . We propose to tackle this problem with a simple stochastic gradient procedure , in which updates are made in the opposite direction of the gradient of Equation ( 9 ) for the minimizing parameters , and in the direction of the gradient for the maximizing parameters . Stochastic estimates of the gradient are made , using a subset of the training samples to compute the averages . Algorithm 1 provides the complete pseudo - code of this learning procedure . 3 In words , during training , the neural network ( parameterized by W , b , V , c ) and the domain regressor ( parameterized by u , z ) are competing against each other , in an adversarial way , over the objective of Equation ( 9 ) . For this reason , we refer to networks trained according to this objective as Domain - Adversarial Neural Networks ( DANN ) . DANN will effectively attempt to learn a hidden layer G f ( \u00b7 ) that maps an example ( either source or target ) into a representation allowing the output layer G y ( \u00b7 ) to accurately classify source samples , but crippling the ability of the domain regressor G d ( \u00b7 ) to detect whether each example belongs to the source or target domains . section : Generalization to Arbitrary Architectures For illustration purposes , we ' ve so far focused on the case of a single hidden layer DANN . However , it is straightforward to generalize to other sophisticated architectures , which might be more appropriate for the data at hand . For example , deep convolutional neural networks are well known for being state - of - the - art models for learning discriminative features of images [ reference ] . Let us now use a more general notation for the different components of DANN . Namely , let G f ( \u00b7 ; \u03b8 f ) be the D - dimensional neural network feature extractor , with parameters \u03b8 f . Also , let G y ( \u00b7 ; \u03b8 y ) be the part of DANN that computes the network 's label prediction output layer , with parameters \u03b8 y , while G d ( \u00b7 ; \u03b8 d ) now corresponds to the computation of the domain prediction output of the network , with parameters \u03b8 d . Note that for preserving the theoretical guarantees of Theorem 2 , the hypothesis class H d generated by the domain prediction component G d should include the hypothesis class H y generated by the label prediction component G y . Thus , We will note the prediction loss and the domain loss respectively by Training DANN then parallels the single layer case and consists in optimizing by finding the saddle point\u03b8 f , \u03b8 y , \u03b8 d such that As suggested previously , a saddle point defined by Equations ( 11 - 12 ) can be found as a stationary point of the following gradient updates : where \u00b5 is the learning rate . We use stochastic estimates of these gradients , by sampling examples from the data set . The updates of Equations ( 13 - 15 ) are very similar to stochastic gradient descent ( SGD ) updates for a feed - forward deep model that comprises feature extractor fed into the label Figure 1 : The proposed architecture includes a deep feature extractor ( green ) and a deep label predictor ( blue ) , which together form a standard feed - forward architecture . Unsupervised domain adaptation is achieved by adding a domain classifier ( red ) connected to the feature extractor via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation - based training . Otherwise , the training proceeds standardly and minimizes the label prediction loss ( for source examples ) and the domain classification loss ( for all samples ) . Gradient reversal ensures that the feature distributions over the two domains are made similar ( as indistinguishable as possible for the domain classifier ) , thus resulting in the domain - invariant features . predictor and into the domain classifier ( with loss weighted by \u03bb ) . The only difference is that in ( 13 ) , the gradients from the class and domain predictors are subtracted , instead of being summed ( the difference is important , as otherwise SGD would try to make features dissimilar across domains in order to minimize the domain classification loss ) . Since SGDand its many variants , such as ADAGRAD [ reference ] or ADADELTA [ reference ]- is the main learning algorithm implemented in most libraries for deep learning , it would be convenient to frame an implementation of our stochastic saddle point procedure as SGD . Fortunately , such a reduction can be accomplished by introducing a special gradient reversal layer ( GRL ) , defined as follows . The gradient reversal layer has no parameters associated with it . During the forward propagation , the GRL acts as an identity transformation . During the backpropagation however , the GRL takes the gradient from the subsequent level and changes its sign , i.e. , multiplies it by \u22121 , before passing it to the preceding layer . Implementing such a layer using existing object - oriented packages for deep learning is simple , requiring only to define procedures for the forward propagation ( identity transformation ) , and backpropagation ( multiplying by \u22121 ) . The layer requires no parameter update . The GRL as defined above is inserted between the feature extractor G f and the domain classifier G d , resulting in the architecture depicted in Figure 1 . As the backpropagation process passes through the GRL , the partial derivatives of the loss that is downstream the GRL ( i.e. , L d ) w.r.t . the layer parameters that are upstream the GRL ( i.e. , \u03b8 f ) get multiplied by \u22121 , i.e. , . Therefore , running SGD in the resulting model implements the updates of Equations ( 13 - 15 ) and converges to a saddle point of Equation ( 10 ) . Mathematically , we can formally treat the gradient reversal layer as a \" pseudo - function \" R ( x ) defined by two ( incompatible ) equations describing its forward and backpropagation behaviour : where I is an identity matrix . We can then define the objective \" pseudo - function \" of ( \u03b8 f , \u03b8 y , \u03b8 d ) that is being optimized by the stochastic gradient descent within our method : Running updates ( 13 - 15 ) can then be implemented as doing SGD for ( 18 ) and leads to the emergence of features that are domain - invariant and discriminative at the same time . After the learning , the label predictor G y ( G f ( x ; \u03b8 f ) ; \u03b8 y ) can be used to predict labels for samples from the target domain ( as well as from the source domain ) . Note that we release the source code for the Gradient Reversal layer along with the usage examples as an extension to Caffe . 4 section : Experiments In this section , we present a variety of empirical results for both shallow domain adversarial neural networks ( Subsection 5.1 ) and deep ones ( Subsections 5.2 and 5.3 ) . section : Experiments with Shallow Neural Networks In this first experiment section , we evaluate the behavior of the simple version of DANN described by Subsection 4.1 . Note that the results reported in the present subsection are obtained using Algorithm 1 . Thus , the stochastic gradient descent approach here consists of sampling a pair of source and target examples and performing a gradient step update of all parameters of DANN . Crucially , while the update of the regular parameters follows as usual the opposite direction of the gradient , for the adversarial parameters the step must follow the gradient 's direction ( since we maximize with respect to them , instead of minimizing ) . section : Experiments on a Toy Problem As a first experiment , we study the behavior of the proposed algorithm on a variant of the inter - twinning moons 2D problem , where the target distribution is a rotation of the source one . As the source sample S , we generate a lower moon and an upper moon labeled 0 and 1 respectively , each of which containing 150 examples . The target sample T is obtained by the following procedure : ( 1 ) we generate a sample S the same way S has been generated ; ( 2 ) we rotate each example by 35 \u2022 ; and ( 3 ) we remove all the labels . Thus , T contains 300 unlabeled examples . We have represented those examples in Figure 2 . We study the adaptation capability of DANN by comparing it to the standard neural network ( NN ) . In these toy experiments , both algorithms share the same network architecture , with a hidden layer size of 15 neurons . We train the NN using the same procedure as the DANN . That is , we keep updating the domain regressor component using target sample T ( with a hyper - parameter \u03bb = 6 ; the same value is used for DANN ) , but we disable the adversarial back - propagation into the hidden layer . To do so , we execute Algorithm 1 by omitting the lines numbered 22 and 31 . This allows recovering the NN learning algorithm - based on the source risk minimization of Equation ( 5 ) without any regularizer - and simultaneously train the domain regressor of Equation ( 7 ) to discriminate between source and target domains . With this toy experience , we will first illustrate how DANN adapts its decision boundary when compared to NN . Moreover , we will also illustrate how the representation given by the hidden layer is less adapted to the source domain task with DANN than with NN ( this is why we need a domain regressor in the NN experiment ) . We recall that this is the founding idea behind our proposed algorithm . The analysis of the experiment appears in Figure 2 , where upper graphs relate to standard NN , and lower graphs relate to DANN . By looking at the lower and upper graphs pairwise , we compare NN and DANN from four different perspectives , described in details below . The column \" Label Classification \" of Figure 2 shows the decision boundaries of DANN and NN on the problem of predicting the labels of both source and the target examples . As expected , NN accurately classifies the two classes of the source sample S , but is not fully adapted to the target sample T . On the contrary , the decision boundary of DANN perfectly classifies examples from both source and target samples . In the studied task , DANN clearly adapts to the target distribution . The column \" Representation PCA \" studies how the domain adaptation regularizer affects the representation G f ( \u00b7 ) provided by the network hidden layer . The graphs are obtained by applying a Principal component analysis ( PCA ) on the set of all representation of source and target data points , i.e. , S ( G f ) \u222a T ( G f ) . Thus , given the trained network ( NN or DANN ) , every point from S and T is mapped into a 15 - dimensional feature space through the hidden layer , and projected back into a two - dimensional plane by the PCA transformation . In the DANN - PCA representation , we observe that target points are homogeneously spread out among source points ; In the NN - PCA representation , a number of target points belong to clusters containing no source points . Hence , labeling the target points seems an easier task given the DANN - PCA representation . To push the analysis further , the PCA graphs tag four crucial data points by the letters A , B , C and D , that correspond to the moon extremities in the original space ( note that the original point locations are tagged in the first column graphs ) . We observe that points A and B are very close to each other in the NN - PCA representation , while they clearly belong to different classes . The same happens to points C and D. Conversely , these four points are at the opposite four corners in the DANN - PCA representation . Note also that the target point A ( resp . D )- that is difficult to classify in the original space - is located in the \" + \" cluster ( resp . \" \u2212 \u2212 \u2212\"cluster ) in the DANN - PCA representation . Therefore , the representation promoted by DANN is better suited to the adaptation problem . The column \" Domain Classification \" shows the decision boundary on the domain classification problem , which is given by the domain regressor G d of Equation ( 7 ) . More precisely , an example x is classified as a source example when G d ( G f ( x ) ) \u2265 0.5 , and is classified as a domain example otherwise . Remember that , during the learning process of DANN , the G d regressor struggles to discriminate between source and target domains , while the hidden representation G f ( \u00b7 ) is adversarially updated to prevent it to succeed . As explained above , we trained a domain regressor during the learning process of NN , but without allowing it to influence the learned representation G f ( \u00b7 ) . On one hand , the DANN domain regressor clearly fails to generalize source and target distribution topologies . On the other hand , the NN domain regressor shows a better ( although imperfect ) generalization capability . Inter alia , it seems to roughly capture the rotation angle of the target distribution . This again corroborates that the DANN representation does not allow discriminating between domains . The column \" Hidden Neurons \" shows the configuration of hidden layer neurons ( by Equation 4 , we have that each neuron is indeed a linear regressor ) . In other words , each of the fifteen plot line corresponds to the coordinates x \u2208 R 2 for which the i - th component of G f ( x ) equals 1 2 , for i \u2208 { 1 , . . . , 15}. We observe that the standard NN neurons are grouped in three clusters , each one allowing to generate a straight line of the zigzag decision boundary for the label classification problem . However , most of these neurons are also able to ( roughly ) capture the rotation angle of the domain classification problem . Hence , we observe that the adaptation regularizer of DANN prevents these kinds of neurons to be produced . It is indeed striking to see that the two predominant patterns in the NN neurons ( i.e. , the two parallel lines crossing the plane from lower left to upper right ) are vanishing in the DANN neurons . section : Unsupervised Hyper - Parameter Selection To perform unsupervised domain adaption , one should provide ways to set hyper - parameters ( such as the domain regularization parameter \u03bb , the learning rate , the network architecture for our method ) in an unsupervised way , i.e. , without referring to labeled data in the target domain . In the following experiments of Sections 5.1.3 and 5.1.4 , we select the hyper - parameters of each algorithm by using a variant of reverse cross - validation approach proposed by [ reference ] , that we call reverse validation . To evaluate the reverse validation risk associated to a tuple of hyper - parameters , we proceed as follows . Given the labeled source sample S and the unlabeled target sample T , we split each set into training sets ( S and T respectively , containing 90 % of the original examples ) and the validation sets ( S V and T V respectively ) . We use the labeled set S and the unlabeled target set T to learn a classifier \u03b7 . Then , using the same algorithm , we learn a reverse classifier \u03b7 r using the self - labeled set { ( x , \u03b7 ( x ) ) } x\u2208T and the unlabeled part of S as target sample . Finally , the reverse classifier \u03b7 r is evaluated on the validation set S V of source sample . We then say that the classifier \u03b7 has a reverse validation risk of R S V ( \u03b7 r ) . The process is repeated with multiple values of hyper - parameters and the selected parameters are those corresponding to the classifier with the lowest reverse validation risk . Note that when we train neural network architectures , the validation set S V is also used as an early stopping criterion during the learning of \u03b7 , and self - labeled validation set { ( x , \u03b7 ( x ) ) } x\u2208T V is used as an early stopping criterion during the learning of \u03b7 r . We also observed better accuracies when we initialized the learning of the reverse classifier \u03b7 r with the configuration learned by the network \u03b7 . section : Experiments on Sentiment Analysis Data Sets We now compare the performance of our proposed DANN algorithm to a standard neural network with one hidden layer ( NN ) described by Equation ( 5 ) , and a Support Vector Machine ( SVM ) with a linear kernel . We compare the algorithms on the Amazon reviews data set , as pre - processed by [ reference ] . This data set includes four domains , each one composed of reviews of a specific kind of product ( books , dvd disks , electronics , and kitchen appliances ) . Reviews are encoded in 5 000 dimensional feature vectors of unigrams and bigrams , and labels are binary : \" 0 \" if the product is ranked up to 3 stars , and \" 1 \" if the product is ranked 4 or 5 stars . We perform twelve domain adaptation tasks . All learning algorithms are given 2 000 labeled source examples and 2 000 unlabeled target examples . Then , we evaluate them on separate target test sets ( between 3 000 and 6 000 examples ) . Note that NN and SVM do not use the unlabeled target sample for learning . Here are more details about the procedure used for each learning algorithms leading to the empirical results of Table 1 : Classification accuracy on the Amazon reviews data set , and Pairwise Poisson binomial test . \u2022 For the DANN algorithm , the adaptation parameter \u03bb is chosen among 9 values between 10 \u22122 and 1 on a logarithmic scale . The hidden layer size l is either 50 or 100 . Finally , the learning rate \u00b5 is fixed at 10 \u22123 . \u2022 For the NN algorithm , we use exactly the same hyper - parameters grid and training procedure as DANN above , except that we do not need an adaptation parameter . Note that one can train NN by using the DANN implementation ( Algorithm 1 ) with \u03bb = 0 . \u2022 For the SVM algorithm , the hyper - parameter C is chosen among 10 values between 10 \u22125 and 1 on a logarithmic scale . This range of values is the same as used by [ reference ] in their experiments . As presented at Section 5.1.2 , we used reverse cross validation selecting the hyper - parameters for all three learning algorithms , with early stopping as the stopping criterion for DANN and NN . The \" Original data \" part of Table 1a shows the target test accuracy of all algorithms , and Table 1b reports the probability that one algorithm is significantly better than the others according to the Poisson binomial test [ reference ] . We note that DANN has a significantly better performance than NN and SVM , with respective probabilities 0.87 and 0.83 . As the only difference between DANN and NN is the domain adaptation regularizer , we conclude that our approach successfully helps to find a representation suitable for the target domain . section : Combining DANN with Denoising Autoencoders We now investigate on whether the DANN algorithm can improve on the representation learned by the state - of - the - art Marginalized Stacked Denoising Autoencoders ( mSDA ) proposed by [ reference ] . In brief , mSDA is an unsupervised algorithm that learns a new robust feature representation of the training samples . It takes the unlabeled parts of both source and target samples to learn a feature map from input space X to a new representation space . As a denoising autoencoders algorithm , it finds a feature representation from which one can ( approximately ) reconstruct the original features of an example from its noisy counterpart . [ reference ] showed that using mSDA with a linear SVM classifier reaches state - of - the - art performance on the Amazon reviews data sets . As an alternative to the SVM , we propose to apply our Shallow DANN algorithm on the same representations generated by mSDA ( using representations of both source and target samples ) . Note that , even if mSDA and DANN are two representation learning approaches , they optimize different objectives , which can be complementary . We perform this experiment on the same Amazon reviews data set described in the previous subsection . For each source - target domain pair , we generate the mSDA representations using a corruption probability of 50 % and a number of layers of 5 . We then execute the three learning algorithms ( DANN , NN , and SVM ) on these representations . More precisely , following the experimental procedure of [ reference ] , we use the concatenation of the output of the 5 layers and the original input as the new representation . Thus , each example is now encoded in a vector of 30 000 dimensions . Note that we use the same grid search as in the previous Subsection 5.1.3 , but use a learning rate \u00b5 of 10 \u22124 for both DANN and the NN . The results of \" mSDA representation \" columns in Table 1a confirm that combining mSDA and DANN is a sound approach . Indeed , the Poisson binomial test shows that DANN has a better performance than the NN and the SVM , with probabilities 0.92 and 0.88 respectively , as reported in Table 1b . We note however that the standard NN and the SVM find the best solution on respectively the second and the fourth tasks . This suggests that DANN and mSDA adaptation strategies are not fully complementary . section : Proxy Distance The theoretical foundation of the DANN algorithm is the domain adaptation theory of [ reference ][ reference ] . We claimed that DANN finds a representation in which the source and the target example are hardly distinguishable . Our toy experiment of Section 5.1.1 already points out some evidence for that and here we provide analysis on real data . To do so , we compare the Proxy A - distance ( PAD ) on various representations of the Amazon Reviews data set ; these representations are obtained by running either NN , DANN , mSDA , or mSDA and DANN combined . Recall that PAD , as described in Section 3.2 , is a metric estimating the similarity of the source and the target representations . More precisely , to obtain a PAD value , we use the following procedure : ( 1 ) we construct the data set U of Equation ( 2 ) using both source and target representations of the training samples ; ( 2 ) we randomly split U in two subsets of equal size ; ( 3 ) we train linear SVMs on the first subset of U using a large range of C values ; ( 4 ) we compute the error of all obtained classifiers on the second subset of U ; and ( 5 ) we use the lowest error to compute the PAD value of Equation ( 3 ) . Firstly , Figure 3a compares the PAD of DANN representations obtained in the experiments of Section 5.1.3 ( using the hyper - parameters values leading to the results of Table 1 ) to the PAD computed on raw data . As expected , the PAD values are driven down by the DANN representations . Secondly , Figure 3b compares the PAD of DANN representations to the PAD of standard NN representations . As the PAD is influenced by the hidden layer size ( the discriminating power tends to increase with the representation length ) , we fix here the size to 100 neurons for both algorithms . We also fix the adaptation parameter of DANN to \u03bb 0.31 ; it was the value that has been selected most of the time during our preceding experiments on the Amazon Reviews data set . Again , DANN is clearly leading to the lowest PAD values . Lastly , Figure 3c presents two sets of results related to Section 5.1.4 experiments . On one hand , we reproduce the results of [ reference ] , which noticed that the mSDA representations have greater PAD values than original ( raw ) data . Although the mSDA approach clearly helps to adapt to the target task , it seems to contradict the theory of BenDavid et al .. On the other hand , we observe that , when running DANN on top of mSDA ( using the hyper - parameters values leading to the results of Table 1 ) , the obtained representations have much lower PAD values . These observations might explain the improvements provided by DANN when combined with the mSDA procedure . section : Experiments with Deep Networks on Image Classification We now perform extensive evaluation of a deep version of DANN ( see Subsection 4.2 ) on a number of popular image data sets and their modifications . These include large - scale data sets of small images popular with deep learning methods , and the Office data sets [ reference ] , which are a de facto standard for domain adaptation in computer vision , but have much fewer images . section : Baselines The following baselines are evaluated in the experiments of this subsection . The source - only model is trained without consideration for target - domain data ( no domain classifier branch included into the network ) . The train - on - target model is trained on the target domain with class labels revealed . This model serves as an upper bound on DA methods , assuming that target data are abundant and the shift between the domains is considerable . In addition , we compare our approach against the recently proposed unsupervised DA method based on subspace alignment ( SA ) [ reference ] , which is simple to setup and test on new data sets , but has also been shown to perform very well in experimental comparisons with other \" shallow \" DA methods . To boost the performance of this baseline , we pick its most important free parameter ( the number of principal components ) from the range { 2 , . . . , 60 } , so that the test performance on the target domain is maximized . To apply SA in our setting , we train a source - only model and then consider the activations of the last hidden layer in the label predictor ( before the final linear classifier ) as descriptors / features , and learn the mapping between the source and the target domains [ reference ] . Since the SA baseline requires training a new classifier after adapting the features , and in order to put all the compared settings on an equal footing , we retrain the last layer of the label predictor using a standard linear SVM [ reference ] for all four considered methods ( including ours ; the performance on the target domain remains approximately the same after the retraining ) . For the Office data set [ reference ] , we directly compare the performance of our full network ( feature extractor and label predictor ) against recent DA approaches using previously published results . section : CNN architectures and Training Procedure In general , we compose feature extractor from two or three convolutional layers , picking their exact configurations from previous works . More precisely , four different architectures were used in our experiments . The first three are shown in Figure 4 . For the Office domains , we use pre - trained AlexNet from the Caffe - package . The adaptation architecture is identical to . [ reference ] For the domain adaption component , we use three ( x\u21921024\u21921024\u21922 ) fully connected layers , except for MNIST where we used a simpler ( x\u2192100\u21922 ) architecture to speed up the experiments . Admittedly these choices for domain classifier are arbitrary , and better adaptation performance might be attained if this part of the architecture is tuned . ( x\u21921024\u21921024\u21922 ) For the loss functions , we set L y and L d to be the logistic regression loss and the binomial cross - entropy respectively . Following [ reference ] we also use dropout and 2 - norm restriction when we train the SVHN architecture . section : A 2 - layer domain classifier The other hyper - parameters are not selected through a grid search as in the small scale experiments of Section 5.1 , which would be computationally costly . Instead , the learning rate is adjusted during the stochastic gradient descent using the following formula : where p is the training progress linearly changing from 0 to 1 , \u00b5 0 = 0.01 , \u03b1 = 10 and \u03b2 = 0.75 ( the schedule was optimized to promote convergence and low error on the source domain ) . A momentum term of 0.9 is also used . The domain adaptation parameter \u03bb is initiated at 0 and is gradually changed to 1 using the following schedule : where \u03b3 was set to 10 in all experiments ( the schedule was not optimized / tweaked ) . This strategy allows the domain classifier to be less sensitive to noisy signal at the early stages of the training procedure . Note however that these \u03bb p were used only for updating the feature extractor component G f . For updating the domain classification component , we used a fixed \u03bb = 1 , to ensure that the latter trains as fast as the label predictor G y . [ reference ] Finally , note that the model is trained on 128 - sized batches ( images are preprocessed by the mean subtraction ) . A half of each batch is populated by the samples from the source domain ( with known labels ) , the rest constitutes the target domain ( with labels not revealed to the algorithms except for the train - on - target baseline ) . section : Visualizations We use t - SNE ( van der Maaten , 2013 ) projection to visualize feature distributions at different points of the network , while color - coding the domains ( Figure 5 ) . As we already observed with the shallow version of DANN ( see Figure 2 ) , there is a strong correspondence 6 . Equivalently , one can use the same \u03bbp for both feature extractor and domain classification components , but use a learning rate of \u00b5 / \u03bbp for the latter . between the success of the adaptation in terms of the classification accuracy for the target domain , and the overlap between the domain distributions in such visualizations . section : Results On Image Data Sets We now discuss the experimental settings and the results . In each case , we train on the source data set and test on a different target domain data set , with considerable shifts between domains ( see Figure 6 ) . The results are summarized in Table 2 and Table 3 . MNIST \u2192 MNIST - M. Our first experiment deals with the MNIST data set [ reference ] . In order to obtain the target domain ( MNIST - M ) we blend digits from the original set over patches randomly extracted from color photos from BSDS500 [ reference ] . This operation is formally defined for two images I 1 , I 2 as I out ijk = |I 1 ijk \u2212 I 2 ijk | , where i , j are the coordinates of a pixel and k is a channel index . In other words , an output sample is produced by taking a patch from a photo and inverting its pixels at positions corresponding to the pixels of a digit . For a human the classification task becomes only slightly harder compared to the original data set ( the digits are still clearly distinguishable ) whereas for a CNN trained on MNIST this domain is quite distinct , as the background and the strokes are no longer constant . Consequently , the source - only model performs poorly . Our approach succeeded at aligning feature distributions ( Figure 5 ) , which led to successful adaptation results ( considering that the adaptation is unsupervised ) . At the same time , the improvement over source - only model achieved by subspace alignment ( SA ) [ reference ] ) is quite modest , thus highlighting the difficulty of the adaptation task . Synthetic numbers \u2192 SVHN . To address a common scenario of training on synthetic data and testing on real data , we use Street - View House Number data set SVHN [ reference ] as the target domain and synthetic digits as the source . The latter ( Syn Numbers ) consists of \u2248 500 , 000 images generated by ourselves from Windows TM fonts by varying the text ( that includes different one - , two - , and three - digit numbers ) , positioning , orientation , background and stroke colors , and the amount of blur . The degrees of variation were chosen manually to simulate SVHN , however the two data sets are still rather distinct , the biggest difference being the structured clutter in the background of SVHN images . The proposed backpropagation - based technique works well covering almost 80 % of the gap between training with source data only and training on target domain data with known target labels . In contrast , SA [ reference ] results in a slight classification accuracy drop ( probably due to the information loss during the dimensionality reduction ) , indicating that the adaptation task is even more challenging than in the case of the MNIST experiment . MNIST \u2194 SVHN . In this experiment , we further increase the gap between distributions , and test on MNIST and SVHN , which are significantly different in appearance . Training on SVHN even without adaptation is challenging - classification error stays high during the first 150 epochs . In order to avoid ending up in a poor local minimum we , therefore , do not use learning rate annealing here . Obviously , the two directions ( MNIST \u2192 SVHN and SVHN \u2192 MNIST ) are not equally difficult . As SVHN is more diverse , a model trained on SVHN is expected to be more generic and to perform reasonably on the MNIST data set . This , indeed , turns out to be the case and is supported by the appearance of the Table 2 : Classification accuracies for digit image classifications for different source and target domains . MNIST - M corresponds to difference - blended digits over nonuniform background . The first row corresponds to the lower performance bound ( i.e. , if no adaptation is performed ) . The last row corresponds to training on the target domain data with known class labels ( upper bound on the DA performance ) . For each of the two DA methods [ reference ] we show how much of the gap between the lower and the upper bounds was covered ( in brackets ) . For all five cases , our approach outperforms [ reference ] considerably , and covers a big portion of the gap . Table 3 : Accuracy evaluation of different DA approaches on the standard Office [ reference ] ) data set . All methods ( except SA ) are evaluated in the \" fullytransductive \" protocol ( some results are reproduced from [ reference ] . Our method ( last row ) outperforms competitors setting the new state - of - the - art . Syn and Real denote available labeled data ( 100 , 000 synthetic and 430 real images respectively ) ; Adapted means that \u2248 31 , 000 unlabeled target domain images were used for adaptation . The best performance is achieved by employing both the labeled samples and the large unlabeled corpus in the target domain . feature distributions . We observe a quite strong separation between the domains when we feed them into the CNN trained solely on MNIST , whereas for the SVHN - trained network the features are much more intermixed . This difference probably explains why our method succeeded in improving the performance by adaptation in the SVHN \u2192 MNIST scenario ( see Table 2 ) but not in the opposite direction ( SA is not able to perform adaptation in this case either ) . Unsupervised adaptation from MNIST to SVHN gives a failure example for our approach : it does n't manage to improve upon the performance of the non - adapted model which achieves \u2248 0.25 accuracy ( we are unaware of any unsupervised DA methods capable of performing such adaptation ) . Synthetic Signs \u2192 GTSRB . Overall , this setting is similar to the Syn Numbers \u2192 SVHN experiment , except the distribution of the features is more complex due to the significantly larger number of classes ( 43 instead of 10 ) . For the source domain we obtained 100 , 000 synthetic images ( which we call Syn Signs ) simulating various imaging conditions . In the target domain , we use 31 , 367 random training samples for unsupervised adaptation and the rest for evaluation . Once again , our method achieves a sensible increase in performance proving its suitability for the synthetic - to - real data adaptation . As an additional experiment , we also evaluate the proposed algorithm for semi - supervised domain adaptation , i.e. , when one is additionally provided with a small amount of labeled target data . Here , we reveal 430 labeled examples ( 10 samples per class ) and add them to the training set for the label predictor . Figure 7 shows the change of the validation error throughout the training . While the graph clearly suggests that our method can be beneficial in the semi - supervised setting , thorough verification of semi - supervised setting is left for future work . Office data set . We finally evaluate our method on Office data set , which is a collection of three distinct domains : Amazon , DSLR , and Webcam . Unlike previously discussed data sets , Office is rather small - scale with only 2817 labeled images spread across 31 different categories in the largest domain . The amount of available data is crucial for a successful training of a deep model , hence we opted for the fine - tuning of the CNN pre - trained on the ImageNet ( AlexNet from the Caffe package , see as it is done in some recent DA works [ reference ][ reference ] . We make our approach more comparable with by using exactly the same network architecture replacing domain mean - based regularization with the domain classifier . Following previous works , we assess the performance of our method across three transfer tasks most commonly used for evaluation . Our training protocol is adopted from ; [ reference ] ; [ reference ] as during adaptation we use all available labeled source examples and unlabeled target examples ( the premise of our method is the abundance of unlabeled data in the target domain ) . Also , all source domain data are used for training . Under this \" fully - transductive \" setting , our method is able to improve previously - reported state - of - the - art accuracy for unsupervised adaptation very considerably ( Table 3 ) , especially in the most challenging Amazon \u2192 Webcam scenario ( the two domains with the largest domain shift ) . Interestingly , in all three experiments we observe a slight over - fitting ( performance on the target domain degrades while accuracy on the source continues to improve ) as training progresses , however , it does n't ruin the validation accuracy . Moreover , switching off the domain classifier branch makes this effect far more apparent , from which we conclude that our technique serves as a regularizer . section : Experiments with Deep Image Descriptors for Re - Identification In this section we discuss the application of the described adaptation method to person re - identification ( re - i d ) problem . The task of person re - identification is to associate people seen from different camera views . More formally , it can be defined as follows : given two sets of images from different cameras ( probe and gallery ) such that each person depicted in the probe set has an image in the gallery set , for each image of a person from the probe set find an image of the same person in the gallery set . Disjoint camera views , different illumination conditions , various poses and low quality of data make this problem difficult even for humans ( e.g. , [ reference ] , reports human performance at Rank1=71.08 % ) . Unlike classification problems that are discussed above , re - identification problem implies that each image is mapped to a vector descriptor . The distance between descriptors is then used to match images from the probe set and the gallery set . To evaluate results of re - i d methods the Cumulative Match Characteristic ( CMC ) curve is commonly used . It is a plot of the identification rate ( recall ) at rank - k , that is the probability of the matching gallery image to be within the closest k images ( in terms of descriptor distance ) to the probe image . Most existing works train descriptor mappings and evaluate them within the same data set containing images from a certain camera network with similar imaging conditions . Several papers , however , observed that the performance of the resulting re - identification systems drops very considerably when descriptors trained on one data set and tested on another . It is therefore natural to handle such cross - domain evaluation as a domain - adaptation problem , where each camera network ( data set ) constitutes a domain . Recently , several papers with significantly improved re - identification performance [ reference ][ reference ][ reference ] have been presented , with [ reference ] reporting good results in cross - data - set evaluation scenario . At the moment , deep learning methods [ reference ] do not achieve state - of - the - art results probably because of the limited size of the training sets . Domain adaptation thus represents a viable direction for improving deep re - identification descriptors . section : Data Sets and Protocols Following [ reference ] , we use PRID [ reference ] , VIPeR [ reference ] , CUHK [ reference ] as target data sets for our experiments . The PRID data set exists in two versions , and as in [ reference ] we use a single - shot variant . It contains images of 385 persons viewed from camera A and images of 749 persons viewed from camera B , 200 persons appear in both cameras . The VIPeR data set also contains images taken with two cameras , and in total 632 persons are captured , for every person there is one image for each of the two camera views . The CUHK data set consists of images from five pairs of cameras , two images for each person from each of the two cameras . We refer to the subset of this data set that includes the first pair of cameras only as CUHK / p1 ( as most papers use this subset ) . See Figure 8 for samples of these data sets . We perform extensive experiments for various pairs of data sets , where one data set serves as a source domain , i.e. , it is used to train a descriptor mapping in a supervised way with known correspondences between probe and gallery images . The second data set is used as a target domain , so that images from that data set are used without probe - gallery correspondence . In more detail , CUHK / p1 is used for experiments when CUHK serves as a target domain and two settings ( \" whole CUHK \" and CUHK / p1 ) are used for experiments when CUHK serves as a source domain . Given PRID as a target data set , we randomly choose 100 persons appearing in both camera views as training set . The images of the other 100 persons from camera A are used as probe , all images from camera B excluding those used in training ( 649 in total ) are used as gallery at test time . For VIPeR , we use random 316 persons for training and all others for testing . For CUHK , 971 persons are split into 485 for training and 486 for testing . Unlike [ reference ] , we use all images in the first pair of cameras of CUHK instead of choosing one image of a person from each camera view . We also performed two experiments with all images of the whole CUHK data set as source domain and VIPeR and PRID data sets as target domains as in the original paper [ reference ] . Following [ reference ] , we augmented our data with mirror images , and during test time we calculate similarity score between two images as the mean of the four scores corresponding to different flips of the two compared images . In case of CUHK , where there are 4 images ( including mirror images ) for each of the two camera views for each person , all 16 combinations ' scores are averaged . section : CNN architectures and Training Procedure In our experiments , we use siamese architecture described in [ reference ] ( Deep Metric Learning or DML ) for learning deep image descriptors on the source data set . This architecture incorporates two convolution layers ( with 7 \u00d7 7 and 5 \u00d7 5 filter banks ) , followed by ReLU and max pooling , and one fully - connected layer , which gives 500 - dimensional descriptors as an output . There are three parallel flows within the CNN for processing three part of an image : the upper , the middle , and the lower one . The first convolution layer shares parameters between three parts , and the outputs of the second convolution layers are concatenated . During training , we follow [ reference ] and calculate pairwise cosine similarities between 500 - dimensional features within each batch and backpropagate the loss for all pairs within batch . To perform domain - adversarial training , we construct a DANN architecture . The feature extractor includes the two convolutional layers ( followed by max - pooling and ReLU ) discussed above . The label predictor in this case is replaced with descriptor predictor that includes one fully - connected layer . The domain classifier includes two fully - connected layers with 500 units in the intermediate representation ( x\u2192500\u21921 ) . For the verification loss function in the descriptor predictor we used Binomial Deviance loss , defined in [ reference ] with similar parameters : \u03b1 = 2 , \u03b2 = 0.5 , c = 2 ( the asymmetric cost parameter for negative pairs ) . The domain classifier is trained with logistic loss as in subsection 5.2.2 . We used learning rate fixed to 0.001 and momentum of 0.9 . The schedule of adaptation similar to the one described in subsection 5.2.2 was used . We also inserted dropout layer with rate 0.5 after the concatenation of outputs of the second max - pooling layer . 128 - sized batches were used for source data and 128 - sized batches for target data . Figure 9 shows results in the form of CMC - curves for eight pairs of data sets . Depending on the hardness of the annotation problem we trained either for 50 , 000 iterations ( CUHK / p1 \u2192 VIPeR , VIPeR \u2192 CUHK / p1 , PRID \u2192 VIPeR ) or for 20 , 000 iterations ( the other five pairs ) . section : Results on Re - identification data sets After the sufficient number of iterations , domain - adversarial training consistently improves the performance of re - identification . For the pairs that involve PRID data set , which is more dissimilar to the other two data sets , the improvement is considerable . Overall , this demonstrates the applicability of the domain - adversarial learning beyond classification problems . section : Conclusion The paper proposes a new approach to domain adaptation of feed - forward neural networks , which allows large - scale training based on large amount of annotated data in the source domain and large amount of unannotated data in the target domain . Similarly to many previous shallow and deep DA techniques , the adaptation is achieved through aligning the distributions of features across the two domains . However , unlike previous approaches , the alignment is accomplished through standard backpropagation training . The approach is motivated and supported by the domain adaptation theory of [ reference ] . The main idea behind DANN is to enjoin the network hidden layer to learn a representation which is predictive of the source example labels , but uninformative about the domain of the input ( source or target ) . We implement this new approach within both shallow and deep feed - forward architectures . The latter allows simple implementation within virtually any deep learning package through the introduction of a simple gradient reversal layer . We have shown that our approach is flexible and achieves state - of - the - art results on a variety of benchmark in domain adaptation , namely for sentiment analysis and image classification tasks . A convenient aspect of our approach is that the domain adaptation component can be added to almost any neural network architecture that is trainable with backpropagation . Towards this end , We have demonstrated experimentally that the approach is not confined to classification tasks but can be used in other feed - forward architectures , e.g. , for descriptor learning for person re - identification . section : section : Acknowledgments This work has been supported by National Science and Engineering Research Council ( NSERC ) Discovery grants 262067 and 0122405 as well as the Russian Ministry of Science and Education grant RFMEFI57914X0071 . Computations were performed on the Colosse supercomputer grid at Universit\u00e9 Laval , under the auspices of Calcul Qu\u00e9bec and Compute Canada . The operations of Colosse are funded by the NSERC , the Canada Foundation for Innovation ( CFI ) , NanoQu\u00e9bec , and the Fonds de recherche du Qu\u00e9bec - Nature et technologies ( FRQNT ) . We also thank the Graphics & Media Lab , Faculty of Computational Mathematics and Cybernetics , Lomonosov Moscow State University for providing the synthetic road signs data set . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["DANN"]]], "Metric": [[["Average"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["DANN"]]], "Metric": [[["Books"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["DANN"]]], "Metric": [[["DVD"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["DANN"]]], "Metric": [[["Electronics"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["DANN"]]], "Metric": [[["Kitchen"]]], "Task": [[["Sentiment_Analysis"]]]}]}
{"docid": "TST3-SREX-0013", "doctext": "document : PixelGAN Autoencoders In this paper , we describe the \u2018 \u2018 PixelGAN autoencoder \u2019 \u2019 , a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels ( PixelCNN ) that is conditioned on a latent code , and the recognition path uses a generative adversarial network ( GAN ) to impose a prior distribution on the latent code . We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder . For example , by imposing a Gaussian distribution as the prior , we can achieve a global vs. local decomposition , or by imposing a categorical distribution as the prior , we can disentangle the style and content information of images in an unsupervised fashion . We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi - supervised settings and achieve competitive semi - supervised classification results on the MNIST , SVHN and NORB datasets . numbers , compressnatbib section : Introduction In recent years , generative models that can be trained via direct back - propagation have enabled remarkable progress in modeling natural images . One of the most successful models is the generative adversarial network ( GAN ) gan , which employs a two player min - max game . The generative model , , samples the prior and generates the sample . The discriminator , , is trained to identify whether a point is a sample from the data distribution or a sample from the generative model . The generator is trained to maximally confuse the discriminator into believing that generated samples come from the data distribution . The cost function of GAN is GANs can be considered within the wider framework of implicit generative models mohamed2016learning , ference , dustin . Implicit distributions can be sampled through their generative path , but their likelihood function is not tractable . Recently , several papers have proposed another application of GAN - style algorithms for approximate inference , mohamed2016learning , ference , dustin , ranganath2016operator , aae , avb , ali , bigan . These algorithms use implicit distributions to learn posterior approximations that are more expressive than the distributions with tractable densities that are often used in variational inference . For example , adversarial autoencoders ( AAE ) aae use a universal approximator posterior as the implicit posterior distribution and use adversarial training to match the aggregated posterior of the latent code to the prior distribution . Adversarial variational Bayes ference , avb uses a more general amortized GAN inference framework within a maximum - likelihood learning setting . Another type of GAN inference technique is used in the ALI ali and BiGAN bigan models , which have been shown to approximate maximum likelihood learning ference . In these models , both the recognition and generative models are implicit and are jointly learnt by an adversarial training process . Variational autoencoders ( VAE ) vae , rezende are another state - of - the - art image modeling technique that use neural networks to parametrize the posterior distribution and pair it with a top - down generative network . Both networks are jointly trained to maximize a variational lower bound on the data log - likelihood . A different framework for learning density models is autoregressive neural networks such as NADE nade , MADE made , PixelRNN pixelrnn and PixelCNN pixelcnn . Unlike variational autoencoders , which capture the statistics of the data in hierarchical latent codes , the autoregressive models learn the image densities directly at the pixel level without learning a hierarchical latent representation . In this paper , we present the PixelGAN autoencoder as a generative autoencoder that combines the benefits of latent variable models with autoregressive architectures . The PixelGAN autoencoder is a generative autoencoder in which the generative path is a PixelCNN that is conditioned on a latent variable . The latent variable is inferred by matching the aggregated posterior distribution to the prior distribution by an adversarial training technique similar to that of the adversarial autoencoder aae . However , whereas in adversarial autoencoders the statistics of the data distribution are captured by the latent code , in the PixelGAN autoencoder they are captured jointly by the latent code and the autoregressive decoder . We show that imposing different distributions as the prior results in different factorizations of information between the latent code and the autoregressive decoder . For example , in : pixelgan_gaussian ] Section [ reference ] , we show that by imposing a Gaussian distribution on the latent code , we can achieve a global vs. local decomposition of information . In this case , the global latent code no longer has to model all the irrelevant and fine details of the image , and can use its capacity to capture more relevant and global statistics of the image . Another type of decomposition of information that can be learnt by PixelGAN autoencoders is a discrete vs. continuous decomposition . In : pixelgan_cat ] Section [ reference ] , we show that we can achieve this decomposition by imposing a categorical prior on the latent code using adversarial training . In this case , the categorical latent code captures the discrete underlying factors of variation in the data , such as class label information , and the autoregressive decoder captures the remaining continuous structure , such as style information , in an unsupervised fashion . We then show how PixelGAN autoencoders with categorical priors can be directly used in clustering and semi - supervised scenarios and achieve very competitive classification results on several datasets in : experiments ] Section [ reference ] . Finally , we present one of the main potential applications of PixelGAN autoencoders in learning cross - domain relations between two different domains in : cross - domain ] Section [ reference ] . section : PixelGAN Autoencoders Let be a datapoint that comes from the distribution and be the hidden code . The recognition path of the PixelGAN autoencoder ( : pixelgan_gaussian ] Figure [ reference ] ) defines an implicit posterior distribution by using a deterministic neural function that takes the input along with random noise with a fixed distribution and outputs . The aggregated posterior of this model is defined as follows : This parametrization of the implicit posterior distribution was originally proposed in the adversarial autoencoder work aae as the universal approximator posterior . We can sample from this implicit distribution , by evaluating at different samples of , but the density function of this posterior distribution is intractable . endix : input_noise ] Appendix [ reference ] discusses the importance of the input noise in training PixelGAN autoencoders . The generative path is a conditional PixelCNN pixelcnn that conditions on the latent vector using an adaptive bias in PixelCNN layers . The inference is done by an amortized GAN inference technique that was originally proposed in the adversarial autoencoder work aae . In this method , an adversarial network is attached on top of the hidden code vector of the autoencoder and matches the aggregated posterior distribution , , to an arbitrary prior , . Samples from and are provided to the adversarial network as the negative and positive examples respectively , and the generator of the adversarial network , which is also the encoder of the autoencoder , tries to match to by the gradient that comes through the discriminative adversarial network . The adversarial network , the PixelCNN decoder and the encoder are trained jointly in two phases \u2013 the reconstruction phase and the adversarial phase \u2013 executed on each mini - batch . In the reconstruction phase , the ground truth input along with the hidden code inferred by the encoder are provided to the PixelCNN decoder . The PixelCNN decoder weights are updated to maximize the log - likelihood of the input . The encoder weights are also updated at this stage by the gradient that comes through the conditioning vector of the PixelCNN . In the adversarial phase , the adversarial network updates both its discriminative network and its generative network ( the encoder ) to match to . Once the training is done , we can sample from the model by first sampling from the prior distribution , and then sampling from the conditional likelihood parametrized by the PixelCNN decoder . We now establish a connection between the PixelGAN autoencoder cost and maximum likelihood learning using a decomposition of the aggregated evidence lower bound ( ELBO ) proposed in surgery : The first term in elbo ] Equation [ reference ] is the reconstruction term and the second term is the marginal KL divergence between the aggregated posterior and the prior distribution . The third term is the mutual information between the latent code and the input . This is a regularization term that encourages and to be decoupled by removing the information of the data distribution from the hidden code . If the training set has examples , is bounded as follows ( see surgery ) . In order to maximize the ELBO , we need to minimize all the three terms of elbo ] Equation [ reference ] . We consider two cases for the decoder : Deterministic Decoder . If the decoder is deterministic or has very limited stochasticity such as the simple factorized decoder of the VAE , the mutual information term acts in the complete opposite direction of the reconstruction term . This is because the only way to minimize the reconstruction error of is to learn a hidden code that is relevant to , which results in maximizing . Indeed , it can be shown that minimizing the reconstruction term maximizes a variational lower bound on i m , infogan . For example , in the case of the VAE trained on MNIST , since the reconstruction is precise , the mutual information term is dominated and is close to its maximum value surgery . Stochastic Decoder . If we use a powerful decoder such as the PixelCNN , the reconstruction term and the mutual information term will not compete with each other anymore and the network can minimize both independently . In this case , the optimal solution for maximizing the ELBO would be to model solely by and thereby minimizing the reconstruction term , and at the same time , minimizing the mutual information term by ignoring the latent code . As a result , even though the model achieves a high likelihood , the latent code does not learn any useful representation , which is undesirable . This problem has been observed in several previous works bowman , vlae and different techniques such as annealing the weight of the KL term bowman or weakening the decoder vlae have been proposed to make and more dependent . As suggested in ference_ml , vlae , we think that the maximum likelihood objective by itself is not a useful objective for representation learning especially when a powerful decoder is used . In PixelGAN autoencoders , in order to encourage learning more useful representations , we modify the ELBO ( elbo ] Equation [ reference ] ) by removing the mutual information term from it , since this term is explicitly encouraging to become independent of . So our cost function only includes the reconstruction term and the marginal KL term . The reconstruction term is optimized by the reconstruction phase of training and the marginal KL term is approximately optimized by the adversarial phase . Note that since the mutual information term is upper bounded by a constant ( ) , we are still maximizing a lower bound on the log - likelihood of data . However , this bound is weaker than the ELBO , which is the price that is paid for learning more useful latent representations by balancing the decomposition of information between the latent code and the autoregressive decoder . For implementing the conditioning adaptive bias in the PixelCNN decoder , we explore two different architectures pixelcnn . In the location - invariant bias , for each PixelCNN layer , we use the latent code to construct a vector that is broadcasted within each feature map of the layer and then added as an adaptive bias to that layer . In the location - dependent bias , we use the latent code to construct a spatial feature map that is broadcasted across different feature maps and then added only to the first layer of the decoder as an adaptive bias . We will discuss the effect of these architectures on the learnt representation in : mnist_code ] Figure [ reference ] of : pixelgan_gaussian ] Section [ reference ] and their implementation details in endix : conditioning_of_pixelcnn ] Appendix [ reference ] . subsection : PixelGAN Autoencoders with Gaussian Priors Here , we show that PixelGAN autoencoders with Gaussian priors can decompose the global and local statistics of the images between the latent code and the autoregressive decoder . : mnist ] Figure [ reference ] a shows the samples of a PixelGAN autoencoder model with the location - dependent bias trained on the MNIST dataset . For the purpose of better illustrating the decomposition of information , we have chosen a 2 - D Gaussian latent code and a limited the receptive field of size 9 for the PixelGAN autoencoder . : mnist ] Figure [ reference ] b shows the samples of a PixelCNN model with the same limited receptive field size of 9 and : mnist ] Figure [ reference ] c shows the samples of an adversarial autoencoder with the 2 - D Gaussian latent code . The PixelCNN can successfully capture the local statistics , but fails to capture the global statistics due to the limited receptive field size . In contrast , the adversarial autoencoder , whose sample quality is very similar to that of the VAE , can successfully capture the global statistics , but fails to generate the details of the images . However , the PixelGAN autoencoder , with the same receptive field and code size , can combine the best of both and generates sharp images with global statistics . In PixelGAN autoencoders , both the PixelCNN depth and the conditioning architecture affect the decomposition of information between the latent code and the autoregressive decoder . We investigate these effects in : mnist_code ] Figure [ reference ] by training a PixelGAN autoencoder on MNIST where the code size is chosen to be for the visualization purpose . As shown in : mnist_code ] Figure [ reference ] a , b , when a shallow decoder is used , most of the information will be encoded in the hidden code and there is a clean separation between the digit clusters . As we make the PixelCNN more powerful ( : mnist_code ] Figure [ reference ] c , d ) , we can see that the hidden code is still used to capture some relevant information of the input , but the separation of digit clusters is not as sharp when the limited code size of 2 is used . In the next section , we will show that by using a larger code size ( e.g. , 30 ) , we can get a much better separation of digit clusters even when a powerful PixelCNN is used . The conditioning architecture also affects the decomposition of information . In the case of the location - invariant bias , the hidden code is encouraged to learn the global information that is location - invariant ( the what information and not the where information ) such as the class label information . For example , we can see in : mnist_code ] Figure [ reference ] a , c that the network has learnt to use one of the axes of the 2D Gaussian code to explicitly encode the digit label even though a continuous prior is imposed . In this case , we can potentially get a much better separation if we impose a discrete prior . This makes this architecture suitable for the discrete vs. continuous decomposition and we use it for our clustering and semi - supervised learning experiments . In the case of the location - dependent bias ( : mnist_code ] Figure [ reference ] b , d ) , the hidden code is encouraged to learn the global information that has location dependent information such as low - frequency content of the image , similar to what the hidden code of an adversarial or variational autoencoder would learn ( : mnist ] Figure [ reference ] c ) . This makes this architecture suitable for the global vs. local decomposition experiments such as : mnist ] Figure [ reference ] a. From : mnist_code ] Figure [ reference ] , we can see that the class label information is mostly captured by while the style information of the images is captured by both and . This decomposition of information has also been studied in other works that combine the latent variable models with autoregressive decoders such as PixelVAE pixelvae and variational lossy autoencoders ( VLAE ) vlae . For example , the VLAE model vlae proposes to use the depth of the PixelCNN decoder to control the decomposition of information . In their model , the PixelCNN decoder is designed to have a shallow depth ( small local receptive field ) so that the latent code is forced to capture more global information . This approach is very similar to our example of the PixelGAN autoencoder in : mnist ] Figure [ reference ] . However , the question that has remained unanswered is whether it is possible to achieve a complete decomposition of content and style in an unsupervised fashion , where the class label or discrete structure information is encoded in the latent code , and the remaining continuous structure such as style is captured by a powerful and deep PixelCNN decoder . This kind of decomposition is particularly interesting as it can be directly used for clustering and semi - supervised classification . In the next section , we show that we can learn this decomposition of content and style by imposing a categorical distribution on the latent representation using adversarial training . Note that this discrete vs. continuous decomposition is very different from the global vs. local decomposition , because a continuous factor of variation such as style can have both global and local effect on the image . Indeed , in order to achieve the discrete vs. continuous decomposition , we have to use very deep and powerful PixelCNN decoders ( up to 20 residual blocks ) to capture both the global and local statistics of the style by the PixelCNN while the discrete content of the image is captured by the categorical latent variable . subsection : PixelGAN Autoencoders with Categorical Priors In this section , we present an architecture of the PixelGAN autoencoder that can separate the discrete information ( e.g. , class label ) from the continuous information ( e.g. , style information ) in the images . We then show how our architecture can be naturally adopted for the semi - supervised settings . The architecture that we use is similar to : pixelgan_gaussian ] Figure [ reference ] , with the difference that we impose a categorical distribution as the prior rather the Gaussian distribution ( : pixelgan_cat ] Figure [ reference ] ) and also use the location - independent bias architecture . Another difference is that we use a convolutional network as the inference network to encourage the encoder to preserve the content and lose the style information of the image . The inference network has a softmax output and predicts a one - hot vector whose dimension is the number of discrete labels or categories that we wish the data to be clustered into . The adversarial network is trained directly on the continuous probability outputs of the softmax layer of the encoder . Imposing a categorical distribution at the output of the encoder imposes two constraints . The first constraint is that the encoder has to make confident decisions about the class labels of the inputs . The adversarial training pushes the output of the encoder to the corners of the softmax simplex , by which it ensures that the autoencoder can not use the latent vector to carry any continuous style information . The second constraint imposed by adversarial training is that the aggregated posterior distribution of should match the categorical prior distribution with uniform outcome probabilities . This constraint enforces the encoder to evenly distribute the class labels across the corners of the softmax simplex . Because of these constraints , the latent variable will only capture the discrete content of the image and all the continuous style information will be captured by the autoregressive decoder . In order to better understand and visualize the effect of the adversarial training on shaping the hidden code distribution , we train a PixelGAN autoencoder on the first three digits of MNIST ( 18000 training and 3000 test points ) and choose the number of clusters to be 3 . Suppose is the hidden code which in this case is the output probabilities of the softmax layer of the inference network . In : pixelgan_cluster_toy ] Figure [ reference ] a , we project the 3D softmax simplex of onto a 2D triangle and plot the hidden codes of the training examples when no distribution is imposed on the hidden code . We can see from this figure that the network has learnt to use the surface of the softmax simplex to encode style information of the digits and thus the three corners of the simplex do not have any meaningful interpretation . : pixelgan_cluster_toy ] Figure [ reference ] b corresponds to the code space of the same network when a categorical distribution is imposed using the adversarial training . In this case , we can see the network has successfully learnt to encode the label information of the three digits in the three corners of the simplex , and all the style information has been separately captured by the autoregressive decoder . This network achieves an almost perfect test error - rate of on the first three digits of MNIST , even though it is trained in a purely unsupervised fashion . Once the PixelGAN autoencoder is trained , its encoder can be used for clustering new points and its decoder can be used to generate samples from each cluster . Figure [ reference ] illustrates the samples of the PixelGAN autoencoder trained on the full MNIST dataset . The number of clusters is set to be 30 and each row corresponds to the conditional samples of one of the clusters ( only 16 are shown ) . We can see that the discrete latent code of the network has learnt discrete factors of variation such as class label information and some discrete style information . For example digit s are put in different clusters based on how much tilted they are . The network is also assigning different clusters to digit s ( based on whether they have a loop ) and digit s ( based on whether they have a dash in the middle ) . In : experiments : unsup ] Section [ reference ] , we will show that by using the encoder of this network , we can obtain about 5 % error rate in classifying digits in an unsupervised fashion , just by matching each cluster to a digit type . Semi - Supervised PixelGAN Autoencoders . The PixelGAN autoencoder can be used in a semi - supervised setting . In order to incorporate the label information , we add a semi - supervised training phase . Specifically , we set the number of clusters to be the same as the number of class labels and after executing the reconstruction and the adversarial phases on an unlabeled mini - batch , the semi - supervised phase is executed on a labeled mini - batch , by updating the weights of the encoder to minimize the cross - entropy cost . The semi - supervised cost also reduces the mode - missing behavior of the GAN training by enforcing the encoder to learn all the modes of the categorical distribution . In : experiments : semi ] Section [ reference ] , we will evaluate the performance of the PixelGAN autoencoders on the semi - supervised classification tasks . section : Experiments In this paper , we presented the PixelGAN autoencoder as a generative model , but the currently available metrics for evaluating the likelihood of GAN - based generative models such as Parzen window estimate are fundamentally flawed theis . So in this section , we only present the performance of the PixelGAN autoencoder on downstream tasks such as unsupervised clustering and semi - supervised classification . The details of all the experiments can be found in endix : experiment ] Appendix [ reference ] . subsection : Unsupervised Clustering We trained a PixelGAN autoencoder in an unsupervised fashion on the MNIST dataset ( : pixelgan_cluster ] Figure [ reference ] ) . We chose the number of clusters to be 30 and used the following evaluation protocol : once the training is done , for each cluster , we found the validation example that maximizes , and assigned the label of to all the points in the cluster . We then computed the test error based on the assigned class labels to each cluster . As shown in the first column of le : semi ] Table [ reference ] , the performance of PixelGAN autoencoders is on par with other GAN - based clustering algorithms such as CatGAN catgan , InfoGAN infogan and adversarial autoencoders aae . subsection : Semi - supervised Classification le : semi ] Table [ reference ] and : plot ] Figure [ reference ] report the results of semi - supervised classification experiments on the MNIST , SVHN and NORB datasets . On the MNIST dataset with 20 , 50 and 100 labels , our classification results are highly competitive . Note that the classification rate of unsupervised clustering of MNIST is better than semi - supervised MNIST with 20 labels . This is because in the unsupervised case , the number of clusters is 30 , but in the semi - supervised case , there are only 10 class labels which makes it more likely to confuse two digits . On the SVHN dataset with 500 and 1000 labels , the PixelGAN autoencoder outperforms all the other methods except the recently proposed temporal ensembling work temporal - ensembling which is not a generative model . On the NORB dataset with 1000 labels , the PixelGAN autoencoder outperforms all the other reported results . : disentangle ] Figure [ reference ] shows the conditional samples of the semi - supervised PixelGAN autoencoder on the MNIST , SVHN and NORB datasets . Each column of this figure presents sampled images conditioned on a fixed one - hot latent code . We can see from this figure that the PixelGAN autoencoder can achieve a rather clean separation of style and content on these datasets with very few labeled data . section : Learning Cross - Domain Relations with PixelGAN Autoencoders In this section , we discuss how the PixelGAN autoencoder can be viewed in the context of learning cross - domain relations between two different domains . We also describe how the problem of clustering or semi - supervised learning can be cast as the problem of finding a smooth cross - domain mapping from the data distribution to the categorical distribution . Recently several GAN - based methods have been developed to learn a cross - domain mapping between two different domains discogan , cyclegan , cross - domain - ilya , aae , cross - domain - nlp . In cross - domain - ilya , an unsupervised cost function called the output distribution matching ( ODM ) is proposed to find a cross - domain mapping between two domains and by imposing the following unsupervised constraint on the uncorrelated samples from and : where denotes the distribution of the random variable . The adversarial training is proposed as one of the methods for matching these distributions . If we have access to a few labeled pairs , then can be further trained on them in a supervised fashion to satisfy . For example , in speech recognition , we want to find a cross - domain mapping from a sequence of phonemes to a sequence of characters . By optimizing the ODM cost function in odm ] Equation [ reference ] , we can find a smooth function that takes phonemes at its input and outputs a sequence of characters that respects the language model . However , the main problem with this method is that the network can learn to ignore part of the input distribution and still satisfy the ODM cost function by its output distribution . This problem has also been observed in other works such as discogan . One way to avoid this problem is to add a reconstruction term to the ODM cost function by introducing a reverse mapping from the output of the encoder to the input domain . The is essentially the idea of the adversarial autoencoder aae which learns a generative model by finding a cross - domain mapping between a Gaussian distribution and the data distribution . Using the ODM cost function along with a reconstruction term to learn cross - domain relations have been explored in several previous works . For example , InfoGAN infogan adds a mutual information term to the ODM cost function and optimizes a variational lower bound on this term . It can be shown that maximizing this variational bound is indeed minimizing the reconstruction cost of an autoencoder i m . Similarly , in cross - domain - nlp , zhangadversarial , an adversarial autoencoder is used to learn the cross - domain relations of the vector representations of words from two different languages . The architecture of the recent works of DiscoGAN discogan and CycleGAN cyclegan are also similar to an adversarial autoencoder in which the latent representation is enforced to have the distribution of the other domain . Here we describe how our proposed PixelGAN autoencoder can be potentially used in all these application areas to learn better cross - domain relations . Suppose we want to learn a mapping from domain to . In the architecture of : pixelgan_gaussian ] Figure [ reference ] , we can use independent samples of at the input and instead of imposing a Gaussian distribution on the latent code , we can impose the distribution of the second domain using its independent samples . Unlike adversarial autoencoders , the encoder of PixelGAN autoencoders does not have to retain all the input information in order to have a lossless reconstruction . So the encoder can use all its capacity to learn the most relevant mapping from to and at the same time , the PixelCNN decoder can capture the remaining information that has been lost by the encoder . We can adopt the ODM idea for semi - supervised learning by assuming is the image domain and is the label domain ( : related ] Figure [ reference ] a ) . Independent samples of and correspond to samples from the data distribution and the categorical distribution . The function can be parametrized by a neural network that is trained to satisfy the ODM cost function by matching the aggregated distribution to the categorical distribution using adversarial training . The few labeled examples are used to further train to satisfy . However , as explained above , the problem with this method is that the network can learn to generate the categorical distribution by ignoring some part of the input distribution . The adversarial autoencoder ( : related ] Figure [ reference ] b ) solves this problem by adding an inverse mapping from the categorical distribution to the data distribution . However , the main drawback of the adversarial autoencoder architecture is that due to the reconstruction term , the latent representation now has to model all the underlying factors of variation in the image . For example , in the architecture of : related ] Figure [ reference ] b , while we are only interested in the one - hot label representation to do semi - supervised learning , we also need to infer the style of the image so that we can have a lossless reconstruction of the image . The PixelGAN autoencoder solves this problem by enabling the encoder to only infer the factor of variation that we are interested in ( i.e. , label information ) , while the remaining structure of the input ( i.e. , style information ) is automatically captured by the autoregressive decoder . section : Conclusion In this paper , we proposed the PixelGAN autoencoder , which is a generative autoencoder that combines a generative PixelCNN with a GAN inference network that can impose arbitrary priors on the latent code . We showed that imposing different distributions as the prior enables us to learn a latent representation that captures the type of statistics that we care about , while the remaining structure of the image is captured by the PixelCNN decoder . Specifically , by imposing a Gaussian prior , we were able to disentangle the low - frequency and high - frequency statistics of the images , and by imposing a categorical prior we were able to disentangle the style and content of images and learn representations that are specifically useful for clustering and semi - supervised learning tasks . While the main focus of this paper was to demonstrate the application of PixelGAN autoencoders in downstream tasks such as semi - supervised learning , we discussed how these architectures have many other potentials such as learning cross - domain relations between two different domains . section : Acknowledgments We would like to thank Nathan Killoran for helpful discussions . We also thank NVIDIA for GPU donations . bibliography : References section : Implementation Details In this section , we describe two important architecture design choices for training PixelGAN autoencoders . subsection : Input noise In all the semi - supervised experiments , we found it crucial to use the universal approximator posterior discussed in : pixelgan ] Section [ reference ] , as opposed to a deterministic posterior . Specifically , the input noise that we use is an additive Gaussian noise , which results in a posterior distribution that is more expressive than that of a model without the input corruption . This is similar to the denoising criterion idea proposed in denoising - vae . We believe this additive noise is also playing an important role in preventing the mode - missing behavior of the GAN when imposing a degenerate distribution such as the categorical distribution . Similar related ideas have been used to stabilize GAN training such as instance noise instance or one - sided label noise improved - gan . subsection : Conditioning of PixelCNN There are three methods to implement how the PixelCNN conditions on the latent vector . Location - Invariant Bias . This is the method that was proposed in the conditional PixelCNN model pixelcnn . Suppose the size of the convolutional layer of the decoder is ( batch , width , height , channels ) . Then the PixelCNN can use a linear mapping to convert the conditioning tensor of size ( batch , condition_size ) to generate a tensor of size ( batch , channels ) that is then broadcasted and added to the feature maps of all the layers of the PixelCNN decoder as an adaptive bias . In this method , the hidden code is encouraged to learn the global information that is location - invariant ( the what information and not the where information ) such as the class label information . We use this method in all the clustering and semi - supervised learning experiments . Location - Dependent Bias . Suppose the size of the convolutional layer of the PixelCNN decoder is ( batch , width , height , channels ) . Then the PixelCNN can use a one layer neural network to convert the conditioning tensor of size ( batch , condition_size ) to generate a spatial tensor of size ( batch , width , height , k ) followed by a convolutional layer to construct a tensor of size ( batch , width , height , channels ) that is then added only to the feature maps of the first layer of the decoder as an adaptive bias ( similar to the VPN model vpn ) . When , we can simply broadcast the tensor of size ( batch , width , height , k=1 ) to get a tensor of size ( batch , width , height , channels ) instead of using the convolution . In this method , the latent vector has spatial and location - dependent information within the feature map . This is the method that we used in experiments of : mnist ] Figure [ reference ] a. Input Channel . Another method for conditioning is proposed in the PixelVAE pixelvae and the variational lossy autoencoder ( VLAE ) vlae . In this method , first a tensor of size ( batch , width , height , k ) is constructed using the conditioning tensor similar to the location - dependent bias . This tensor is then concatenated to the input of the PixelCNN . The performance and computational complexity of this method is very similar to that of the location - dependent bias method . section : Experiment Details We used TensorFlow tensorflow2015 - whitepaper in all of our experiments . As suggested in gan , in order to improve the stability of GAN training , the generator of the GAN in all our experiments is trained to maximize rather than minimizing . subsection : MNIST Dataset The MNIST dataset has 50 K training points , 10 K validation points and 10 K test points . We perform experiments on both the binary MNIST and the real - valued MNIST . In the real valued MNIST experiments , we subtract 127.5 from the data points and then divide them by 127.5 and use the discretized logistic mixture likelihood pixelcnn ++ as the cost function for the PixelCNN . In the case of binary MNIST , the data points are binarized by setting pixel values larger than 0.5 to 1 , and values smaller than 0.5 to 0 . subsubsection : PixelGAN Autoencoders with Gaussian Prior on MNIST Here we describe the model architecture used for training the PixelGAN autoencoder with a Gaussian prior on the binary MNIST dataset in : mnist ] Figure [ reference ] a. The PixelCNN decoder uses both the vertical and horizontal stacks similar to pixelcnn . The cost function of the PixelCNN is the cross - entropy cost function . The PixelCNN uses the location - dependent bias as described in endix : conditioning_of_pixelcnn ] Appendix [ reference ] . Specifically , a tensor of size ( batch , width , height , 1 ) is constructed from the conditioning vector by using a one - layer neural network with 1000 hidden units , ReLU activation and linear output . This tensor is then broadcasted and added only to the feature maps of the first layer of the PixelCNN decoder . The PixelCNN is designed to have a local receptive field by having 3 residual blocks ( filter size of 3x5 , 32 feature maps , ReLU non - linearity as in pixelcnn ) . The adversarial discriminator has two layers of 2000 hidden units with ReLU activation function . The encoder architecture has two fully - connected layers of size 2000 with ReLU non - linearity . The last layer of the encoder has a linear activation function . On the latent representation of size , we impose a Gaussian distribution with standard deviation of . We used the gradient descent with momentum algorithm for optimizing all the cost functions of the network . For the PixelCNN reconstruction cost , we used the learning rate of 0.001 and the momentum value of 0.9 . After 25 epochs we reduce the learning rate to 0.0001 . For both of the generator and the discriminator costs , the learning rates and the momentum values were set to 0.1 . subsubsection : Unsupervised Clustering of MNIST Here we describe the model architecture used for clustering the binary MNIST dataset in : pixelgan_cluster ] Figure [ reference ] and : experiments : unsup ] Section [ reference ] . The PixelCNN decoder uses both the vertical and horizontal stacks similar to pixelcnn . The cost function of the PixelCNN is the cross - entropy cost function . The PixelCNN uses the location - invariant bias as described in endix : conditioning_of_pixelcnn ] Appendix [ reference ] and has 15 residual blocks ( filter size of 3x5 , 32 feature maps , ReLU non - linearity as in pixelcnn ) . The adversarial discriminator has two layers of 3000 hidden units with ReLU activation function . The encoder architecture has a convolutional layer ( filter size of 7 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) , followed by another convolutional layer ( filter size of 7 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) with no fully - connected layer . The last layer of the encoder has the softmax activation function . We found it important to use batch - normalization batch for all the layers of the encoder including the softmax layer . The number of clusters is chosen to be . The clusters are represented by a discrete one - hot variable of size 30 . On the continuous probability output of the softmax , we impose a categorical distribution with uniform probabilities . We use Adam Adam optimizer with learning rate of for optimizing the PixelCNN reconstruction cost function , but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network . For both of the generator and the discriminator costs , the momentum values were set to 0.1 and the learning rates were set to 0.01 . We use an input dropout noise with the keep probability of at the input layer and only at the training time . The model architecture used for : pixelgan_cluster_toy ] Figure [ reference ] is the same as this architecture except that the number of clusters is chosen to be . subsubsection : Semi - Supervised MNIST We performed semi - supervised learning experiments on both binary and real - valued MNIST dataset . We found that the semi - supervised error - rate of the real - valued MNIST is roughly the same as the binary MNIST ( about 1.10 % with 100 labels ) , but it takes longer to train due to the logistic mixture likelihood cost function pixelcnn ++ . So in le : semi ] Table [ reference ] , we only report the performance with the binary MNIST , but in : disentangle ] Figure [ reference ] b we are showing the samples of the real - valued MNIST with 100 labels . Binary MNIST . Here we describe the model architecture used for the semi - supervised learning experiments on the binary MNIST in : experiments : semi ] Section [ reference ] and le : semi ] Table [ reference ] . The PixelCNN decoder uses both the vertical and horizontal stacks similar to pixelcnn and uses the cross - entropy cost function . The PixelCNN uses the location - invariant bias as described in endix : conditioning_of_pixelcnn ] Appendix [ reference ] . The PixelCNN has 6 residual blocks ( filter size of 3x5 , 32 feature maps , ReLU non - linearity as in pixelcnn ) . The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function . The encoder architecture has three convolutional layers ( filter size of 5 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) , followed by another three convolutional layers ( filter size of 5 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) with no fully - connected layer . The last layer of the encoder has the softmax activation function . All the convolutional layers of the encoder except the softmax layer use batch - normalization batch . On the latent representation , we impose a categorical distribution with uniform probabilities . The semi - supervised cost is the cross - entropy cost function at the output of . We use Adam Adam optimizer with learning rate of for optimizing the PixelCNN cost and the cross - entropy cost , but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network . For both of the generator and the discriminator costs , the momentum values were set to 0.1 and the learning rates were set to 0.1 . We add a Gaussian noise with standard deviation of to the input layer as described in endix : input_noise ] Appendix [ reference ] . The labeled examples were chosen at random but evenly distributed across the classes . Real - valued MNIST . Here we describe the model architecture used for the semi - supervised learning experiments on the real - valued MNIST in : disentangle ] Figure [ reference ] b. The PixelCNN decoder uses both the vertical and horizontal stacks similar to pixelcnn and uses a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in pixelcnn ++ . The PixelCNN uses the location - invariant bias as described in endix : conditioning_of_pixelcnn ] Appendix [ reference ] . The PixelCNN has 20 residual blocks ( filter size of 2x3 , 64 feature maps , gated sigmoid - tanh non - linearity as in pixelcnn ) . The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function . The encoder architecture has three convolutional layers ( filter size of 5 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) , followed by another three convolutional layers ( filter size of 5 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) with no fully - connected layer . The last layer of the encoder has the softmax activation function . All the convolutional layers of the encoder except the softmax layer use batch - normalization batch . On the latent representation , we impose a categorical distribution with uniform probabilities . The semi - supervised cost is the cross - entropy cost function at the output of . We use Adam Adam optimizer with learning rate of for optimizing the PixelCNN cost and the cross - entropy cost , but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network . For both of the generator and the discriminator costs , the momentum values were set to 0.1 and the learning rates were set to 0.1 . After 150 epochs , we divide all the learning rates by 10 . We add a Gaussian noise with standard deviation of to the input layer as described in endix : input_noise ] Appendix [ reference ] . The labeled examples were chosen at random but evenly distributed across the classes . subsection : SVHN Dataset The SVHN dataset has about 530 K training points and 26 K test points . We use 10 K points for the validation set . Similar to vat , we downsample the images from to and then subtracte 127.5 from the data points and then divide them by 127.5 . subsubsection : Semi - Supervised SVHN Here we describe the model architecture used for the semi - supervised learning experiments on the SVHN dataset in : experiments : semi ] Section [ reference ] . The PixelCNN decoder uses both the vertical and horizontal stacks similar to pixelcnn . The cost function of the PixelCNN is a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in pixelcnn ++ . The PixelCNN uses the location - invariant bias as described in endix : conditioning_of_pixelcnn ] Appendix [ reference ] and has 20 residual blocks ( filter size of 3x5 , 32 feature maps , gated sigmoid - tanh non - linearity as in pixelcnn ) . The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function . The encoder architecture has two convolutional layers ( filter size of 5 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) , followed by another two convolutional layers ( filter size of 5 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) with no fully - connected layer . The last layer of the encoder has the softmax activation function . All the convolutional layers of the encoder except the softmax layer use batch - normalization batch . On the latent representation , we impose a categorical distribution with uniform probabilities . The semi - supervised cost is the cross - entropy cost function at the output of . We use Adam Adam optimizer for optimizing all the cost function . For the PixelCNN cost and the cross - entropy cost we use the learning rate of and for the generator and the discriminator costs of the adversarial network we use the learning rate of . We add a Gaussian noise with standard deviation of to the input layer as described in endix : input_noise ] Appendix [ reference ] . subsection : NORB Dataset The NORB dataset has about 24 K training points and 24 K test points . We use 4 K points for the validation set . This dataset has 5 object categories : animals , human figures , airplanes , trucks and cars . We downsample the images to have the size of , subtract 127.5 from the data points and then divide them by 127.5 . subsubsection : Semi - Supervised NORB The PixelCNN decoder uses both the vertical and horizontal stacks similar to pixelcnn . The cost function of the PixelCNN is a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in pixelcnn ++ . The PixelCNN uses the location - invariant bias as described in endix : conditioning_of_pixelcnn ] Appendix [ reference ] and has 15 residual blocks ( filter size of 3x5 , 32 feature maps , gated sigmoid - tanh non - linearity as in pixelcnn ) . The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function . The encoder architecture has a convolutional layer ( filter size of 7 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) , followed by another convolutional layer ( filter size of 7 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) , followed by another convolutional layer ( filter size of 7 , 32 feature maps , ReLU activation ) and a max - pooling layer ( pooling size 2 ) with no fully - connected layer . The last layer of the encoder has the softmax activation function . All the convolutional layers of the encoder except the softmax layer use batch - normalization batch . On the latent representation , we impose a categorical distribution with uniform probabilities . The semi - supervised cost is the cross - entropy cost function at the output of . We use Adam Adam optimizer for optimizing all the cost function . For the PixelCNN cost and the cross - entropy cost we use the learning rate of and for the generator and the discriminator costs of the adversarial network we use the learning rate of . We add a Gaussian noise with standard deviation of to the input layer as described in endix : input_noise ] Appendix [ reference ] . The labeled examples were chosen at random but evenly distributed across the classes . In the case of NORB with 1000 labels , the test error after 10 epochs is 12.97 % , after 100 epochs is 11.63 % and after 500 epochs is 8.17 % .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["MNIST"]]], "Method": [[["PixelGAN_Autoencoders"]]], "Metric": [[["Accuracy"]]], "Task": [[["Unsupervised_MNIST"]]]}, {"incident_type": "SciREX_incident", "Material": [[["MNIST"]]], "Method": [[["PixelGAN_Autoencoders"]]], "Metric": [[["Accuracy"]]], "Task": [[["Unsupervised_image_classification"]]]}]}
{"docid": "TST3-SREX-0014", "doctext": "document : Submanifold Sparse Convolutional Networks Convolutional network are the de - facto standard for analysing spatio - temporal data such as images , videos , 3D shapes , etc . Whilst some of this data is naturally dense ( for instance , photos ) , many other data sources are inherently sparse . Examples include pen - strokes forming on a piece of paper , or ( colored ) 3D point clouds that were obtained using a LiDAR scanner or RGB - D camera . Standard \u2018 \u2018 dense \u2019 \u2019 implementations of convolutional networks are very inefficient when applied on such sparse data . We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds , rather than \u2018 \u2018 dilating \u2019 \u2019 the observation with every layer in the network . Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state - of - the - art methods whilst requiring substantially less computation . section : Introduction Convolutional networks constitute the state - of - the art method for a wide range of tasks that involve the analysis of data with spatial and / or temporal structure , such as photographs , videos , or three - dimensional surface models . While such data frequently comprises a densely filled ( 2D or 3D ) grid , other spatio - temporal datasets are naturally sparse . For instance , handwriting is made up of one - dimensional lines in two - dimensional space , pictures made by RGB - D cameras are three - dimensional point clouds , and OFF models form two - dimensional surfaces in 3D space . The curse of dimensionality applies , in particular , on data that lives on grids that have three or more dimensions : the number of points on the grid grows exponentially with its dimensionality . In such scenarios , it becomes increasingly important to exploit data sparsity whenever possible in order to reduce the computational resources needed for data processing . Indeed , exploiting sparsity is paramount when analyzing , for instance , RGB - D videos which are sparsely populated 4D structures . Traditional convolutional network implementations are optimized for data that lives on densely populated grids , and can not process sparse data efficiently . More recently , a number of convolutional network implementations have been presented that are tailored to work efficiently on sparse data . Mathematically , some of these implementations are identical to a regular convolutional network , but they require fewer computational resources in terms of FLOPs and / or in terms of memory . OctNets slightly modify the convolution operator to produce \u2018 \u2018 averaged \u2019 \u2019 hidden states in parts of the grid that are away from regions of interest . One of the downsides of prior sparse implementations of convolutional networks is that they \u2018 \u2018 dilate \u2019 \u2019 the sparse data in every layer , because they implement a \u2018 \u2018 full \u2019 \u2019 convolution . In this work , we show that it is possible to successfully train convolutional networks that keep the same sparsity pattern throughout the layers of the network , without dilating the feature maps . To this end , we explore two novel convolution operators : sparse convolution ( SC ) and valid sparse convolution ( VSC ) . In our experiments with recognizing handwritten digits and 3D shapes , networks using SC and VSC achieve state - of - the - art performance whilst reducing the computation and memory requirements by . section : Motivation We define a - dimensional convolutional network as a network that takes as input that is a - dimensional tensor : the input tensor contains spatiotemporal dimensions ( such as length , width , height , time , etc . ) and one additional feature space dimension ( for instance , RGB color channels , surface normal vectors , etc . ) . A sparse input corresponds to a - dimensional grid of sites that is associated with a feature vector . We define a site in the input to be active if any element in the feature vector is not in its ground state , for instance , if it is non - zero . In many practical problems , thresholding may be used to eliminate sites at which the feature vector is within a very small distance from the ground state . Note that even though the input tensor is - dimensional , activity is a - dimensional phenomenon : entire planes along the feature dimension are either active or not . The hidden layers of a convolutional network are also represented by - dimensional grids of feature - space vectors . When propagating the input data through the network , a site in a hidden layer is active if any of the sites in the layer that it takes as input is active . ( Note that when using convolutions , each site is connected to sites in the hidden layer below . ) Activity in a hidden layer thus follows an inductive definition in which each layer determines the set of active states in the next . In each hidden layer , inactive sites all have the same feature vector : the one corresponding to the ground state . Note that the ground state in a hidden layer is often not equal to zero , in particular , when convolutions with a bias term are used . However , irrespective of the value of the ground state , the ground - state value only needs to be calculated once per forward pass during training ( and only once for all forward passes at test time ) . This allows for substantial savings in computational and memory requirements ; the exact savings depend on the data sparsity and the network depth . In this paper , we argue that the framework described above is unduly restrictive , in particular , because the convolution operation has not been modified to accommodate the sparsity of the input data . If the input data contains a single active site , then after applying a convolution , there will be active sites . Applying a second convolution of the same size will yield active sites , and so on . This rapid growth of the number of active sites is a poor prospect when implementing modern convolutional network architectures that comprise tens or even hundreds of convolutions , such as VGG networks , ResNets , and DenseNets . Of course , convolutional networks are not often applied to inputs that only have a single active site , but the aforementioned \u2018 \u2018 dilation \u2019 \u2019 problems are equally problematic when the input data comprises one - dimensional curves in spaces with two or more dimensions , or two - dimensional surfaces in three or more dimensions . To address the problems with dilation of active sites , we propose two slightly different convolution operations for use in convolutional networks . What the two operations have in common is that they both ignore the ground state : they replace the ground state with a zero vector to simplify the convolution operations . The difference between both operations is in how they handle active sites : instead of automatically making a site active if any of the inputs to its receptive field is active ( thereby dilating the set of active sites ) , our most efficient convolutional operation only considers the central input . As a result , the output set of active sites exactly mirrors that of the input set . We empirically demonstrate that use of our adapted convolutional operators allows us to build much deeper networks that achieve state - of - the - art results whilst requiring much fewer resources by preserving sparsity . subsection : Submanifold Dilation In Figure [ reference ] , we show an example of a one - dimensional curve that is embedded on a two - dimensional grid . The figure shows that even when we apply small convolutions on this grid , the sparsity on the grid rapidly disappears . At the same time , if we restrict the output of the convolution only to the set of active input points , hidden layers in the network can not capture a lot of information that may relevant to the classification of the curve . In particular , two neighboring connected components will be treated completely independently . Luckily , nearly all convolutional networks incorporate some form of pooling , or use strided convolutions . These operations are essential in the sparse convolutional networks we investigate , as they allow neighboring components to merge . In particular , the closer the components are , the smaller the number of poolings / strided convolutions is that is necessary for the components to merge in the hidden - layer representations . subsection : Very Deep Convolutional Networks In image classification , very deep convolutional networks with small filters , often of size pixels and a padding of pixel ( to preserve the size of the feature maps ) , have proven to be very effective . Such small filters were used successfully in VGG networks , which have relatively wide layers . The introduction of residual networks ( ResNets ) showed that deeper but narrow networks with small filters are more efficient . The success of very deep ResNets , ResNeXt models , and DenseNets with bottleneck connections shows that it can be useful to calculate a relatively small number of features at a time and amalgamate these features into a larger state variable , either by vector - addition or feature - vector concatenation . Unfortunately , these techniques are impractical using existing sparse convolutional network implementations . One problem is that networks with multiple paths will tend to generate different sets of active paths , which would have to be merged to reconnect the outputs . It seems that this would be difficult to perform this merging efficiently . More importantly , ResNets and DenseNets generate such large receptive fields that sparsity would almost immediately be destroyed by the explosion in the number of active sites . section : ( Valid ) Sparse Convolutions : SC and VSC We define a sparse convolution SC ( with input feature planes , output feature planes , a filter size of , and stride . We assume and to be odd integers , but we can allow generalization to non - square filters , e.g. , or , if we want to implement Inception - style factorised convolutions . An SC convolution computes the set of active sites in the same way as a regular convolution : it looks for the presence of any active sites in its receptive field of size . If the input has size then the output will have size . An SC convolution differs from a regular convolution in that it discards the ground state for non - active sites by assuming that the input from those sites is exactly zero . Whereas this is a seemingly small change to the convolution operation , it may bring computational benefits in practice . Next , we define a second type of sparse convolution , which forms the main contribution of this paper . Again , let denote an odd number , or collection of odd numbers , e.g. , or . We define a valid sparse convolution VSC as a modified SC convolution . First , we pad the input with on each side , so that the output will have the same size as the input . Next , we restrict an output site to be active if and only if the site at the corresponding site in the input is active ( i.e. , if the central site in the receptive field is active ) . Whenever an output site is determined to be active , its output feature vector is calculated by the SC operation . Table [ reference ] presents the computational and memory requirements of a regular convolution ( C ) and of our SC and VSC convolutions . To construct convolutional networks using SC and VSC , we also need activation functions , batch normalization , and pooling . Activation functions are defined as usual , but are restricted to the set of active sites . Similarly , we define batch normalization in terms of regular batch - normalization applied over the set of active sites . Max - pooling MP and average - pooling AP operations are defined as a variant of SC . MP takes the maximum of the zero vector and the input feature vectors in the receptive field . AP calculates times the sum of the active input vectors . We also define a deconvolution operation DC as an inverse of the SC convolution . The set of active output sites from a DC convolution is exactly the set of input active sites to the matching SC convolution . The set of connections between input - output sites is simply inverted . subsection : Submanifold Convolutional Networks We use a combination of VSC convolutions , strided SC convolutions , and sparse pooling operations to build sparse versions of the popular VGG , ResNet , and DenseNet convolutional networks . The blocks we use in our networks are presented in Figure [ reference ] . We refer to our networks as submanifold convolutional networks , because they are optimised to process low - dimensional data living in a space of higher dimensionality . We use the name VGG to refer to networks that contain a number of VSC ( , , 3 , 1 ) convolutions , separated by max - pooling . Each convolution is followed by batch normalization and a ReLU non - linearity . Similarly , we define \u2018 \u2018 pre - activated ResNets \u2019 \u2019 in which most data processing is performed by pairs of VSC ( , , 3 , 1 ) convolutions , and in which the residual connections are identity functions . Whenever the number of input / output features is different , we use a VSC ( , , 1 , 1 ) instead . Whenever there is change of scale , we replace the first convolution and the residual connection by a SC ( , , 3 , 2 ) convolution . This ensures that two branches can use the same hash table of active sites , and reduces additions to a simple sum of two equally sized matrices . The increased size of the residual connection \u2019s receptive field also prevents excessive information loss . We also experiment with submanifold DenseNets . Herein , the word dense does not refer to a lack of spatial sparsity but rather to the pattern of connections between convolution operations . A simple DenseNet module is a sequence of convolutions in which each convolution takes as input the concatenated output of all the previous convolution operations . The bottleneck layers in our submanifold DenseNets are implemented in the same way as for ResNets . section : Implementation To implement ( V ) SC convolutions efficiently , we store the state of a input / hidden layer in two parts : a hash table and a matrix . The matrix has size and contains one row for each of the active sites . The hash table contains ( location , row ) pairs for all active sites : the location is a tuple of integer coordinates , and the row number indicates the corresponding row in the feature matrix . Given a convolution with filter size , we define a rule book to be a collection of integer matrices of size . To implement an SC ( convolution , we : Iterate once through the the input hash - table . We build the output hash table and rule book on - the - fly by iterating over points in the output layer that receive input from a given point in the input layer . When an output site is visited for the first time , a new entry is created in the output hash table . Based on the spatial offset between the input and output points , a ( input index , output index ) pair is added to the rule book . Initialize the output matrix to all zeros . For each , there is a parameter matrix with size . For each , multiply the - th row of the input feature matrix by and add it to the - th row of the the output feature matrix . This can be implemented very efficiently on GPUs because it is a matrix - matrix multiply - add operation . To implement a VSC convolution , we re - use the input hash table for the output , and construct an appropriate rule book . Note that because the sparsity pattern does not change , the same rule book can be re - used in VGG / ResNet / DenseNet networks until a pooling or subsampling layer is encountered . If there are active points in the input layer , the cost of building the input hash - table is . For VGG / ResNet / DenseNet networks , assuming the number of active sites reduces by a multiplicative factor with each pooling operation , the cost of building all the hash - tables and rule - books is also , regardless of the depth of the network . section : Experiments We perform experiments on a 2D and a 3D dataset with sparse images . The CASIA dataset contains samples of 3755 GBK level - 1 characters with approximately 240 train and 60 test images per class . CJVK characters are good test cases for our models because they are a worst - case scenario for sparse convolutional networks : when drawn at scale , about 8 % of the pixels are active , but this percentage rapidly decreases after pooling due to the small density of the pen strokes . This makes them a good test case for our models . The ModelNet - 40 datasethttp: // 3dshapenets.cs.princeton.edu / contain 2468 CAD models that contain shapes corresponding to 40 classes . We follow the preprocessing of before feeding the models into our convolutional networks . All CAD models were rendered as surfaces at size . subsection : Results on CASIA We first experiment with two VGG architectures on the CASIA dataset . We trained all models for 100 epochs using batches of size 100 , SGD with momentum 0.9 , a weight decay of , and a learning rate decay of 5 % per epoch . For simplicity , we do not employ any data augmentation . The architectures of our VGG networks and their performances are presented in Table [ reference ] . We observe that \u2018 \u2018 regular \u2019 \u2019 C convolutions and \u2018 \u2018 sparse \u2019 \u2019 SC convolutions achieve the same error : this result suggests that discarding the ground state has essentially no negative impact on performance . This is an argument for always discarding ground states , as it makes things easier computationally and algorithmically . Comparing SC with VSC convolutions , we observe a minimal loss in performance by considering only the valid part of the convolution . This minimal loss in accuracy does facilitate great computational improvements : networks using VSC use 2 to 3 less computation and memory . Next , we performed experiments on CASIA with submanifold ResNets . The key difference between our implementation of ResNets and regular ResNets is that stride - 2 ResNet modules use SC ( , , 3 , 2 ) convolutions for the strided convolution , rather than SC ( , , 1 , 2 ) . This change is necessary to ensure the two branches produce the same set of active sites , which simplifies bookkeeping and turns the addition operation into a simple matrix - matrix addition . Unlike the VSC convolutions that are used in most layers , the SC ( , , 3 , 2 ) we use after downsampling leads sites to be active if any of its inputs are active , which avoids information loss in the transition . The architectures of our ResNet networks and their performances are presented in Table [ reference ] . The results with ResNets are in line with those obtained using VGG networks : we obtain reductions in computational and memory requirements by at least a factor of 2 at a minimal loss in accuracy . We also performed experiments with DenseNets ; please see Table [ reference ] . Next we experimented with adding extra connections to VGG networks to increase the effective receptive fields of the hidden states ; see Table [ reference ] for results . In the table , denotes a VSC convolution performed in parallel with a chain of SC - VSC - DC operations ; outputs are concatenated to produce output feature planes . To simplify the network design , we switched to size - 3 stride - 2 max - pooling , matching the SC convolutions in the SC - VSC - DC branches , and reduce the input size from 64 64 to 63 63 . Figure [ reference ] presents an overview of all our results on the CASIA dataset . subsection : Results on ModelNet In a second set of experiments , we compare two submanifold VGG networks with a state - of - the - art dense convolutional network on the ModelNet - 40 dataset . The results of these experiments are shown in Table [ reference ] : the left part of the table shows the architecture and performance of our submanifold VGG networks , whereas the right part of the table shows that of the dense 3DNiN network . The results clearly demonstrate that submanifold have the potential for designing convolutional networks for sparse data that obtain state - of - the - art performance with limited computational requirements : in particular , our VGG - A network makes 2 % more errors at 13 fewer computations , and our VGG - B performs roughly on par with the dense 3DNiN whilst performing fewer computations . section : Related Work This paper is not the first to study sparse convolutional networks . Most prior networks for sparse data implements a standard convolutional operator that increases the number of active sites with each layer . By contrast , our submanifold convolutional networks allows sparse data to be processed whilst retraining a much greater degree of sparsity . We have shown that this makes it practical to train deep and efficient VGG and ResNet models . Submanifold convolutional networks are also much sparser than OctNets . OctNet stores data in oct - trees : a data structure in which the grid cube is progressively subdivided into smaller sub - cubes until the sub - cubes are either empty or contain a single active site . To compare the efficiency of OctNets with that of submanifold convolutional networks , we picked a random sample from the ModelNet - 40 dataset and rendered it in a cube with grid points . The resulting grid had 423 active sites , which corresponds to 1.3 % of the total number of sites . Each active site had on average 12.4 active neighbors ( the maximum possible number of neighbors is 27 ) . VSC convolutions , therefore , require only 0.6 % of the work of a dense ( C ) convolution . However , in the OctTree , 80 % , 13 % , 4 % , and 3 % of the volume of the cube is covered by sub - cubes of size , , and , respectively . As a result , an OctNet convolution , which operates over the surfaces of the smaller cubes , requires about 35 % of the computations that a dense ( C ) convolution requires . In this particular example , an OctNet convolution thus has a computational cost that is 60 times higher than that of a VSC convolution . Submanifold convolutional networks also have advantages in terms of memory requirements . In particular , a submanifold network stores a single feature vector for each of the active sites . By contrast , OctTrees have about twice as many empty child nodes as active nodes , which implies they have to store roughly three times as many features as a submanifold convolutional network . Having said that , some of the ideas of may be combined with VSC convolutions . In particular , it is possible to use oct - trees as a specialized hash function in VSC convolutions . Such an oct - tree - based hash function has the potential to be faster than a standard universal hash function that operates on integer tuple keys , like in our implementation of VSC . section : Conclusion We introduced a new sparse convolutional operator , called valid sparse convolution ( VSC ) , that facilitates the design of efficient , deep convolutional networks for sparse data . We have shown that VSC convolutions lead to substantial computational savings whilst maintain state - of - the - art accuracies on two datasets : a dataset comprising one - dimensional manifolds embedded in two - dimensional space , and a dataset comprising two - dimensional surfaces embedded in three - dimensional space . As part of this paper , we are releasing easy - to - use implementations of VSC and the other sparse operations we used in the networks described in this paper . We will also release code to reproduce the results of our experiments . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["ShapeNet-Part"]]], "Method": [[["SSCN"]]], "Metric": [[["Instance_Average_IoU"]]], "Task": [[["3D_Part_Segmentation"]]]}]}
{"docid": "TST3-SREX-0015", "doctext": "document : Natural Language Inference by Tree - Based Convolution and Heuristic Matching In this paper , we propose the TBCNN - pair model to recognize entailment and contradiction between two sentences . In our model , a tree - based convolutional neural network ( TBCNN ) captures sentence - level semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences . Experimental results show that our model outperforms existing sentence encoding - based approaches by a large margin . section : Introduction Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in inference . Provided with a premise sentence , the task is to judge whether the hypothesis can be inferred ( entailment ) , or the hypothesis can not be true ( contradiction ) . Several examples are illustrated in Table [ reference ] . NLI is in the core of natural language understanding and has wide applications in NLP , e.g. , question answering and automatic summarization . Moreover , NLI is also related to other tasks of sentence pair modeling , including paraphrase detection , relation recognition of discourse units , etc . Traditional approaches to NLI mainly fall into two groups : feature - rich models and formal reasoning methods . Feature - based approaches typically leverage machine learning models , but require intensive human engineering to represent lexical and syntactic information in two sentences . Formal reasoning , on the other hand , converts a sentence into a formal logical representation and uses interpreters to search for a proof . However , such approaches are limited in terms of scope and accuracy . The renewed prosperity of neural networks has made significant achievements in various NLP applications , including individual sentence modeling as well as sentence matching . A typical neural architecture to model sentence pairs is the \u201c Siamese \u201d structure , which involves an underlying sentence model and a matching layer to determine the relationship between two sentences . Prevailing sentence models include convolutional networks and recurrent / recursive networks . Although they have achieved high performance , they may either fail to fully make use of the syntactical information in sentences or be difficult to train due to the long propagation path . Recently , we propose a novel tree - based convolutional neural network ( TBCNN ) to alleviate the aforementioned problems and have achieved higher performance in two sentence classification tasks . However , it is less clear whether TBCNN can be harnessed to model sentence pairs for implicit logical inference , as is in the NLI task . In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences . We leverage our newly proposed TBCNN model to capture structural information in sentences , which is important to NLI . For example , the phrase \u201c riding bicycles on the streets \u201d in Table [ reference ] can be well recognized by TBCNN via the dependency relations dobj ( riding , bicycles ) and prep_on ( riding , street ) . As we can see , TBCNN is more robust than sequential convolution in terms of word order distortion , which may be introduced by determinators , modifiers , etc . A pooling layer then aggregates information along the tree , serving as a way of semantic compositonality . Finally , two sentences \u2019 information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity . To sum up , the main contributions of this paper are two - fold : ( 1 ) We are the first to introduce tree - based convolution to sentence pair modeling tasks like NLI ; ( 2 ) Leveraging additional heuristics further improves the accuracy while remaining low complexity , outperforming existing sentence encoding - based approaches to a large extent , including feature - rich methods and long short term memory ( LSTM )- based recurrent networks . section : Related Work Entailment recognition can be viewed as a task of sentence pair modeling . Most neural networks in this field involve a sentence - level model , followed by one or a few matching layers . They are sometimes called \u201c Siamese \u201d architectures . CNN : NIPS and CNN : NAACL apply convolutional neural networks ( CNNs ) as the individual sentence model , where a set of feature detectors over successive words are designed to extract local features . LSTM : AAAI build sentence pair models upon recurrent neural networks ( RNNs ) to iteratively integrate information along a sentence . recurparaphrase dynamically construct tree structures ( analogous to parse trees ) by recursive autoencoders to detect paraphrase between two sentences . As shown , inherent structural information in sentences is oftentimes important to natural language understanding . The simplest approach to match two sentences , perhaps , is to concatenate their vector representations . Concatenation is also applied in our previous work of matching the subject and object in relation classification . CNN : EMNLP apply additional heuristics , namely Euclidean distance , cosine measure , and element - wise absolute difference . The above methods operate on a fixed - size vector representation of a sentence , categorized as sentence encoding - based approaches . Thus the matching complexity is , i.e. , independent of the sentence length . Word - by - word similarity matrices are introduced to enhance interaction . To obtain the similarity matrix , CNN : NIPS ( Arc - II ) concatenate two words \u2019 vectors ( after convolution ) , recurparaphrase compute Euclidean distance , and LSTM : AAAI apply tensor product . In this way , the complexity is of , where is the length of a sentence ; hence similarity matrices are difficult to scale and less efficient for large datasets . Recently , attention introduce several context - aware methods for sentence matching . They report that RNNs over a single chain of two sentences are more informative than separate RNNs ; a static attention over the first sentence is also useful when modeling the second one . Such context - awareness interweaves the sentence modeling and matching steps . In some scenarios like sentence pair re - ranking , it is not feasible to pre - calculate the vector representations of sentences , so the matching complexity is of . attention further develop a word - by - word attention mechanism and obtain a higher accuracy with a complexity order of . section : Our Approach We follow the \u201c Siamese \u201d architecture ( like most work in Section [ reference ] ) and adopt a two - step strategy to classify the relation between two sentences . Concretely , our model comprises two parts : A tree - based convolutional neural network models each individual sentence ( Figure [ reference ] a ) . Notice that , the two sentences , premise and hypothesis , share a same TBCNN model ( with same parameters ) , because this part aims to capture general semantics of sentences . A matching layer combines two sentences \u2019 information by heuristics ( Figure [ reference ] b ) . After individual sentence models , we design a sentence matching layer to aggregate information . We use simple heuristics , including concatenation , element - wise product and difference , which are effective and efficient . Finally , we add a softmax layer for output . The training objective is cross - entropy loss , and we adopt mini - batch stochastic gradient descent , computed by back - propagation . subsection : Tree - Based Convolution The tree - based convolutoinal neural network ( TBCNN ) is first proposed in our previous work to classify program source code ; later , we further propose TBCNN variants to model sentences . This subsection details the tree - based convolution process . The basic idea of TBCNN is to design a set of subtree feature detectors sliding over the parse tree of a sentence ; either a constituency tree or a dependency tree applies . In this paper , we prefer the dependency tree - based convolution for its efficiency and compact expressiveness . Concretely , a sentence is first converted to a dependency parse tree . Each node in the dependency tree corresponds to a word in the sentence ; an edge indicates is governed by . Edges are labeled with grammatical relations ( e.g. , nsubj ) between the parent node and its children . Words are represented by pretrained vector representations , also known as word embeddings . Now , we consider a set of two - layer subtree feature detectors sliding over the dependency tree . At a position where the parent node is with child nodes , the output of the feature detector , , is Let us assume word embeddings ( and ) are of dimensions ; that the convolutional layer is - dimensional . is the weight matrix ; is the bias vector . denotes the dependency relation between and . is the non - linear activation function , and we apply ReLU in our experiments . After tree - based convolution , we obtain a set of feature maps , which are one - one corresponding to original words in the sentence . Therefore , they may vary in size and length . A dynamic pooling layer is applied to aggregate information along different parts of the tree , serving as a way of semantic compositionality . We use the pooling operation , which takes the maximum value in each dimension . Then we add a fully - connected hidden layer to further mix the information in a sentence . The obtained vector representation of a sentence is denoted as ( also called a sentence embedding ) . Notice that the same tree - based convolution applies to both the premise and hypothesis . Tree - based convolution along with pooling enables structural features to reach the output layer with short propagation paths , as opposed to the recursive network , which is also structure - sensitive but may suffer from the problem of long propagation path . By contrast , TBCNN is effective and efficient in learning such structural information . subsection : Matching Heuristics In this part , we introduce how vector representations of individual sentences are combined to capture the relation between the premise and hypothesis . As the dataset is large , we prefer matching operations because of efficiency concerns . Concretely , we have three matching heuristics : Concatenation of the two sentence vectors , Element - wise product , and Element - wise difference . The first heuristic follows the most standard procedure of the \u201c Siamese \u201d architectures , while the latter two are certain measures of \u201c similarity \u201d or \u201c closeness . \u201d These matching layers are further concatenated ( Figure [ reference ] b ) , given by where and are the sentence vectors of the premise and hypothesis , respectively ; \u201c \u201d denotes element - wise product ; semicolons refer to column vector concatenation . is the output of the matching layer . We would like to point out that , with subsequent linear transformation , element - wise difference is a special case of concatenation . If we assume the subsequent transformation takes the form of , where is the weights for concatenated sentence representations , then element - wise difference can be viewed as such that . ( is the weights corresponding to element - wise difference . ) Thus , our third heuristic can be absorbed into the first one in terms of model capacity . However , as will be shown in the experiment , explicitly specifying this heuristic significantly improves the performance , indicating that optimization differs , despite the same model capacity . Moreover , word embedding studies show that linear offset of vectors can capture relationships between two words , but it has not been exploited in sentence - pair relation recognition . Although element - wise distance is used to detect paraphrase in CNN : EMNLP , it mainly reflects \u201c similarity \u201d information . Our study verifies that vector offset is useful in capturing generic sentence relationships , akin to the word analogy task . section : Evaluation subsection : Dataset To evaluate our TBCNN - pair model , we used the newly published Stanford Natural Language Inference ( SNLI ) dataset . The dataset is constructed by crowdsourced efforts , each sentence written by humans . Moreover , the SNLI dataset is magnitudes of larger than previous resources , and hence is particularly suitable for comparing neural models . The target labels comprise three classes : Entailment , Contradiction , and Neutral ( two irrelevant sentences ) . We applied the standard train / validation / test split , contraining 550k , 10k , and 10k samples , respectively . Figure [ reference ] presents additional dataset statistics , especially those relevant to dependency parse trees . subsection : Hyperparameter Settings All our neural layers , including embeddings , were set to 300 dimensions . The model is mostly robust when the dimension is large , e.g. , several hundred . Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as a part of model parameters . We applied penalty of ; dropout was chosen by validation with a granularity of 0.1 ( Figure [ reference ] ) . We see that a large dropout rate ( 0.3 ) hurts the performance ( and also makes training slow ) for such a large dataset as opposed to small datasets in other tasks . Initial learning rate was set to 1 , and a power decay was applied . We used stochastic gradient descent with a batch size of 50 . subsection : Performance Table [ reference ] compares our model with previous results . As seen , the TBCNN sentence pair model , followed by simple concatenation alone , outperforms existing sentence encoding - based approaches ( without pretraining ) , including a feature - rich method using 6 groups of human - engineered features , long short term memory ( LSTM )- based RNNs , and traditional CNNs . This verifies the rationale for using tree - based convolution as the sentence - level neural model for NLI . Table [ reference ] compares different heuristics of matching . We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other . Combining different matching heuristics improves the result : the TBCNN - pair model with concatenation , element - wise product and difference yields the highest performance of 82.1 % . As analyzed in Section [ reference ] , the element - wise difference matching layer does not add to model complexity and can be absorbed as a special case into simple concatenation . However , explicitly using such heuristic yields an accuracy boost of 1\u20132 % . Further applying element - wise product improves the accuracy by another 0.5 % . The full TBCNN - pair model outperforms all existing sentence encoding - based approaches , including a 1024d gated recurrent unit ( GRU )- based RNN with \u201c skip - thought \u201d pretraining . The results obtained by our model are also comparable to several attention - based LSTMs , which are more computationally intensive than ours in terms of complexity order . subsection : Complexity Concerns For most sentence models including TBCNN , the overall complexity is at least . However , an efficient matching approach is still important , especially to retrieval - and - reranking systems . For example , in a retrieval - based question - answering or conversation system , we can largely reduce response time by performing sentence matching based on precomputed candidates \u2019 embeddings . By contrast , context - aware matching approaches as described in Section [ reference ] involve processing each candidate given a new user - issued query , which is time - consuming in terms of most industrial products . In our experiments , the matching part ( Figure [ reference ] b ) counts 1.71 % of the total time during prediction ( single - CPU , C ++ implementation ) , showing the potential applications of our approach in efficient retrieval of semantically related sentences . section : Conclusion In this paper , we proposed the TBCNN - pair model for natural language inference . Our model relies on the tree - based convolutional neural network ( TBCNN ) to capture sentence - level semantics ; then two sentences \u2019 information is combined by several heuristics including concatenation , element - wise product and difference . Experimental results on a large dataset show a high performance of our TBCNN - pair model while remaining a low complexity order . section : Acknowledgments We thank all anonymous reviewers for their constructive comments , especially those on complexity issues . We also thank Sam Bowman , Edward Grefenstette , and Tim Rockt\u00e4schel for their discussion . This research was supported by the National Basic Research Program of China ( the 973 Program ) under Grant No . 2015CB352201 and the National Natural Science Foundation of China under Grant Nos . 61232015 , 61421091 , and 61502014 . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_Tree-based_CNN_encoders"]]], "Metric": [[["__Test_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_Tree-based_CNN_encoders"]]], "Metric": [[["__Train_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_Tree-based_CNN_encoders"]]], "Metric": [[["Parameters"]]], "Task": [[["Natural_Language_Inference"]]]}]}
{"docid": "TST3-SREX-0016", "doctext": "document : Grounded Textual Entailment Capturing semantic relations between sentences , such as entailment , is a long - standing challenge for computational semantics . Logic - based models analyse entailment in terms of possible worlds ( interpretations , or situations ) where a premise P entails a hypothesis H iff in all worlds where P is true , H is also true . Statistical models view this relationship probabilistically , addressing it in terms of whether a human would likely infer H from P. In this paper , we wish to bridge these two perspectives , by arguing for a visually - grounded version of the Textual Entailment task . Specifically , we ask whether models can perform better if , in addition to P and H , there is also an image ( corresponding to the relevant \u201c world \u201d or \u201c situation \u201d ) . We use a multimodal version of the SNLI dataset and we compare \u201c blind \u201d and visually - augmented models of textual entailment . We show that visual information is beneficial , but we also conduct an in - depth error analysis that reveals that current multimodal models are not performing \u201c grounding \u201d in an optimal fashion . section : Introduction Correspondence should be addressed to Raffaella Bernardi ( raffaella.bernardi@unitn.it ) and Albert Gatt ( albert.gatt@um.edu.mt ) . This work is licensed under a Creative Commons Attribution 4.0 International License . License details : . The dataset , annotation and code is available from . Evaluating the ability to infer information from a text is a crucial test of the capability of models to grasp meaning . As a result , the computational linguistics community has invested huge efforts into developing textual entailment ( TE ) datasets . After formal semanticists developed FraCas in the mid \u2019 90 , an increase in statistical approaches to computational semantics gave rise to the need for suitable evaluation datasets . Hence , Recognizing Textual Entailment ( RTE ) shared tasks have been organized regularly . Recent work on compositional distributional models has motivated the development of the SICK dataset of sentence pairs in entailment relations for evaluating such models . Further advances with Neural Networks ( NNs ) have once more motivated efforts to develop a large natural language inference dataset , SNLI , since NNs need to be trained on big data . However , meaning is not something we obtain just from text and the ability to reason is not unimodal either . The importance of enriching meaning representations with other modalities has been advocated by cognitive scientists , ( e.g. , ) and computational linguists ( e.g. , ) . While efforts have been put into developing multimodal datasets for the task of checking Semantic Text Similarity Text , we are not aware of any available datasets to tackle the problem of Grounded Textual Entailment ( GTE ) . Our paper is a first effort in this direction . [ b ] 0.45 [ scale=0.5 ] figures / warm - entailment [ b ] 0.45 [ scale=0.5 ] figures / neutral - mis Textual Entailment is defined in terms of the likelihood of two sentences ( a premise P and an hypothesis H ) to be in a certain relation : P entails , contradicts or is unrelated to H. For instance , the premise \u201c People trying to get warm in front of a chimney \u201d and the hypothesis \u201c People trying to get warm at home \u201d are highly likely to be in an entailment relation . Our question is whether having an image that illustrates the event ( e.g. , Figure [ reference ] ) can help a model to capture the relation . In order to answer this question , we augment the largest available TE dataset with images , we enhance a state of the art model of textual entailment to take images into account and we evaluate it against the GTE task . The inclusion of images can also alter relations which , based on text alone , would seem likely . For example , to a \u201c blind \u201d model the sentences of the sentence pair in Figure [ reference ] would seem to be unrelated , but when the two sentences are viewed in the context of the image , they do become related . A suitable GTE model therefore has to perform two sub - tasks : ( a ) it needs to ground its linguistic representations of P , H or both in non - linguistic ( visual ) data ; ( b ) it needs to reason about the possible relationship between P and H ( modulo the visual information ) . section : Related Work Grounding language through vision has recently become the focus of several tasks , including Image Captioning ( IC , e.g. ) and Visual Question Answering ( VQA , eg . ) , and even more recently , Visual Reasoning and Visual Dialog . Our focus is on Grounded Textual Entailment ( GTE ) . While the literature on TE is rather vast , GTE is still rather unexplored territory . paragraph : Textual Entailment Throughout the history of Computational Linguistics various datasets have been built to evaluate Computational Semantics models on the TE task . Usually they contain data divided into entailment , contradiction or unknown classes . The \u201c unknown \u201d label has sometimes been replaced with the \u201c unrelated \u201d or \u201c neutral \u201d label , capturing slightly different types of phenomena . Interestingly , the \u201c entailment \u201d and \u201c contradiction \u201d classes also differ across datasets . In the mid - \u201990s a group of formal semanticists developed FraCaS ( Framework for Computational Semantics ) . The dataset contains logical entailment problems in which a conclusion has to be derived from one or more premises ( but not necessarily all premises are needed to verify the entailment ) . The entailments are driven by logical properties of linguistic expressions , like the monotonicity of quantifiers , or their conservativity property etc . Hence , the set of premises entails the conclusion iff in all the interpretations ( worlds ) in which the premises are true the conclusion is also true ; otherwise the conclusion contradicts the premises . In 2005 , the PASCAL RTE ( Recognizing Textual Entailment ) challenge was launched , to become a task organized annually . In 2008 , the RTE - 4 committee made the task more fine - grained by requiring the classification of the pairs as \u201c entailment \u201d , \u201c contradiction \u201d and \u201c unknown \u201d . The RTE datasets , unlike FraCaS , contain real - life natural language sentences and the sort of entailment problems which occur in corpora collected from the web . Importantly , the sentence pair relations are annotated as entailment , contradiction or neutral based on a likelihood condition : if a human reading the premise would typically infer that the conclusion ( called the hypothesis ) is most likely true ( entailment ) , its negation is most likely true ( contradiction ) or the conclusion can be either true or false ( neutral ) . At SemEval 2014 , in order to evaluate Compositional Distributional Semantics Models focusing on the compositionality ability of those models , the SICK dataset ( Sentences Involving Compositional Knowledge ) was used in a shared entailment task . Sentence pairs were obtained through re - writing rules and annotated with the three RTE labels via a crowdsourcing platform . Both in RTE and SICK the label assigned to the sentence pairs captures the relation holding between the two sentences . A different approach has been used to build the much larger SNLI ( Stanford Natural Language Inference ) dataset : Premises are taken from a dataset of images annotated with descriptive captions ; the corresponding hypotheses were produced through crowdsourcing , where for a given premise , annotators provided a sentence which is true or not true with respect to a possible image which the premise could describe . A consequence of this choice is that the contradiction relation can be assigned to pairs which are rather unrelated ( \u201c A person in a black wetsuit is surfing a small wave \u201d and \u201c A woman is trying to sleep on her bed \u201d ) , differently from what happens in RTE and SICK . Since the inception of RTE shared tasks , there has been an increasing emphasis on data - driven approaches which , given the hypothesis H and premise P , seek to classify the semantic relation ( see for a review ) . More recently , neural approaches have come to dominate the scene , as shown by the recent RepEval 2017 task , where all submissions relied on bidirectional LSTM models , with or without pretrained embeddings . RTE also intersects with a number of related inference problems , including semantic text similarity and Question Answering , and some models have been proposed to address several such problems . In one popular approach , both P and H are encoded within the same embedding space , using a single RNN , with a decision made based on the encodings of the two sentences . This is the approach we adopt for our baseline LSTM in Section [ reference ] , based on the model proposed by snli : emnlp2015 , albeit with some modifications ( see also ) . A second promising approach , based on which we adapt our state of the art model , relies on matching and aggregation . Here , the decision concerning the relationship between P and H is based on an aggregate representation achieved after the two sentences are matched . Yet another area where neural approaches are being applied to sentence pairs in an entailment relationship is generation , where an RNN generates an entailed hypothesis ( or a chain of such hypotheses ) given an encoding of the premise . paragraph : Vision and Textual Entailment In recent years , several models have been proposed to integrate the language and vision modalities ; usually the integration is operationalized by element - wise multiplication between linguistic and visual vectors . Though the interest in these modalities has spread in an astonishing way thanks to various multimodal tasks proposed , including the IC , VQA , Visual Reasoning and Visual Dialogue tasks mentioned above , very little work has been done on grounding entailment . Interestingly , youn : from14 has proposed the idea of considering images as the \u201c possible worlds \u201d on which sentences find their denotation . Hence , they released a \u201c visual denotation graph \u201d which associates sentences with their denotation ( sets of images ) . The idea has been further exploited by lai : lear17 and han : visd17 . vend : orde16 look at hypernymy , textual entailment and image captioning as special cases of a single visual - semantic hierarchy over words , sentences and images , and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spaces improves model performance on those three tasks . We share with this work the idea that the image can be taken as a possible world . However , we do n\u2019t use sets of images to obtain the visual denotation of text in order to check whether entailment is logically valid / highly likely . Rather , we take the image to be the world / situation in which the text finds its interpretation . The only work that is close to ours is an unpublished student report , which however lacks the in - depth analysis presented here . section : Annotated dataset of images and sentence pairs We took as our starting point the Stanford Natural Language Inference ( SNLI ) dataset , the largest natural language inference dataset available with sentence pairs labelled with entailment , contradiction and neutral relations . We augmented this dataset with images . It has been shown very recently that SNLI contains language bias , such that a simple classifier can achieve high accuracy in predicting the three classes just by having as input the hypothesis sentence . A subset of the SNLI test set with \u2018 hard \u2019 cases , where such a simplistic classifier fails ( hereafter SNLI ) has been released . Hence , in this paper we will report our results on both the full dataset and the hard test set , but then zoom in on SNLI to understand the models \u2019 behaviour . We briefly introduce SNLI and the new test set and compare them through our annotation of linguistic phenomena . subsection : Dataset construction paragraph : SNLI and SNLI test set The SNLI dataset was built through Amazon Mechanical Turk . Workers were shown captions of photographs without the photo and were asked to write a new caption that ( a ) is definitely a true description of the photo ( entailment ) ; ( b ) might be a true description of the photo ( neutral ) ; ( c ) is definitely a false description of the photo ( contradiction ) . Examples were provided for each of the three cases . The premises are captions which come mostly from Flickr30 K ; only 4 K captions are from VisualGenome . In total , the dataset contains 570 , 152 sentence pairs , balanced with respect to the three labels . Around 10 % of these data have been validated ( 4 annotators for each example plus the label assigned through the previous data collection phase ) . The development and test datasets contain 10 K examples each . Moreover , each Image / Flickr caption occurs in only one of the three sets , and all the examples in the development and test sets have been validated . paragraph : V - SNLI and V - SNLI test set Our grounded version of SNLI , V - SNLI , has been built by matching each sentence pair in SNLI with the corresponding image coming from the Flickr30 K dataset ; thus the V - SNLI dataset is slightly smaller than the original , which also contains captions from VisualGenome . V - SNLI consists of 565 , 286 pairs ( 187 , 969 neutral , 188 , 453 contradiction , and 188 , 864 entailment ) . Training , test , and development splits have been built according to the splits in SNLI . The main statistics of the splits of the dataset are reported in Table [ reference ] together with statistics for the visual counterpart of Hard SNLI , namely V - SNLI . By construction , V - SNLI contains datapoints such that the premise is always true with respect to the image , whereas the hypothesis can be either true ( entailment or neutral cases ) or false ( contradiction or neutral cases . ) subsection : Dataset annotation For deeper analysis and comparison of the contents of SNLI and SNLI , we have annotated the SNLI dataset by both automatically detecting some surface linguistic cues and manually labelling less trivial phenomena . Using an in - house annotation interface , we collected human judgments aiming to ( a ) filter out those cases for which the gold - standard annotation was considered to be wrong ; ( b ) connect the three ungrounded relations to various linguistic phenomena . To achieve this , we annotated a random sample of the SNLI test set containing 527 sentence pairs ( 185 entailment , 171 contradiction , 171 neutral ) , out of which 176 were from the hard test set ( 56 entailment , 62 contradiction , 58 neutral ) . All the pairs were annotated by at least two annotators , as follows : ( a ) We filtered out all the pairs which had a wrong gold label ( see Table [ reference ] for details ) . When our annotators did not agree whether a given relation holds for a specific pair , we appealed to the corresponding five judgments coming from the validation stage of the SNLI dataset to reach a consensus based on the majority of labels . ( b ) We considered as valid any linguistic tag assigned by at least one annotator . Since the annotation for ( a ) is binary whereas for ( b ) it is multi - class , we used Cohen \u2019s for the former and also Scott \u2019s and Krippendorff \u2019s for the latter as suggested by Passonneau pass : inte06 . The inter - annotator agreement for the relation type ( a ) was ; for ( b ) linguistic tags it was , , and . paragraph : Linguistic phenomena Following the error analysis approach described in recent work , we compiled a new list of linguistic features that can be of interest when contrasting SNLI and SNLI , as well as for evaluating RTE models . Some of these were detected automatically , while others were assigned manually . Automatic tags included Synonym and Antonym , which were detected using WordNet . Quantifier , Pronoun , Diff Tense , Superlative and Bare NP were identified using Penn treebank labels , while labels such as Negation were found with a straightforward keyword search . The tag Long has been assigned to sentence pairs with a premise containing more than 30 tokens , or a hypothesis with more than 16 tokens . Details about the tags used in the manual annotation are presented in Table [ reference ] . We examined the differences in the tags distributions between the SNLI and SNLI test sets ( Table [ reference ] ) . Interestingly , the hard sentence pairs from our random sample include proportionately more antonyms but fewer pronouns , as well as examples with different verb tenses in the premise and hypothesis , compared to the full test set . Furthermore , SNLI contains a significantly larger proportion of gold - standard labels which become wrong when the image is factored in ( - test with ) . section : Models In this section , we describe a variety of models that were compared on both V - SNLI and V - SNLI , ranging from baseline models based on snli : emnlp2015 to a state of the art model by Wang2017 . We compare the original \u2018 blind \u2019 version of a model with a visually - augmented counterpart . In what follows , we use P and H to refer to a premise and hypothesis , respectively . paragraph : LSTM baseline ( Blind ) This model exploits a Recurrent Neural Network with Long Short - Term Memory units to encode both P and H in 512D vectors . The two vectors are then concatenated in a stack of three 512D layers having a ReLU activation function , with a final softmax layer to classify the relation between the two sentences as entailment , contradiction or neutral . The model is inspired by the LSTM baseline proposed by snli : emnlp2015 . The model exploits the 300 , 000 most frequent pretrained GloVe embeddings and improves them during the training process . To regularize the model , Dropout is applied to the inputs and outputs of the recurrent layers and to the ReLU fully connected layers with a keeping probability of 0.5 . The model is trained using the Adam optimizer with a learning rate of 0.001 until its accuracy on the development set drops for three successive iterations . paragraph : V - LSTM baseline The LSTM model described above is augmented with a visual component following a standard Visual Question Answering baseline model . Following initial representation of P and H in 512D vectors through an LSTM , a fully - connected layer projects the L2 - normalized 4096D image vector coming from the penultimate layer of a VGGnet16 Convolutional Neural Network to a reduced 512D vector . A fully - connected layer with a ReLU activation function is also applied to P and H to obtain two 512D vectors . The multimodal fusion between the text and the image is obtained by performing an element - wise multiplication between the vector of the text representation and the reduced vector of the image . The multimodal fusion is performed between the image and both the premise and the hypothesis , resulting in two multimodal representations . The relation between them is captured as in the model described above . This model uses GloVe embeddings and the same optimization and procedure described above . We have also adapted a state - of - the - art attention - based model for IC and VQA to the GTE task . It obtains results comparable to the V - LSTM . This lack of improvement might be due to the need of further parameter tuning . We report the details of our implementation and its results in the Supplementary Material . paragraph : BiMPM The Bilateral Multi - Perspective Matching ( BiMPM ) model obtains state - of - the - art performance on the SNLI dataset , achieving a maximum accuracy of 86.9 % , and going up to 88.8 % in an ensemble setup . An initial embedding layer vectorises words in P and H using pretrained GLoVe embeddings , and passing them to a context representation layer , which uses bidirectional LSTMs ( BiLSTMs ) to encode context vectors for each time - step . The core part of the model is the subsequent matching layer , where each contextual embedding or time - step of P is matched against all the embeddings of H , and vice versa . The output of this layer is composed of two sequences of matching vectors , which constitute the input to another BiLSTM at the aggregation layer . The vectors from the last time - step of the BiLSTM are concatenated into a fixed - length vector , which is passed to the final prediction tier , a two - layer feed - forward network which classifies the relation between P and H via softmax . Matching is performed via a cosine operation , which yields an - dimensional vector , where is the number of perspectives . Wang2017 experiment with four different matching strategies . In their results , the best - performing version of the BiMPM model used all four matching strategies . We adopt this version of the model in what follows . paragraph : V - BiMPM model We enhanced BiMPM to account for the image , too . Our version of this model is referred to as the V - BiMPM . Here , the feature vector for an image is obtained from the layer before the fully - connected layer of a VGGnet - 16 . This results in a tensor , which we consider as 49 512 - dimensional vectors . The same matching operations are performed , except that matching occurs between P , H , and the image . Since the textual and visual vectors have different dimensionality and belong to different spaces , we first map them to a mutual space using an affine transformation . We match textual and image vectors using a cosine operation , as before . Full details of the model are reported in the Supplementary Materials for this paper . section : Experiments and Results The models described in the previous sections were evaluated on both ( V -) SNLI and ( V -) SNLI . For the visually - augmented models , we experimented with configurations where image vectors were combined with both P and H ( namely P + I and H + I ) , or only with H ( P and H + I ) . The best setting was invariably the one where only H was grounded ; hence , we focus on these results in what follows , comparing them to \u201c blind \u201d models . In view of recent results suggesting that biases in SNLI afford a high accuracy in the prediction task with only the hypothesis sentence as input , we also include results for the blind models without the premise ( denoted with [ H ] in what follows ) . Table [ reference ] shows the results of the various models on the full V - SNLI dataset . The same models are compared in Table [ reference ] on V - SNLI . First , note that the LSTM [ H ] model evinces a drop in performance compared to LSTM ( from 81.49 % to 68.49 % ) , though the drop is much greater on the unbiased SNLI subset ( from 60.99 to 25.57 % ) . This confirms the results reported by guru : anno18 and justifies our additional focus on this subset of the data . The effect of grounding in these models is less clear . The LSTM baseline performs worse when it is visually augmented ; this is the case of V - SNLI and , even more drastically , V - SNLI . It is also true irrespective of the relationship type . On the other hand , the V - BiMPM model improves marginally across the board , compared to BiMPM , on the V - SNLI data . On the hard subset , the images appear to hurt performance somewhat in the case of contradiction ( from 77.62 % to 76.12 % ) , but improve it by a substantial margin on neutral cases ( from 59.36 % to 63.67 % ) . The neutral case is the hardest for all models , with the possible exception of LSTM [ H ] on the full dataset . Overall , the results suggest that factoring in images either hinders performance ( as in the case of the V - LSTM baseline ) , or helps only marginally ( as in the case of V - BiMPM ) . In the latter case , we also observe instances where factoring in images hurts performance . In an effort to understand the results , we turn to a more detailed error analysis of the V - BiMPM model , first in relation to the dataset annotations , and then by zooming in somewhat closer on V - SNLI . subsection : Error analysis by linguistic annotation label In Table [ reference ] , accuracies for the blind and grounded version of BiMPM are broken down by the labels given to the sentence pairs in the annotated subset of SNLI described in Section [ reference ] . We only observe a significant difference in the Entity case , that is , where the referents in P and H are inconsistent . Here , the blind model outperforms the grounded one , an unexpected result , since one would assume a grounded model to be better equipped to identify mismatched referents . Hence , in the following we aim to understand whether the models properly deal with the grounding sub - task . subsection : Error analysis on grounding in the SNLI We next turn to the \u201c hard \u201d subset of the data , where V - BiMPM showed some improvement over the blind case , but suffered on contradiction cases ( Table [ reference ] ) . We analysed the 207 cases in SNLI where the V - BiMPM made incorrect predictions compared to the blind model , that is , where the image hurt performance . These were annotated independently by two of the authors ( raw inter - annotator agreement : 96 % ) who ( a ) read the two sentences , P and H ; ( b ) checked whether the relation annotated in the dataset actually held or whether it was an annotation error ; ( c ) in those cases where it held , checked whether including the image actually resulted in a change in the relation . Table [ reference ] displays the proportions of image mismatch and incorrect annotations . As the table suggests , in the cases where images hinder performance in the V - BiMPM , it is usually because the image changes the relation ( thus , these are cases of image mismatch ; see Section [ reference ] for an example ) ; this occurs in a large proportion of cases labelled as neutral in the dataset . Inspired by the work in , we further explored the impact of visual grounding in both the V - LSTM and V - BiMPM by comparing their performance on SNLI , with the same subset incorporating image \u201c foils \u201d . Vectors for the images in the V - SNLI test set were compared pairwise using cosine , and for each test case in V - SNLI , the actual image was replaced with the most dissimilar image in the full test set . The rationale is that , if visual grounding is really helpful in recognising the semantic relationship between P and H , we should observe a drop in performance when the images are unrelated to the scenario described by the sentences . The results are displayed in Table [ reference ] , which also reproduces the original results on V - SNLI from Table [ reference ] for ease of reference . As the results show , models are not hurt by the foil image , contrary to our expectations . V - BiMPM overall drops just by 0.67 % whereas V - LSTM drop is somewhat higher ( - 2.11 % ) showing it might be doing a better job on the grounding sub - task . As a final check , we sought to isolate the grounding from the reasoning sub - task , focusing only on the former . We compared the models when grounding only the hypothesis [ H + I ] , while leaving out the premise . Note that this test is different from the evaluation of the model using only the hypothesis [ H ] : Whereas in that case the input is not expected to provide any useful information to perform the task , here it is . As we noted in Section [ reference ] , by construction the premise is always true with respect to the image while the hypothesis can be either true ( entailment or neutral cases ) or false ( contradiction or neutral cases ) . A model that is grounding the text adequately would be expected to confuse both entailment and contradiction cases with neutral ones ; on the other hand , neutral cases should be confused with entailments or contradictions . Confusing contradictions with entailments would be a sign that a model is grounding inadequately , since it is not recognising that H is false with respect to the image . As the left panel of Table [ reference ] shows , V - BiMPM outperforms V - LSTM by a substantial margin , though the performance of both models drops substantially with this setup . The right panel in the table shows that neither model is free of implausible errors ( confusing entailments and contradictions ) , though V - BiMPM makes substantially fewer of these . section : Conclusion This paper has investigated the potential of grounding the textual entailment task in visual data . We argued that a Grounded Textual Entailment model needs to perform two tasks : ( a ) the grounding itself , and ( b ) reasoning about the relation between the sentences , against the visual information . Our results suggest that a model based on matching and aggregation like the BiMPM model can perform very well at the reasoning task , classifying entailment relations correctly much more frequently than a baseline V - LSTM . On the other hand , it is not clear that grounding is being performed adequately in this model . It is primarily in the case of contradictions that the image seems to play a direct role in biasing the classification towards the right or wrong class , depending on whether the image is correct . In summary , two conclusions can be drawn from these results . First , in those cases where the inclusion of visual information results in a loss of accuracy , this is often due to the image resulting in a change in the original relation annotated in the dataset . A related observation is that using foil images results in a greater drop in performance on contradiction cases , possibly because in such cases , grounding serves to identify a mismatch between the hypothesis and the scene described by the premise , a situation which is rendered opaque by the introduction of foils . Second , in those cases where improvements are observed in the state of the art V - BiMPM , the precise role played by the image is not straightforward . Indeed , we find that this model still marginally outperforms the \u2018 blind \u2019 , text - only model overall , when the images involved are foils rather than actual images . We believe that further research on grounded TE is worthy of the NLP community \u2019s attention . While linking language with perception is currently a topical issue , there has been relatively little work on linking grounding directly with inference . By drawing closer to a joint solution to the grounding and inference tasks , models will also be better able to address language understanding in the real world . The present paper presented a first step in this direction using a version of an existing TE dataset which was augmented with images that could be paired directly with the premises , since these were originally captions for those images . However , it is important to note that in this dataset premise - hypotheses pairs were not generated directly with reference to the images themselves . An important issue to consider in future work on GTE , besides the development of better models , is the development of datasets in which the role of perceptual information is controlled , ensuring that the data on which models are trained represents truly grounded inferences . section : Acknowledgements We kindly acknowledge the European Network on Integrating Vision and Language ( iV & L Net ) ICT COST Action IC1307 . Moreover , we thank the Erasmus Mundus European Program in Language and Communication Technology . Marc Tanti \u2019s work is partially funded by the Endeavour Scholarship Scheme ( Malta ) , part - financed by the European Union \u2019s European Social Fund ( ESF ) . Finally , we gratefully acknowledge the support of NVIDIA Corporation with the donations to the University of Trento of the GPUs used in our research . section : Appendix A : Bottom - up top - down attention ( VQA ) We adapted the Visual Question Answering model proposed in to the Grounded Textual Entailment task . The model presents a more fine - grained attention mechanism which allows to identify the most important regions discovered in the image and to perform attention over each of them . The model uses a a Recurrent Neural Network with Long Short - Term Memory units to encode the premise P and hypothesis H in 512D vectors . A bottom - up attention mechanism exploits a Fast R - CNN based on a ResNet - 101 convolutional neural network to obtain region proposals corresponding to the 36 most informative regions of the image . A top - down attention mechanism is used between the premise ( resp . hypothesis ) and each of the L2 - normalized 2048D image vectors corresponding to the region proposals to obtain an attention score for each of them . Then , a 2048D image vector encoding the most interesting visual features for the premise ( hypothesis ) is obtained as a sum of the 36 image vectors weighted by the corresponding attention scores for the premise ( hypothesis ) . A fully - connected layer with a gated tanh activation function is applied to the image vector of the most interesting visual features for the premise and for the hypothesis to obtain a reduced 512D vector for each of them . A fully - connected layer with a gated tanh activation function is also applied to the premise and to the hypothesis in order to obtain a reduced 512D vector for each of them . The multimodal fusion between the premise ( hypothesis ) and the image vector of the most interesting visual features for the premise ( hypothesis ) is obtained by performing an element - wise multiplication between the reduced vector of the premise ( hypothesis ) and the reduced vector of the most interesting visual features for the premise ( hypothesis ) . After that , the model feeds the concatenation of the two resulting multimodal representations to a stack of three 512D layers having a gated tanh activation function , with a final softmax layer to classify the relation between the two sentences as entailment , contradiction or neutral . This model uses GloVe embeddings and the same optimization tricks and procedure of the LSTM and V - LSTM models . We report the accuracies of the VQA models against the various tests reported in the paper . For ease of comparison we reproduce the full table from the main paper , with the addition of the VQA results . section : Appendix B : V - biMPM Model details arrows , calc , fit positioning , shapes.multipart , shapes.callouts decorations.pathreplacing hbox= [ rectangle , minimum width=7pt , minimum height=25pt ] vbox= [ rectangle , fill = black!25 , minimum width=35pt , minimum height=10pt ] blue hbox= [ hbox , fill = blue ] dots hbox= [ hbox ] green hbox= [ hbox , fill = green ] green vbox= [ vbox , fill = green ] tbox= [ rectangle , minimum width=45pt , minimum height=20pt , draw = black , text centered , text = black ] red arrow= [- \u00bf , red ] match arrow= [- \u00bf , black ] red matching= [ text = red , inner sep=0 ] black matching= [ text = black , inner sep=0 ] context container / .style = draw = orange , rectangle callout , inner sep=0.6em 0.95 ! [ scale=0.2 ] [ ] ( prem_text ) at ( 10 ,- 7 ) Premise ; [ ] ( hypo_text ) at ( 35 ,- 7 ) Hypothesis ; [ blue hbox , label = below : ] ( w - 1 ) at ( 0 , 0 ) ; [ blue hbox , right of = w - 1 , node distance=0.7 cm , label = below : ] ( w - 2 ) ; [ dots hbox , right of = w - 2 , node distance=0.7 cm , label = below : \u2026 ] ( w - 3 ) \u2026 ; [ blue hbox , right of = w - 3 , node distance=0.7 cm , label = below : ] ( w - 4 ) ; [ dots hbox , right of = w - 4 , node distance=0.7 cm , label = below : \u2026 ] ( w - 5 ) \u2026 ; [ blue hbox , right of = w - 5 , node distance=0.7 cm , label = below : ] ( w - 6 ) ; [ blue hbox , right of = w - 6 , node distance=1.5cm*1.2 , label = below : ] ( w - 7 ) ; [ blue hbox , right of = w - 7 , node distance=0.7 cm , label = below : ] ( w - 8 ) ; [ dots hbox , right of = w - 8 , node distance=0.7 cm , label = below : \u2026 ] ( w - 9 ) \u2026 ; [ blue hbox , right of = w - 9 , node distance=0.7 cm , label = below : ] ( w - 10 ) ; [ dots hbox , right of = w - 10 , node distance=0.7 cm , label = below : \u2026 ] ( w - 11 ) \u2026 ; [ blue hbox , right of = w - 11 , node distance=0.7 cm , label = below : ] ( w - 12 ) ; [ inner sep=0pt , right of = w - 12 , node distance=1.5cm*2.15 ] ( image ) [ width=.10 ] figures / cat.jpg ; [ blue hbox ] ( c - 1 ) at ( 0 , 1.5cm*7 ) ; [ blue hbox , right of = c - 1 , node distance=0.7 cm ] ( c - 2 ) ; [ dots hbox , right of = c - 2 , node distance=0.7 cm ] ( c - 3 ) \u2026 ; [ blue hbox , right of = c - 3 , node distance=0.7 cm ] ( c - 4 ) ; [ dots hbox , right of = c - 4 , node distance=0.7 cm ] ( c - 5 ) \u2026 ; [ blue hbox , right of = c - 5 , node distance=0.7 cm ] ( c - 6 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 1 ) \u2013 ( c - 2 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 2 ) \u2013 ( c - 3 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 3 ) \u2013 ( c - 4 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 4 ) \u2013 ( c - 5 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 5 ) \u2013 ( c - 6 ) ; [ blue hbox , above of = c - 1 , node distance=1.5cm / 1.2 ] ( c - 1u ) ; [ blue hbox , right of = c - 1u , node distance=0.7 cm ] ( c - 2u ) ; [ dots hbox , right of = c - 2u , node distance=0.7 cm ] ( c - 3u ) \u2026 ; [ blue hbox , right of = c - 3u , node distance=0.7 cm ] ( c - 4u ) ; [ dots hbox , right of = c - 4u , node distance=0.7 cm ] ( c - 5u ) \u2026 ; [ blue hbox , right of = c - 5u , node distance=0.7 cm ] ( c - 6u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 2u ) \u2013 ( c - 1u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 3u ) \u2013 ( c - 2u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 4u ) \u2013 ( c - 3u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 5u ) \u2013 ( c - 4u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 6u ) \u2013 ( c - 5u ) ; [ red arrow , shorten \u00bf = 10pt , shorten \u00a1 = - 3pt ] ( w - 3 ) \u2013 ( c - 3 ) ; [ inner sep=0 ] ( p - callout - pointer ) at ( ) ; [ context container , callout absolute pointer= ( p - callout - pointer ) , fit= ( c - 1 ) ( c - 6u ) ] ( premise - callout ) ; [ blue hbox , right of = c - 6 , node distance=1.5cm*1.2 ] ( c - 7 ) ; [ blue hbox , right of = c - 7 , node distance=0.7 cm ] ( c - 8 ) ; [ dots hbox , right of = c - 8 , node distance=0.7 cm ] ( c - 9 ) \u2026 ; [ blue hbox , right of = c - 9 , node distance=0.7 cm ] ( c - 10 ) ; [ dots hbox , right of = c - 10 , node distance=0.7 cm ] ( c - 11 ) \u2026 ; [ blue hbox , right of = c - 11 , node distance=0.7 cm ] ( c - 12 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 7 ) \u2013 ( c - 8 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 8 ) \u2013 ( c - 9 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 9 ) \u2013 ( c - 10 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 10 ) \u2013 ( c - 11 ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 11 ) \u2013 ( c - 12 ) ; [ blue hbox , right of = c - 6u , node distance=1.5cm*1.2 ] ( c - 7u ) ; [ blue hbox , right of = c - 7u , node distance=0.7 cm ] ( c - 8u ) ; [ dots hbox , right of = c - 8u , node distance=0.7 cm ] ( c - 9u ) \u2026 ; [ blue hbox , right of = c - 9u , node distance=0.7 cm ] ( c - 10u ) ; [ dots hbox , right of = c - 10u , node distance=0.7 cm ] ( c - 11u ) \u2026 ; [ blue hbox , right of = c - 11u , node distance=0.7 cm ] ( c - 12u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 8u ) \u2013 ( c - 7u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 9u ) \u2013 ( c - 8u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 10u ) \u2013 ( c - 9u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 11u ) \u2013 ( c - 10u ) ; [ red arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( c - 12u ) \u2013 ( c - 11u ) ; [ red arrow , shorten \u00bf = 10pt , shorten \u00a1 = - 3pt ] ( w - 9 ) \u2013 ( c - 9 ) ; [ inner sep=0 ] ( h - callout - pointer ) at ( ) ; [ context container , callout absolute pointer= ( h - callout - pointer ) , fit= ( c - 7 )( c - 12u ) ] ( left - callout ) ; [ blue hbox , right of = c - 12 , node distance=1.5cm*1.2 , yshift=1.2 cm , label = below : ] ( ci - 1 ) ; [ blue hbox , right of = ci - 1 , node distance=0.7 cm , label = below : ] ( ci - 2 ) ; [ dots hbox , right of = ci - 2 , node distance=0.7 cm , label = below : \u2026 ] ( ci - 3 ) \u2026 ; [ blue hbox , right of = ci - 3 , node distance=0.7 cm , label = below : ] ( ci - 4 ) ; [ dots hbox , right of = ci - 4 , node distance=0.7 cm , label = below : \u2026 ] ( ci - 5 ) \u2026 ; [ blue hbox , right of = ci - 5 , node distance=0.7 cm , label = below : ] ( ci - 6 ) ; [ red arrow , shorten \u00bf = 9pt , shorten \u00a1 = 5pt ] ( image ) \u2013 ( ci - 3 ) node [ pos=0.35 , right , font= ] VGGnet ; [ inner sep=0em ] ( i - callout - pointer ) at ( -(- \u2062ci1 )( \u20623cm ,- \u20621 cm ) ) ; container , callout absolute pointer= ( i - callout - pointer ) , fit= ( ci - 1 )( ci - 6 ) , inner sep=0.26em ] ( left - callout ) ; [ blue hbox , above of = c - 1u , node distance=1.5cm*2.9 ] ( m - 1 ) ; hbox , right of = m - 1 , node distance=0.5 cm ] ( m - 2 ) ; hbox , right of = m - 2 , node distance=0.5 cm , ] ( m - 3 ) \u2026 ; hbox , right of = m - 3 , node distance=0.5 cm ] ( m - 4 ) ; hbox , right of = m - 4 , node distance=0.5 cm ] ( m - 5 ) \u2026 ; hbox , right of = m - 5 , node distance=0.5 cm , ] ( m - 6 ) ; ( [ xshift=0pt , yshift= - 0.2cm ] m - 6.south east ) \u2013 ( [ xshift=0pt , yshift= - 0.2cm ] m - 1.south west ) node [ black , midway , yshift= - 0.35 cm ] P vs H ; [ blue hbox , right of = m - 6 , node distance=0.9cm*1.5 ] ( m - 7 ) ; hbox , right of = m - 7 , node distance=0.5 cm ] ( m - 8 ) ; hbox , right of = m - 8 , node distance=0.5 cm ] ( m - 9 ) \u2026 ; hbox , right of = m - 9 , node distance=0.5 cm ] ( m - 10 ) ; hbox , right of = m - 10 , node distance=0.5 cm ] ( m - 11 ) \u2026 ; hbox , right of = m - 11 , node distance=0.5 cm ] ( m - 12 ) ; ( [ xshift=0pt , yshift= - 0.2cm ] m - 12.south east ) \u2013 ( [ xshift=0pt , yshift= - 0.2cm ] m - 7.south west ) node [ black , midway , yshift= - 0.35 cm ] H vs P ; [ blue hbox , right of = m - 12 , node distance=0.9cm*1.5 ] ( mi - 13 ) ; hbox , right of = mi - 13 , node distance=0.5 cm ] ( mi - 14 ) ; hbox , right of = mi - 14 , node distance=0.5 cm ] ( mi - 15 ) \u2026 ; hbox , right of = mi - 15 , node distance=0.5 cm ] ( mi - 16 ) ; hbox , right of = mi - 16 , node distance=0.5 cm ] ( mi - 17 ) \u2026 ; hbox , right of = mi - 17 , node distance=0.5 cm ] ( mi - 18 ) ; ( [ xshift=0pt , yshift= - 0.2cm ] mi - 18.south east ) \u2013 ( [ xshift=0pt , yshift= - 0.2cm ] mi - 13.south west ) node [ black , midway , yshift= - 0.35 cm ] H vs Image ; [ blue hbox , right of = mi - 18 , node distance=0.9cm*1.5 ] ( mi - 19 ) ; hbox , right of = mi - 19 , node distance=0.5 cm ] ( mi - 20 ) ; hbox , right of = mi - 20 , node distance=0.5 cm ] ( mi - 21 ) \u2026 ; hbox , right of = mi - 21 , node distance=0.5 cm ] ( mi - 22 ) ; hbox , right of = mi - 22 , node distance=0.5 cm ] ( mi - 23 ) \u2026 ; hbox , right of = mi - 23 , node distance=0.5 cm ] ( mi - 24 ) ; ( [ xshift=0pt , yshift= - 0.2cm ] mi - 24.south east ) \u2013 ( [ xshift=0pt , yshift= - 0.2cm ] mi - 19.south west ) node [ black , midway , yshift= - 0.35 cm ] Image vs H ; [ red matching , below of = m - 1 , node distance=1.5cm*1.5 ] ( mp - 1 ) \u2297 ; matching , below of = m - 2 , node distance=1.5cm*1.5 ] ( mp - 2 ) \u2297 ; matching , below of = m - 4 , node distance=1.5cm*1.5 ] ( mp - 3 ) \u2297 ; matching , below of = m - 6 , node distance=1.5cm*1.5 ] ( mp - 4 ) \u2297 ; [ red matching , below of = m - 7 , node distance=1.5cm*1.5 ] ( mp - 5 ) \u2297 ; matching , below of = m - 8 , node distance=1.5cm*1.5 ] ( mp - 6 ) \u2297 ; matching , below of = m - 10 , node distance=1.5cm*1.5 ] ( mp - 7 ) \u2297 ; matching , below of = m - 12 , node distance=1.5cm*1.5 ] ( mp - 8 ) \u2297 ; [ black matching , below of = mi - 13 , node distance=1.5cm*1.5 ] ( mp - 9 ) \u2297 ; matching , below of = mi - 14 , node distance=1.5cm*1.5 ] ( mp - 10 ) \u2297 ; matching , below of = mi - 16 , node distance=1.5cm*1.5 ] ( mp - 11 ) \u2297 ; matching , below of = mi - 18 , node distance=1.5cm*1.5 ] ( mp - 12 ) \u2297 ; [ black matching , below of = mi - 19 , node distance=1.5cm*1.5 ] ( mp - 13 ) \u2297 ; matching , below of = mi - 20 , node distance=1.5cm*1.5 ] ( mp - 14 ) \u2297 ; matching , below of = mi - 22 , node distance=1.5cm*1.5 ] ( mp - 15 ) \u2297 ; matching , below of = mi - 24 , node distance=1.5cm*1.5 ] ( mp - 16 ) \u2297 ; mp - 1 ) ; h - callout - pointer ) \u2423 edge [- > , \u2423 out=165 , \u2423 in= - 70 ] mp - 2 ) ; h - callout - pointer ) \u2423 edge [- > , \u2423 out=160 , \u2423 in= - 75 ] mp - 3 ) ; h - callout - pointer ) \u2423 edge [- > , \u2423 out=155 , \u2423 in= - 80 ] mp - 4 ) ; mp - 5 ) ; p - callout - pointer ) \u2423 edge [- > , \u2423 out=110 , \u2423 in= - 100 ] mp - 6 ) ; p - callout - pointer ) \u2423 edge [- > , \u2423 out=65 , \u2423 in= - 90 ] mp - 7 ) ; p - callout - pointer ) \u2423 edge [- > , \u2423 out=60 , \u2423 in= - 90 ] mp - 8 ) ; mp - 13 ) ; h - callout - pointer ) \u2423 edge [- > , \u2423 out=20 , \u2423 in= - 100 ] mp - 14 ) ; h - callout - pointer ) \u2423 edge [- > , \u2423 out=15 , \u2423 in= - 105 ] mp - 15 ) ; h - callout - pointer ) \u2423 edge [- > , \u2423 out=10 , \u2423 in= - 110 ] mp - 16 ) ; mp - 9 ) ; i - callout - pointer ) \u2423 edge [- > , \u2423 out=145 , \u2423 in= - 60 ] mp - 10 ) ; i - callout - pointer ) \u2423 edge [- > , \u2423 out=140 , \u2423 in= - 105 ] mp - 11 ) ; i - callout - pointer ) \u2423 edge [- > , \u2423 out=65 , \u2423 in= - 60 ] mp - 12 ) ; [ red arrow , shorten \u00bf = 18pt ] ( mp - 1 ) \u2013 ( m - 1 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 2 ) \u2013 ( m - 2 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 3 ) \u2013 ( m - 4 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 4 ) \u2013 ( m - 6 ) ; [ red arrow , shorten \u00bf = 18pt ] ( mp - 5 ) \u2013 ( m - 7 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 6 ) \u2013 ( m - 8 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 7 ) \u2013 ( m - 10 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 8 ) \u2013 ( m - 12 ) ; [ red arrow , shorten \u00bf = 18pt ] ( mp - 9 ) \u2013 ( mi - 13 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 10 ) \u2013 ( mi - 14 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 11 ) \u2013 ( mi - 16 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 12 ) \u2013 ( mi - 18 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 13 ) \u2013 ( mi - 19 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 14 ) \u2013 ( mi - 20 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 15 ) \u2013 ( mi - 22 ) ; arrow , shorten \u00bf = 18pt ] ( mp - 16 ) \u2013 ( mi - 24 ) ; [ match arrow , shorten \u00a1 = 6pt ] ( c - 1u ) \u2013 ( mp - 1 ) ; arrow , shorten \u00a1 = 6pt ] ( c - 2u ) \u2013 ( mp - 2 ) ; arrow , shorten \u00a1 = 7pt ] ( c - 4u ) to [ out=80 , in= - 45 ] ( mp - 3 ) ; arrow , shorten \u00a1 = 6pt ] ( c - 6u ) to [ out=100 , in= - 45 ] ( mp - 4 ) ; [ match arrow , shorten \u00a1 = 9pt ] ( c - 7u ) to [ out=110 , in= - 40 ] ( mp - 5 ) ; arrow , shorten \u00a1 = 7pt ] ( c - 8u ) to [ out=100 , in= - 35 ] ( mp - 6 ) ; arrow , shorten \u00a1 = 20pt ] ( c - 10u ) \u2013 ( mp - 7 ) ; arrow , shorten \u00a1 = 22pt ] ( c - 12u ) \u2013 ( mp - 8 ) ; [ match arrow , shorten \u00a1 = 22pt ] ( c - 7u ) \u2013 ( mp - 9 ) ; arrow , shorten \u00a1 = 20pt ] ( c - 8u ) \u2013 ( mp - 10 ) ; arrow , shorten \u00a1 = 7pt ] ( c - 10u ) to [ out=80 , in=220 ] ( mp - 11 ) ; arrow , shorten \u00a1 = 7pt ] ( c - 12u ) to [ out=80 , in=230 ] ( mp - 12 ) ; [ match arrow , shorten \u00a1 = 3pt ] ( ci - 1 ) to [ out=80 , in=220 ] ( mp - 13 ) ; arrow , shorten \u00a1 = 3pt ] ( ci - 2 ) to [ out=90 , in=220 ] ( mp - 14 ) ; arrow , shorten \u00a1 = 10pt ] ( ci - 4 ) to [ out=60 , in= - 70 ] ( mp - 15 ) ; arrow , shorten \u00a1 = 3pt ] ( ci - 6 ) \u2013 ( mp - 16 ) ; [ blue hbox , above of = m - 1 , node distance=1.5cm*1.2 ] ( a - 1 ) ; hbox , right of = a - 1 , node distance=0.5 cm ] ( a - 2 ) ; hbox , right of = a - 2 , node distance=0.5 cm ] ( a - 3 ) \u2026 ; hbox , right of = a - 3 , node distance=0.5 cm ] ( a - 4 ) ; hbox , right of = a - 4 , node distance=0.5 cm ] ( a - 5 ) \u2026 ; hbox , right of = a - 5 , node distance=0.5 cm ] ( a - 6 ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( a - 1 ) \u2013 ( a - 2 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 2 ) \u2013 ( a - 3 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 3 ) \u2013 ( a - 4 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 4 ) \u2013 ( a - 5 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 5 ) \u2013 ( a - 6 ) ; [ green hbox , above of = a - 1 , node distance=1.5cm / 1.2 ] ( a - 1u ) ; hbox , right of = a - 1u , node distance=0.5 cm ] ( a - 2u ) ; hbox , right of = a - 2u , node distance=0.5 cm ] ( a - 3u ) \u2026 ; hbox , right of = a - 3u , node distance=0.5 cm ] ( a - 4u ) ; hbox , right of = a - 4u , node distance=0.5 cm ] ( a - 5u ) \u2026 ; hbox , right of = a - 5u , node distance=0.5 cm ] ( a - 6u ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( a - 2u ) \u2013 ( a - 1u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 3u ) \u2013 ( a - 2u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 4u ) \u2013 ( a - 3u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 5u ) \u2013 ( a - 4u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 6u ) \u2013 ( a - 5u ) ; [ blue hbox , right of = a - 6 , node distance=0.9cm*1.5 ] ( a - 7 ) ; hbox , right of = a - 7 , node distance=0.5 cm ] ( a - 8 ) ; hbox , right of = a - 8 , node distance=0.5 cm ] ( a - 9 ) \u2026 ; hbox , right of = a - 9 , node distance=0.5 cm ] ( a - 10 ) ; hbox , right of = a - 10 , node distance=0.5 cm ] ( a - 11 ) \u2026 ; hbox , right of = a - 11 , node distance=0.5 cm ] ( a - 12 ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( a - 7 ) \u2013 ( a - 8 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 8 ) \u2013 ( a - 9 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 9 ) \u2013 ( a - 10 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 10 ) \u2013 ( a - 11 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 11 ) \u2013 ( a - 12 ) ; [ green hbox , right of = a - 6u , node distance=0.9cm*1.5 ] ( a - 7u ) ; hbox , right of = a - 7u , node distance=0.5 cm ] ( a - 8u ) ; hbox , right of = a - 8u , node distance=0.5 cm ] ( a - 9u ) \u2026 ; hbox , right of = a - 9u , node distance=0.5 cm ] ( a - 10u ) ; hbox , right of = a - 10u , node distance=0.5 cm ] ( a - 11u ) \u2026 ; hbox , right of = a - 11u , node distance=0.5 cm ] ( a - 12u ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( a - 8u ) \u2013 ( a - 7u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 9u ) \u2013 ( a - 8u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 10u ) \u2013 ( a - 9u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( a - 11u ) \u2013 ( a - 10u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( a - 12u ) \u2013 ( a - 11u ) ; [ blue hbox , right of = a - 12 , node distance=0.9cm*1.5 ] ( ai - 13 ) ; hbox , right of = ai - 13 , node distance=0.5 cm ] ( ai - 14 ) ; hbox , right of = ai - 14 , node distance=0.5 cm ] ( ai - 15 ) \u2026 ; hbox , right of = ai - 15 , node distance=0.5 cm ] ( ai - 16 ) ; hbox , right of = ai - 16 , node distance=0.5 cm ] ( ai - 17 ) \u2026 ; hbox , right of = ai - 17 , node distance=0.5 cm ] ( ai - 18 ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( ai - 13 ) \u2013 ( ai - 14 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 14 ) \u2013 ( ai - 15 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 15 ) \u2013 ( ai - 16 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 16 ) \u2013 ( ai - 17 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 17 ) \u2013 ( ai - 18 ) ; [ green hbox , right of = a - 12u , node distance=0.9cm*1.5 ] ( ai - 13u ) ; hbox , right of = ai - 13u , node distance=0.5 cm ] ( ai - 14u ) ; hbox , right of = ai - 14u , node distance=0.5 cm ] ( ai - 15u ) \u2026 ; hbox , right of = ai - 15u , node distance=0.5 cm ] ( ai - 16u ) ; hbox , right of = ai - 16u , node distance=0.5 cm ] ( ai - 17u ) \u2026 ; hbox , right of = ai - 17u , node distance=0.5 cm ] ( ai - 18u ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( ai - 14u ) \u2013 ( ai - 13u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 15u ) \u2013 ( ai - 14u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 16u ) \u2013 ( ai - 15u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 17u ) \u2013 ( ai - 16u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 18u ) \u2013 ( ai - 17u ) ; [ blue hbox , right of = ai - 18 , node distance=0.9cm*1.5 ] ( ai - 19 ) ; hbox , right of = ai - 19 , node distance=0.5 cm ] ( ai - 20 ) ; hbox , right of = ai - 20 , node distance=0.5 cm ] ( ai - 21 ) \u2026 ; hbox , right of = ai - 21 , node distance=0.5 cm ] ( ai - 22 ) ; hbox , right of = ai - 22 , node distance=0.5 cm ] ( ai - 23 ) \u2026 ; hbox , right of = ai - 23 , node distance=0.5 cm ] ( ai - 24 ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( ai - 19 ) \u2013 ( ai - 20 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 20 ) \u2013 ( ai - 21 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 21 ) \u2013 ( ai - 22 ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 22 ) \u2013 ( ai - 23 ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 23 ) \u2013 ( ai - 24 ) ; [ green hbox , right of = ai - 18u , node distance=0.9cm*1.5 ] ( ai - 19u ) ; hbox , right of = ai - 19u , node distance=0.5 cm ] ( ai - 20u ) ; hbox , right of = ai - 20u , node distance=0.5 cm ] ( ai - 21u ) \u2026 ; hbox , right of = ai - 21u , node distance=0.5 cm ] ( ai - 22u ) ; hbox , right of = ai - 22u , node distance=0.5 cm ] ( ai - 23u ) \u2026 ; hbox , right of = ai - 23u , node distance=0.5 cm ] ( ai - 24u ) ; arrow , shorten \u00bf = 1pt , shorten \u00a1 = 1pt ] ( ai - 20u ) \u2013 ( ai - 19u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 21u ) \u2013 ( ai - 20u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 22u ) \u2013 ( ai - 21u ) ; arrow , shorten \u00bf = 0pt , shorten \u00a1 = - 2pt ] ( ai - 23u ) \u2013 ( ai - 22u ) ; arrow , shorten \u00bf = - 2pt , shorten \u00a1 = 0pt ] ( ai - 24u ) \u2013 ( ai - 23u ) ; arrow , shorten \u00bf = 6pt , shorten \u00a1 = - 5pt ] ( m - 3 ) \u2013 ( a - 3 ) ; arrow , shorten \u00bf = 6pt , shorten \u00a1 = - 5pt ] ( m - 9 ) \u2013 ( a - 9 ) ; arrow , shorten \u00bf = 6pt , shorten \u00a1 = - 5pt ] ( mi - 15 ) \u2013 ( ai - 15 ) ; arrow , shorten \u00bf = 6pt , shorten \u00a1 = - 5pt ] ( mi - 21 ) \u2013 ( ai - 21 ) ; [ green vbox , above of = a - 3u , node distance=1.5cm*1.2 , xshift= - 0.3 cm ] ( ap - 1 ) ; vbox , right of = ap - 1 , node distance=0.9cm*2 ] ( ap - 2 ) ; vbox , right of = ap - 2 , node distance=0.9cm*2 ] ( ap - 3 ) ; vbox , right of = ap - 3 , node distance=0.9cm*2 ] ( ap - 4 ) ; vbox , right of = ap - 4 , node distance=0.9cm*2 ] ( ap - 5 ) ; vbox , right of = ap - 5 , node distance=0.9cm*2 ] ( ap - 6 ) ; vbox , right of = ap - 6 , node distance=0.9cm*2 ] ( ap - 7 ) ; vbox , right of = ap - 7 , node distance=0.9cm*2 ] ( ap - 8 ) ; [ red arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( a - 1u ) \u2013 ( ap - 1 ) ; arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( a - 6.east ) \u2013 ( [ xshift= - 1 cm ] ap - 2.south east ) ; [ red arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( a - 7u ) \u2013 ( ap - 3 ) ; arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( a - 12.west ) \u2013 ( [ xshift= - 0.5cm ] ap - 4.south ) ; [ red arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( ai - 13u ) \u2013 ( ap - 5 ) ; arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( ai - 18.west ) \u2013 ( [ xshift= - 1cm ] ap - 6 ) ; [ red arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( ai - 19u ) \u2013 ( ap - 7 ) ; arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( ai - 24.west ) \u2013 ( ap - 8 ) ; [ tbox , above of = ap - 4 , node distance=0.9cm*1.6 , xshift=1 cm ] ( softmax ) softmax ; of = softmax , node distance=0.9 cm ] ( output ) P ( y|premise , image , hypothesis ) ; arrow , shorten \u00bf = 3pt , shorten \u00a1 = 3pt ] ( [ xshift=1.5cm ] ap - 4.north east ) \u2013 ( softmax ) ; [ decorate , thick , decoration = brace , amplitude=6pt , yshift=0pt ] ( [ yshift= - 65pt , xshift= - 50pt ] w - 1.south west ) \u2013 ( [ xshift= - 50pt , yshift= - 125pt ] c - 1.south west ) node [ black , midway , xshift= - 1 cm , yshift= - 0.2 cm , rotate=90 , anchor = north ] Embedding layer ; [ decorate , thick , decoration = brace , amplitude=6pt , yshift=0pt ] ( [ yshift= - 5pt , xshift= - 50pt ] c - 1.south west ) \u2013 ( [ xshift= - 50pt , yshift=25pt ] c - 1u.north west ) node [ black , midway , xshift= - 1 cm , rotate=90 , anchor = north ] Context layer ; [ decorate , thick , decoration = brace , amplitude=6pt , yshift=0pt ] ( [ yshift= - 95pt , xshift= - 50pt ] mp - 1.south west ) \u2013 ( [ xshift= - 50pt , yshift= - 45pt ] m - 1.north west ) node [ black , midway , xshift= - 1 cm , rotate=90 , anchor = north ] Matching layer ; [ decorate , thick , decoration = brace , amplitude=6pt , yshift=0pt ] ( [ xshift= - 50pt ] a - 1.south west ) \u2013 ( [ xshift= - 50pt , yshift=1.5cm*3 ] a - 1u.north west ) node [ black , midway , xshift= - 1 cm , rotate=90 , anchor = north ] Aggregation layer ; [ decorate , thick , decoration = brace , amplitude=6pt , yshift=0pt ] ( [ xshift= - 50pt , yshift=1.5cm*5.5 ] a - 1u.north west ) \u2013 ( [ xshift= - 50pt , yshift=1.5cm*12.5 ] a - 1u.north west ) node [ black , midway , xshift= - 1 cm , rotate=90 , anchor = north ] Prediction layer ; Here , we report some further details of our implementation of the V - BiMPM model described in Section 4 of the main paper , based on the work of Wang2017 . Our model is displayed in Figure [ reference ] . The core part of the original BiMPM is the matching layer . Given two - dimensional vectors and , each replicated times ( is the number of \u2018 perspectives \u2019 ) and a trainable weight matrix , matching involves a cosine similarity computation that yields an - dimensional matching vector , whose elements are defined as follows : The matching operations included are the following : full - matching , where each forward or backward contextual embedding of the premise P ( resp . the hypothesis H ) is matched to the last time - step of H ( resp . P ) ; max - pooling , where each forward / backward contextual embedding of one sentence is compared to the embeddings of the other , retaining the maximum value for each dimension ; attentive matching , where first , the pairwise cosine similarity between forward / backward embeddings of P and H is estimated , before calculating an attentive vector over the weighted sum of contextual embeddings for H and matching each forward / backward embedding of P against the attentive vector ; max - attentive matching , a version of attentive matching where the contextual embedding with the highest cosine is used as the attentive vector , instead of the weighted sum . The visually - augmented version of the original model , V - BiMPM , is displayed in Figure [ reference ] . To perform multimodal matching , the visual and textual vectors are mapped to a mutual space using the following affine transformation : where , , , and are the weight matrix , the bias , the input features and output features , respectively , and is any text ( P or H ) . Given weight matrices for text and for images , we compute the matching vector between a textual vector and image vector as : bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["V-SNLI"]]], "Method": [[["BiMPM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["V-SNLI"]]], "Method": [[["V-BiMPM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}]}
{"docid": "TST3-SREX-0017", "doctext": "document : Going deeper with convolutions We propose a deep convolutional neural network architecture codenamed \u00e2\u0080\u009cInception\u00e2\u0080\u009d , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large - Scale Visual Recognition Challenge 2014 ( ILSVRC\u00e2\u0080\u009914 ) . The main hallmark of this architecture is the improved utilization of the computing resources inside the network . This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant . To optimize quality , the architectural decisions were based on the Hebbian principle and the intuition of multi - scale processing . One particular incarnation used in our submission for ILSVRC\u00e2\u0080\u009914 is called GoogLeNet , a 22 layers deep network , the quality of which is assessed in the context of classification and detection . parskip [ ] section : Introduction In the last three years , mainly due to the advances of deep learning , more concretely convolutional networks , the quality of image recognition and object detection has been progressing at a dramatic pace . One encouraging news is that most of this progress is not just the result of more powerful hardware , larger datasets and bigger models , but mainly a consequence of new ideas , algorithms and improved network architectures . No new data sources were used , for example , by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes . Our GoogLeNet submission to ILSVRC 2014 actually uses fewer parameters than the winning architecture of Krizhevsky et al from two years ago , while being significantly more accurate . The biggest gains in object - detection have not come from the utilization of deep networks alone or bigger models , but from the synergy of deep architectures and classical computer vision , like the R - CNN algorithm by Girshick et al . Another notable factor is that with the ongoing traction of mobile and embedded computing , the efficiency of our algorithms \u2013 especially their power and memory use \u2013 gains importance . It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers . For most of the experiments , the models were designed to keep a computational budget of billion multiply - adds at inference time , so that the they do not end up to be a purely academic curiosity , but could be put to real world use , even on large datasets , at a reasonable cost . In this paper , we will focus on an efficient deep neural network architecture for computer vision , codenamed \u00e2\u0080\u009cInception\u00e2\u0080\u009d , which derives its name from the \u00e2\u0080\u009cNetwork in network\u00e2\u0080\u009d paper by Lin et al in conjunction with the famous \u201c we need to go deeper \u201d internet meme . In our case , the word \u201c deep \u201d is used in two different meanings : first of all , in the sense that we introduce a new level of organization in the form of the \u201c Inception module \u201d and also in the more direct sense of increased network depth . In general , one can view the Inception model as a logical culmination of while taking inspiration and guidance from the theoretical work by Arora et al . The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges , on which it significantly outperforms the current state of the art . section : Related Work Starting with LeNet - 5 , convolutional neural networks ( CNN ) have typically had a standard structure \u2013 stacked convolutional layers ( optionally followed by contrast normalization and max - pooling ) are followed by one or more fully - connected layers . Variants of this basic design are prevalent in the image classification literature and have yielded the best results to - date on MNIST , CIFAR and most notably on the ImageNet classification challenge . \u00c2 For larger datasets such as Imagenet , the recent trend has been to increase the number of layers and layer size , while using dropout to address the problem of overfitting . Despite concerns that max - pooling layers result in loss of accurate spatial information , the same convolutional network architecture as has also been successfully employed for localization , object detection and human pose estimation . Inspired by a neuroscience model of the primate visual cortex , Serre et al . use a series of fixed Gabor filters of different sizes in order to handle multiple scales , similarly to the Inception model . However , contrary to the fixed 2 - layer deep model of , all filters in the Inception model are learned . Furthermore , Inception layers are repeated many times , leading to a 22 - layer deep model in the case of the GoogLeNet model . Network - in - Network is an approach proposed by Lin et al . in order to increase the representational power of neural networks . When applied to convolutional layers , the method could be viewed as additional convolutional layers followed typically by the rectified linear activation . This enables it to be easily integrated in the current CNN pipelines . We use this approach heavily in our architecture . However , in our setting , convolutions have dual purpose : most critically , they are used mainly as dimension reduction modules to remove computational bottlenecks , that would otherwise limit the size of our networks . \u00c2 This allows for not just increasing the depth , but also the width of our networks without significant performance penalty . The current leading approach for object detection is the Regions with Convolutional Neural Networks ( R - CNN ) proposed by Girshick et al . . R - CNN decomposes the overall detection problem into two subproblems : to first utilize low - level cues such as color and superpixel consistency for potential object proposals in a category - agnostic fashion , and to then use CNN classifiers to identify object categories at those locations . Such a two stage approach leverages the accuracy of bounding box segmentation with low - level cues , as well as the highly powerful classification power of state - of - the - art CNNs . We adopted a similar pipeline in our detection submissions , but have explored enhancements in both stages , such as multi - box prediction for higher object bounding box recall , and ensemble approaches for better categorization of bounding box proposals . section : Motivation and High Level Considerations The most straightforward way of improving the performance of deep neural networks is by increasing their size . This includes both increasing the depth \u2013 the number of levels \u2013 of the network and its width : the number of units at each level . This is as an easy and safe way of training higher quality models , especially given the availability of a large amount of labeled training data . However this simple solution comes with two major drawbacks . Bigger size typically means a larger number of parameters , which makes the enlarged network more prone to overfitting , especially if the number of labeled examples in the training set is limited . This can become a major bottleneck , since the creation of high quality training sets can be tricky and expensive , especially if expert human raters are necessary to distinguish between fine - grained visual categories like those in ImageNet ( even in the 1000 - class ILSVRC subset ) as demonstrated by Figure [ reference ] . Another drawback of uniformly increased network size is the dramatically increased use of computational resources . For example , in a deep vision network , if two convolutional layers are chained , any uniform increase in the number of their filters results in a quadratic increase of computation . If the added capacity is used inefficiently ( for example , if most weights end up to be close to zero ) , then a lot of computation is wasted . Since in practice the computational budget is always finite , an efficient distribution of computing resources is preferred to an indiscriminate increase of size , even when the main objective is to increase the quality of results . The fundamental way of solving both issues would be by ultimately moving from fully connected to sparsely connected architectures , even inside the convolutions . Besides mimicking biological systems , this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al . . Their main result states that if the probability distribution of the data - set is representable by a large , very sparse deep neural network , then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs . Although the strict mathematical proof requires very strong conditions , the fact that this statement resonates with the well known Hebbian principle \u2013 neurons that fire together , wire together \u2013 suggests that the underlying idea is applicable even under less strict conditions , in practice . On the downside , today\u00e2\u0080\u0099s computing infrastructures are very inefficient when it comes to numerical calculation on non - uniform sparse data structures . Even if the number of arithmetic operations is reduced by , the overhead of lookups and cache misses is so dominant that switching to sparse matrices would not pay off . The gap is widened even further by the use of steadily improving , highly tuned , numerical libraries that allow for extremely fast dense matrix multiplication , exploiting the minute details of the underlying CPU or GPU hardware . Also , non - uniform sparse models require more sophisticated engineering and computing infrastructure . Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions . However , convolutions are implemented as collections of dense connections to the patches in the earlier layer . ConvNets have traditionally used random and sparse connection tables in the feature dimensions since in order to break the symmetry and improve learning , the trend changed back to full connections with in order to better optimize parallel computing . The uniformity of the structure and a large number of filters and greater batch size allow for utilizing efficient dense computation . This raises the question whether there is any hope for a next , intermediate step : an architecture that makes use of the extra sparsity , even at filter level , as suggested by the theory , but exploits our current hardware by utilizing computations on dense matrices . The vast literature on sparse matrix computations ( e.g. ) suggests that clustering sparse matrices into relatively dense submatrices tends to give state of the art practical performance for sparse matrix multiplication . It does not seem far - fetched to think that similar methods would be utilized for the automated construction of non - uniform deep - learning architectures in the near future . The Inception architecture started out as a case study of the first author for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by for vision networks and covering the hypothesized outcome by dense , readily available components . Despite being a highly speculative undertaking , only after two iterations on the exact choice of topology , we could already see modest gains against the reference architecture based on . After further tuning of learning rate , hyperparameters and improved training methodology , we established that the resulting Inception architecture was especially useful in the context of localization and object detection as the base network for and . Interestingly , while most of the original architectural choices have been questioned and tested thoroughly , they turned out to be at least locally optimal . One must be cautious though : although the proposed architecture has become a success for computer vision , it is still questionable whether its quality can be attributed to the guiding principles that have lead to its construction . Making sure would require much more thorough analysis and verification : for example , if automated tools based on the principles described below would find similar , but better topology for the vision networks . The most convincing proof would be if an automated system would create network topologies resulting in similar gains in other domains using the same algorithm but with very differently looking global architecture . At very least , the initial success of the Inception architecture yields firm motivation for exciting future work in this direction . section : Architectural Details The main idea of the Inception architecture is based on finding out how an optimal local sparse structure in a convolutional vision network can be approximated and covered by readily available dense components . Note that assuming translation invariance means that our network will be built from convolutional building blocks . All we need is to find the optimal local construction and to repeat it spatially . Arora et al . suggests a layer - by layer construction in which one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation . These clusters form the units of the next layer and are connected to the units in the previous layer . We assume that each unit from the earlier layer corresponds to some region of the input image and these units are grouped into filter banks . In the lower layers ( the ones close to the input ) correlated units would concentrate in local regions . This means , we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of convolutions in the next layer , as suggested in . However , one can also expect that there will be a smaller number of more spatially spread out clusters that can be covered by convolutions over larger patches , and there will be a decreasing number of patches over larger and larger regions . In order to avoid patch - alignment issues , current incarnations of the Inception architecture are restricted to filter sizes , and , however this decision was based more on convenience rather than necessity . It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage . Additionally , since pooling operations have been essential for the success in current state of the art convolutional networks , it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect , too ( see Figure [ reference ] ) . As these \u201c Inception modules \u201d are stacked on top of each other , their output correlation statistics are bound to vary : as features of higher abstraction are captured by higher layers , their spatial concentration is expected to decrease suggesting that the ratio of and convolutions should increase as we move to higher layers . One big problem with the above modules , at least in this na\u00efve form , is that even a modest number of convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters . This problem becomes even more pronounced once pooling units are added to the mix : their number of output filters equals to the number of filters in the previous stage . The merging of the output of the pooling layer with the outputs of convolutional layers would lead to an inevitable increase in the number of outputs from stage to stage . Even while this architecture might cover the optimal sparse structure , it would do it very inefficiently , leading to a computational blow up within a few stages . This leads to the second idea of the proposed architecture : judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise . This is based on the success of embeddings : even low dimensional embeddings might contain a lot of information about a relatively large image patch . However , embeddings represent information in a dense , compressed form and compressed information is harder to model . We would like to keep our representation sparse at most places ( as required by the conditions of ) and compress the signals only whenever they have to be aggregated en masse . That is , convolutions are used to compute reductions before the expensive and convolutions . Besides being used as reductions , they also include the use of rectified linear activation which makes them dual - purpose . The final result is depicted in Figure [ reference ] . In general , an Inception network is a network consisting of modules of the above type stacked upon each other , with occasional max - pooling layers with stride 2 to halve the resolution of the grid . For technical reasons ( memory efficiency during training ) , it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion . This is not strictly necessary , simply reflecting some infrastructural inefficiencies in our current implementation . One of the main beneficial aspects of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow - up in computational complexity . The ubiquitous use of dimension reduction allows for shielding the large number of input filters of the last stage to the next layer , first reducing their dimension before convolving over them with a large patch size . Another practically useful aspect of this design is that it aligns with the intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from different scales simultaneously . The improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties . Another way to utilize the inception architecture is to create slightly inferior , but computationally cheaper versions of it . We have found that all the included the knobs and levers allow for a controlled balancing of computational resources that can result in networks that are faster than similarly performing networks with non - Inception architecture , however this requires careful manual design at this point . section : GoogLeNet We chose GoogLeNet as our team - name in the ILSVRC14 competition . This name is an homage to Yann LeCun\u00e2\u0080\u0099s pioneering LeNet 5 network . We also use GoogLeNet to refer to the particular incarnation of the Inception architecture used in our submission for the competition . We have also used a deeper and wider Inception network , the quality of which was slightly inferior , but adding it to the ensemble seemed to improve the results marginally . We omit the details of that network , since our experiments have shown that the influence of the exact architectural parameters is relatively minor . Here , the most successful particular instance ( named GoogLeNet ) is described in Table [ reference ] for demonstrational purposes . The exact same topology ( trained with different sampling methods ) was used for 6 out of the 7 models in our ensemble . All the convolutions , including those inside the Inception modules , use rectified linear activation . The size of the receptive field in our network is taking RGB color channels with mean subtraction . \u201c reduce \u201d and \u201c reduce \u201d stands for the number of filters in the reduction layer used before the and convolutions . One can see the number of filters in the projection layer after the built - in max - pooling in the \u00e2\u0080\u009cpool proj\u00e2\u0080\u009d column . All these reduction / projection layers use rectified linear activation as well . The network was designed with computational efficiency and practicality in mind , so that inference can be run on individual devices including even those with limited computational resources , especially with low - memory footprint . The network is 22 layers deep when counting only layers with parameters ( or 27 layers if we also count pooling ) . The overall number of \u00e2\u0080\u009clayers\u00e2\u0080\u009d ( independent building blocks ) used for the construction of the network is about 100 . However this number depends on the machine learning infrastructure system used . The use of average pooling before the classifier is based on , although our implementation differs in that we use an extra linear layer . This enables adapting and fine - tuning our networks for other label sets easily , but it is mostly convenience and we do not expect it to have a major effect . It was found that a move from fully connected layers to average pooling improved the top - 1 accuracy by about 0.6 % , however the use of dropout remained essential even after removing the fully connected layers . Given the relatively large depth of the network , the ability to propagate gradients back through all the layers in an effective manner was a concern . One interesting insight is that the strong performance of relatively shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative . By adding auxiliary classifiers connected to these intermediate layers , we would expect to encourage discrimination in the lower stages in the classifier , increase the gradient signal that gets propagated back , and provide additional regularization . These classifiers take the form of smaller convolutional networks put on top of the output of the Inception ( 4a ) and ( 4d ) modules . During training , their loss gets added to the total loss of the network with a discount weight ( the losses of the auxiliary classifiers were weighted by 0.3 ) . At inference time , these auxiliary networks are discarded . The exact structure of the extra network on the side , including the auxiliary classifier , is as follows : An average pooling layer with filter size and stride , resulting in an output for the ( 4a ) , and for the ( 4d ) stage . A convolution with 128 filters for dimension reduction and rectified linear activation . A fully connected layer with 1024 units and rectified linear activation . A dropout layer with 70 % ratio of dropped outputs . A linear layer with softmax loss as the classifier ( predicting the same 1000 classes as the main classifier , but removed at inference time ) . A schematic view of the resulting network is depicted in Figure [ reference ] . section : Training Methodology Our networks were trained using the DistBelief distributed machine learning system using modest amount of model and data - parallelism . Although we used CPU based implementation only , a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high - end GPUs within a week , the main limitation being the memory usage . Our training used asynchronous stochastic gradient descent with 0.9 momentum , fixed learning rate schedule ( decreasing the learning rate by 4 % every 8 epochs ) . Polyak averaging was used to create the final model used at inference time . Our image sampling methods have changed substantially over the months leading to the competition , and already converged models were trained on with other options , sometimes in conjunction with changed hyperparameters , like dropout and learning rate , so it is hard to give a definitive guidance to the most effective single way to train these networks . To complicate matters further , some of the models were mainly trained on smaller relative crops , others on larger ones , inspired by . Still , one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly between 8 % and 100 % of the image area and whose aspect ratio is chosen randomly between and . Also , we found that the photometric distortions by Andrew Howard were useful to combat overfitting to some extent . In addition , we started to use random interpolation methods ( bilinear , area , nearest neighbor and cubic , with equal probability ) for resizing relatively late and in conjunction with other hyperparameter changes , so we could not tell definitely whether the final results were affected positively by their use . section : ILSVRC 2014 Classification Challenge Setup and Results The ILSVRC 2014 classification challenge involves the task of classifying the image into one of 1000 leaf - node categories in the Imagenet hierarchy . There are about 1.2 million images for training , 50 , 000 for validation and 100 , 000 images for testing . Each image is associated with one ground truth category , and performance is measured based on the highest scoring classifier predictions . Two numbers are usually reported : the top - 1 accuracy rate , which compares the ground truth against the first predicted class , and the top - 5 error rate , which compares the ground truth against the first 5 predicted classes : an image is deemed correctly classified if the ground truth is among the top - 5 , regardless of its rank in them . The challenge uses the top - 5 error rate for ranking purposes . We participated in the challenge with no external data used for training . In addition to the training techniques aforementioned in this paper , we adopted a set of techniques during testing to obtain a higher performance , which we elaborate below . We independently trained 7 versions of the same GoogLeNet model ( including one wider version ) , and performed ensemble prediction with them . These models were trained with the same initialization ( even with the same initial weights , mainly because of an oversight ) and learning rate policies , and they only differ in sampling methodologies and the random order in which they see input images . During testing , we adopted a more aggressive cropping approach than that of Krizhevsky et al . . Specifically , we resize the image to 4 scales where the shorter dimension ( height or width ) is 256 , 288 , 320 and 352 respectively , take the left , center and right square of these resized images ( in the case of portrait images , we take the top , center and bottom squares ) . For each square , we then take the 4 corners and the center crop as well as the square resized to , and their mirrored versions . This results in crops per image . A similar approach was used by Andrew Howard in the previous year \u2019s entry , which we empirically verified to perform slightly worse than the proposed scheme . We note that such aggressive cropping may not be necessary in real applications , as the benefit of more crops becomes marginal after a reasonable number of crops are present ( as we will show later on ) . The softmax probabilities are averaged over multiple crops and over all the individual classifiers to obtain the final prediction . In our experiments we analyzed alternative approaches on the validation data , such as max pooling over crops and averaging over classifiers , but they lead to inferior performance than the simple averaging . In the remainder of this paper , we analyze the multiple factors that contribute to the overall performance of the final submission . Our final submission in the challenge obtains a top - 5 error of 6.67 % on both the validation and testing data , ranking the first among other participants . This is a 56.5 % relative reduction compared to the SuperVision approach in 2012 , and about 40 % relative reduction compared to the previous year \u2019s best approach ( Clarifai ) , both of which used external data for training the classifiers . The following table shows the statistics of some of the top - performing approaches . We also analyze and report the performance of multiple testing choices , by varying the number of models and the number of crops used when predicting an image in the following table . When we use one model , we chose the one with the lowest top - 1 error rate on the validation data . All numbers are reported on the validation dataset in order to not overfit to the testing data statistics . section : ILSVRC 2014 Detection Challenge Setup and Results The ILSVRC detection task is to produce bounding boxes around objects in images among 200 possible classes . Detected objects count as correct if they match the class of the groundtruth and their bounding boxes overlap by at least 50 % ( using the Jaccard index ) . Extraneous detections count as false positives and are penalized . Contrary to the classification task , each image may contain many objects or none , and their scale may vary from large to tiny . Results are reported using the mean average precision ( mAP ) . The approach taken by GoogLeNet for detection is similar to the R - CNN by , but is augmented with the Inception model as the region classifier . Additionally , the region proposal step is improved by combining the Selective Search approach with multi - box predictions for higher object bounding box recall . In order to cut down the number of false positives , the superpixel size was increased by . This halves the proposals coming from the selective search algorithm . We added back 200 region proposals coming from multi - box resulting , in total , in about 60 % of the proposals used by , while increasing the coverage from 92 % to 93 % . The overall effect of cutting the number of proposals with increased coverage is a 1 % improvement of the mean average precision for the single model case . Finally , we use an ensemble of 6 ConvNets when classifying each region which improves results from 40 % to 43.9 % accuracy . Note that contrary to R - CNN , we did not use bounding box regression due to lack of time . We first report the top detection results and show the progress since the first edition of the detection task . Compared to the 2013 result , the accuracy has almost doubled . The top performing teams all use Convolutional Networks . We report the official scores in Table [ reference ] and common strategies for each team : the use of external data , ensemble models or contextual models . The external data is typically the ILSVRC12 classification data for pre - training a model that is later refined on the detection data . Some teams also mention the use of the localization data . Since a good portion of the localization task bounding boxes are not included in the detection dataset , one can pre - train a general bounding box regressor with this data the same way classification is used for pre - training . The GoogLeNet entry did not use the localization data for pretraining . In Table [ reference ] , we compare results using a single model only . The top performing model is by Deep Insight and surprisingly only improves by 0.3 points with an ensemble of 3 models while the GoogLeNet obtains significantly stronger results with the ensemble . section : Conclusions Our results seem to yield a solid evidence that approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision . The main advantage of this method is a significant quality gain at a modest increase of computational requirements compared to shallower and less wide networks . Also note that our detection work was competitive despite of neither utilizing context nor performing bounding box regression and this fact provides further evidence of the strength of the Inception architecture . Although it is expected that similar quality of result can be achieved by much more expensive networks of similar depth and width , our approach yields solid evidence that moving to sparser architectures is feasible and useful idea in general . This suggest promising future work towards creating sparser and more refined structures in automated ways on the basis of . section : Acknowledgements We would like to thank Sanjeev Arora and Aditya Bhaskara for fruitful discussions on . Also we are indebted to the DistBelief team for their support especially to Rajat Monga , Jon Shlens , Alex Krizhevsky , Jeff Dean , Ilya Sutskever and Andrea Frome . We would also like to thank to Tom Duerig and Ning Ye for their help on photometric distortions . Also our work would not have been possible without the support of Chuck Rosenberg and Hartwig Adam . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["ImageNet"]]], "Method": [[["Inception_V1"]]], "Metric": [[["Top_1_Accuracy"]]], "Task": [[["Image_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ImageNet"]]], "Method": [[["Inception_V1"]]], "Metric": [[["Top_5_Accuracy"]]], "Task": [[["Image_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ImageNet_Detection"]]], "Method": [[["Inception_V1"]]], "Metric": [[["MAP"]]], "Task": [[["Object_Detection"]]]}]}
{"docid": "TST3-SREX-0018", "doctext": "Image Super - resolution via Feature - augmented Random Forest section : Abstract - Recent random - forest ( RF )- based image super - resolution approaches inherit some properties from dictionary - learning - based algorithms , but the effectiveness of the properties in RF is overlooked in the literature . In this paper , we present a novel feature - augmented random forest ( FARF ) for image super - resolution , where the conventional gradient - based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF . The advantages of our method are that , firstly , the dictionary - learning - based features are enhanced by adding gradient magnitudes , based on the observation that the non - linear gradient magnitude are with highly discriminative property . Secondly , generalized locality - sensitive hashing ( LSH ) is used to replace principal component analysis ( PCA ) for feature dimensionality reduction and original high - dimensional features are employed , instead of the compressed ones , for the leaf - nodes ' regressors , since regressors can benefit from higher dimensional features . This original - compressed coupled feature sets scheme unifies the unsupervised LSH evaluation on both image super - resolution and content - based image retrieval ( CBIR ) . Finally , we present a generalized weighted ridge regression ( GWRR ) model for the leaf - nodes ' regressors . Experiment results on several public benchmark datasets show that our FARF method can achieve an average gain of about 0.3 dB , compared to traditional RF - based methods . Furthermore , a fine - tuned FARF model can compare to or ( in many cases ) outperform some recent stateof - the - art deep - learning - based algorithms . section : INTRODUCTION In the past few years , random forest ( RF ) [ reference ][ reference ] as a machine - learning tool , working via an ensemble of multiple decision trees , has been employed for efficient classification or regression problems , and applied to a large variety of computer - vision applications , such as object recognition [ reference ] , face alignment [ reference ][ reference ][ reference ] , data clustering [ reference ] , single image super - resolution ( SISR ) [ reference ][ reference ] , and so on . The RF method , which benefits from its simple implementation of binary trees , has been widely used , and exhibits a number of merits , including ( 1 ) it works with an ensemble of multiple decision trees to express the principle that \" two heads are better than one \" , [ reference ] it is easy to be sped up with parallel processing technology , on both the training and inference stages , ( 3 ) it has sub - linear search complexity , because of the use of the binary tree structure , ( 4 ) the bagging strategy for feature candidates on splitnodes enable it to handle high - dimensional features and avoid over - fitting on regression , and ( 5 ) the clustering - regression scheme employs the \" divide and conquer \" strategy , which can tackle the classification and regression tasks with more stable performance . The RF - based image super - resolution approach can be considered as a clustering / classificationbased method , as shown in Fig . 1 . But the clustering and regression problems in RF require with different discriminative features , which have not been systematically studied in existing literature . Feature engineering has been a research hotspot for decades . Several features have been proposed for learning the mapping functions from low - resolution ( LR ) patches to high - resolution ( HR ) patches on image restoration problems . Pioneer work in [ reference ] used a simple high - pass filter as simple as subtracting a low - pass filtered values from the input image raw values . Meanwhile , most algorithms [ reference ][ reference ][ reference ][ reference ][ reference ] follow the approach in [ reference ] , which concatenates the first - and second - order gradients to form the features , as an inexpensive solution to approximating high - pass filtering . Since RF is used as a dictionarylearning - based tool , it inherits many properties from the conventional dictionary - learning - based algorithms on feature extraction . However , the discriminative ability of those gradient - based features for random forest has been overlooked in the literature . We found , from experiments , that augmented features based on two gradient - magnitude filters can achieve more than 0.1dB quality improvement in RF based SISR , with the same parameter setting . In most dictionary - learning - based algorithms , principal component analysis ( PCA ) is used for dimensionality reduction before classification and regression processes . The impact of using PCA has also been paid less attention in the literature . PCA projection may damage the structure of features , which are originally discriminative for clustering at the split - nodes and regression at the leaf - nodes . Motivated from content - based image retrieval ( CBIR ) [ reference ][ reference ] , where the coarse - level search uses compressed features , while the fine - level search uses augmented features . Therefore , in our method , we use the original features rather than the compressed features generated by PCA as worked in [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] , so that more accurate regression and higher image quality improvement can be achieved . Moreover , the unsupervised locality - sensitive hashing ( LSH ) model , instead of PCA , is employed for feature dimensionality reduction , which can reduce the damage on the feature structure for the compressed features used on clustering at the split - nodes and thus improve the final image quality . For regression problems at the leaf - nodes , we propose a generalized weighted ridge regression ( GWRR ) as an extension of the work in [ reference ] . GWRR models are generated based on the data distributions from the leaf - nodes . The main contribution of our method is on feature augmentation , so we call our method featureaugmented random forest ( FARF ) . The pipeline of our FARF method , which includes feature extraction , the training stage , and inference stages for SISR , is shown in Fig . 1 . In the FARF - based image SR scheme , higher discriminative features are extracted by using the first - and second - order gradients and their magnitudes . Then , the conventional PCA is replaced by the generalized LSH for dimensionality reduction , and the compressed features are used for clustering in the split - nodes on an RF . Finally , the respective regressors at the leaf - nodes are learned by using the original high dimensional features with the GWRR models . Having introduced the main idea of our paper , the remainder of this paper is organized as follows . In Section 2 , we review the related works on SISR , particularly the RF - based approaches and our insights . In Section 3 , we introduce the proposed method FARF , including the discriminative feature augmented by the gradient - magnitude filters , the generalized weighted ridge regression ( GWRR ) model , and the fine - tuned FARF version . In Section 4 , we evaluate our FARF scheme on public datasets , and conclusions are given in Section 5 . section : IMAGE SUPER - RESOLUTION VIA RANDOM FOREST section : Image Super - Resolution Image SR attempts to achieve an impressive HR quality image from one or a set of LR images via artistic skills , which has been an active research topic for decades in the image restoration area . Generalized SR includes interpolation algorithms , such as the classic bicubic interpolation , and other edge - preserving algorithms [ reference ][ reference ][ reference ][ reference ][ reference ] . The traditional super - resolution algorithms are based on pixel operations . Intuitively , operating on a \" big pixel \" , i.e. a patch [ reference ] , is more effective . Since patch - based algorithms can preserve the local texture structure of an image , various methods based on image patches , such as non - local means [ reference ] , self - similarity [ reference ] , manifold learning [ reference ] , block - matching and 3D filtering ( BM3D ) [ reference ] , sparse representation [ reference ] , etc . have been proposed . The neighbor - embedding ( NE ) methods [ reference ][ reference ] are the milestone for patch - based dictionary learning methods . NE learns the mapping between low - and high - resolution patches , with the use of manifold learning . Based on the locally linear embedding ( LLE ) theory , an LR patch can be represented as a linear combination of its nearest neighbors in a learned dictionary , and its HR counterpart can be approximated as a linear combination of the corresponding HR patches of its LR neighbors , with the same coefficients . Although the NE method is simple and sounds practical , a problem with the method is how to build a feasible patch dictionary . For example , for a patch size of 5\u00d75 , with 256 gray levels , it is necessary to have a massive dataset , which has millions of patches , in order to achieve high - quality reconstructed HR patches , if the patches are collected directly from natural scene images . Because of the large dictionary size , it is time consuming to search for a neighbor in such a large dataset . Other method to reduce the dictionary size is to learn a relatively smaller dictionary with discrete cosine transform ( DCT ) or wavelet fixed basis , which the adaptiveness is sacrificed . In 2010 , Yang et al . [ reference ] proposed a sparse prior for dictionary learning . Using sparse coding , image representation can work with a relatively smaller dictionary while keep the adaptiveness by learning the basis from data directly , which opens the era for sparse coding in the image inverse problems . With the sparse constraint used in the sparse - coding super - resolution ( ScSR ) framework , an LR patch and its corresponding HR patch can both be reconstructed through two learned coupled dictionaries , with the same coefficients as following : where and denote an LR patch and its HR counterpart , respectively , and D and D \u210e are the low and high - resolution coupled dictionaries trained jointly from LR and HR patch samples . The value of in \u2016 \u2016 \u03d1 is the sparsity factor of the coefficients . \u2016 \u2016 0 , called the 0 - norm , is the non - zero count of the coefficients in . The LR and HR coupled dictionaries are trained jointly with a sparsity constraint , as following : an LR patch of an input LR image Y can be formulated in terms of D as following : where is a feature - extraction operator on the LR patches , which aims to extract discriminative features from LR patches , rather than using the raw pixel intensity . Although the 0 - norm of \u03b1 is an ideal regularization term for the sparse constraint , this strong constraint leads to an NP - hard problem in solving the coefficients \u03b1 . Yang et al . [ reference ] relaxed the 0 - norm to 1 - norm , so as to achieve a feasible solution as following : and an equivalent formulation can be achieved by using the Lagrange multiplier , where the parameter balances the sparsity of the solution and the fidelity of the approximation to . As the sparse constraint in [ reference ] is still a bottleneck on training dictionaries considering the computation , an intuitive way to solve it is to relax the constraint again to 2 - norm . Meanwhile , the effectiveness of sparsity is challenged [ reference ][ reference ] by researchers as to whether sparsity or collaborative representation really helps in image classification and restoration . As a natural solution to that , Timofte et al . proposed an anchored neighborhood regression ( ANR ) [ reference ] framework , where there is no sparse constraint in the model . ANR replaces the sparse - decomposition optimization ( 1 - norm ) with a ridge regression ( i.e. 2 - norm ) , where the coefficients can be computed offline and each coefficient can be stored as an atom ( anchor ) in the dictionary . This offline learning can greatly speed - up the prediction stage , and this approach has subsequently led to several variant algorithms . Timofte et al . later extended the ANR approach to the A + [ reference ] . In A + [ reference ] , the coupled dictionaries are trained from a large pool of training samples ( in the order of millions ) rather than only from the anchoring atoms , which greatly improves the image quality . After that , more extensions based on ANR and A + have emerged [ reference ][ reference ][ reference ][ reference ][ reference ] . However , in the above - mentioned dictionary - learning methods , the complexity of finding those similar patches by comparing an input patch with all the dictionary items has been overlooked . Recently , algorithms using random forest ( RF ) [ reference ][ reference ][ reference ] have achieved state - of - the - art performances , in terms of both accuracy and efficiency for classification and regression tasks . This is mainly due to the use of ensemble learning and sublinear search based on binary trees . Schulter et al . [ reference ] adopted random forest and the clustering - regression scheme to learn regressors from the patches in leaf - nodes for SISR . With the same number of regressors , the RF - based algorithm can outperform or achieve comparable performance with A + and its variants , in terms of accuracy but with less computational complexity . In recent years , deep learning has achieved promising performances on image super - resolution [ reference ][ reference ][ reference ][ reference ] . In [ reference ][ reference ] , milestone works on image super - resolution based on deep learning were presented , where a convolutional neural network ( SRCNN ) was proposed to learn an end - to - end mapping between LR and HR images for image super - resolution . Later a scheme with very deep networks for SISR was proposed in [ reference ] , where the convergence rate of the deep network is improved by using residual learning and extremely high learning rates . In addition , Ledig et al . [ reference ] introduced a generative adversarial network ( GAN ) based image super - resolution model ( SRGAN ) , where the image perceptual loss function is reformulated as the combination of content loss and adversarial loss . Although deeplearning - based approaches have achieved promising progress on SISR , the heavy computational requirement is still a large burden even though the implementation is accelerated by GPU . This may limit them from those applications without powerful GPU , such as smart mobile terminals . cluster all the feature data assigned to this node . This results in separating the three data samples ( quadrangle , pentagon and hexagon ) into three leaf nodes . section : Image Super - Resolution via Random Forest In the inference stage , each decision tree returns a class probability ( | ) for a given test sample \u2208 , and the final class label * is then obtained via averaging , as follows : A splitting function ( ; \u0398 ) is typically parameterized by two values : ( i ) a feature dimensional index : \u0398 \uf0ce{1 , . . . , } , and ( ii ) a threshold \u0398 \uf0ce\u211d. The splitting function is defined as follows : where the outcome defines to which child node is routed , and 0 and 1 are the two labels belonging to the left and right child node , respectively . Each node chooses the best splitting function \u0398 * out of a randomly sampled set { \u0398 } , and the threshold \u0398 is determined by optimizing the following function : where and are the sets of samples that are routed to the left and right child nodes , respectively , and | | represents the number of samples in the set . During the training of an RF , the decision trees are provided with a random subset of the training data ( i.e. bagging ) , and are trained independently . Training a single decision tree involves recursively splitting each node , such that the training data in the newly created child node is clustered conforming to class labels . Each tree is grown until a stopping criterion is reached ( e.g. the number of samples in a node is less than a threshold or the tree depth reaches a maximum value ) and the class probability distributions are estimated in the leaf nodes . After fulfilling one of the stopping criteria , the density model ( ) in each leaf node is estimated by using all the samples falling into the leaf node , which will be used as a prediction of class probabilities in the inference stage . A simple way to estimate the probability distribution function ( ) is by averaging all the samples in the leaf node , and there are many variants , such as fitting a Gaussian distribution , kernel density estimation , etc . In ( 9 ) , ( ) is the local score for a set of samples in S ( S is either L or R ) , which is usually calculated by entropy , as shown in Eqn . [ reference ] , and it can be replaced by variance [ reference ][ reference ][ reference ] or by the Gini index [ reference ] . where is the number of classes , and ( | ) is the probability for class , which is estimated from the set . For the regression problem , the differential entropy is used , and is defined as , where ( | ) denotes the conditional probability of a target variable given an input sample . Assuming that ( . , . ) is of Gaussian distribution , and has only a set of finite samples , the differential entropy can be written as , where det ( \u03a3 ) is the determinant of the estimated covariance matrix of the target variables in . RF - based approaches hold some properties , which make them powerful classifiers as SVM ( support vector machine ) [ reference ] and AdaBoost ( short for \" Adaptive Boosting \" ) [ reference ] . Both SVM and AdaBoost work as to approximate the Bayes decision rule - known to be the optimal classifiers - via minimizing a margin - based global loss function . RF - based image super - resolution ( SR ) , following a recent emerging stream [ reference ][ reference ] on single - image SR , formulates the SR problem as a clustering - regression problem . These emerging approaches attempt to reconstruct an HR image from patches with the aid of an external database . These methods first decompose an image into patches , then classify the patches into different clusters , and later regressors are trained for all the clusters respectively , which generate mappings from an input LR patch 's features to its corresponding HR patch . In the inference stage , an LR image follows the same procedures , such that it is divided into patches and features are extracted from each patch . Then , the patches are classified into different clusters using K - NN [ reference ][ reference ] or RF [ reference ][ reference ][ reference ] , and their super - resolved HR patches are computed through regression in the leaf nodes ( see Fig . 1 ) . This kind of clustering - regression - based random forest [ reference ][ reference ][ reference ] methods has achieved state - of - the - art performance in SISR , both in terms of accuracy and efficiency . section : FEATURE - AUGMENTED RANDOM FOREST Classification and regression can be regarded as probability problems from the statistical theory . Historical frequentist probability is the probability obtained from the relative frequency in a large number of trials . In contrast , the Bayesian probability is an interpretation of the concept of probability , in which probability is interpreted as an expectation taking the knowledge and personal belief into account . From the Bayesian theory , the posterior probability of a random event is a conditional probability , which can be calculated if the relevant evidence or context is considered . Therefore , the posterior probability is the probability ( | ) of the parameters given the evidence . We denote the probability distribution function of the prior for parameters as ( ) , and the likelihood as ( | ) , which is the probability of given . Then , based on the Bayesian rule , the posterior probability can be defined as follows : The posterior probability can be denoted in a memorable form as : Based on the Bayesian framework , the likelihood term and the prior term are both required to be determined in order to solve the inverse problems , and the extracted features are normally worked as prior or likelihood , particularly on some image restoration problems . From this point of view , most research works , from classic feature extractors to deep - learning neural networks , are essentially done under the Bayesian inference framework . Since SISR is a well - known ill - posed problem , researchers have put their efforts into the priors of the problem with skills from mathematics , computer vision and machine learning . One of the obvious and most studied priors is the edge prior , which can be found in many pioneering works : new edgedirected interpolation ( NEDI ) [ reference ] , soft - decision adaptive interpolation ( SAI ) [ reference ] , directional filtering and data - fusion ( DFDF ) [ reference ] , modified edge - directed interpolation ( MEDI ) [ reference ] , and so on . The edge prior is effective on image processing , and the first and second - order gradients are studied and employed by Yang et al . [ reference ] in a pioneering dictionary - learning - based algorithm . However , the effect of edgebased features has not been investigated in depth . For the clustering and classification problems , feature engineering is a critical research point , and in some cases , the chosen feature may dominate the performance . As shown in Eqn . ( 6 ) , a feature filter , whose coefficients are computed to fit the most relevant parts in the LR image patches , is employed , and the generated features can achieve more accurate predictions for reconstructing their counterpart HR image patches , as shown in Fig . 3 . section : Augmented Features via Gradient Magnitude Filters Normally it is unstable to directly use pixel intensities as features , which are susceptible to the environmental lighting variations and camera noise . Instead , the differences between the neighboring pixels ' intensity values , which are computationally efficient , and are immune to lighting changes and noise , are examined . This type of features can be implemented efficiently through convolutional filters . Typically , the feature filter can be chosen as a high - pass filter , while in [ reference ][ reference ][ reference ][ reference ] , the first and second - order gradient operators are used to generate an up - sampled version from a low - resolution image , then four patches are extracted from the gradient maps at each location , and finally the patches are concatenated to form feature vectors . The four 1 - D filters used to extract the derivatives are described in Eqn . ( 14 ) , These features can work well on dictionary - learning - based methods , because when searching a matched patch in a dictionary , the distance is calculated based on the whole feature vectors with the Euclidean distance . However , when training a split node in a decision tree of an RF , only one or a few of the feature dimensions are chosen as candidate features for comparison . Therefore , more discriminative features are required for RF , when compared with dictionary - learning - based methods . The first and second - order gradients of an image can provide the directions of edges in a perceptual manner as shown in Fig . 4 and Fig . 5 , which can be calculated as Eqn . ( 15 ) , where / and / are the gradients in the x - axis and y - axis directions , respectively , at a given pixel . Meanwhile , the gradient magnitude image can provide the edge strength , as described in Eqn . [ reference ] . Fig . 4 shows a toy example of a man - made \" circle \" image , to demonstrate its discriminative property . With a natural image shown in Fig . 5 , it can be observed that the gradient magnitude image has more detailed textures than the gradient images ( / and / ) , as well as the sum of the horizontal gradient and vertical gradient image , i.e. / + / , perceptually . An explanation for this phenomenon is that non - linear features are usually more discriminative . Thus , in our work , all the first and second - order gradients , and gradient magnitude are employed , and are concatenated to form more discriminative , augmented features . On the other hand , the image orientation ( gradient angle ) is defined by the following formulation , where atan ( ) is the gradient orientation , with a value between - 90\uf0b0 and 90\uf0b0. As shown in Eqn . ( 17 ) , when the value of is equal to 0 or close to 0 , the value of \u2220\u2207 becomes infinitely large and unstable , i.e. , different will result in approximately the same \u2220\u2207 value . Based on this analysis , we only use the two gradient magnitude filters derived from the four gradient filters [ reference ] to generate the augmented features . Experiments validate that the use of the augmented features can improve the conventional RF algorithm [ reference ] to achieve a performance gain of more than 0.1dB , which is a remarkable improvement , with the same setting and parameters . section : Fine - grained Features for Regression The inference stage of the RF - based image super - resolution process is similar to the content - based image retrieval ( CBIR ) framework , as shown in Fig . 1 . The general approximated nearest neighbor ( ANN ) search framework [ reference ][ reference ] is an efficient strategy for large - scale image retrieval , which mainly consists of 4 parts : ( 1 ) extracting compact features ( e.g. , locality - sensitive Hashing ( LSH ) [ reference ] feature ) for a query image ; ( 2 ) coarse - level search using Hamming distance to measure the similarity between binary compact Hash features , then narrow the search scope into a smaller candidate group ; ( 3 ) fine - level search by using Euclidean distance to measure the similarity between their corresponding feature vectors ; and ( 4 ) finding the object in the smaller candidate group that is the nearest one to the query image . In the inference stage of conventional RF - based SISR , PCA projection is worked as a Hash - like function to compress the feature dimension for decreasing the search range , which can speed up the searching as the coarse - level search in a CBIR framework , but the impact of using PCA on feature dimensionality reduction has been overlooked in previous works [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] . Inspired by the finelevel search using augmented features in CBIR frameworks , the high dimensional features in the leaf nodes in an RF can further improve the prediction accuracy in the regression step , which has not been studied previously . Consequently , we use the original features , rather than PCA or the LSH compressed features , to perform ridge regression in the leaf nodes . Experimental results show that the new RF scheme can greatly improve the quality of super - resolved images , by using this augmented feature . Another explanation for this is that the regression problems can benefit more from higher dimensional features than classification problems . Based on the observation that the original edge - like features are used for the final regressors in the leaf nodes and the compressed features ( either produced by PCA or LSH ) are used for clustering in the split nodes , a new clustering - regression - based SISR approach can be designed as shown in Fig . 6 . In this new scheme , the original - compressed coupled feature sets are worked for different purposes at different stages , i.e. , the original edge features are used for regression in the leaf nodes , and the compressed features derived from the LSH - like functions are employed for node splitting ( clustering ) in the training stage , and node searching in the inference stage in the split nodes . section : Fig . 6 : Augmented features for regressors and the LSH compressed features for searching in a random forest In the new scheme , we unify the research of LSH - based SISR and image retrieval ( CBIR ) [ reference ][ reference ] . In brief , the new achievement on unsupervised LSH can be evaluated not only in CBIR systems , but also in the clustering - regression RF - based SISR methods . Moreover , as evidence from [ reference ] , proper unsupervised LSH models , e.g. , iterative quantization ( ITQ ) [ reference ] used for feature dimension reduction instead of PCA , can reduce the damage on the image structure . This can further improve the superresolved image quality . Different from [ reference ] using an ITQ - like algorithm to rotate the original features into a new feature space , with the use of the proposed original - compressed coupled feature sets , any unsupervised LSH generated features can directly be employed . section : Generalized Weighted Ridge Regression Model In this sub - section , we further analyze the ridge regression employed in the RF leaf nodes . The anchored neighborhood regression ( ANR ) [ reference ] model relaxes the 1 - norm in Eqn . ( 6 ) to the 2 - norm constraint , with least - squares minimization as the following equation , Based on the ridge regression [ reference ] theory , this 2 - norm constrained least square regression regularized problem has a closed - form solution , according to the Tikhonov regularization theory , as follows : With the assumption in [ reference ] , where HR patches and their counterpart LR patches share the same reconstructed coefficient \u03b1 , i.e. = D \u210e , from Eqn . [ reference ] we have If we define as a pre - calculated projection matrix , as follows , then the HR patches can be reconstructed with = . Having studied the model in Eqn . ( 18 ) , the authors in [ reference ] argued that different weights should be given to different atoms when reconstructing an HR patch so as to emphasize the similarity to the anchor atom . Based on this idea , [ reference ] proposed a weighted collaborative representation ( WCR ) model by generalizing the normal collaborative representation ( CR ) model in the ANR , where is a diagonal weight matrix , in which the non - zero entries are proportional to the similarities between the atoms and the anchor atom . Same as the ANR model , a new closed - form solution can be computed offline through the following and the new projection matrix can be derived as The WCR model further improves the ANR / A + model in terms of image quality , while keeping the same level of computation . In [ reference ] , the local geometry prior of the data sub - space is used . However , all the weighted ridge regression models [ reference ][ reference ] are constructed based on an existing dictionary , e.g. , Zeyde et al . [ reference ] used K - SVD to train a sparse - coding - based dictionary with 1024 items . This limits the models to collect samples in a smaller sub - space when constructing linear regressors based on existing anchor points . section : Fig . 7 : Gaussian mixture model ( GMM ) is used to generate the weights for weighted ridge regression , and the weight of each entry lies on its belonging cluster 's weight and its weight in the belonging cluster . When training the regressors in an RF , there is no existing anchor point in the clustered groups of the leaf nodes , similar to the previous models [ reference ][ reference ] . A solution to mentioned problem is inspired from the work on image classification using locality - constrained linear coding ( LLC ) [ reference ] , where Gaussian mixture model ( GMM ) is used to describe the locality - constrained affine subspace coding ( LASC ) [ reference ] . We employ GMM to construct the data distribution in the sub - space for each leaf node , which derives the weights of all the entries in the ridge regression models . Through the derived weights , we can obtain a generalized weighted ridge regression ( GWRR ) model for ridge regression . The new projection matrix is given as follows : where is a diagonal weight matrix , and the weight of each diagonal entry is related to its belonging cluster 's weight and its local weight in its belongingwhi cluster , as illustrated in the right part of Fig . 7 . Obviously , a query entry falling into a bigger cluster and closer to the center of the belonging cluster achieves a larger weight . In a rough form , the diagonal weight matrix is given as follows : where is the weight of the th entry , is number of samples in the leaf nodes , is the th cluster 's weight for the th entry , is the th entry 's local weight in the th cluster , which is approximated with the inverse value of the distance to the center of the belonging cluster , and is the number of clusters generated by the GMM model for a leaf node . Experimental results in Table - 1 show that the proposed GWRR model can achieve the same level of performance as WCR [ reference ] , and obtain 0.2dB gain more than the ANR [ 1 ] model . Note that when the number of samples in a leaf node becomes bigger , the performance of the GWRR model will achieve less advantage than the normal regression model , because the higher weights will be averaged by a large number of other samples . Theoretically , the regression of a leaf node can benefit from the GWRR model , particularly when there are a few samples falling into the leaf node . section : Initial Estimation with Iterative Back Projection Generally speaking , SISR is a low - level computer vision task , which attempts to restore an HR image from a single input LR image . A mathematical model for image degradation can be formulated as follows : where \u212c is a low - pass ( blur ) filter and \uf069 denotes the down - sampling operator with factor . Based on a given LR image , how to achieve an approximated HR image \u0302 is a classic inverse problem , which requires priors based on the Bayesian theory . Irani and Peleg [ reference ] firstly proposed an iterative back projection ( IBP ) method for SR reconstruction , and IBP is the most effective way to obtain an HR image when comparing it with other SR methods . In the IBP method , the reconstruction error of an estimated LR image \u0302 is the difference between the input LR and the synthesized image \u0302 generated from the estimated HR image \u0302 as follows : IBP is an efficient approach to obtain the HR image by minimizing the reconstruction error defined by Eqn . [ reference ] . For the IBP approach on SISR , the updating procedure can be summarized as the following two steps , performed iteratively : \u2022 Compute the reconstruction error ( \u0302 ) with the following equation : where \u2191 is the up - sampling operator and is a constant back - projection kernel to approximate the inverse operation of the low - pass filter \u212c. \u2022 Update the estimating HR image \u0302 by back - projecting errors as follows : where \u0302 is the estimated HR image at the - th iteration . Most learning - based algorithms [ reference ][ reference ][ reference ][ reference ] follow the milestone work in [ reference ] , which uses the coarse estimation firstly obtained via bicubic interpolation . As we know , the classic IBP algorithm is an efficient way to obtain high - quality up - scaled images , but it will inevitably produce artifacts ( such as ringing , jaggy effects , and noise ) at the output , because the kernel operator in Eqn . ( 29 ) is hard to estimate accurately . That is the reason why algorithms with IBP need an additional denoising process [ reference ][ reference ][ reference ] . However , the sparse - constraint - based approach [ reference ] does not have this denoising capability . As the 2 - norm constraint - based ridge regression has the denoising effect , due to its averaging - like process , this means that the ridge regression - based RF scheme has the denoise capability intrinsically . Based on this observation , we obtain the coarse estimation of an HR image \u0302 by applying IBP to the corresponding input LR image . Experimental results in Table - 2 and Table - 3 validate that using IBP , instead of bicubic , to obtain the initial coarse estimation can help the RF - based SR method obtain a remarkable improvement . As the number of trees is an important parameter in RF - based approaches , we plot the performance with respect to the number of trees . As shown in Fig . 8 , the performance of the RF - based image superresolution method increases as expected , but the increment becomes relatively smaller after a certain number of trees are used . The experimental results in Fig . 8 section : Fine - Tuning with Proper Trees in Random Forest section : PSNR section : Set14 million samples from the dataset are used for all training stages . It shows that using 45 trees is an optimal number , as a trade - off between performance and computational cost . Therefore , we set the number of trees for the proposed FARF method at 45 , and our method with this number is denoted as FARF*. The performances of our methods , and other methods , are tabulated in Table - 2 and Table - 3 . We also compare our methods with a recently proposed deep - learning - based algorithm , SRCNN algorithm [ reference ][ reference ] , and our methods outperform it in some cases . section : Algorithm Workflow The training and inference stages of the proposed FARF algorithm are described in Algorithm 1 and Algorithm 2 , respectively . To help the readers understand our paper , the source code of our algorithm will be available at : https: // github.com / HarleyHK / FARF , for reference . section : EXPERIMENTS In this section , we evaluate our algorithm on standard super - resolution benchmarks Set 5 , Set14 and B100 [ reference ] , and compare it with some state - of - the - art methods . They are bicubic interpolation , adjusted anchored neighborhood regression ( A + ) [ reference ] , standard RF [ reference ] , alternating regression forests ( ARF ) [ reference ] , and the convolutional neural - network - based image super - resolution ( SRCNN ) [ reference ][ reference ] , as listed in Table - 2 and Table - Table - 2 : Results of the proposed method compared with state - of - the - art works on 3 datasets in terms of PSNR ( dB ) using three different magnification factors ( # ) ( \u00d72 , \u00d73 , \u00d74 ) . Table - 2 summarizes the performances of our proposed algorithm on the 3 datasets , in terms of the average peak signal to noise ratio ( PSNR ) scores , with different magnification factors ( \u00d72 , \u00d73 , \u00d74 ) . The objective quality metric , PSNR , in Table - 2 also shows that the fine - tuned FARF , i.e. FARF * , can further improve the image quality , which is comparable to recently proposed state - of - the - art deeplearning - based algorithms , such as SRCNN [ reference ][ reference ] . Comparing our proposed FARF algorithm to other methods , the improved visual quality of our results is obvious , as shown in Fig . 9 . This shows that our method can produce more details , particularly on some texture - rich regions . Fig . 9 : Super - resolution ( \u00d73 ) images from B100 , bicubic , A + ( ACCV - 2014 ) [ reference ] , ARF ( CVPR - 2015 ) [ reference ] , SRCNN ( PAMI - 2016 ) [ reference ] , our proposed algorithm FARF , and ground truth . The results show that our FARF algorithm can produce more details and its performance is comparable to a recent state - of - the - art deep - learning method [ reference ] . section : CONCLUSIONS This paper presents a feature - augmented random forest ( FARF ) scheme for the single image superresolution ( SISR ) task by augmenting features and redesigning the inner structure of a random forest ( RF ) , with different feature recipes at different stages , where the compressed features are used for clustering in the split nodes and the original features are used for regression in the leaf nodes . The contributions of this paper are threefold : ( 1 ) the more discriminative gradient magnitude - based augmented features are proposed for clustering on split nodes and regression on leaf nodes ; ( 2 ) By extending principal component analysis ( PCA ) to a generalized unsupervised locality - sensitive hashing ( LSH ) model for dimensionality reduction , we lay out an original compressed coupled feature set for tackling the clustering - regression tasks , which unify SISR and content - based image retrieval ( CBIR ) for LSH evaluation ; and ( 3 ) we have extended WCR model to a generalized GWRR model for ridge regression . The proposed FAFR scheme can achieve highly competitive quality results , e.g. , obtaining about a 0.3dB gain in PSNR , on average , when compared to conventional RF - based super - resolution approaches . Furthermore , a fine - tuned version of our proposed FARF approach is provided , whose performance is comparable to recent state - of - the - art deep - learning - based algorithms . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["BSD100_-_4x_upscaling"]]], "Method": [[["FAFR_"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set14_-_4x_upscaling"]]], "Method": [[["FAFR_"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set5_-_4x_upscaling"]]], "Method": [[["FARF_"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}]}
{"docid": "TST3-SREX-0019", "doctext": "document : Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different language families and written in 28 different scripts . Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages , which is coupled with an auxiliary decoder and trained on publicly available parallel corpora . This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only , and transfer it to any of the 93 languages without any modification . Our approach sets a new state - of - the - art on zero - shot cross - lingual natural language inference for all the 14 languages in the XNLI dataset but one . We also achieve very competitive results in cross - lingual document classification ( MLDoc dataset ) . Our sentence embeddings are also strong at parallel corpus mining , establishing a new state - of - the - art in the BUCC shared task for 3 of its 4 language pairs . Finally , we introduce a new test set of aligned sentences in 122 languages based on the Tatoeba corpus , and show that our sentence embeddings obtain strong results in multilingual similarity search even for low - resource languages . Our PyTorch implementation , pre - trained encoder and the multilingual test set will be freely available . section : Introduction While the recent advent of deep learning has led to impressive progress in Natural Language Processing ( NLP ) , these techniques are known to be particularly data hungry , limiting their applicability in many practical scenarios . An increasingly popular approach to alleviate this issue is to first learn general language representations on unlabeled data , which are then integrated in task - specific downstream systems . This approach was first popularized by word embeddings mikolov2013distributed , pennington2014glove , but has recently been superseded by sentence - level representations Alexis:2017:emnlp , Peters:2018:naacl_elmo , Devlin:2018:arxiv_bert . Nevertheless , all these works learn a separate model for each language and are thus unable to leverage information across different languages , greatly limiting their potential performance for low - resource languages . In this work , we are interested in universal language agnostic sentence embeddings , that is , vector representations of sentences that are general with respect to two dimensions : the input language and the NLP task . The motivations for such a representation are multiple : the hope that languages with limited resources benefit from joint training over many languages , the desire to perform zero - shot transfer of an NLP model from one language ( e.g. English ) to another , and the possibility to handle code - switching . We achieve this by using a single encoder that can handle multiple languages , so that semantically similar sentences in different languages are close in the resulting embedding space . Most research in multilingual NLP focuses on high - resource languages like Chinese , Arabic or major European languages , and is usually limited to a few ( most often only two ) languages . In contrast , we learn joint sentence representations for 93 different languages , including under - resourced and minority languages ( see Tables [ reference ] and [ reference ] ) . Our system is trained on freely available parallel texts only . The contributions of this paper are as follows : We substantially improve on previous work to learn joint multilingual sentence representations . We learn one shared encoder that can handle 93 different languages . All languages are jointly embedded in a shared space , in contrast to most other works which usually consider separate English / foreign alignments . We cover 34 language families and 28 different scripts . We outperform the state - of - the - art on zero - shot cross - lingual natural language inference ( XNLI dataset ) and classification ( MLDoc dataset ) , bitext mining ( BUCC dataset ) and multilingual similarity search ( Tatoeba dataset ) , for almost all considered languages . These results were obtained with a single pre - trained BiLSTM encoder for all 93 languages and tasks , without any fine - tuning . We define a new test set based on the freely available Tatoeba corpus and provide baseline results for 122 languages . We report accuracy for multilingual similarity search on this test set , but the corpus could also be used for MT evaluation . The remaining of this paper is organized as follows . In the next section , we first summarize related work . Section [ reference ] then describe our approach in detail . All experimental results are given in Sections [ reference ] and [ reference ] , and the paper concludes with a discussion and directions for future research . Dataset details and additional result analysis can be found in the appendix . section : Related work Following the success of word embeddings mikolov2013distributed , pennington2014glove , there has been an increasing interest in learning continuous vector representations of longer linguistic units like sentences . These sentence embeddings are commonly obtained using a Recurrent Neural Network ( RNN ) encoder , which is typically trained in an unsupervised way over large collections of unlabelled corpora . For instance , the skip - thought model of Kiros:2015:nips_skipthought couple the encoder with an auxiliary decoder , and train the entire system end - to - end to predict the surrounding sentences over a large collection of books . It was later shown that more competitive results could be obtained by training the encoder over labeled Natural Language Inference ( NLI ) data Alexis:2017:emnlp . This was recently extended to multitask learning , combining different training objectives like that of skip - thought , NLI and machine translation Google:2018:arxiv_srep , MLIA - MSR:2018:iclr_srep . While the previous methods consider a single language at a time , multilingual representations have attracted a large attention in recent times . Most of this research focuses on cross - lingual word embeddings ruder2017survey , which are commonly learned jointly from parallel corpora gouws2015bilbowa , luong2015bilingual . An alternative approach that is becoming increasingly popular is to train word embeddings independently for each language over monolingual corpora , and then map them to a shared space based on a bilingual dictionary mikolov2013exploiting , artetxe2018generalizing or even in a fully unsupervised manner . Cross - lingual word embeddings are often used to build bag - of - word representations of longer linguistic units by taking their respective centroid Klementiev:2012:coling_reuters . While this approach has the advantage of requiring a weak ( or even no ) cross - lingual signal , it has been shown that the resulting sentence embeddings works rather poorly in practical cross - lingual transfer settings Conneau:2018:emnlp_xnli . A more competitive approach that we follow here is to use a sequence - to - sequence encoder - decoder architecture Schwenk:2017:repl4nlp , hassan2018achieving . The full system is trained end - to - end on parallel corpora akin to neural machine translation : the encoder maps the source sequence into a fixed - length vector representation , which is used by the decoder to create the target sequence . This decoder is then discarded , and the encoder is kept to embed sentences in any of the training languages . While some proposals use a separate encoder for each language Schwenk:2017:repl4nlp , sharing a single encoder for all languages also gives strong results Schwenk:2018:acl_mine . Nevertheless , most existing work is either limited to few , rather close languages or , more commonly , consider pairwise joint embeddings with English and one foreign language only . To the best of our knowledge , all existing work on learning multilingual representations for a large number of languages is limited to word embeddings ammar2016massively , ours being the first paper exploring massively multilingual sentence representations . Finally , while all the previous approaches learn a fixed - length representation for each sentence , a recent research line has obtained very strong results using variable - length representations instead , consisting of contextualized embeddings of the words in the sentence Peters:2018:naacl_elmo , howard2018universal , Devlin:2018:arxiv_bert . For that purpose , these methods train either an RNN or self - attentional encoder over unnanotated corpora using some form of language modeling . A classifier can then be learned on top of the resulting encoder , which is commonly further fine - tuned during this supervised training . Despite the strong performance of these approaches in monolingual settings , we argue that fixed - length approaches provide a more generic , flexible and compatible representation form for our multilingual scenario , and our model indeed outperforms the multilingual BERT model Devlin:2018:arxiv_bert in zero - shot transfer ( see Section [ reference ] ) . section : Proposed method We use a single , language agnostic BiLSTM encoder to build our sentence embeddings , which is coupled with an auxiliary decoder and trained over parallel corpora . From Section [ reference ] to [ reference ] , we describe its architecture , our training strategy to scale to up to 93 languages , and the training data used for that purpose . subsection : Architecture Figure [ reference ] illustrates the architecture of the proposed system , which is based on Schwenk:2018:acl_mine . As it can be seen , sentence embeddings are obtained by applying a max - pooling operation over the output of a BiLSTM encoder . These sentence embeddings are used to initialize the decoder LSTM through a linear transformation , and are also concatenated to its input embeddings at every time step . Note that there is no other connection between the encoder and the decoder , as we want all relevant information of the input sequence to be captured by the sentence embedding . We use a single encoder and decoder in our system , which are shared by all languages involved . For that purpose , we build a joint byte - pair encoding ( BPE ) vocabulary with 50k operations , which is learned on the concatenation of all training corpora . This way , the encoder has no explicit signal on what the input language is , encouraging it to learn language independent representations . In contrast , the decoder takes a language ID embedding that specifies the language to generate , which is concatenated to the input and sentence embeddings at every time step . Scaling up to almost hundred languages , which use very different syntax , writing scripts and linguistic concepts , naturally calls for an encoder with sufficient capacity . In this paper , we limit our study to a stacked BiLSTM with 1 to 5 layers , each 512 - dimensional . The resulting sentence representations ( after concatenating both directions ) are 1024 dimensional . The decoder has always one layer of dimension 2048 . The input embedding size is set to 320 , while the language ID embedding has 32 dimensions . Training [ - 1pt ] corpus [ - 2pt ] size Tatoeba [ - 1pt ] test set [ - 2pt ] size Training [ - 1pt ] corpus [ - 2pt ] size Tatoeba [ - 1pt ] test set [ - 2pt ] size subsection : Training strategies In preceding work , each sentence at the input was jointly translated into all other languages . While this approach was shown to learn high - quality representations , it poses two obvious drawbacks when trying to scale to a large number of languages . First , it requires an N - way parallel corpus , which is difficult to obtain for all languages . Second , it has a quadratic cost with respect to the number of languages , making training prohibitively slow as the number of languages is increased . In our preliminary experiments , we observed that similar results can be obtained by using less target languages \u2013 two seem to be enough . At the same time , we relax the requirement for N - way parallel corpora by considering independent alignments with the two target languages , e.g. we do not require each source sentence to be translated into the two target languages . Training minimizes the cross - entropy loss on the training corpus , alternating over all combinations of the languages involved . For that purpose , we use Adam with a constant learning rate of 0.001 and dropout set to 0.1 , and train for a fixed number of epochs . Our implementation is based on fairseq , and we make use of its multi - GPU support to train on 16 NVIDIA V100 GPUs with a total batch size of 128 , 000 tokens . Unless otherwise specified , we train our model for 17 epochs , which takes about 5 days . Stopping training early decreases the overall performance only slightly . subsection : Training data and pre - processing As described in Section [ reference ] , training requires bitexts aligned with two target languages . We choose English and Spanish for that purpose , as most of the data is aligned with these languages . We collect training corpora for 93 input languages by combining the Europarl , United Nations , OpenSubtitles2018 , Global Voices , Tanzil and Tatoeba corpus , which are all publicly available on the OPUS website . Appendix [ reference ] provides a more detailed description of this training data , while Tables [ reference ] and [ reference ] summarize the list of all languages used for training , their language family , writing script and the size of the bitexts . Our training data comprises a total of 223 million parallel sentences . In preliminary experiments , we observed that the domain of the training data played a key role in the performance of our sentence embeddings in different tasks . Some tasks ( BUCC , MLDoc ) tend to perform better when the encoder is trained on long and formal sentences , whereas other tasks ( XNLI , Tatoeba ) benefit from training on shorter and more informal sentences . In an attempt to achieve a general purpose sentence encoder that performs well on all tasks , we aimed at balancing the size of training corpora with long and short sentences . For that purpose , we used at most two million sentences from OpenSubtitles , although more data is available for some languages . All pre - processing is done with Moses tools : punctuation normalization , removing non - printing characters and tokenization . As the only exception , Chinese and Japanese texts were segmented with Jieba and Mecab , respectively . All the languages are kept in their original script with the exception of Greek , which we romanize into the Latin alphabet . section : Experimental evaluation In contrast with the well - established evaluation frameworks for English sentence representations , there is not yet a commonly accepted standard to evaluate multilingual sentence embeddings . The most notable effort in this regard is probably the XNLI corpus , an NLI test set similar to MultiNLI for which the premises and hypotheses were translated into 14 languages by professional translators . We train an NLI classifier on top of our multilingual sentence embedding using English training data , and evaluate its zero - shot transfer performance in the remaining languages ( Section [ reference ] ) . So as to obtain a more complete picture of the behavior of our multilingual sentence representations , we also evaluate them in cross - lingual document classification ( MLDoc , Section [ reference ] ) , and bitext mining ( BUCC , Section [ reference ] ) . However , all these datasets only cover a subset of our 93 languages , so we also introduce a new test set for multilingual similarity search in 122 languages , including several languages for which we have no training data but whose language family is covered ( Section [ reference ] ) . We remark that we use the same pre - trained BiLSTM encoder for all tasks and languages without any fine - tuning . subsection : XNLI : cross - lingual NLI NLI has become a widely used task to evaluate sentence representations snli:2015 , multinli:2017 . Given two sentences , a premise and a hypothesis , the task consists in deciding whether there is an entailment , contradiction or neutral relationship between them . XNLI is a recent effort to create a dataset similar to the English MultiNLI for several languages . 2 , 500 development and 5 , 000 test sentences have been translated from English into 14 languages by professional translators , making results across different languages directly comparable . Note that no human translated training data is provided ; instead , different systems are to use English training data from MultiNLI , and their transfer performance is evaluated on the rest of languages . We train a classifier on top of our multilingual encoder using the usual combination of the two sentence embeddings : , where and are the premise and hypothesis . For that purpose , we use a feed - forward neural network with two hidden layers of size 512 and 384 , trained with Adam . All hyperparameters were optimized on the English XNLI development corpus , and then , the same classifier was applied to all languages of the XNLI test set . As such , we did not use any training or development data in any of the foreign languages . Note , moreover , that the multilingual sentence embeddings are fixed and not fine - tuned on the task or the language . We report our results in Table [ reference ] , along with several baselines from Conneau:2018:emnlp_xnli and the recently released multilingual BERT model Devlin:2018:arxiv_bert . As it can be seen , our proposed method establishes a new state - of - the - art in zero - shot cross - lingual transfer ( i.e. training a classifier on English data and applying it to all other languages ) for all languages but Spanish . Our transfer results are strong and homogeneous across all languages : for 11 of them , the zero - short performance is ( at most ) 5 % lower than the one on English , including distant languages like Arabic , Chinese and Vietnamese , and we also achieve remarkable good results on low - resource languages like Swahili . In contrast , BERT achieves excellent results on English , outperforming our system by 7.5 points , but its zero - shot cross - lingual transfer performance is much weaker . For instance , the loss in accuracy for both Arabic and Chinese is 2.5 points for our system , compared to 19.3 and 17.6 points for BERT . Finally , we also outperform all baselines of Conneau:2018:emnlp_xnli by a substantial margin , with the additional advantage that we use a single pre - trained encoder , whereas X - BiLSTM learns a separate encoder for each language by aligning it to the English one . For completeness , we also provide results that include the use of Machine Translation ( MT ) . This can be done in two ways : 1 ) translate the test data into English and apply the English NLI classifier , or 2 ) translate the English training data and train a language specific NLI classifier for each language . It should be stressed that we are not evaluating multilingual sentence embeddings anymore , but rather the quality of the MT system and a monolingual model . Moreover , the use of MT incurs in an important overhead with either strategy : translating test makes inference substantially more expensive , whereas translating train results in a separate model for each language . As shown in Table [ reference ] , our approach outperforms all translation baselines of Conneau:2018:emnlp_xnli with the exception of Urdu . We also outperform MT BERT for Arabic and Thai , and are very close for Urdu . Finally , it is worth mentioning that , thanks to its multilingual nature , our system can also handle premises and hypothesis in different languages . As reported in Appendix [ reference ] , the proposed method obtains very strong results in these settings , even for distant language combinations like French - Chinese . subsection : MLDoc : cross - lingual classification Cross - lingual document classification is a typical application of multilingual representations . In order to evaluate our sentence embeddings in this task , we use the MLDoc dataset of Schwenk:2018:lrec_mldoc , which is an improved version of the Reuters benchmark Lewis : Reuters:2004 , Klementiev:2012:coling_reuters with uniform class priors and a wider language coverage . There are 1 , 000 training and development documents and 4 , 000 test documents for each language , divided in 4 different genders . Just as with the XNLI evaluation , we consider the zero - shot transfer scenario : we train a classifier on top of our multilingual encoder using the English training data , optimizing hyper - parameters on the English development set , and evaluating the resulting system in the remaining languages . We use a feed - forward neural network with one hidden layer of 10 units . As shown in Table [ reference ] , our system obtains the best published results for 5 of the 7 transfer languages . We believe that our weaker performance on Japanese can be attributed to the domain and sentence length mismatch between MLDoc and the parallel corpus we use for this language ( OpenSubtitles ) . subsection : BUCC : bitext mining Bitext mining is another natural application for multilingual sentence embeddings . Given two comparable corpora in different languages , the task consists in identifying sentence pairs that are translations of each other . For that purpose , one would commonly score sentence pairs by taking the cosine similarity of their respective embeddings , so parallel sentences can be extracted through nearest neighbor retrieval and filtered by setting a fixed threshold over this cosine score Schwenk:2018:acl_mine . However , it was recently shown that this approach suffers from scale inconsistency issues guo2018effective , and artetxe2018margin proposed the following alternative score addressing it : where and are the source and target sentences , and denotes the nearest neighbors of in the other language . The paper explores different margin functions , with ratio ( ) yielding the best results . This notion of margin is related to CSLS as proposed in Conneau:2018:iclr_muse . The reader is referred to artetxe2018margin for a detailed discussion . We use this method to evaluate our sentence embeddings on the BUCC mining task zweigenbaum2017overview , zweigenbaum2018overview , using exact same hyper - parameters as artetxe2018margin . The goal is to extract parallel sentences from a comparable corpus between English and four foreign languages : German , French , Russian and Chinese . The dataset consists of 150 K to 1.2 M sentences for each language , split into a sample , training and test set , with about 2\u20133 % of the sentences being parallel . As shown in our results in Table [ reference ] , our sentence embeddings establish a new state - of - the - art for all language pairs with the exception of English - Chinese test . Quite remarkably , we also outperform artetxe2018margin themselves , who use two separate models covering 4 languages each ( English / French / Spanish / German and English / French / Russian / Chinese ) . The average performance over the four languages increased from 93.27 to 93.92 . Not only are our results better , but our model also covers many more languages , so it can potentially be used to mine bitext for any combination of the 93 languages supported . subsection : Tatoeba : similarity search While XNLI , MLDoc and BUCC are well established benchmarks with comparative results available , they only cover a small subset of our 93 languages . So as to better assess the performance of our model in all these different languages , we introduce a new test set of similarity search for 122 languages based on the Tatoeba corpus . The dataset consists of up to 1 , 000 English - aligned sentence pairs for each language . Appendix [ reference ] describes how the dataset was constructed in more details . Evaluation is done by finding the nearest neighbor for each sentence in the other language according to cosine similarity and computing the error rate . We report our results in Tables [ reference ] and [ reference ] . Contrasting these results with those of XNLI , one would assume that similarity error rates below 5 % are indicative of strong downstream performance . This is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % , covering 22 different families and 15 different scripts . There are only 15 languages with error rates above 50 % . We believe that our competitive results for many low - resource languages are indicative of the benefits of joint training , which is also supported by our ablation results in Section [ reference ] . In relation to that , Appendix [ reference ] reports similarity search results for 29 additional languages without any training data , showing that our encoder can also generalize to unseen languages to some extent as long as it was trained in related languages . section : Ablation experiments In this section , we explore different variants of our approach and study the impact on the performance for all our evaluation tasks . We report average results across all languages . For XNLI , we also report the accuracy on English . subsection : Encoder depth Table [ reference ] reports the performance on the different tasks for encoders with one , three or five layers . We were not able to achieve good convergence with deeper models . It can be seen that all tasks benefit from deeper models , in particular XNLI and Tatoeba , suggesting that a single layer BiLSTM has not enough capacity to encode so many languages . subsection : Multitask learning Multitask learning has been shown to be helpful to learn English sentence embeddings . The most important task in this approach is arguably NLI , so we explored adding an additional NLI objective to our system with different weighting schemes . As shown in Table [ reference ] , the NLI objective leads to a better performance on the English NLI test set , but this comes at the cost of a worse cross - lingual transfer performance in XNLI and Tatoeba . The effect in BUCC is negligible . subsection : Number of training languages So as to better understand how our architecture scales to a large amount of languages , we train a separate model on a subset of 18 evaluation languages , and compare it to our main model trained on 93 languages . We replaced the Tatoeba corpus with the WMT 2014 test set to evaluate the multilingual similarity error rate . This covers English , Czech , French , German and Spanish , so results between both models are directly comparable . As shown in Table [ reference ] , the full model equals or outperforms the one covering the evaluation languages only for all tasks but MLDoc . This suggests that the joint training also yields to overall better representations . section : Conclusions In this paper , we propose an architecture to learn multilingual sentence embeddings for 93 languages . We use a single language - agnostic BiLSTM encoder for all languages , which is trained on publicly available parallel corpora and applied to different downstream tasks without any fine - tuning . Our model sets a new state - of - the - art for most languages in zero - shot cross - lingual natural language inference ( XNLI ) , cross - lingual document classification ( MLDoc ) , and bitext mining ( BUCC ) . We also introduce a new test set of cross - lingual similarity search in 122 languages , and show that our approach is competitive even for low - resource languages . To the best of our knowledge , this is the first successful exploration of massively multilingual sentence representations . In the future , we would like to explore alternative architectures for the encoder . In particular , we plan to replace our BiLSTM with the Transformer , which has been shown to work better in different settings vaswani2017attention , Devlin:2018:arxiv_bert . Moreover , we would like to explore possible strategies to exploit monolingual training data in addition to parallel corpora , such as using pre - trained word embeddings , backtranslation sennrich2016improving , edunov2018understanding , or other ideas from unsupervised machine translation Artetxe:2018:emnlp_unsupmt , Lample:2018:emnlp_unsupmt . Finally , we would like to replace our language - specific tokenization and BPE segmentation with a language agnostic approach similar to SentencePiece . The model and code used in this paper will be freely available in the framework of the LASER toolkit . bibliography : References appendix : Training data Our training data consists of the combination of the following publicly available parallel corpora : Europarl provides high - quality translations for 21 European languages . The size varies from 400k to 2 M sentence pairs , in function of the date the respective country joined the European Union . United Nations : More than 11 million sentences in the six official languages of the United Nations . We only use the first two million sentences in Arabic , Russian and Chinese . OpenSubtitles2018 : A collection of translations of movie subtitles in 57 languages . The corpus size varies from few thousand sentences ( e.g. Armenian or Kazakh ) to more than 50 million ( e.g. Spanish or Romanian ) . We keep at most 2 million entries for each language pair . Global Voices : A parallel corpus of news stories from the Global Voices website ( 38 languages ) . This is a rather small corpus with less than 100k sentence in most of the languages . Tanzil : A collection of Quran translations in 42 languages . The style and vocabulary is very different from news texts . The average size is 135k sentences . Tatoeba : A community supported collection of English sentences and translations into more than 300 languages . We use this corpus to extract a separate test set of up to 1 , 000 sentences for many languages ( see Section [ reference ] and [ reference ] ) . For languages with more than 1 , 000 entries , we use the remaining ones for training . Using all these corpora would provide parallel data for more than hundred languages . However , we finally only kept 93 different languages to train the multilingual sentence embeddings . In particular , we discarded several constructed languages with little practical use ( Klingon , Kotava , Lojban , Toki Pona and Volap\u00fck ) . appendix : XNLI results for all language combinations Table [ reference ] reports the accuracies of our system on the XNLI test set when the premises and hypothesis are in a different language ( e.g. premise in Russian and hypothesis in Thai ) . The numbers in the diagonal correspond to the main results reported in Table [ reference ] . We observe that our approach seems to handle the combination of different languages very well . We do not have evidence that very distant languages perform considerably worse . It rather seems that the combined performance is mostly bounded by the accuracy of the language which performs worst when used alone . As an example , Greek - Russian achieves very similar results than Bulgarian - Russian , two Slavic languages . Combing French with Chinese , two totally different languages , is only 1.5 points worse than French / Spanish , two very close languages . appendix : Tatoeba dataset Tatoeba is an open collection of English sentences and high quality translations into more than three hundred languages . The number of available translations is updated every Saturday . We downloaded the snapshot on November 19th 2018 and performed the following processing : Removal of sentences that contain \u201c @ \u201d or \u201c http \u201d . This is motivated by the fact that emails and web addresses are not language specific . Removal of sentences with less than three words ( before tokenization ) . These are usually sentences with limited semantic information . Removal of sentences that appear multiple times , either in the source or the target . After filtering , we created test sets of up to 1 , 000 aligned sentences with English . This amount of texts is available for 78 languages . Limiting the number of sentences to 500 , we increase the coverage to 101 languages , and even 141 languages with 100 parallel sentences . It should be stressed that , in general , the English sentences are not the same for the different languages . This implies that the error rates are not necessarily comparable between the languages . appendix : Tatoeba : result analysis We provide here some analysis on the results given in Tables [ reference ] and [ reference ] . We have 48 languages with an error rate below 10 % and 55 with less than 20 % , respectively ( English included ) . The languages with less than 20 % error belong to 20 different families and use 12 different scripts . It is nice to find six languages in this list for which we have only small amounts of bitexts ( less than 400k ) , namely Esperanto , Galician , Hindi , Interlingua , Malayam and Marathi . The two constructed languages probably benefit from their inspiration by other European languages . Overall , we observe low similarity error rates on the Indo - Aryan languages , namely Hindi , Bengali , Marathi and Urdu . The performance on Berber languages ( \u201c ber \u201d and \u201c kab \u201d ) is remarkable , although we have less than 100 thousand sentences to train them . This is a typical example of languages which are spoken by several millions of people , but for which the amount of written resources is very limited . It is quite unlikely that we would be able to train a good sentence embedding with language specific corpora only . This clearly shows the benefit of joint training on many languages . Fifteen languages have similarity error rates of more than 50 % . Four of them are low - resource languages with their own script and which are alone in their family : Amharic , Armenian , Khmer and Georgian . This makes it difficult to benefit from joint training . On the other hand , one can also argue that is surprising that a language like Khmer performs much better than random ( 99.9 % error rate ) with only 625 training examples . Khmer probably benefits of the fact that he have trained our model on other languages of the region which have influenced Khmer , namely Thai and Vietnamese . There are also several Turkic languages ( Kazakh , Tatar , Uighur and Uzbek ) and Celtic languages ( Breton and Cornish ) with high error rates . We hope to improve their performance in the future . appendix : Tatoeba : results for unseen languages We extend our Tatoeba experiments to 29 languages without any training data ( see Table [ reference ] ) . Many of them are recognized minority languages spoken in specific regions , e.g. Asturian , Faroese , Frisian , Kashubian , North Moluccan Malay , Piemontese , Swabian or Sorbian . All share some similarities , at various degrees , with other major languages , but also differ by their own grammar or specific vocabulary . This enables our encoder to perform reasonably well . We can probably assume that these are mainly spoken languages with limited resources in written form . The six languages which perform worst are Mongolian , Welsh , Xhosa Pampangan , Yiddish and Gaelic . We include these results here as baseline for future research . Premise Training [ - 1pt ] corpus [ - 2pt ] size Tatoeba [ - 1pt ] test set [ - 2pt ] size", "templates": [{"incident_type": "SciREX_incident", "Material": [[["BUCC_French-to-English"]]], "Method": [[["Massively_Multilingual_Sentence_Embeddings"]]], "Metric": [[["F1_score"]]], "Task": [[["Cross-Lingual_Bitext_Mining"]]]}, {"incident_type": "SciREX_incident", "Material": [[["BUCC_German-to-English"]]], "Method": [[["Massively_Multilingual_Sentence_Embeddings"]]], "Metric": [[["F1_score"]]], "Task": [[["Cross-Lingual_Bitext_Mining"]]]}, {"incident_type": "SciREX_incident", "Material": [[["MLDoc_Zero-Shot_English-to-French"]]], "Method": [[["Massively_Multilingual_Sentence_Embeddings"]]], "Metric": [[["Accuracy"]]], "Task": [[["Cross-Lingual_Document_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["MLDoc_Zero-Shot_English-to-German"]]], "Method": [[["Massively_Multilingual_Sentence_Embeddings"]]], "Metric": [[["Accuracy"]]], "Task": [[["Cross-Lingual_Document_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["MLDoc_Zero-Shot_English-to-Spanish"]]], "Method": [[["Massively_Multilingual_Sentence_Embeddings"]]], "Metric": [[["Accuracy"]]], "Task": [[["Cross-Lingual_Document_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["XNLI_Zero-Shot_English-to-French"]]], "Method": [[["BiLSTM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Cross-Lingual_Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["XNLI_Zero-Shot_English-to-German"]]], "Method": [[["BiLSTM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Cross-Lingual_Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["XNLI_Zero-Shot_English-to-Spanish"]]], "Method": [[["BiLSTM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Cross-Lingual_Natural_Language_Inference"]]]}]}
{"docid": "TST3-SREX-0020", "doctext": "document : Image Super - Resolution via Dual - State Recurrent Networks Advances in image super - resolution ( SR ) have recently benefited significantly from rapid developments in deep neural networks . Inspired by these recent discoveries , we note that many state - of - the - art deep SR architectures can be reformulated as a single - state recurrent neural network ( RNN ) with finite unfoldings . In this paper , we explore new structures for SR based on this compact RNN view , leading us to a dual - state design , the Dual - State Recurrent Network ( DSRN ) . Compared to its single - state counterparts that operate at a fixed spatial resolution , DSRN exploits both low - resolution ( LR ) and high - resolution ( HR ) signals jointly . Recurrent signals are exchanged between these states in both directions ( both LR to HR and HR to LR ) via delayed feedback . Extensive quantitative and qualitative evaluations on benchmark datasets and on a recent challenge demonstrate that the proposed DSRN performs favorably against state - of - the - art algorithms in terms of both memory consumption and predictive accuracy . section : Introduction In the problem of single - image super - resolution ( SR ) , the aim is to recover a high - resolution ( HR ) image from a single low - resolution ( LR ) image . In recent years , SR performance has been significantly improved due to rapid developments in deep neural networks ( DNNs ) . Specifically , convolutional neural networks ( CNNs ) and residual learning have been widely applied in much recent SR work . In these approaches , two principles have been consistently observed . The first is that increasing the depth of a CNN model improves SR performance ; a deeper model with more parameters can represent a more complex mapping from LR to HR images . In addition , increasing network depth enlarges the size of receptive fields , providing more contextual information that can be exploited to reconstruct missing HR components . The second principle is that adding residual connections ( globally , locally or jointly ) prevents the problems of vanishing and exploding gradients , facilitating the training of deep models . While these recent models have demonstrated promising results , there are also drawbacks . One major issue is that increasing the depth of models by adding new layers introduces more parameters , and thus raises the likelihood of model overfitting . At the same time , larger models demand more storage space , which is a hurdle to deployment in resource - constrained environments ( e.g. mobile systems ) . To resolve this issue , the Deep Recursive Residual Network ( DRRN ) inspired by the Deeply - Recursive Convolutional Network ( DRCN ) shares weights across different residual units and achieves state - of - the - art performance with a small number of parameters . Separate efforts in neural architectural design have recently shown that commonly - used deep structures can be represented more compactly using recurrent neural networks ( RNNs ) . Specifically , Liao and Poggio demonstrated that a weight - sharing Residual Neural Network ( ResNet ) is equivalent to a shallow RNN . Inspired by their findings , we first explore the connections between the neural architectures of existing SR algorithms and their compact RNN formulations . We note that previous SR models with recursive computation and weight sharing , including DRRN and DRCN , work at a single spatial resolution ( bicubic interpolation is first applied to upscale LR images to a desired spatial resolution ) . This enables their model structures to be represented as a unified single - state RNN . Thus , both DRRN and DRCN can be viewed as a finite unfolding in time of the same RNN structure , but with different transition functions . This is illustrated in Figure [ reference ] , and will be discussed in detail in Section [ reference ] . It is worth mentioning that we follow the terminology used in , where a \u2018 \u2018 state \u2019 \u2019 can be considered as corresponding to a \u2018 \u2018 layer \u2019 \u2019 in the normal RNN setting . Based on this compact RNN view of state - of - the - art SR models , in this paper we explore new structures to extend the frontier of SR . The first approach in improving a conventional RNN model is generally to make it multi - layer . We apply this experience in designing the SR architecture in our compact RNN view by adding an additional state , rendering our model a Dual - State Recurrent Network ( DSRN ) , where the two states operate at different spatial resolutions . Specifically , the bottom state captures information at LR , while the top state operates in the HR regime . As with a conventional two - layer stacked RNN , there is a connection from the bottom to the top state via deconvolutional operations . This provides information flow from LR to HR at every single unrolling time . In addition , to allow information flow from previously predicted HR features to LR features , we incorporate a delayed feedback mechanism from the top ( HR ) state to the bottom one . The overall structure of the proposed DSRN is shown in Figure [ reference ] , which not only utilizes parameters efficiently but also allows both LR and HR signals to contribute jointly to learning the mappings . To demonstrate the effectiveness of the proposed method , we compare DSRN with other recent image SR approaches on four common benchmarks as well as on the DIV2 K dataset from the \" New Trends in Image Restoration and Enhancement workshop and challenge on image super - resolution ( NTIRE SR 2017 ) \" . Extensive experimental results validate that DSRN delivers higher parameter efficiency , low memory consumption and high restoration accuracy . section : Related Work Single image SR has been widely studied in the past few decades and has an extensive literature . In recent years , due to the fast development of deep learning , significant progress has been made in this field . Dong et al . first exploited a fully convolutional neural network , termed SRCNN , to predict the nonlinear LR - HR mapping . It demonstrated superior performance to many other example - based learning paradigms , such as nearest neighbor , sparse representation , neighborhood embedding , random forest , etc . Although all layers of a SRCNN are trained jointly in an end - to - end fashion , conceptually the network is split into three stages : patch representation , non - linear mapping , and reconstruction . Much of the later work follows a similar network design with more complicated building blocks or advanced optimization techniques . Wang et al . proposed a sparse coding network ( SCN ) that encodes a sparse representation prior for image SR and can be trained end - to - end , demonstrating the benefit of domain expertise in sparse coding for image SR . Both external and self examples were utilized to synthesize the HR prediction via a neural network in . Inspired by the success of very deep models on ImageNet challenges , Kim et al . proposed a very deep CNN , VDSR , which stacks 20 convolutional layers with kernels . Both residual learning and adjustable gradient clipping are used to prevent vanishing and exploding gradients . However , as the model gets deeper , the number of parameters increases . To control the size of the model , DRCN introduces 16 recursive layers , each with the same structure and shared parameters . Moreover , DRCN makes use of skip connections and recursive supervision to mitigate the difficulty of training . Tai et al . discovered that many residual SR learning algorithms are based on either global residual learning or local residual learning , which are insufficient for very deep models . Instead , they proposed the DRRN that applies both global and local learning while remaining parameter efficient via recursive learning . More recently , Tong et al . proposed making use of Densely Connected Networks ( DenseNet ) instead of ResNet as the building block for image SR . They demonstrated that the DenseNet structure is better at combining features at different levels , which boosts SR performance . Apart from deep models working on bicubic upscaled input images , Shi et al . used a compact network model to conduct convolutions on LR images directly and learned upscaling filters in the last layer , which considerably reduces the computation cost . Similarly , Dong et al . adopted deconvolution layers to accelerate SRCNN in combination with smaller filter sizes and more convolution layers . However , these networks are relatively small and have difficulty capturing complicated mappings owing to limited network capacity . The Laplacian Pyramid Super - Resolution Network ( LapSRN ) works on LR images directly and progressively predicts sub - band residuals on various scales . Lim et al . proposed the Enhanced Deep Super - Resolution ( EDSR ) network and a multi - scale variant , which learns different scaled mapping functions in parallel via weight sharing . It is noteworthy that most SR algorithms minimize the mean squared reconstruction error ( i.e. via loss ) . They often suffer from regression - to - the - mean due to the ill - posed nature of single image SR , resulting in blurry predictions and poor subjective scores . To overcome this drawback , Generative Adversarial Networks have been used along with perceptual loss for SR . Subjective evaluation by mean - opinion - score showed huge improvement over other regression - based methods . Our work is also strongly related to and built upon the idea of viewing a ResNet as an unrolled RNN . It was first proposed in , which aids understanding of a family of deep structures from the perspective of RNNs . Later , Chen et al . unified several different residual functions to provide a better understanding of the design of DNNs with high learning capacity . Recently , the equivalence to RNNs has been further extended to DenseNet . Based on this finding , Dual Path Networks were proposed and showed superior performance to DenseNet and ResNet in a varity of applications . section : Single - State Recurrent Networks In this section , we first revisit the discovery that a ResNet with shared weights can be reformulated as a recurrent system . Then , based on this view , we unite the recent development of SR models with such RNN reformulations to show DRCN and DRRN are structurally equivalent to an unrolled single - state RNN . To establish the equivalence , we adopt the commonly used definition of a RNN , which is characterized by a set of states and transition functions among the states . A RNN often consists of the input state , output state , and the recurrent states . Depending on the number recurrent states , we describe RNNs as \u2018 \u2018 single - state \u2019 \u2019 ( i.e. one recurrent state ) or \u2018 \u2018 dual - state \u2019 \u2019 ( i.e. two recurrent states ) . An illustration of a single - state RNN is shown in Figure [ reference ] ( a ) . The input , output , and recurrent states are represented as , and respectively . The arrow link indicates the state transition function . The square on the directed cycle indicates that the recurrent function travels one time step forward during the unfolding . Interested readers are referred to for detailed information on this general formulation of a RNN . Based on Figure [ reference ] ( a ) , we unfold along the temporal direction to a fixed length . The unfolded graph is shown in figure [ reference ] ( b ) , and the dynamics of a single - state RNN can be characterized by : where the upper script indicates the - th unrolling . The parameters of , , and are often time - independent , which means these parameters are reused at every unfolding step . This allows us to unify ResNet , DRCN , and DRRN as unrolled networks with the same recurrent structure but with the different realizations of and different rules of parameter sharing . ResNet : We consider a ResNet in its simplest form without any down - sampling or up - sampling operations . In other words , both of the spatial dimensions and feature dimensions remain the same across all intermediate layers . To render Figure [ reference ] ( b ) equivalent to a ResNet with residual blocks , one possible technique is to make : be the input image or a function of . , and . Thus , the state transition becomes . The recurrent function be the same as a conventional residual block , which contains two convolutional layers with skip connections as shown in Figure [ reference ] ( c ) . Differences in color indicate different sets of parameters . The prediction state be calculated only at the time as the final output . It is worth mentioning that the only difference between an unrolled RNN following the above definitions and a conventional ResNet is that the parameters in need to be reused among all residual blocks . DRCN : To realize the DRCN expressible by the same single - state RNN , we define and in the same way as for the ResNet . Since DRCN recursively applies only a single convolutional layer to the input feature map 16 times , with the parameters of the layer reused across the whole network , we could use a single convolutional layer to express . The graph is illustrated in Figure [ reference ] ( d ) . Moreover , unlike the ResNet where the output is predicted only at the end of unfolding , DRCN utilizes recursive supervision , which generates an output at every unfolding . The final HR prediction of DRCN is the weighted sum of the outputs at every unfolding . DRRN : The recurrent structure of DRRN differs only slightly from a ResNet . In a ResNet , the skip connection comes from the previous residual block , whereas in a DRRN the skip connection always comes from the first unrolled state . Figure [ reference ] ( e ) shows the equivalent recurrent function for a DRRN with one recursive block ( i.e. ) using the definition in the original paper . section : Dual - State Recurrent Networks Drawing on the connections between state - of - the - art SR models and RNNs , we have investigated new compact RNN architectures for image SR . Specifically , we propose a dual - state design , which adopts two recurrent states enable use of features from both LR and HR spaces . The RNN view of our DSRN is shown in Figure [ reference ] ( a ) and is introduced as follows . Dual - state design : Unlike single - state models working at the same spatial resolution , DSRN incorporates information from both the LR and HR spaces . Specifically , and in Figure [ reference ] ( a ) indicate the LR state and HR state , respectively . Four colored arrows indicate the transition functions between these two states . The blue ( ) , orange ( ) and yellow ( ) links exist in a conventional two - layer RNN , providing information flow from LR to LR , HR to HR , and LR to HR , respectively . To further enable two - way information flows between and , we add the green link , which is inspired by the delayed feedback mechanism of traditional multi - layer RNNs . Here , it introduces a delayed HR to LR connection . The overall dynamics of our DSRN is given as : Figure [ reference ] ( b ) demonstrates the same concept via an unfolded graph , where the top row represents HR state while the bottom one is LR . This design choice encourages feature specialization for different resolutions and information sharing across different resolutions . Transition functions : Our model is characterized by six transition functions . , , , and as illustrated in Figure [ reference ] ( b ) . Specifically , we use the standard residual block for both self - transitions . A single convolutional layer is used for the down - sampling transition and a single transposed convolutional ( or deconvolutional ) layer is used for the up - sampling transition . The strides in both inter - state layers are set to be the same as the SR upscaling factor . Unfolding details : Similarly to unfolding a single - state RNN to obtain a ResNet , for image SR , we let have no contribution to calculating the state transition . In other words , for any choice of ( e.g. choose ) . Furthermore , we set as the output of two convolutional layers with skip connections , which takes the LR input image and transform it into a desired feature space . In addition , is set to zero . Finally , we use deep supervision for the HR prediction , as discussed below . Deep supervision : The unrolled DSRN is capable of making a prediction at every time step . Denote as a prediction at the unfolding , where is characterized by a single convolutional layer . Then , instead of taking the prediction only at the final unfolding , we average all the predictions as Thus , every unrolled layer directly connects to the loss layer to facilitate the training of such a very deep network . Moreover , the model predicts the residual image and minimizes the following mean square error where is the group - truth image in HR and is the residual map between the ground truth and bicubic upsampled LR image . section : Experiments In this section , we first provide implementation details , including both model hyper - parameters and training data augmentation . Then we analyze a number of design choices and their contributions to final performance . Finally , we compare DSRN to other state - of - the - art methods on several benchmark datasets . subsection : Datasets To evaluate the proposed DSRN algorithm , we train our model using 91 images proposed in and test on the following datasets : Set5 , Set14 , B100 and Urban100 . The training data is augmented in a similar way to previous methods , which includes 1 ) random flipping along the vertical or horizontal axis ; 2 ) random rotation by 90 , 180 or 270 ; and 3 ) random scaling by a factor from [ 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1 ] . Tensorflow is used for our full data processing pipeline ; the LR training images are generated by the built - in bicubic down - sampling function . We additionally test our algorithm on the DIV2 K dataset of the NTIRE SR 2017 challenge , where we use the provided training and validation sets with all of the aforementioned data augmentations except random scaling . subsection : Implementation Details We use our model to super - resolve only the luminance channel of images , and use bicubic interpolation to upscale the other two color channels , following . We train independent models for each scale ( 2 , 3 , and 4 ) with 64 filters on the first input convolutional layer and 128 filters in the rest of the network . All layers use convolution filters . Due to our dual - state design , the feature maps of and in each time step have the same spatial dimensions as the LR and HR images , respectively . We zero - pad the boundaries of feature maps to ensure the spatial size of each feature map is the same as the input size after the convolution is applied . All the weights in the network are initialized with a uniform distribution using the method proposed in . We use standard stochastic gradient descent ( SGD ) with momentum 0.95 as our optimizer to minimize the MSE loss function in Equation ( [ reference ] ) . We search for the best initial learning rate from and reduce it by a factor of 10 three times during the entire training process . This learning rate annealing is driven by observing that the loss on the validation set stops decreasing . Gradient clipping at is adopted during training to prevent the gradient explosion . We sample image patches with a size of and use a mini - batch size of to train our network . We observe that the recursion defined in Equation ( [ reference ] ) may lead to an exponential increase in the scale of feature values , especially when is large . In , the authors proposed the use of unshared batch normalization at every unfolding time to resolve this issue . Batch normalization is not used in our network ; we found that normalizing the scale with two scalar parameters was sufficient . Specifically , we use one unshared PReLU activation for each recurrent state after every unrolling step . All other layers have ordinary ReLU as the activation function . Ours Others .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 .24 subsection : Model Analysis In this section , we analyze our proposed model in the following respects : Unrolling length : The unrolling length changes the maximum effective depth of the unrolled network . In particular , for a DSRN with times unrolling , the maximum number of convolution layers between input and output of the network is . The multiplier comes from the two layers in a residual block , while the extra 4 is from the auxiliary input and output layers . However , the number of model parameters remains independent of the length of unrolling . Essentially , controls the trade - off between model capacity and computation cost . We study the influence of by training the model with different unrolling lengths . The empirical results are shown in Figure [ reference ] . The test performance increases when the number of unfolding steps increases , but the benefit seems to diminish after . Unless otherwise mentioned , we use for all our models . It is worth mentioning that we also experimented with stochastic depth by randomly sampling during training , but we observed no improvement in validation accuracy . Parameter sharing : We empirically find parameter sharing to be crucial for training a deep recursive model . As shown in Table [ reference ] , the same model with untied weights performs much more poorly than its weight - sharing counterpart . Specifically , we observe around 0.2dB performance drop across all three upscaling scales when changing from shared weights to untied weights . We speculate that the model with untied weights suffers a larger risk of model over - fitting and much slower training convergence , both of which diminish the model \u2019s restoration accuracy . Dual - state and delayed feedback : We compare our DSRN with two baselines under the same unrolling time steps to understand how each module of our model contributes to the final performance : 1 ) a single - state RNN unrolled ResNet ; and 2 ) a dual - state RNN without delayed feedback connections . The quantitative comparison on the NTIRE SR 2017 challenge is shown in Table [ reference ] . Comparing the single - state baseline and the DSRN without feedback , it is clear that considering information from both LR and HR spaces as two separated states provides performance gains . In addition , comparing our models with and without feedback , we realize that incorporating such an information flow from HR space back to LR space consistently improves performance on all three different scales . In all , both the dual - state and delayed feedback designs are beneficial to our model . State visualization Since DSRN has independent scaling parameters on each unrolled state , the model implicitly learns a weighted - average of all the unrolled states for the final prediction . Empirically we observe that this strategy performs better than output from the last state only . To demonstrate how the network aggregates different unrolled states , we show feature response maps at different unrolling steps in Figure [ reference ] , demonstrating that the network distributes slightly different features to each unrolled state . subsection : Comparison with the State - of - the - Art We provide results of evaluation of our model on several public benchmark datasets in Table [ reference ] , with three commonly - used evaluation metrics : Peak Signal - to - Noise Ratio ( PSNR ) , Structural SIMilarity ( SSIM ) and the Information Fidelity Criterion ( IFC ) . Specifically , we perform a comprehensive comparison between our method and 10 other existing SR algorithms , including both deep learning and non - deep - learning based methods . Note that many recent deep learning based competitors , including VDSR , LapSRN and DRRN , use 291 training samples with the additional 200 from the training set of Berkeley Segmentation Dataset , while our model was trained on only the 91 images . Still , our DSRN method achieves competitive performance across all datasets and scales . It achieves particularly strong performance in the and settings . In addition , we report quantitative evaluations on the recently developed DIV2 K dataset and comparisons with top - ranking algorithms in Table [ reference ] . Our method achieves competitive performance with the best algorithm , EDSR + , and outperforms all the other algorithms by a large margin , which demonstrates the effectiveness of our proposed dual - state recurrent structure . To further analyze the proposed DSRN against other state - of - the - art SR approaches in a qualitative manner , in Figure [ reference ] we present several visual examples of super - resolved images on Set14 with upscaling among different SR approaches . For these competing methods , we use SR results publicly released by the authors . As shown in Figure [ reference ] , our method can construct sharp and detailed structures and is less prone to generating spurious artifacts . Furthermore , the proposed DSRN benefits from inherent parameter sharing and therefore obtains higher parameter efficiency compared to other methods . In Figure [ reference ] , we illustrate the parameters - to - PSNR relationship of our model and several state - of - the - art methods , including SRCNN , VDSR , DRCN , DRRN and RED30 . Our method represents a favorable trade - off between model size and SR performance , and has modest inference time . The DSRN takes 0.4s on the x4 task with a 288x288 output image size , on an NVIDIA Titan X GPU . section : Conclusion In this work , we have provided a unique formulation that expresses many state - of - the - art SR models as a finite unfolding of a single - state RNN with various recurrent functions . Based on this , we extend existing methods by considering a dual - state design ; the two hidden states of our proposed DSRN operate at different spatial resolutions . One captures the LR information while the other one targets the HR domains . To ensure two - way communication between states , we integrate a delayed feedback mechanism . Thus , the predicted features from both LR and HR states can be exploited jointly for final predictions . Extensive experiments on benchmark datasets have demonstrated that the proposed DSRN performs favorably against state - of - the - art SR models in terms of both efficiency and accuracy . For the future work , we will explore use of our proposed DSRN to capture temporal dependencies for video SR . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["BSD100_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["BSD100_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["SSIM"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set14_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set14_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["SSIM"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set5_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set5_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["SSIM"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Urban100_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Urban100_-_4x_upscaling"]]], "Method": [[["DSRN"]]], "Metric": [[["SSIM"]]], "Task": [[["Image_Super-Resolution"]]]}]}
{"docid": "TST3-SREX-0021", "doctext": "document : Natural Language Inference over Interaction Space Natural Language Inference ( NLI ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis . We introduce Interactive Inference Network ( IIN ) , a novel class of neural network architectures that is able to achieve high - level understanding of the sentence pair by hierarchically extracting semantic features from interaction space . We show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a denser interaction tensor contains richer semantic information . One instance of such architecture , Densely Interactive Inference Network ( DIIN ) , demonstrates the state - of - the - art performance on large scale NLI copora and large - scale NLI alike corpus . It \u2019s noteworthy that DIIN achieve a greater than 20 % error reduction on the challenging Multi - Genre NLI ( MultiNLI ; ) dataset with respect to the strongest published system . section : Introduction Natural Language Inference ( NLI also known as recognizing textual entiailment , or RTE ) task requires one to determine whether the logical relationship between two sentences is among entailment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither entailment nor contradiction ) . NLI is known as a fundamental and yet challenging task for natural language understanding , not only because it requires one to identify the language pattern , but also to understand certain common sense knowledge . In Table [ reference ] , three samples from MultiNLI corpus show solving the task requires one to handle the full complexity of lexical and compositional semantics . The previous work on NLI ( or RTE ) has extensively researched on conventional approaches . Recent progress on NLI is enabled by the availability of 570k human annotated dataset and the advancement of representation learning technique . Among the core representation learning techniques , attention mechanism is broadly applied in many NLU tasks since its introduction : machine translation , abstractive summarization , Reading Comprehension , dialog system , etc . As described by , \u201c An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors . The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key \u201d . Attention mechanism is known for its alignment between representations , focusing one part of representation over another , and modeling the dependency regardless of sequence length . Observing attention \u2019s powerful capability , we hypothesize that the attention weight can assist with machine to understanding the text . A regular attention weight , the core component of the attention mechanism , encodes the cross - sentence word relationship into a alignment matrix . However , a multi - head attention weight can encode such interaction into multiple alignment matrices , which shows a more powerful alignment . In this work , we push the multi - head attention to a extreme by building a word - by - word dimension - wise alignment tensor which we call interaction tensor . The interaction tensor encodes the high - order alignment relationship between sentences pair . Our experiments demonstrate that by capturing the rich semantic features in the interaction tensor , we are able to solve natural language inference task well , especially in cases with paraphrase , antonyms and overlapping words . We dub the general framework as Interactive Inference Network ( IIN ) . To the best of our knowledge , it is the first attempt to solve natural language inference task in the interaction space . We further explore one instance of Interactive Inference Network , Densely Interactive Inference Network ( DIIN ) , which achieves new state - of - the - art performance on both SNLI and MultiNLI copora . To test the generality of the architecture , we interpret the paraphrase identification task as natural language inference task where matching as entailment , not - matching as neutral . We test the model on Quora Question Pair dataset , which contains over 400k real world question pair , and achieves new state - of - the - art performance . We introduce the related work in Section 2 , and discuss the general framework of IIN along with a specific instance that enjoys state - of - the - art performance on multiple datasets in Section 3 . We describe experiments and analysis in Section 4 . Finally , we conclude and discuss future work in Section 5 . section : Related Work The early exploration on NLI mainly rely on conventional methods and small scale datasets . The availability of SNLI dataset with 570k human annotated sentence pairs has enabled a good deal of progress on natural language understanding . The essential representation learning techniques for NLU such as attention , memory and the use of parse structure are studied on the SNLI which serves as an important benchmark for sentence understanding . The models trained on NLI task can be divided into two categories : ( i ) sentence encoding - based model which aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation along with their absolute element - wise difference and element - wise product . ( ii ) Joint feature models which use the cross sentence feature or attention from one sentence to another . After neural attention mechanism is successfully applied on the machine translation task , such technique has became widely used in both natural language process and computer vision domains . Many variants of attention technique such as hard - attention , self - attention , multi - hop attention , bidirectional attention and multi - head attention are also introduced to tackle more complicated tasks . Before this work , neural attention mechanism is mainly used to make alignment , focusing on specific part of the representation . In this work , we want to show that attention weight contains rich semantic information required for understanding the logical relationship between sentence pair . Though RNN or LSTM are very good for variable length sequence modeling , using Convolutional neural network in NLU tasks is very desirable because of its parallelism in computation . Convolutional structure has been successfully applied in various domain such as machine translation , sentence classification , text matching and sentiment analysis , etc . The convolution structure is also applied on different level of granularity such as byte , character , word and sentences levels . section : Model subsection : Interactive Inference Network The Interactive Inference Network ( IIN ) is a hierarchical multi - stage process and consists of five components . Each of the components is compatible with different type of implementations . Potentially all exiting approaches in machine learning , such as decision tree , support vector machine and neural network approach , can be transfer to replace certain component in this architecture . We focus on neural network approaches below . Figure [ reference ] provides a visual illustration of Interactive Inference Network . Embedding Layer converts each word or phrase to a vector representation and construct the representation matrix for sentences . In embedding layer , a model can map tokens to vectors with the pre - trained word representation such as GloVe , word2Vec and fasttext . It can also utilize the pre - processing tool , e.g. named entity recognizer , part - of - speech recognizer , lexical parser and coreference identifier etc . , to incorporate more lexical and syntactical information into the feature vector . Encoding Layer encodes the representations by incorporating the context information or enriching the representation with desirable features for future use . For instance , a model can adopt bidirectional recurrent neural network to model the temporal interaction on both direction , recursive neural network ( also known as TreeRNN ) to model the compositionality and the recursive structure of language , or self - attention to model the long - term dependency on sentence . Different components of encoder can be combined to obtain a better sentence matrix representation . Interaction Layer creates an word - by - word interaction tensor by both premise and hypothesis representation matrix . The interaction can be modeled in different ways . A common approach is to compute the cosine similarity or dot product between each pair of feature vector . On the other hand , a high - order interaction tensor can be constructed with the outer product between two matrix representations . Feature Extraction Layer adopts feature extractor to extract the semantic feature from interaction tensor . The convolutional feature extractors , such as AlexNet , VGG , Inception , ResNet and DenseNet , proven work well on image recognition are completely compatible under such architecture . Unlike the work who employs 1 - D sliding window , our CNN architecture allows 2 - D kernel to extract semantic interaction feature from the word - by - word interaction between n - gram pair . Sequential or tree - like feature extractors are also applicable in the feature extraction layer . Output Layer decodes the acquired features to give prediction . Under the setting of NLI , the output layer predicts the confidence on each class . subsection : Densely Interactive Inference Network Here we introduce Densely Interactive Inference Network ( DIIN ) , which is a relatively simple instantiation of IIN but produces state - of - the - art performance on multiple datasets . paragraph : Embedding Layer : For DIIN , we use the concatenation of word embedding , character feature and syntactical features . The word embedding is obtained by mapping token to high dimensional vector space by pre - trained word vector ( 840B GloVe ) . The word embedding is updated during training . As in , we filter character embedding with 1D convolution kernel . The character convolutional feature maps are then max pooled over time dimension for each token to obtain a vector . The character features supplies extra information for some out - of - vocabulary ( OOV ) words . Syntactical features include one - hot part - of - speech ( POS ) tagging feature and binary exact match ( EM ) feature . The EM value is activated if there are tokens with same stem or lemma in the other sentence as the corresponding token . The EM feature is simple while found useful as in reading comprehension task . In analysis section , we study how EM feature helps text understanding . Now we have premise representation and hypothesis representation , where refers to the sequence length of premise , refers to the sequence length of hypothesis and means the dimension of both representation . The 1 - D convolutional neural network and character features weights share the same set of parameters between premise and hypothesis . paragraph : Encoding Layer : In the encoding layer , the premise representation and the hypothesis representation are passed through a two - layer highway network , thus having and for new premise representation and new hypothesis representation . These new representation are then passed to self - attention layer to take into account the word order and context information . Take premise as example , we model self - attention by where is a weighted summation of . We choose , where is a trainable weight , is element - wise multiplication , [ ; ] is vector concatenation across row , and the implicit multiplication is matrix multiplication . Then both and are fed into a semantic composite fuse gate ( fuse gate in short ) , which acts as a skip connection . The fuse gate is implemented as where , , and , are trainable weights , is sigmoid nonlinear operation . We do the same operation on hypothesis representation , thus having . The weights of intra - attention and fuse gate for premise and hypothesis are not shared , but the difference between the weights of are penalized . The penalization aims to ensure the parallel structure learns the similar functionality but is aware of the subtle semantic difference between premise and hypothesis . paragraph : Interaction Layer : The interaction layer models the interaction between premise encoded representation and hypothesis encoded representation as follows : where is the - th row vector of , and is the - th row vector of . Though there are many implementations of interaction , we find very useful . paragraph : Feature Extraction Layer : We adopt DenseNet as convolutional feature extractor in DIIN . Though our experiments show ResNet works well in the architecture , we choose DenseNet because it is effective in saving parameters . One interesting observation with ResNet is that if we remove the skip connection in residual structure , the model does not converge at all . We found batch normalization delays convergence without contributing to accuracy , therefore we does not use it in our case . A ReLU activation function is applied after all convolution unless otherwise noted . Once we have the interaction tensor , we use a convolution with kernel to scale down the tensor in a ratio , , without following ReLU . If the input channel is then the output channel is . Then the generated feature map is feed into three sets of Dense block and transition block pair . The DenseNet block contains n layers of convolution layer with growth rate of g . The transition layer has a convolution layer with kernel for scaling down purpose , followed by a max pooling layer with stride . The transition scale down ratio in transition layer is . paragraph : Output Layer : DIIN uses a linear layer to classify final flattened feature representation to three classes . section : Experiments In this section , we present the evaluation of our model . We first perform quantitative evaluation , comparing our model with other competitive models . We then conduct some qualitative analyses to understand how DIIN achieve the high level understanding through interaction . subsection : Data Here we introduce three datasets we evaluate our model on . The evaluation metric for all dataset is accuracy . paragraph : SNLI Stanford Natural Language Inference ( SNLI ; ) has 570k human annotated sentence pairs . The premise data is draw from the captions of the Flickr30k corpus , and the hypothesis data is manually composed . The labels provided in are \u201c entailment \u201d , \u201c neutral \u2019 , \u201c contradiction \u201d and \u201c - \u201d . \u201c - \u201d shows that annotators can not reach consensus with each other , thus removed during training and testing as in other works . We use the same data split as in . paragraph : MultiNLI Multi - Genre NLI Corpus ( MultiNLI ; ) has 433k sentence pairs , whose collection process and task detail are modeled closely to SNLI . The premise data is collected from maximally broad range of genre of American English such as written non - fiction genres ( SLATE , OUP , GOVERNMENT , VERBATIM , TRAVEL ) , spoken genres ( TELEPHONE , FACE - TO - FACE ) , less formal written genres ( FICTION , LETTERS ) and a specialized one for 9 / 11 . Half of these selected genres appear in training set while the rest are not , creating in - domain ( matched ) and cross - domain ( mismatched ) development / test sets . We use the same data split as provided by . Since test set labels are not provided , the test performance is obtained through submission on Kaggle.com . Each team is limited to two submissions per day . paragraph : Quora question pair Quora question pair dataset contains over 400k real world question pair selected from Quora.com . A binary annotation which stands for match ( duplicate ) or not match ( not duplicate ) is provided for each question pair . In our case , duplicate question pair can be interpreted as entailment relation and not duplicate as neutral . We use the same split ratio as mentioned in . subsection : Experiments setting We implement our algorithm with Tensorflow framework . An Adadelta optimizer with as 0.95 and as is used to optimize all the trainable weights . The initial learning rate is set to 0.5 and batch size to 70 . When the model does not improve best in - domain performance for 30 , 000 steps , an SGD optimizer with learning rate of is used to help model to find a better local optimum . Dropout layers are applied before all linear layers and after word - embedding layer . We use an exponential decayed keep rate during training , where the initial keep rate is 1.0 and the decay rate is 0.977 for every 10 , 000 step . We initialize our word embeddings with pre - trained 300D GloVe 840B vectors while the out - of - vocabulary word are randomly initialized with uniform distribution . The character embeddings are randomly initialized with 100D. We crop or pad each token to have 16 characters . The 1D convolution kernel size for character embedding is 5 . All weights are constraint by L2 regularization , and the L2 regularization at step is calculated as follows : where determines the maximum L2 regularization ratio , and determines at which step the maximum L2 regularization ratio would be applied on the L2 regularization . We choose as and as 100 , 000 . The ratio of L2 penalty between the difference of two encoder weights is set to . For a dense block in feature extraction layer , the number of layer is set to and growth rate g is set to . The first scale down ratio in feature extraction layer is set to and transitional scale down ratio is set to . The sequence length is set as a hard cutoff on all experiments : 48 for MultiNLI , 32 for SNLI and 24 for Quora Question Pair Dataset . During the experiments on MultiNLI , we use 15 % of data from SNLI as in . We select the parameter by the best run of development accuracy . Our ensembling approach considers the majority vote of the predictions given by multiple runs of the same model under different random parameter initialization . subsection : Experiment on MultiNLI We compare our result with all other published systems in Table [ reference ] . Besides ESIM , the state - of - the - art model on SNLI , all other models appear at RepEval 2017 workshop . RepEval 2017 workshop requires all submitted model to be sentence encoding - based model therefore alignment between sentences and memory module are not eligible for competition . All models except ours share one common feature that they use LSTM as a essential building block as encoder . Our approach , without using any recurrent structure , achieves the new state - of - the - art performance of 80.0 % , exceeding current state - of - the - art performance by more than 5 % . Unlike the observation from , we find the out - of - domain test performance is consistently lower than in - domain test performance . Selecting parameters from the best in - domain development accuracy partially contributes to this result . subsection : Experiment on SNLI In Table [ reference ] , we compare our model to other model performance on SNLI . Experiments ( 2 - 7 ) are sentence encoding based model . provides a BiLSTM baseline . adopts two layer GRU encoder with pre - trained \u201d skip - thoughts \u201d vectors . To capture sentence - level semantics , use tree - based CNN and propose a stack - augmented parser - interpreter neural network ( SPINN ) which incorporates parsing information in a sequential manner . uses intra - attention on top of BiLSTM to generate sentence representation , and proposes an memory augmented neural network to encode the sentence . The next group of model , experiments ( 8 - 18 ) , uses cross sentence feature . aligns each sentence word - by - word with attention on top of LSTMs . enforces cross sentence attention word - by - word matching with the proprosed mLSTM model . proposes long short - term memory - network ( LSTMN ) with deep attention fusion that links the current word to previous word stored in memory . decomposes the task into sub - problems and conquer them respectively . proposes neural tree indexer , a full n - ary tree whose subtrees can be overlapped . Re - read LSTM proposed by considers the attention vector of one sentence as the inner - state of LSTM for another sentence . propose a sequential model that infers locally , and a ensemble with tree - like inference module that further improves performance . We show our model , DIIN , achieves state - of - the - art performance on the competitive leaderboard . subsection : Experiment on Quora Question Pair dataset In this subsection , we evaluate the effectiveness of our model for paraphrase identification as natural language inference task . Other than our baselines , we compare with and . BiMPM models different perspective of matching between sentence pair on both direction , then aggregates matching vector with LSTM . DecAtt\u2062word and DecAtt\u2062char uses automatically collected in - domain paraphrase data to noisy pretrain - gram word embedding and - gram subword embedding correspondingly on decomposable attention model proposed by . In Table [ reference ] , our experiment shows DIIN has better performance than all other models and an ensemble score is higher than the former best result for more than 1 percent . subsection : Analysis paragraph : Ablation Study We conduct a ablation study on our base model to examine the effectiveness of each component . We study our model on MultiNLI dataset and we use Matched validation score as the standard for model selection . The result is shown in Table [ reference ] . We studies how EM feature contributes to the system . After removing the exact match binary feature , we find the performance degrade to 78.2 on matched score on development set and 78.0 on mismatched score . As observed in reading comprehension task , the simple exact match feature does help the model to better understand the sentences . In the experiment 3 , we remove the convolutional feature extractor and then model is structured as a sentence - encoding based model . The sentence representation matrix is max - pooled over time to obtain a feature vector . Once we have the feature vector for premise and for hypothesis , we use as final feature vector to classify the relationship . We obtain 73.2 for matched score and 73.6 on mismatched data . The result is competitive among other sentence - encoding based model . We further study how encoding layer contribute in enriching the feature space in interaction tensor . If we remove encoding layer completely , then we \u2019ll obtain a 73.5 for matched score and 73.2 for mismatched score . The result demonstrate the feature extraction layer have powerful capability to capture the semantic feature . In experiment 5 , we remove both self - attention and fuse gate , thus retaining only highway network . The result improves to 77.7 and 77.3 respectively on matched and mismatched development set . However , in experiment 6 , when we only remove fuse gate , to our surprise , the performance degrade to 73.5 for matched score and 73.8 for mismatched . On the other hand , if we use the addition of the representation after highway network and the representation after self - attention as skip connection as in experiment 7 , the performance increase to 77.3 and 76.3 . The comparison indicates self - attention layer makes the training harder to converge while a skip connection could ease the gradient flow for both highway layer and self - attention layer . By comparing the base model and the model the in experiment 6 , we show that the fuse gate not only well serves as a skip connection , but also makes good decision upon which information the fuse for both representation . To show that dense interaction tensor contains more semantic information , we replace the dense interaction tensor with dot product similarity matrix between the encoded representation of premise and hypothesis . The result shows that the dot product similarity matrix has an inferior capacity of semantic information . Another dimensionality study is provided in supplementary material . In experiment 9 , we share the encoding layer weight , and the result decrease from the baseline . The result shows that the two set of encoding weights learn the subtle difference between premise and hypothesis . paragraph : Error analysis To analyze the model prediction , we use annotated subset of development set provided by that consists of 1 , 000 examples each tagged with zero or more following tags : CONDITIONAL : whether the sentence contains a conditional . WORD OVERLAP : whether both sentences share more than 70 % of their tokens . NEGATION : whether a negation shows up in either sentence . ANTO : whether two sentences contain antonym pair . LONG SENTENCE : whether premise or hypothesis is longer than 30 or 16 tokens respectively . TENSE DIFFERENCE : whether any verb in two sentences uses different tense . ACTIVE / PASSIVE : whether there is an active - to - passive ( or vice versa ) transformation from the premise to the hypothesis . PARAPHRASE : whether the two sentences are close paraphrases QUANTITY / TIME REASONING : whether understanding the pair requires quantity or time reasoning . COREF : Whether the hypothesis contains a pronoun or referring expression that needs to be resolved using the premise . QUANTIFIER : Whether either sentence contains one of the following quantifier : much , enough , more , most , less , least , no , none , some , any , many , few , several , almost , nearly . MODAL : Whether one of the following modal verbs appears in either sentence : can , could , may , might , must , will , would , should . BELIEF : Whether one of the following belief verbs appear in either sentence : know , believe , understand , doubt , think , suppose , recognize , forget , remember , imagine , mean , agree , disagree , deny , promise . For more detailed descriptions , please resort to . The result is shown in Table [ reference ] . We find DIIN is consistently better on sentence pair with WORD OVERLAP , ANTO , LONG SENTENCE , PARAPHRASE and BELIEF tags by a large margin . During investigation , we hypothesize exact match feature helps the model to better understand paraphrase , therefore we study the result from second ablation ablation study where exact match feature is not used . Surprisingly , the model without exact model feature does not work worse on PARAPHRASE , instead , the accuracy on ANTO drops about 10 % . DIIN is also work well on LONG SENTENCE , partially because the receptive field is large enough to cover all tokens . paragraph : Visualization We also visualize the hidden representation from interaction tensor and the feature map from first dense block in Figure [ reference ] . We pick a sentence pair whose premise is \u201c South Carolina has no referendum right , so the Supreme Court canceled the vote and upheld the ban . \u201d and hypothesis is \u201c South Carolina has a referendum right , so the Supreme Court was powerless over the state . \u201d . The upper row of figures are sampled from hidden representation of interaction tensor . We observe the values of neurons are highly correlated row - wise and column - wise in the interaction tensor and different channel of hidden representation shows different aspect of interaction . Though in certain channel same words , \u201c referendum \u201d , or phrases , \u201c supreme court \u201d , cause activation , different word or phrase pair , such as \u201c ban \u201d and \u201c powerless over \u201d , also cause activation in other activation . It shows the model \u2019s strong capacity of understanding text in different perspective . The lower row of Figure [ reference ] shows the feature map from first dense block . After being convolved from the interaction tensor and previous feature map , new feature maps shows activation in different position , demonstrating different semantic features are found . The first figure in the lower row has similar pattern as normal attention weight whereas others has no obvious pattern . Different channels of feature maps indicate different kinds of semantic feature . section : Conclusion and Future Work We show the interaction tensor ( or attention weight ) contains semantic information to understand the natural language . We introduce Interactive Inference Network , a novel class of architecture that allows the model to solve NLI or NLI alike tasks via extracting semantic feature from interaction tensor end - to - end . One instance of such architecture , Densely Interactive Inference Network ( DIIN ) , achieves state - of - the - art performance on multiple datasets . By ablating each component in DIIN and changing the dimensionality , we show the effectiveness of each component in DIIN . Though we have the initial exploration of natural language inference in interaction space , the full potential is not yet clear . We will keep exploring the potential of interaction space . Incorporating common - sense knowledge from external resources such as knowledge base to leverage the capacity of the mode is another research goal of ours . subsubsection : Acknowledgments We thank Yuchen Lu , Chang Huang and Kai Yu for their sincere and insightful advice . bibliography : References appendix : Supplementary Material paragraph : Dimensionality and Parameter number study To study the influence of the model dimension which is also the channel number of interaction tensor , we design experiments to find out whether dimension has influence on performance . We also present the parameter count of these models . The dimensionality is 448 where 300 comes from word embedding , 100 comes from char feature , 47 comes from Part of speech tagging and 1 comes from the binary exact match feature . Since Highway network sets the output dimensionality default as that in input , we design a variant to highway network so that different output size could be obtained . The variant of highway layer is designed as follows : where is the - th vector of input matrix , is the - th vector of output matrix , , , and , , are trainable weights . The result shows that higher dimension number have better performance when the dimension number is lower certain threshold , however , when the number of dimensionality is greater than the threshold , larger number of parameter and higher dimensionality does n\u2019t contribute to performance . In the case of SNLI , due to its simplicity in language pattern , 250D would be suffice to obtain a good performance . On the other hand , it requires 350D to achieve a competitive performance on MultiNLI . We fail to reproduce our best performance with the new structure on MultiNLI . It shows that the additional layer on highway network does n\u2019t helps convergence .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Quora_Question_Pairs"]]], "Method": [[["DIIN"]]], "Metric": [[["Accuracy"]]], "Task": [[["Paraphrase_Identification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["448D_Densely_Interactive_Inference_Network__DIIN__code_"]]], "Metric": [[["__Test_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["448D_Densely_Interactive_Inference_Network__DIIN__code__Ensemble"]]], "Metric": [[["__Test_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["448D_Densely_Interactive_Inference_Network__DIIN__code_"]]], "Metric": [[["__Train_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["448D_Densely_Interactive_Inference_Network__DIIN__code__Ensemble"]]], "Metric": [[["__Train_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["448D_Densely_Interactive_Inference_Network__DIIN__code_"]]], "Metric": [[["Parameters"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["448D_Densely_Interactive_Inference_Network__DIIN__code__Ensemble"]]], "Metric": [[["Parameters"]]], "Task": [[["Natural_Language_Inference"]]]}]}
{"docid": "TST3-SREX-0022", "doctext": "document : Fraternal Dropout Recurrent neural networks ( RNNs ) form an important class of architectures among neural networks useful for language modeling and sequential prediction . However , optimizing RNNs is known to be harder compared to feed - forward neural networks . A number of techniques have been proposed in literature to address this problem . In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal . Specifically , we propose to train two identical copies of an RNN ( that share parameters ) with different dropout masks while minimizing the difference between their ( pre - softmax ) predictions . In this way our regularization encourages the representations of RNNs to be invariant to dropout mask , thus being robust . We show that our regularization term is upper bounded by the expectation - linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout . We evaluate our model and achieve state - of - the - art results in sequence modeling tasks on two benchmark datasets \u2013 Penn Treebank and Wikitext - 2 . We also show that our approach leads to performance improvement by a significant margin in image captioning ( Microsoft COCO ) and semi - supervised ( CIFAR - 10 ) tasks . section : Introduction Recurrent neural networks ( RNNs ) like long short - term memory ( LSTM ; ) networks and gated recurrent unit ( GRU ; ) are popular architectures for sequence modeling tasks like language generation , translation , speech synthesis , and machine comprehension . However , they are harder to optimize compared to feed - forward networks due to challenges like variable length input sequences , repeated application of the same transition operator at each time step , and largely - dense embedding matrix that depends on the vocabulary size . Due to these optimization challenges in RNNs , the application of batch normalization and its variants ( layer normalization , recurrent batch normalization , recurrent normalization propagation ) have not been as successful as their counterparts in feed - forward networks , although they do considerably provide performance gains . Similarly , naive application of dropout has been shown to be ineffective in RNNs . Therefore , regularization techniques for RNNs is an active area of research . To address these challenges , proposed to apply dropout only to the non - recurrent connections in multi - layer RNNs . Variational dropout ( ) uses the same dropout mask throughout a sequence during training . DropConnect applies the dropout operation on the weight matrices . Zoneout ( ) , in a similar spirit with dropout , randomly chooses to use the previous time step hidden state instead of using the current one . Similarly as a substitute for batch normalization , layer normalization normalizes the hidden units within each sample to have zero mean and unit standard deviation . Recurrent batch normalization applies batch normalization but with unshared mini - batch statistics for each time step . and on the other hand show that activity regularization ( AR ) and temporal activation regularization ( TAR ) are also effective methods for regularizing LSTMs . Another more recent way of regularizing RNNs , that is similar in spirit to the approach we take , involves minimizing the difference between the hidden states of the original and the auxiliary network . In this paper we propose a simple regularization based on dropout that we call fraternal dropout , where we minimize an equally weighted sum of prediction losses from two identical copies of the same LSTM with different dropout masks , and add as a regularization the difference between the predictions ( pre - softmax ) of the two networks . We analytically show that our regularization objective is equivalent to minimizing the variance in predictions from different i.i.d . dropout masks ; thus encouraging the predictions to be invariant to dropout masks . We also discuss how our regularization is related to expectation linear dropout , - model and activity regularization , and empirically show that our method provides non - trivial gains over these related methods which we explain furthermore in our ablation study ( Section [ reference ] ) . section : Fraternal dropout Dropout is a powerful regularization for neural networks . It is usually more effective on densely connected layers because they suffer more from overfitting compared with convolution layers where the parameters are shared . For this reason dropout is an important regularization for RNNs . However , dropout has a gap between its training and inference phase since the latter phase assumes linear activations to correct for the factor by which the expected value of each activation would be different . In addition , the prediction of models with dropout generally vary with different dropout mask . However , the desirable property in such cases would be to have final predictions be invariant to dropout masks . As such , the idea behind fraternal dropout is to train a neural network model in a way that encourages the variance in predictions under different dropout masks to be as small as possible . Specifically , consider we have an RNN model denoted by that takes as input , where denotes the model parameters . Let be the prediction of the model for input sample at time , for dropout mask and current input , where is a function of and the hidden states corresponding to the previous time steps . Similarly , let be the corresponding time step loss value for the overall input - target sample pair . Then in fraternal dropout , we simultaneously feed - forward the input sample through two identical copies of the RNN that share the same parameters but with different dropout masks and at each time step . This yields two loss values at each time step given by , and . Then the overall loss function of fraternal dropout is given by , where is the regularization coefficient , is the dimensions of and is the fraternal dropout regularization given by , We use Monte Carlo sampling to approximate where and are the same as the one used to calculate values . Hence , the additional computation is negligible . We note that the regularization term of our objective is equivalent to minimizing the variance in the prediction function with different dropout masks as shown below ( proof in the appendix ) . theorem : . Let sit and sjt be i.i.d . dropout masks and \u2208\u2062pt ( zt , sit;\u03b8 ) Rm be the prediction function as described above . Then , Note that a generalization of our approach would be to minimize the difference between the predictions of the two networks with different data / model augmentations . However , in this paper we focus on using different dropout masks and experiment mainly with RNNs . section : Related Work subsection : Relation to Expectation Linear Dropout ( ELD ) analytically showed that the expected error ( over samples ) between a model \u2019s expected prediction over all dropout masks , and the prediction using the average mask , is upper bounded . Based on this result , they propose to explicitly minimize the difference ( we have adapted their regularization to our notations ) , where is the dropout mask . However , due to feasibility consideration , they instead propose to use the following regularization in practice , Specifically , this is achieved by feed - forwarding the input twice through the network , with and without dropout mask , and minimizing the main network loss ( with dropout ) along with the regularization term specified above ( but without back - propagating the gradients through the network without dropout ) . The goal of is to minimize the network loss along with the expected difference between the prediction from individual dropout mask and the prediction from the expected dropout mask . We note that our regularization objective is upper bounded by the expectation - linear dropout regularization as shown below ( proof in the appendix ) . theorem : . . This result shows that minimizing the ELD objective indirectly minimizes our regularization term . Finally as indicated above , they apply the target loss only on the network with dropout . In fact , in our own ablation studies ( see Section [ reference ] ) we find that back - propagating target loss through the network ( without dropout ) makes optimizing the model harder . However , in our setting , simultaneously back - propagating target loss through both networks yields both performance gain as well as convergence gain . We believe convergence is faster for our regularization because network weights are more likely to get target based updates from back - propagation in our case . This is especially true for weight dropout since in this case dropped weights do not get updated in the training iteration . subsection : Relation to - model propose - model with the goal of improving performance on classification tasks in the semi - supervised setting . They propose a model similar to ours ( considering the equivalent deep feed - forward version of our model ) except they apply target loss only on one of the networks and use time - dependent weighting function ( while we use constant ) . The intuition in their case is to leverage unlabeled data by using them to minimize the difference in prediction between the two copies of the network with different dropout masks . Further , they also test their model in the supervised setting but fail to explain the improvements they obtain by using this regularization . We note that in our case we analytically show that minimizing our regularizer ( also used in - model ) is equivalent to minimizing the variance in the model predictions ( Remark [ reference ] ) . Furthermore , we also show the relation of our regularizer to expectation linear dropout ( Proposition [ reference ] ) . In Section [ reference ] , we study the effects of target based loss on both networks , which is not used in the - model . We find that applying target loss on both the networks leads to significantly faster convergence . Finally , we bring to attention that temporal embedding ( another model proposed by , claimed to be a better version of - model for semi - supervised , learning ) is intractable in natural language processing applications because storing averaged predictions over all of the time steps would be memory exhaustive ( since predictions are usually huge - tens of thousands values ) . On a final note , we argue that in the supervised case , using a time - dependent weighting function instead of a constant value is not needed . Since the ground truth labels are known , we have not observed the problem mentioned by , that the network gets stuck in a degenerate solution when is too large in earlier epochs of training . We note that it is much easier to search for an optimal constant value , which is true in our case , as opposed to tuning the time - dependent function . Similarity to - model makes our method related to other semi - supervised works , mainly and . Since semi - supervised learning is not a primary focus of this paper , we refer to for more details . We note that the idea of adding a penalty encouraging the representation to be similar for two different masks was previously implemented by the authors of a Multi - Prediction Deep Boltzmann Machines . Nevertheless , the idea is not discussed in their paper . Another way to address the gap between the train and evaluation mode of dropout is to perform Monte Carlo sampling of masks and average the predictions during evaluation , and this has been used for feed - forward networks . We find that this technique does not work well for RNNs . The details of these experiments can be found in the appendix . section : Experiments subsection : Language Models In the case of language modeling we test our model on two benchmark datasets \u2013 Penn Tree - bank ( PTB ) dataset and WikiText - 2 ( WT2 ) dataset . We use preprocessing as specified by ( for PTB corpus ) and Moses tokenizer ( for the WT2 dataset ) . For both datasets we use the AWD - LSTM 3 - layer architecture described in which we call the baseline model . The number of parameters in the model used for PTB is 24 million as compared to 34 million in the case of WT2 because WT2 has a larger vocabulary size for which we use a larger embedding matrix . Apart from those differences , the architectures are identical . When we use fraternal dropout , we simply add our regularization on top of this baseline model . Word level Penn Treebank ( PTB ) . Influenced by , our goal here is to make sure that fraternal dropout outperforms existing methods not simply because of extensive hyper - parameter grid search but rather due to its regularization effects . Hence , in our experiments we leave a vast majority of hyper - parameters used in the baseline model unchanged i.e. embedding and hidden states sizes , gradient clipping value , weight decay and the values used for all dropout layers ( dropout on the word vectors , the output between LSTM layers , the output of the final LSTM , and embedding dropout ) . However , a few changes are necessary : the coefficients for AR and TAR needed to be altered because fraternal dropout also affects RNNs activation ( as explained in Subsection [ reference ] ) \u2013 we did not run grid search to obtain the best values but simply deactivated AR and TAR regularizers ; since fraternal dropout needs twice as much memory , batch size is halved so the model needs approximately the same amount of memory and hence fits on the same GPU . The final change in hyper - parameters is to alter the non - monotone interval used in non - monotonically triggered averaged SGD ( NT - ASGD ) optimizer . We run a grid search on and obtain very similar results for the largest values ( 40 , 50 and 60 ) in the candidate set . Hence , our model is trained longer using ordinary SGD optimizer as compared to the baseline model . We evaluate our model using the perplexity metric and compare the results that we obtain against the existing state - of - the - art results . The results are reported in Table [ reference ] . Our approach achieves the state - of - the - art performance compared with existing benchmarks . To confirm that the gains are robust to initialization , we run ten experiments for the baseline model with different seeds ( without fine - tuning ) for PTB dataset to compute confidence intervals . The average best validation perplexity is with the minimum value equals . The same for test perplexity is and , respectively . Our score ( validation and test perplexity ) beats ordinal dropout minimum values . We also perform experiments using fraternal dropout with a grid search on all the hyper - parameters and find that it leads to further improvements in performance . The details of this experiment can be found in section [ reference ] . Word level WikiText - 2 ( WT2 ) . In the case of WikiText - 2 language modeling task , we outperform the current state - of - the - art using the perplexity metric by a significant margin . Due to the lack of computational power , we run a single training procedure for fraternal dropout on WT2 dataset because it is larger than PTB . In this experiment , we use the best hyper - parameters found for PTB dataset ( , non - monotone interval and halved batch size ; the rest of the hyper - parameters are the same as described in for WT2 ) . The final results are presented in Table [ reference ] . subsection : Image captioning We also apply fraternal dropout on an image captioning task . We use the well - known show and tell model as a baseline . We emphasize that in the image captioning task , the image encoder and sentence decoder architectures are usually learned together . Since we want to focus on the benefits of using fraternal dropout in RNNs we use frozen pretrained ResNet - 101 model as our image encoder . It means that our results are not directly comparable with other state - of - the - art methods , however we report results for the original methods so readers can see that our baseline performs well . The final results are presented in Table [ reference ] . We argue that in this task smaller values are optimal because the image captioning encoder is given all information in the beginning and hence the variance of consecutive predictions is smaller that in unconditioned natural language processing tasks . Fraternal dropout may benefits here mainly due to averaging gradients for different mask and hence updating weights more frequently . section : Ablation Studies In this section , the goal is to study existing methods closely related to ours \u2013 expectation linear dropout , - model and activity regularization . All of our experiments for ablation studies , which apply a single layer LSTM , use the same hyper - parameters and model architecture as . subsection : Expectation - linear dropout ( ELD ) The relation with expectation - linear dropout has been discussed in Section [ reference ] . Here we perform experiments to study the difference in performance when using the ELD regularization versus our regularization ( FD ) . In addition to ELD , we also study a modification ( ELDM ) of ELD which applies target loss to both copies of LSTMs in ELD similar to FD ( notice in their case they only have dropout on one LSTM ) . Finally we also evaluate a baseline model without any of these regularizations . The learning dynamics curves are shown in Figure [ reference ] . Our regularization performs better in terms of convergence compared with other methods . In terms of generalization , we find that FD is similar to ELD , but baseline and ELDM are much worse . Interestingly , looking at the train and validation curves together , ELDM seems to be suffering from optimization problems . subsection : - model Since - model is similar to our algorithm ( even though it is designed for semi - supervised learning in feed - forward networks ) , we study the difference in performance with - model both qualitatively and quantitatively to establish the advantage of our approach . First , we run both single layer LSTM and 3 - layer AWD - LSTM on PTB task to check how their model compares with ours in the case of language modeling . The results are shown in Figure [ reference ] and [ reference ] . We find that our model converges significantly faster than - model . We believe this happens because we back - propagate the target loss through both networks ( in contrast to - model ) that leads to weights getting updated using target - based gradients more often . Even though we designed our algorithm specifically to address problems in RNNs , to have a fair comparison , we compare with - model on a semi - supervised task which is their goal . Specifically , we use the CIFAR - 10 dataset that consists of images from 10 classes . Following the usual splits used in semi - supervised learning literature , we use 4 thousand labeled and 41 thousand unlabeled samples for training , 5 thousand labeled samples for validation and 10 thousand labeled samples for test set . We use the original ResNet - 56 architecture . We run grid search on , dropout rates in and leave the rest of the hyper - parameters unchanged . We additionally check importance of using unlabeled data . The results are reported in Table [ reference ] . We find that our algorithm performs at par with - model . When unlabeled data is not used , fraternal dropout provides slightly better results as compared to traditional dropout . subsection : Activity regularization and temporal activity regularization analysis The authors of study the importance of activity regularization ( AR ) and temporal activity regularization ( TAR ) in LSTMs given as , where is the LSTM \u2019s output activation at time step ( hence depends on both current input and the model parameters ) . Notice that AR and TAR regularizations are applied on the output of the LSTM , while our regularization is applied on the pre - softmax output of the LSTM . However , since our regularization can be decomposed as and encapsulates an term along with the dot product term , we perform experiments to confirm that the gains in our approach is not due to the regularization alone . A similar argument goes for the TAR objective . We run a grid search on , , which include the hyper - parameters mentioned in . For our regularization , we use . Furthermore , we also compare with a regularization ( PR ) that regularizes to further rule - out any gains only from regularization . Based on this grid search , we pick the best model on the validation set for all the regularizations , and additionally report a baseline model without any of these four mentioned regularizations . The learning dynamics is shown in Figure [ reference ] . Our regularization performs better both in terms of convergence and generalization compared with other methods . Average hidden state activation is reduced when any of the regularizer described is applied ( see Figure [ reference ] ) . subsection : Improvements using fine - tuning We confirm that models trained with fraternal dropout benefit from the NT - ASGD fine - tuning step ( as also used in ) . However , this is a very time - consuming practice and since different hyper - parameters may be used in this additional part of the learning procedure , the probability of obtaining better results due to the extensive grid search is higher . Hence , in our experiments we use the same fine - tuning procedure as implemented in the official repository ( even fraternal dropout was not used ) . We present the importance of fine - tuning in Table [ reference ] . subsection : Fraternal dropout and expectation linear dropout comparison We perform extensive grid search for the baseline model from Subsection [ reference ] ( an AWD - LSTM 3 - layer architecture ) trained with either fraternal dropout or expectation linear dropout regularizations , to further contrast the performance of these two methods . The experiments are run without fine - tuning on the PTB dataset . In each run , all five dropout rates are randomly altered ( they are set to their original value , as in , multiplied by a value drawn from the uniform distribution on the interval ) and the rest of the hyper - parameters are drawn as shown in Table [ reference ] . As in Subsection [ reference ] , AR and TAR regularizers are deactivated . Together we run more than 400 experiments . The results are presented in Table [ reference ] . Both FD and ELD perform better than the baseline model that instead uses AR and TAR regularizers . Hence , we confirm our previous finding ( see Subsection [ reference ] ) that both FD and ELD are better . However , as found previously for smaller model in Subsection [ reference ] , the convergence of FD is faster than that of ELD . Additionally , fraternal dropout is more robust to different hyper - parameters choice ( more runs performing better than the baseline and better average for top performing runs ) . section : Conclusion In this paper we propose a simple regularization method for RNNs called fraternal dropout that acts as a regularization by reducing the variance in model predictions across different dropout masks . We show that our model achieves state - of - the - art results on benchmark language modeling tasks along with faster convergence . We also analytically study the relationship between our regularization and expectation linear dropout . We perform a number of ablation studies to evaluate our model from different aspects and carefully compare it with related methods both qualitatively and quantitatively . section : Acknowledgements The authors would like to acknowledge the support of the following agencies for research funding and computing support : NSERC , CIFAR , and IVADO . We would like to thank Rosemary Nan Ke and Philippe Lacaille for their thoughts and comments throughout the project . We would also like to thank Stanis\u0142aw Jastrz\u0119bski and Evan Racah for useful discussions . bibliography : References appendix : Appendix subsection : Monte Carlo evaluation A well known way to address the gap between the train and evaluation mode of dropout is to perform Monte Carlo sampling of masks and average the predictions during evaluation ( MC - eval ) , and this has been used for feed - forward networks . Since fraternal dropout addresses the same problem , we would like to clarify that it is not straight - forward and feasible to apply MC - eval for RNNs . In feed - forward networks , we average the output prediction scores from different masks . However , in the case RNNs ( for next step predictions ) , there is more than one way to perform such evaluation , but each one is problematic . They are as follows : 1 . Online averaging Consider that we first make the prediction at time step 1 using different masks by averaging the prediction score . Then we use this output to feed as input to the time step 2 , then use different masks at time step 2 to generate the output at time step 2 , and so on . But in order to do so , because of the way RNNs work , we also need to feed the previous time hidden state to time step 2 . One way would be to average the hidden states over different masks at time step 1 . But the hidden space can in general be highly nonlinear , and it is not clear if averaging in this space is a good strategy . This approach is not justified . Besides , this strategy as a whole is extremely time consuming because we would need to sequentially make predictions with multiple masks at each time step . 2 . Sequence averaging Let \u2019s consider that we use a different mask each time we want to generate a sequence , and then we average the prediction scores , and compute the argmax ( at each time step ) to get the actual generated sequence . In this case , notice it is not guaranteed that the predicted word at time step due to averaging the predictions would lead to the next word ( generated by the same process ) if we were to feed the time step output as input to the time step . For example , with different dropout masks , if the probability of time step outputs are : I 40 % ) , he ( 30 % ) , she ( 30 % ) , and the probability of the 2nd time step outputs are : am ( 30 % ) , is ( 60 % ) , was ( 10 % ) . Then the averaged prediction score followed by argmax will result in the prediction \u2018 \u2018 I is \u2019 \u2019 , but this would be incorrect . A similar concern applies for output predictions varying in temporal length . Hence , this approach can not be used to generate a sequence ( it has to be done by by sampling a mask and generating a single sequence ) . However , this approach may be used to estimate the probability assigned by the model to a given sequence . Nonetheless , we run experiments on the PTB dataset using MC - eval ( the results are summarized in Table [ reference ] ) . We start with a simple comparison that compares fraternal dropout with the averaged mask and the AWD - LSTM 3 - layer baseline with a single fixed mask that we call MC1 . The MC1 model performs much worse than fraternal dropout . Hence , it would be hard to use MC1 model in practice because a single sample is inaccurate . We also check MC - eval for a larger number of models ( MC50 ) ( 50 models were used since we were not able to fit more models simultaneously on a single GPU ) . The final results for MC50 are worse than the baseline which uses the averaged mask . For comparison , we also evaluate MC10 . Note that no fine - tuning is used for the above experiments . subsection : Reasons for focusing on RNNs The fraternal dropout method is general and may be applied in feed - forward architectures ( as shown in Subsection [ reference ] for CIFAR - 10 semisupervised example ) . However , we believe that it is more powerful in the case of RNNs because : Variance in prediction accumulates among time steps in RNNs and since we share parameters for all time steps , one may use the same value at each step . In feed - forward networks the layers usually do not share parameters and hence one may want to use different values for different layers ( which may be hard to tune ) . The simple way to alleviate this problem is to apply the regularization term on the pre - softmax predictions only ( as shown in the paper ) or use the same value for all layers . However , we believe that it may limit possible gains . The best performing RNN architectures ( state - of - the - art ) usually use some kind of dropout ( embedding dropout , word dropout , weight dropout etc . ) , very often with high dropout rates ( even larger than 50 % for input word embedding in NLP tasks ) . However , this is not true for feed - forward networks . For instance , ResNet architectures very often do not use dropout at all ( probably because batch normalization is often better to use ) . It can be seen in the paper ( Subsection [ reference ] , semisupervised CIFAR - 10 task ) that when unlabeled data is not used the regular dropout hurts performance and using fraternal dropout seems to improve just a little . On a final note , the Monte Carlo sampling ( a well known method that adresses the gap betweem the train and evaluation mode of dropout ) can not be easily applied for RNNs and fraternal dropout may be seen as an alternative . To conclude , we believe that when the use of dropout benefits in a given architecture , applying fraternal dropout should improve performance even more . As mentioned before , in image recognition tasks , one may experiment with something what we would temporarily dub fraternal augmentation ( even though dropout is not used , one can use random data augmentation such as random crop or random flip ) . Hence , one may force a given neural network to have the same predictions for different augmentations . subsection : Proofs theorem : . Let sit and sjt be i.i.d . dropout masks and \u2208\u2062pt ( zt , sit;\u03b8 ) Rm be the prediction function as described above . Then , proof : Proof . For simplicity of notation , we omit the time index . \u220e theorem : . . proof : Proof . Let , then Then using Jensen \u2019s inequality , \u220e", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank__Word_Level_"]]], "Method": [[["AWD-LSTM_3-layer_with_Fraternal_dropout"]]], "Metric": [[["Params"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank__Word_Level_"]]], "Method": [[["AWD-LSTM_3-layer_with_Fraternal_dropout"]]], "Metric": [[["Test_perplexity"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank__Word_Level_"]]], "Method": [[["AWD-LSTM_3-layer_with_Fraternal_dropout"]]], "Metric": [[["Validation_perplexity"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WikiText-2"]]], "Method": [[["AWD-LSTM_3-layer_with_Fraternal_dropout"]]], "Metric": [[["Number_of_params"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WikiText-2"]]], "Method": [[["AWD-LSTM_3-layer_with_Fraternal_dropout"]]], "Metric": [[["Test_perplexity"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WikiText-2"]]], "Method": [[["AWD-LSTM_3-layer_with_Fraternal_dropout"]]], "Metric": [[["Validation_perplexity"]]], "Task": [[["Language_Modelling"]]]}]}
{"docid": "TST3-SREX-0023", "doctext": "document : Speech Recognition with Deep Recurrent Neural Networks Recurrent neural networks ( RNNs ) are a powerful model for sequential data . End - to - end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input - output alignment is unknown . The combination of these methods with the Long Short - term Memory RNN architecture has proved particularly fruitful , delivering state - of - the - art results in cursive handwriting recognition . However RNN performance in speech recognition has so far been disappointing , with better results returned by deep feedforward networks . This paper investigates deep recurrent neural networks , which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs . When trained end - to - end with suitable regularisation , we find that deep Long Short - term Memory RNNs achieve a test set error of 17.7 % on the TIMIT phoneme recognition benchmark , which to our knowledge is the best recorded score . AlexGraves , Abdel - rahmanMohamedandGeoffreyHinton DepartmentofComputerScience , UniversityofToronto recurrent neural networks , deep neural networks , speech recognition section : Introduction Neural networks have a long history in speech recognition , usually in combination with hidden Markov models . They have gained attention in recent years with the dramatic improvements in acoustic modelling yielded by deep feedforward networks . Given that speech is an inherently dynamic process , it seems natural to consider recurrent neural networks ( RNNs ) as an alternative model . HMM - RNN systems have also seen a recent revival , but do not currently perform as well as deep networks . Instead of combining RNNs with HMMs , it is possible to train RNNs \u2018 end - to - end \u2019 for speech recognition . This approach exploits the larger state - space and richer dynamics of RNNs compared to HMMs , and avoids the problem of using potentially incorrect alignments as training targets . The combination of Long Short - term Memory , an RNN architecture with an improved memory , with end - to - end training has proved especially effective for cursive handwriting recognition . However it has so far made little impact on speech recognition . RNNs are inherently deep in time , since their hidden state is a function of all previous hidden states . The question that inspired this paper was whether RNNs could also benefit from depth in space ; that is from stacking multiple recurrent hidden layers on top of each other , just as feedforward layers are stacked in conventional deep networks . To answer this question we introduce deep Long Short - term Memory RNNs and assess their potential for speech recognition . We also present an enhancement to a recently introduced end - to - end learning method that jointly trains two separate RNNs as acoustic and linguistic models . Sections [ reference ] and [ reference ] describe the network architectures and training methods , sec : experiments provides experimental results and concluding remarks are given in sec : conclusion . section : Recurrent Neural Networks Given an input sequence , a standard recurrent neural network ( RNN ) computes the hidden vector sequence and output vector sequence by iterating the following equations from to : where the terms denote weight matrices ( e.g. is the input - hidden weight matrix ) , the terms denote bias vectors ( e.g. is hidden bias vector ) and is the hidden layer function . is usually an elementwise application of a sigmoid function . However we have found that the Long Short - Term Memory ( LSTM ) architecture , which uses purpose - built memory cells to store information , is better at finding and exploiting long range context . fig : lstm illustrates a single LSTM memory cell . For the version of LSTM used in this paper is implemented by the following composite function : where is the logistic sigmoid function , and , , and are respectively the input gate , forget gate , output gate and cell activation vectors , all of which are the same size as the hidden vector . The weight matrices from the cell to gate vectors ( e.g. ) are diagonal , so element in each gate vector only receives input from element of the cell vector . One shortcoming of conventional RNNs is that they are only able to make use of previous context . In speech recognition , where whole utterances are transcribed at once , there is no reason not to exploit future context as well . Bidirectional RNNs ( BRNNs ) do this by processing the data in both directions with two separate hidden layers , which are then fed forwards to the same output layer . As illustrated in fig : brnn , a BRNN computes the forward hidden sequence , the backward hidden sequence and the output sequence by iterating the backward layer from to , the forward layer from to and then updating the output layer : Combing BRNNs with LSTM gives bidirectional LSTM , which can access long - range context in both input directions . A crucial element of the recent success of hybrid HMM - neural network systems is the use of deep architectures , which are able to build up progressively higher level representations of acoustic data . Deep RNNs can be created by stacking multiple RNN hidden layers on top of each other , with the output sequence of one layer forming the input sequence for the next . Assuming the same hidden layer function is used for all layers in the stack , the hidden vector sequences are iteratively computed from to and to : where we define . The network outputs are Deep bidirectional RNNs can be implemented by replacing each hidden sequence with the forward and backward sequences and , and ensuring that every hidden layer receives input from both the forward and backward layers at the level below . If LSTM is used for the hidden layers we get deep bidirectional LSTM , the main architecture used in this paper . As far as we are aware this is the first time deep LSTM has been applied to speech recognition , and we find that it yields a dramatic improvement over single - layer LSTM . section : Network Training We focus on end - to - end training , where RNNs learn to map directly from acoustic to phonetic sequences . One advantage of this approach is that it removes the need for a predefined ( and error - prone ) alignment to create the training targets . The first step is to to use the network outputs to parameterise a differentiable distribution over all possible phonetic output sequences given an acoustic input sequence . The log - probability of the target output sequence can then be differentiated with respect to the network weights using backpropagation through time , and the whole system can be optimised with gradient descent . We now describe two ways to define the output distribution and hence train the network . We refer throughout to the length of as , the length of as , and the number of possible phonemes as . subsection : Connectionist Temporal Classification The first method , known as Connectionist Temporal Classification ( CTC ) , uses a softmax layer to define a separate output distribution at every step along the input sequence . This distribution covers the phonemes plus an extra blank symbol which represents a non - output ( the softmax layer is therefore size ) . Intuitively the network decides whether to emit any label , or no label , at every timestep . Taken together these decisions define a distribution over alignments between the input and target sequences . CTC then uses a forward - backward algorithm to sum over all possible alignments and determine the normalised probability of the target sequence given the input sequence . Similar procedures have been used elsewhere in speech and handwriting recognition to integrate out over possible segmentations ; however CTC differs in that it ignores segmentation altogether and sums over single - timestep label decisions instead . RNNs trained with CTC are generally bidirectional , to ensure that every depends on the entire input sequence , and not just the inputs up to . In this work we focus on deep bidirectional networks , with defined as follows : where is the element of the length unnormalised output vector , and is the number of bidirectional levels . subsection : RNN Transducer CTC defines a distribution over phoneme sequences that depends only on the acoustic input sequence . It is therefore an acoustic - only model . A recent augmentation , known as an RNN transducer combines a CTC - like network with a separate RNN that predicts each phoneme given the previous ones , thereby yielding a jointly trained acoustic and language model . Joint LM - acoustic training has proved beneficial in the past for speech recognition . Whereas CTC determines an output distribution at every input timestep , an RNN transducer determines a separate distribution for every combination of input timestep and output timestep . As with CTC , each distribution covers the phonemes plus . Intuitively the network \u2018 decides \u2019 what to output depending both on where it is in the input sequence and the outputs it has already emitted . For a length target sequence , the complete set of decisions jointly determines a distribution over all possible alignments between and , which can then be integrated out with a forward - backward algorithm to determine . In the original formulation was defined by taking an \u2018 acoustic \u2019 distribution from the CTC network , a \u2018 linguistic \u2019 distribution from the prediction network , then multiplying the two together and renormalising . An improvement introduced in this paper is to instead feed the hidden activations of both networks into a separate feedforward output network , whose outputs are then normalised with a softmax function to yield . This allows a richer set of possibilities for combining linguistic and acoustic information , and appears to lead to better generalisation . In particular we have found that the number of deletion errors encountered during decoding is reduced . Denote by and the uppermost forward and backward hidden sequences of the CTC network , and by the hidden sequence of the prediction network . At each the output network is implemented by feeding and to a linear layer to generate the vector , then feeding and to a hidden layer to yield , and finally feeding to a size softmax layer to determine : where is the element of the length unnormalised output vector . For simplicity we constrained all non - output layers to be the same size ( ; however they could be varied independently . RNN transducers can be trained from random initial weights . However they appear to work better when initialised with the weights of a pretrained CTC network and a pretrained next - step prediction network ( so that only the output network starts from random weights ) . The output layers ( and all associated weights ) used by the networks during pretraining are removed during retraining . In this work we pretrain the prediction network on the phonetic transcriptions of the audio training data ; however for large - scale applications it would make more sense to pretrain on a separate text corpus . subsection : Decoding RNN transducers can be decoded with beam search to yield an n - best list of candidate transcriptions . In the past CTC networks have been decoded using either a form of best - first decoding known as prefix search , or by simply taking the most active output at every timestep . In this work however we exploit the same beam search as the transducer , with the modification that the output label probabilities do not depend on the previous outputs ( so ) . We find beam search both faster and more effective than prefix search for CTC . Note the n - best list from the transducer was originally sorted by the length normalised log - probabilty ; in the current work we dispense with the normalisation ( which only helps when there are many more deletions than insertions ) and sort by . subsection : Regularisation Regularisation is vital for good performance with RNNs , as their flexibility makes them prone to overfitting . Two regularisers were used in this paper : early stopping and weight noise ( the addition of Gaussian noise to the network weights during training ) . Weight noise was added once per training sequence , rather than at every timestep . Weight noise tends to \u2018 simplify \u2019 neural networks , in the sense of reducing the amount of information required to transmit the parameters , which improves generalisation . section : Experiments Phoneme recognition experiments were performed on the TIMIT corpus . The standard 462 speaker set with all SA records removed was used for training , and a separate development set of 50 speakers was used for early stopping . Results are reported for the 24 - speaker core test set . The audio data was encoded using a Fourier - transform - based filter - bank with 40 coefficients ( plus energy ) distributed on a mel - scale , together with their first and second temporal derivatives . Each input vector was therefore size 123 . The data were normalised so that every element of the input vectors had zero mean and unit variance over the training set . All 61 phoneme labels were used during training and decoding ( so ) , then mapped to 39 classes for scoring . Note that all experiments were run only once , so the variance due to random weight initialisation and weight noise is unknown . As shown in tab : timit , nine RNNs were evaluated , varying along three main dimensions : the training method used ( CTC , Transducer or pretrained Transducer ) , the number of hidden levels ( 1\u20135 ) , and the number of LSTM cells in each hidden layer . Bidirectional LSTM was used for all networks except CTC - 3l - 500h - tanh , which had units instead of LSTM cells , and CTC - 3l - 421h - uni where the LSTM layers were unidirectional . All networks were trained using stochastic gradient descent , with learning rate , momentum and random initial weights drawn uniformly from . All networks except CTC - 3l - 500h - tanh and PreTrans - 3l - 250h were first trained with no noise and then , starting from the point of highest log - probability on the development set , retrained with Gaussian weight noise ( ) until the point of lowest phoneme error rate on the development set . PreTrans - 3l - 250h was initialised with the weights of CTC - 3l - 250h , along with the weights of a phoneme prediction network ( which also had a hidden layer of 250 LSTM cells ) , both of which were trained without noise , retrained with noise , and stopped at the point of highest log - probability . PreTrans - 3l - 250h was trained from this point with noise added . CTC - 3l - 500h - tanh was entirely trained without weight noise because it failed to learn with noise added . Beam search decoding was used for all networks , with a beam width of 100 . The advantage of deep networks is immediately obvious , with the error rate for CTC dropping from 23.9 % to 18.4 % as the number of hidden levels increases from one to five . The four networks CTC - 3l - 500h - tanh , CTC - 1l - 622h , CTC - 3l - 421h - uni and CTC - 3l - 250h all had approximately the same number of weights , but give radically different results . The three main conclusions we can draw from this are ( a ) LSTM works much better than for this task , ( b ) bidirectional LSTM has a slight advantage over unidirectional LSTMand ( c ) depth is more important than layer size ( which supports previous findings for deep networks ) . Although the advantage of the transducer is slight when the weights are randomly initialised , it becomes more substantial when pretraining is used . section : Conclusions and future work We have shown that the combination of deep , bidirectional Long Short - term Memory RNNs with end - to - end training and weight noise gives state - of - the - art results in phoneme recognition on the TIMIT database . An obvious next step is to extend the system to large vocabulary speech recognition . Another interesting direction would be to combine frequency - domain convolutional neural networks with deep LSTM . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["TIMIT"]]], "Method": [[["Bi-LSTM___skip_connections_w__CTC"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}]}
{"docid": "TST3-SREX-0024", "doctext": "document : Image Restoration Using Very Deep Convolutional Encoder - Decoder Networks with Symmetric Skip Connections In this paper , we propose a very deep fully convolutional encoding - decoding framework for image restoration such as denoising and super - resolution . The network is composed of multiple layers of convolution and de - convolution operators , learning end - to - end mappings from corrupted images to the original ones . The convolutional layers act as the feature extractor , which capture the abstraction of image contents while eliminating noises / corruptions . De - convolutional layers are then used to recover the image details . We propose to symmetrically link convolutional and de - convolutional layers with skip - layer connections , with which the training converges much faster and attains a higher - quality local optimum . First , The skip connections allow the signal to be back - propagated to bottom layers directly , and thus tackles the problem of gradient vanishing , making training deep networks easier and achieving restoration performance gains consequently . Second , these skip connections pass image details from convolutional layers to de - convolutional layers , which is beneficial in recovering the original image . Significantly , with the large capacity , we can handle different levels of noises using a single model . Experimental results show that our network achieves better performance than all previously reported state - of - the - art methods . . / figs / section : Introduction The task of image restoration is to recover an clean image from its corrupted observation , which is known to be an ill - posed inverse problem . By accommodating different types of corruption distributions , the same mathematical model applies to problems such as image denoising and super - resolution . Recently , deep neural networks ( DNNs ) have shown their superior performance in image processing and computer vision tasks , ranging from high - level recognition , semantic segmentation to low - level denoising , super - resolution , deblur , inpainting and recovering raw images from compressed images . Despite the progress that DNNs achieve , there still are some problems . For example , can a deeper network in general achieve better performance ; can we design a single model to handle different levels of corruption . Observing recent superior performance of DNNs on image processing tasks , we propose a convolutional neural network ( CNN )- based framework for image restoration . We observe that in order to obtain good restoration performance , it is beneficial to train a very deep model . Meanwhile , we show that it is possible to achieve good performance with a single network when processing multiple different levels of corruptions due to the benefits of large - capacity networks . Specifically , the proposed framework learns end - to - end fully convolutional mappings from corrupted images to the clean ones . The network is composed of multiple layers of convolution and de - convolution operators . As deeper networks tend to be more difficult to train , we propose to symmetrically link convolutional and de - convolutional layers with skip - layer connections , with which the training converges much faster and attains a higher - quality local optimum . Our main contributions are briefly outlined as follows : 1 ) A very deep network architecture , which consists of a chain of symmetric convolutional and deconvolutional layers , for image restoration is proposed in this paper . The convolutional layers act as the feature extractor which encode the primary components of image contents while eliminating the corruption . The deconvolutional layers then decode the image abstraction to recover the image content details . 2 ) We propose to add skip connections between corresponding convolutional and de - convolutional layers . These skip connections help to back - propagate the gradients to bottom layers and pass image details to the top layers , making training of the end - to - end mapping more easier and effective , and thus achieve performance improvement while the network going deeper . 3 ) Relying on the large capacity and fitting ability of our very deep network , we propose to handle different level of noises / corruption using a single model . To our knowledge , this is the first approach that achieves good accuracy for processing different levels of noises with a single model . 4 ) Experimental results demonstrate the advantages of our network over other recent state - of - the - art methods on image denoising and super - resolution , setting new records on these topics . Related work Extensive work has been done on image restoration in the literature . See detail reviews in a survey . Traditional methods such as Total variation , BM3D algorithm and dictionary learning based methods have shown very good performance on image restoration topics such as image denoising and super - resolution . Since image restoration is in general an ill - posed problem , the use of regularization has been proved to be essential . An active ( and probably more promising ) category for image restoration is the DNN based methods . Stacked denoising auto - encoder is one of the most well - known DNN models which can be used for image restoration . Xie et al . combined sparse coding and DNN pre - trained with denoising auto - encoder for low - level vision tasks such as image denoising and inpainting . Other neural networks based methods such as multi - layer perceptron and CNN for image denoising , as well as DNN for image or video super - resolution and compression artifacts reduction have been actively studied in these years . Burger et al . presented a patch - based algorithm learned with a plain multi - layer perceptron . They also concluded that with large networks , large training data , neural networks can achieve state - of - the - art image denoising performance . Jain and Seung proposed fully convolutional CNN for denoising . They found that CNN provide comparable or even superior performance to wavelet and Markov Random Field ( MRF ) methods . Cui et al . employed non - local self - similarity ( NLSS ) search on the input image in multi - scale , and then used collaborative local auto - encoder for super - resolution in a layer by layer fashion . Dong et al . proposed to directly learn an end - to - end mapping between the low / high - resolution images . Wang et al . argued that domain expertise represented by the conventional sparse coding can be combined to achieve further improved results . In general , DNN - based methods learn restoration parameters directly from data , which tends to been more effective in real - world image restoration applications . An advantage of DNN methods is that these methods are purely data driven and no assumption about the noise distributions are made . section : Very deep RED - Net for Image Restoration The proposed framework mainly contains a chain of convolutional layers and symmetric deconvolutional layers , as shown in Figure [ reference ] . We term our method \u2018 \u2018 RED - Net\u2019\u2019\u2014very deep Residual Encoder - Decoder Networks . subsection : Architecture The framework is fully convolutional and deconvolutional . Rectification layers are added after each convolution and deconvolution . The convolutional layers act as feature extractor , which preserve the primary components of objects in the image and meanwhile eliminating the corruptions . The deconvolutional layers are then combined to recover the details of image contents . The output of the deconvolutional layers is the \u2018 \u2018 clean \u2019 \u2019 version of the input image . Moreover , skip connections are also added from a convolutional layer to its corresponding mirrored deconvolutional layer . The passed convolutional feature maps are summed to the deconvolutional feature maps element - wise , and passed to the next layer after rectification . For low - level image restoration problems , we use neither pooling nor unpooling in the network as usually pooling discards useful image details that are essential for these tasks . Motivated by the VGG model , the kernel size for convolution and deconvolution is set to , which has shown excellent image recognition performance . It is worth mentioning that the size of input image can be arbitrary since our network is essentially a pixel - wise prediction . The input and output of the network are images of the same size , where , and are width , height and number of channels . In this paper , we use although it is straightforward to apply to images with . We found that using 64 feature maps for convolutional and deconvolutional layers achieves satisfactory results , although more feature maps leads to slightly better performance . Deriving from the above architecture , we propose two networks , which are 20 - layer and 30 - layer respectively . subsubsection : Deconvolution decoder Architectures combining layers of convolution and deconvolution have been proposed for semantic segmentation lately . In contrast to convolutional layers , in which multiple input activations within a filter window are fused to output a single activation , deconvolutional layers associate a single input activation with multiple outputs . One can simply replace deconvolution with convolution , which results in a architecture that is very similar to recently proposed very deep fully convolutional neural networks . However , there exist essential differences between a fully convolution model and our model . In the fully convolution case , the noise is eliminated step by step , i.e. , the noise level is reduced after each layer . During this process , the details of the image content may be lost . Nevertheless , in our network , convolution preserves the primary image content . Then deconvolution is used to compensate the details . We compare the 5 - layer and 10 - layer fully convolutional network with our network ( combining convolution and deconvolution , but without skip connection ) . For fully convolutional networks , we use padding and up - sample the input to make the input and output the same size . For our network , the first 5 layers are convolutional and the second 5 layers are deconvolutional . All the other parameters for training are the same , i.e. , trained with SGD and learning rate of , noise level . In terms of PSNR , using deconvolution works better than the fully convolutional counterpart . We see that , the fully convolutional network reduces noise layer by layer , and our network preserve primary image contents by convolution and recover some details by using deconvolution . Detailed results are in the supplementary materials . subsubsection : Skip connections An intuitive question is that , is deconvolution able to recover image details from the image abstraction only ? We find that in shallow networks with only a few layers of convolution , deconvolution is able to recover the details . However , when the network goes deeper or using operations such as max pooling , deconvolution does not work so well , possibly because too much details are already lost in the convolution . The second question is that , when our network goes deeper , does it achieve performance gain ? We observe that deeper networks often suffer from gradients vanishing and become hard to train \u2014 a problem that is well addressed in the literature . To address the above two problems , inspired by highway networks and deep residual networks , we add skip connections between two corresponding convolutional and deconvolutional layers as shown in Figure [ reference ] . A building block is shown in Figure [ reference ] . There are two reasons for using such connections . First , when the network goes deeper , as mentioned above , image details can be lost , making deconvolution weaker in recovering them . However , the feature maps passed by skip connections carry much image detail , which helps deconvolution to recover a better clean image . Second , the skip connections also achieve benefits on back - propagating the gradient to bottom layer , which makes training deeper network much easier as observed in and . Note that our skip layer connections are very different from the ones proposed in and , where the only concern is on the optimization side . In our case , we want to pass information of the convolutional feature maps to the corresponding deconvolutional layers . Instead of directly learning the mappings from input to the output , we would like the network to fit the residual of the problem , which is denoted as . Such a learning strategy is applied to inner blocks of the encoding - decoding network to make training more effective . Skip connections are passed every two convolutional layers to their mirrored deconvolutional layers . Other configurations are possible and our experiments show that this configuration already works very well . Using such shortcuts makes the network easier to be trained and gains restoration performance via increasing network depth . The very deep highway networks are essentially feed - forward long short - term memory ( LSTMs ) with forget gates ; and the CNN layers of deep residual network are feed - forward LSTMs without gates . Note that our deep residual networks are in general not in the format of standard feed - forward LSTMs . subsection : Discussions Training with symmetric skip connections As mentioned above , using skip connections mainly has two benefits : ( 1 ) passing image detail forwardly , which helps recovering clean images and ( 2 ) passing gradient backwardly , which helps finding better local minimum . We design experiments to show these observations . We first compare two networks trained for denoising noises of . In the first network , we use 5 layers of convolution with stride 3 . The input size of training data is , which results in a vector after 5 layers of convolution . Then deconvolution is used to recover the input . The second network uses the same settings as the first one , except for adding skip connections . The results are show in Figure [ reference ] ( a ) . We can observe that it is hard for deconvolution to recover details from only a vector encoding the abstraction of the input , which shows that the ability on recovering image details for deconvolution is limited . However , if we use skip connections , the network can still recover the input , because details are passed to topper layers in the network . We also train five networks to show that using skip connections help to back - propagate gradient in training to better fit the end - to - end mapping , as shown in Figure [ reference ] ( b ) . The five networks are : 10 , 20 and 30 layer networks without skip connections , and 20 , 30 layer networks with skip connections . As we can see , the training loss increases when the network going deeper without shortcuts ( similar phenomenon is also observed in ) , but we obtain smaller loss when using skip connections . Comparison with deep residual networks [ ] One may use different types of skip connections in our network , a straightforward alternate is that in . In , the skip connections are added to divide the network into sequential blocks . A benefit of our model is that our skip connections have element - wise correspondence , which can be very important in pixel - wise prediction problems . We carry out experiments to compare the two types of skip connections . Here the block size indicates the span of the connections . The results are shown in Figure [ reference ] ( c ) . We can observe that our connections often converge to a better optimum , demonstrating that element - wise correspondence can be important . Dealing with different levels of noises / corruption An important question is , can we handle different levels of corruption with a single model . Almost all existing methods need to train different models for different levels of corruption and estimate the corruption level at first . We use a trained model in , to denoise different levels of noises with being 10 , 30 , 50 and 70 . The obtained average PSNR on the 14 images are 29.95dB , 27.81dB , 18.62dB and 14.84dB , respectively . The results show that the parameters trained on a single noise level can not handle different levels of noises well . Therefore , in this paper , we aim to train a single model for recovering different levels of corruption , which are different noise levels in the task of image denoising and different scaling parameters in image super - resolution . The large capacity of the network is the key to this success . subsection : Training Learning the end - to - end mapping from corrupted images to clean ones needs to estimate the weights represented by the convolutional and deconvolutional kernels . This is achieved by minimizing the Euclidean loss between the outputs of the network and the clean image . In specific , given a collection of training sample pairs , where is a corrupted image and is the clean version as the groundtruth . We minimize the following Mean Squared Error ( MSE ) : We implement and train our network using Caffe . In practice , we find that using Adam with learning rate for training converges faster than traditional stochastic gradient descent ( SGD ) . The base learning rate for all layers are the same , different from , in which a smaller learning rate is set for the last layer . This trick is not necessary in our network . As general settings in the literature , we use gray - scale image for denoising and the luminance channel for super - resolution in this paper . 300 images from the Berkeley Segmentation Dataset ( BSD ) are used to generate the training set . For each image , patches of size are sampled as ground truth . For denoising , we add additive Gaussian noise to the patches multiple times to generate a large training set ( about 0.5 M ) . For super - resolution , we first down - sample a patch and then up - sample it to its original size , obtaining a low - resolution version as the input of the network . subsection : Testing Although trained on local patches , our network can perform denoising and super - resolution on images of arbitrary size . Given a testing image , one can simply go forward through the network , which is able to obtain a better performance than existing methods . To achieve more smooth results , we propose to process a corrupted image on multiple orientations . Different from segmentation , the filter kernels in our network only eliminate the corruptions , which is not sensitive to the orientation of image contents . Therefore , we can rotate and mirror flip the kernels and perform forward multiple times , and then average the output to get a more smooth image . We see that this can lead to slightly better denoising and super - resolution performance . section : Experiments In this section , we provide evaluation of denoising and super - resolution performance of our models against a few existing state - of - the - art methods . Denoising experiments are performed on two datasets : 14 common benchmark images and the BSD200 dataset . We test additive Gaussian noises with zero mean and standard deviation 10 , 30 , 50 and 70 respectively . BM3D , NCSR , EPLL , PCLR , PDPD and WMMN are compared with our method . For super - resolution , we compare our network with SRCNN , NBSRF , CSCN , CSC , TSE and ARFL + on three dataset : Set5 , Set14 and BSD100 . The scaling parameter are tested with 2 , 3 and 4 . Peak Signal - to - Noise Ratio ( PSNR ) and Structural SIMilarity ( SSIM ) index are calculated for evaluation . For our method , which is denoted as RED - Net , we implement three versions : RED10 contains 5 convolutional and deconvolutional layers without shortcuts , RED20 contains 10 convolutional and deconvolutional layers with shortcuts , and RED30 contains 15 convolutional and deconvolutional layers with shortcuts . subsection : Image Denoising Evaluation on the 14 images Table [ reference ] presents the PSNR and SSIM results of 10 , 30 , 50 , and 70 . We can make some observations from the results . First of all , the 10 layer convolutional and deconvolutional network has already achieved better results than the state - of - the - art methods , which demonstrates that combining convolution and deconvolution for denoising works well , even without any skip connections . Moreover , when the network goes deeper , the skip connections proposed in this paper help to achieve even better denoising performance , which exceeds the existing best method WNNM by 0.32dB , 0.43dB , 0.49dB and 0.51dB on noise levels of being 10 , 30 , 50 and 70 respectively . While WNNM is only slightly better than the second best existing method PCLR by 0.01dB , 0.06dB , 0.03dB and 0.01dB respectively , which shows the large improvement of our model . Last , we can observe that the more complex the noise is , the more improvement our model achieves than other methods . Similar observations can be made on the evaluation of SSIM . Evaluation on BSD200 For testing efficiency , we convert the images to gray - scale and resize them to smaller ones on BSD - 200 . Then all the methods are run on these images to get average PSNR and SSIM results of 10 , 30 , 50 , and 70 , as shown in Table [ reference ] . For existing methods , their denoising performance does not differ much , while our model achieves 0.38dB , 0.47dB , 0.49dB and 0.42dB higher of PSNR over WNNM . subsection : Image super - resolution The evaluation on Set5 is shown in Table [ reference ] . Our 10 - layer network outperforms the compared methods already , and we achieve better performance with deeper networks . The 30 - layer network exceeds the second best method CSCN for 0.52dB , 0.56dB and 0.47dB on scale 2 , 3 and 4 respectively . The evaluation on Set14 is shown in Table [ reference ] . The improvement on Set14 in not as significant as that on Set5 , but we can still observe that the 30 layer network achieves higher PSNR than the second best CSCN for 0.23dB , 0.06dB and 0.1dB. The results on BSD100 , as shown in Table [ reference ] , is similar than that on Set5 . The second best method is still CSCN , the performance of which is not as good as our 10 layer network . Our deeper network obtains much more performance gain than the others . subsection : Evaluation with a single model To construct the training set , we extract image patches with different noise levels and scaling parameters for denoising and super - resolution . Then a 30 - layer network is trained for the two tasks respectively . The evaluation results are shown in Table [ reference ] and Table [ reference ] . Although training with different levels of corruption , we can observe that the performance of our network only slightly degrades comparing to the case in which using separate models for denoising and super - resolution . This may be due the fact that the network has to fit much more complex mappings . Except that CSCN works slightly better on super - resolution with scales 3 and 4 , our network still beats the existing methods , showing that our network works much better in image denoising and super - resolution even using only one single model to deal with complex corruption . section : Conclusions In this paper we have proposed a deep encoding and decoding framework for image restoration . Convolution and deconvolution are combined , modeling the restoration problem by extracting primary image content and recovering details . More importantly , we propose to use skip connections , which helps on recovering clean images and tackles the optimization difficulty caused by gradient vanishing , and thus obtains performance gains when the network goes deeper . Experimental results and our analysis show that our network achieves better performance than state - of - the - art methods on image denoising and super - resolution . X. - J. Mao \u2019s contribution was made when visiting The University of Adelaide . This work was in part supported by ARC Future Fellowship ( FT120100969 ) . Correspondence should be addressed to C. Shen . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["BSD100_-_4x_upscaling"]]], "Method": [[["RED30"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["BSD100_-_4x_upscaling"]]], "Method": [[["RED30"]]], "Metric": [[["SSIM"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set5_-_4x_upscaling"]]], "Method": [[["RED30"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set5_-_4x_upscaling"]]], "Method": [[["RED30"]]], "Metric": [[["SSIM"]]], "Task": [[["Image_Super-Resolution"]]]}]}
{"docid": "TST3-SREX-0025", "doctext": "document : A Discriminatively Learned CNN Embedding for Person Re - identification In this paper , we revisit two popular convolutional neural networks ( CNN ) in person re - identification ( re - ID ) , i.e. , verification and identification models . The two models have their respective advantages and limitations due to different loss functions . In this paper , we shed light on how to combine the two models to learn more discriminative pedestrian descriptors . Specifically , we propose a siamese network that simultaneously computes the identification loss and verification loss . Given a pair of training images , the network predicts the identities of the two input images and whether they belong to the same identity . Our network learns a discriminative embedding and a similarity measurement at the same time , thus making full usage of the re - ID annotations . Our method can be easily applied on different pre - trained networks . Albeit simple , the learned embedding improves the state - of - the - art performance on two public person re - ID benchmarks . Further , we show our architecture can also be applied in image retrieval . Large - scale Person Re - identification , Convolutional Neural Networks . section : Introduction Person re - identification ( re - ID ) is usually viewed as an image retrieval problem , which matches pedestrians from different cameras . Given a person - of - interest ( query ) , person re - ID determines whether the person has been observed by another camera . Recent progress in this area has been due to two factors : 1 ) the availability of the large - scale pedestrian datasets . The datasets contain the general visual variance of pedestrian and provide a comprehensive evaluation . 2 ) the learned embedding of pedestrian using a convolutional neural network ( CNN ) . Recently , the convolutional neural network ( CNN ) has shown potential for learning state - of - the - art feature embeddings or deep metrics . As shown in Fig . [ reference ] , there are two major types of CNN structures , i.e. , verification models and identification models . The two models are different in terms of input , feature extraction and loss function for training . Our motivation is to combine the strengths of the two models and learn a more discriminative pedestrian embedding . Verification models take a pair of images as input and determine whether they belong to the same person or not . A number of previous works treat person re - ID as a binary - class classification task or a similarity regression task . Given a label , the verification network forces two images of the same person to be mapped to nearby points in the feature space . If the images are of different people , the points are far apart . However , the major problem in the verification models is that they only use weak re - ID labels , and do not take all the annotated information into consideration . Therefore , the verification network lacks the consideration of the relationship between the image pairs and other images in the dataset . In the attempt to take full advantages of the re - ID labels , identification models which treat person re - identification as a multi - class recognition task , are employed for feature learning . They directly learn the non - linear functions from an input image to the person ID and the cross - entropy loss is used following the final layer . During testing , the feature is extracted from a fully connected layer and then normalized . The similarity of two images is thus computed by the Euclidean distance between their normalized CNN embeddings . The major drawback of the identification model is that the training objective is different from the testing procedure , i.e. , it does not account for the similarity measurement between image pairs , which can be problematic during the pedestrian retrieval process . The above - mentioned observations demonstrate that the two types of models have complementary advantages and limitations as shown in Table [ reference ] . Motivated by these properties , this work proposes to combine the strengths of the two networks and leverage their complementary nature to improve the discriminative ability of the learned embeddings . The proposed model is a siamese network that predicts person identities and similarity scores at the same time . Compared to previous networks , we take full advantages of the annotated data in terms of pair - wise similarity and image identities . During testing , the final convolutional activations are extracted for Euclidepdfan distance based pedestrian retrieval . To summarize , our contributions are : We propose a siamese network that has two losses : identification loss and verification loss . This network simultaneously learns a discriminative CNN embedding and a similarity metric , thus improving pedestrian retrieval accuracy . We report competitive accuracy compared to the state - of - art methods on two large - scale person re - ID datasets ( Market1501 and CUHK03 ) and one instance retrieval dataset ( Oxford5k ) . The paper is organized as follows . We first review some related works in Section [ reference ] . In Section [ reference ] , we describe how we combine the two losses and define the CNN structure . The implementation details are provided . In Section [ reference ] , we present the experimental results on two large - scale person re - identification datasets and one instance retrieval dataset . We conclude this paper in Section [ reference ] . section : Related Work In this section we describe previous works relevant to the approach discussed in this paper . They are mainly based on verification models or identification models . subsection : Verification Models In 1993 , Bromley et al . first used verification models to deep metric learning in signature verification . Verification models usually take a pair of images as input and output a similarity score by calculating the cosine distance between low - dimensional features , which can be penalized by the contrastive loss . Recently researchers have begun to apply verification models to person re - identification with a focus on data augmentation and image matching . Yi et al . split a pedestrian image into three horizontal parts and train three part - CNNs to extract features . The similarity of two images is computed by the cosine distance of their features . Similarly , Cheng et al . split the convolutional map into four parts and fuse the part features with the global features . Li et al . add a patch - matching layer that multiplies the activation of two images in different horizontal stripes . They use it to find similar locations and treat similarity regression as binary - class penalized by softmax loss . Later , Ahmed et al . improve the verification model by adding a different matching layer that compares the activation of two images in neighboring pixels . Besides , Wu et al . use smaller filters and a deeper network to extract features . Varior et al . combine CNN with some gate functions , similar to long - short - term memory ( LSTM ) in spirit , which aims to adaptively focus on the similar parts of input image pairs . But it is limited by the computational inefficiency because the query image has to pair with every gallery image to pass through the network . Moreover , Ding et al . use triplet samples for training the network which considers the images from the same people and the different people at the same time . subsection : Identification Models Recent datasets such as CUHK03 and Market1501 provide large - scale training sets , which make it possible to train a deeper classification model without over - fitting . Every identity has 9.6 training images on average in CUHK03 and has 17.2 images in Market1501 . CNN can learn discriminative embeddings by itself without part - matching . Zheng et al . directly use a conventional fine - tuning approach on Market1501 , PRW and MARS and outperform many recent results . Wu et al . combine CNN embeddings with the hand - crafted features in the FC layer . Besides , Xiao et al . jointly train a classification model using multiple datasets and propose a new dropout function to deal with the hundreds of classes . In , Xiao et al . train a classification model similar to the faster - RCNN method and automatically predict the location of the candidate pedestrian from the whole image , which alleviates the pedestrian detection errors . subsection : Verification - identification Models In face recognition , the \u201c DeepID networks \u201d train the network with the verification and identification losses , which is similar to our network . In , Sun et al . jointly train face identification and verification . Then more verification supervision is added into the model and a deeper network is used . Our method is different from their models in the following aspects . First , in face recognition , the training dataset contains 202 , 599 face images of 10 , 177 identities while the current largest person re - i d training dataset contains 12 , 936 images of 751 identities . DeepID networks apply contrastive loss to the verification problem , wile our model uses the cross - entropy loss . We find that the contrastive loss leads to over - fitting when the number of images is limited . In the experiment , we show the proposed method learns more robust person representative and outperforms using contrastive loss . Second , dropout can not be applied on the embedding before the contrastive loss , which introduces zero values at random locations . On the contrary , we can add dropout regularization on the embedding in the proposed model . Third , the DeepID networks are trained from scratch , while our model benefits from the networks pretrained on ImageNet . Finally , we evaluate our method on the tasks of person re - ID and instance retrieval , providing more insights in the verification - classification models . section : Proposed Method subsection : Preview Fig . [ reference ] ( a ) and Fig . [ reference ] ( b ) illustrate the relational graph built by verification and identification models . In a sample batch of size , red edges represent the positive pairs ( the same person ) and blue edges represent the negative pairs ( different persons ) . The dotted edges denote implicit relationships built by the identification loss and the solid edges denote explicit relationships built by the verification loss . In verification models , there are several operations between the two inputs . The explicit relationship between data is built by the pair - wise comparison , such as part matching or contrastive loss . For example , contrastive loss directly calculates the Euclidean distance between two embeddings . In identification models , the input is independent to each other . But there is implicit relationship between the learned embeddings built by the cross - entropy loss . The cross - entropy loss can be formulated as . is the weight of the linear function . are the embeddings of the two images from the same class . To maximize , , the network converges when and have similar vector direction with . In , similar observation and visualization are shown . So the learned embeddings are eventually close for images within the same class and far away for images in the different classes . The relationship is implicitly built between and bridged by the weight . Due to the usage of the weak labels , verification models take limited relationships into consideration . On the other hand , classification models do not explicitly consider similarity measurements . Fig . [ reference ] ( c ) illustrates how our model works in a batch . We benefit from simultaneously considering the verification and identification losses . The proposed model thus combines the strength of the two models ( see Table [ reference ] ) . subsection : Overall Network Our network is basically a convolutional siamese network that combines the verification and identification losses . Fig . [ reference ] briefly illustrates the architecture of the proposed network . Given an input pair of images resized to , the proposed network simultaneously predicts the IDs of the two images and the similarity score . The network consists of two ImageNet pre - trained CNN models , three additional Convolutional Layers , one Square Layer and three losses . It is supervised by the identification label and the verification label . The pre - trained CNN model can be CaffeNet , VGG16 or ResNet - 50 , from which we have removed the final fully - connected ( FC ) layer . The re - ID performance of the three models is comprehensively evaluated in Section [ reference ] . Here , we do not provide detailed descriptions of the architecture of the CNN models and only take CaffeNet as an example in the following subsections . The three optimization objectives include two identification losses and one verification loss . We use the final convolutional activations as the discriminative descriptor for person re - ID , which is directly supervised by three objectives . subsection : Identification Loss There are two CaffeNets in our architecture . They share weights and predict the two identity labels of the input image pair simultaneously . In order to fine - tune the network on a new dataset , we replace the final fully - connected layer ( 1 , 000 - dim ) of the pre - trained CNN model with a convolutional layer . The number of the training identities in Market - 1501 is 751 . So this convolutional layer has kernels of size connected to the output of CaffeNet and then we add a softmax unit to normalize the output . The size of the result tensor is . The Rectified Linear Unit ( ReLU ) is not added after this convolution . Similar to conventional multi - class recognition approaches , we use the cross - entropy loss for identity prediction , which is Here denotes the convolutional operation . is a tensor , is the target class and denotes the parameters of the added convolutional layer . is the predicted probability , is the target probability . for all except . subsection : Verification Loss While some previous works contain a matching function in the intermediate layers , our work directly compares the high - level features for similarity estimation . The high - level feature from the fine - tuned CNN has shown a discriminative ability and it is more compact than the activations in the intermediate layers . So in our model , the pedestrian descriptor in the identification model are directly supervised by the verification loss . As shown in Fig . [ reference ] , we introduce a non - parametric layer called Square Layer to compare the high - level features . It takes two tensors as inputs and outputs one tensor after subtracting and squaring element - wisely . The Square Layer is denoted as , where are the 4 , 096 - dim embeddings and is the output tensor of the Square Layer . We then add a convolutional layer and the softmax output function to embed the resulting tensor to a 2 - dim vector ( , ) which represents the predicted probability of the two input images belonging to the same identity . The convolutional layer takes as input and filters it with kernels of size . The ReLU is not added after this convolution . We treat pedestrian verification as a binary classification problem and use the cross - entropy loss that is similar to the one in the identification loss , which is Here are the two tensors of size . is the target class ( same / different ) , denotes the parameters of the added convolutional layer and is the predicted probability . If the image pair depicts the same person , ; otherwise , . Departing from , we do not use the contrastive loss . On the one hand , the contrastive loss , as a regression loss , forces the same - class embeddings to be as close as possible . It may make the model over - fitting because the number of training of each identity is limited in person re - ID . On the other hand , dropout , which introduces zero values at random locations , can not be applied on the embedding before the contrastive loss . But the cross - entropy loss in our model can work with dropout to regularize the model . In Section [ reference ] , we show that the result using contrastive loss is 4.39 % and 6.55 % lower than the one using the cross - entropy loss on rank - 1 accuracy and mAP respectively . subsection : Identification vs. Verification The proposed network is trained to minimize the three cross - entropy losses jointly . To figure out which objective contributes more , we train the identification model and verification model separately . Following the learning rate setting in Section [ reference ] , we train the models until convergence . We also train the network with the two losses jointly until two objectives both converge . As the quantitative results shown in Table [ reference ] , the fine - tuned CNN model with two kinds of losses outperforms the one trained individually . This result has been confirmed on the three different network structures . Further , we visualize the intermediate feature maps that are trained using ResNet - 50 as the pretrained model and try to find the differences between identification loss and verification loss . We select three test images in the Market1501 . One image is considered to be well detected and the other two images are not well aligned . Given one image as input , we get its activation in the intermediate layer \u201c res4fx \u201d , the size of which is . We visualize the sum of several activation maps . As shown in Fig . [ reference ] , the identification and the verification networks exhibit different activation patterns to the pedestrian . We find that if we use only one kind of loss , the network tends to find one discriminative part . The proposed model takes advantages of both networks , so the new activation map is mostly a union of the two individual maps . This also illustrates the complementary nature of the two baseline networks . The proposed model makes more neurons activated . Moreover , as shown in Fig . [ reference ] we visualize the embedding by plot them to the 2 - dimension map . In regard to Fig . [ reference ] , we find the network usually has strong attention on the center part of the human ( usually clothes ) and it also illustrates the color of the clothes is the major clue for the person re - identification . subsection : Training and Optimization Input preparation . We resize all the training images to . The mean image computed from all the training images is subtracted from all the images . During training , all the images are randomly cropped to for CaffeNet and mirrored horizontally . For ResNet - 50 and VGG16 , we randomly crop images to . We shuffle the dataset and use a random order of the images . Then we sample another image from the same / different class to compose a positive / negative pair . The initial ratio between negative pairs and positive pairs is to alleviate the prediction bias and we multiple it by a factor of every epoch until it reaches , since the number of positive pairs is so limited that the network risks over - fitting . Training . We use the Matconvnet package for training and testing the embedding with CaffeNet , VGG16 and ResNet - 50 , respectively . The maximum number of training epochs is set to 75 for ResNet - 50 , 65 for VGG16net and 155 for CaffeNet . The batch size ( in image pairs ) is set to 128 for CaffeNet , 48 for VGG16 and ResNet - 50 . The learning rate is initialized as 0.001 and then set to 0.0001 for the final 5 epochs . We adopt the mini - batch stochastic gradient descent ( SGD ) to update the parameters of the network . There are three objectives in our network . Therefore , we first compute all the gradients produced by every objectives respectively and add the weighted gradients together to update the network . We assign a weight of 1 to the gradient produced by the verification loss and 0.5 for the two gradients produced by two identification losses . Moreover , we insert the dropout function before the final convolutional layer . Testing . We adopt an efficient method to extract features as well as the activation in the intermediate layer . Because two CaffeNet share weights , our model has nearly the same memory consumption with the pretrained model . So we extract features by only activating one fine - tuned model . Given a image , we feed forward the image to one CaffeNet in our network and obtain a 4 , 096 - dim pedestrian descriptor . Once the descriptors for the gallery sets are obtained , they are stored offline . Given a query image , its descriptor is extracted online . We sort the cosine distance between the query and all the gallery features to obtain the final ranking result . Note that the cosine distance is equivalent to Euclidean distance when the feature is L2 - normalized . section : Experiments We mainly verify the proposed model on two large - scale datasets Market1501 and CUHK03 . We report the results trained by three network structures . Besides , we also report the result on Market1501 + 500k dataset . Meanwhile , the proposed architecture is also applied on the image retrieval task . We modify our model and test it on a popular image retrieval dataset , i.e. , Oxford Buildings . The performance is comparable to the state of the art . subsection : Dataset Market1501 contains 32 , 668 annotated bounding boxes of 1 , 501 identities . Images of each identity are captured by at most six cameras . According to the dataset setting , the training set contains 12 , 936 cropped images of 751 identities and testing set contains 19 , 732 cropped images of 750 identities and distractors . They are directly detected by the Deformable Part Model ( DPM ) instead of using hand - drawn bboxes , which is closer to the realistic setting . For each query , we aim to retrieve the ground truth images from the 19 , 732 candidate images . The searching pool ( gallery ) is important to person re - identification . In the realistic setting , the scale of the gallery is usually large . The distractor dataset of Market1501 provides extra 500 , 000 bboxes , consisting of false alarms on the background as well as the persons not belonging to any of the original 1 , 501 identities . When testing , we add the 500k images to the original gallery , which makes the retrieval more difficult . CUHK03 dataset contains 14 , 097 cropped images of 1 , 467 identities collected in the CUHK campus . Each identity is observed by two camera views and has 4.8 images in average for each view . The Author provides two kinds of bounding boxes . We evaluate our model on the bounding boxes detected by DPM , which is closer to the realistic setting . Following the setting of the dataset , the dataset is partitioned into a training set of 1 , 367 persons and a testing set of 100 persons . The experiment is repeated with 20 random splits . Both the single - shot and multiple - shot results will be reported . Oxford5k buildings consists of 5062 images collected from the internet and corresponding to particular Oxford landmarks . Some images have complex structures and may contain other buildings . The images corresponding to 11 Oxford landmarks are manually annotated and a set of 55 queries for 11 different landmarks are provided . This benchmark contains many high - resolution images and the mean image size of this dataset is . We use the rank - 1 accuracy and mean average precision ( mAP ) for performance evaluation on Market1501 ( + 100k ) and CUHK03 , while on Oxford , we use mAP . subsection : Person Re - i d Evaluation Comparison with the CNN baseline . We train the baseline networks according the conventional fine - tuning method . The baseline networks are pretrained on ImageNet and fine - tuned to predict the person identities . As shown in Tab . [ reference ] , we obtain 50.89 % , 65.02 % and 73.69 % rank - 1 accuracy by CaffeNet , VGG16 and ResNet - 50 , respectively on Market1501 . Note that using the baseline alone exceeds many previous works . Our model further improves these baselines on Market1501 . The improvement can be observed on three network architectures . To be specific , we obtain 11.25 % , 5.14 % and 5.82 % improvement , respectively , using CaffeNet , VGG16 and ResNet - 50 on Market1501 . Similarly , we observe 35.8 % , 49.1 % and 71.5 % baseline rank - 1 accuracy on CUHK03 in single - shot setting . As show in Tab . [ reference ] , these baseline results exceed some previous works as well . We further get 14.0 % , 22.7 % and 11.9 % improvement on the baseline by our method . These results show that our method can work with different networks and improve their results . It indicates that the proposed model helps the network to learn more discriminative features . Cross - entropy vs. Contrastive loss . We replace the cross - entropy loss with the contrastive loss as used in \u201c DeepID network \u201d . However , we find a 4.39 % and 6.55 % drop in rank - 1 and mAP . The ResNet - 50 model using the contrastive loss has 75.12 % rank - 1 accuracy and 53.32 % mAP . We speculate that the contrastive loss tends to over - fit on the re - ID dataset because no regularization is added to the verification . Cross - entropy loss designed in our model can work with the dropout function and avoid the over - fitting . Comparison with the state of the art . As shown in Table [ reference ] , we compare our method with other state - of - the - art algorithms in terms of mean average precision ( mAP ) and rank - 1 accuracy on Market1501 . We report the single - query as well as multiple - query evaluation results . Our model ( CaffeNet ) achieves 62.14 % rank - 1 accuracy and 39.61 % mAP , which is comparable to the state of the art 65.88 % rank - 1 accuracy and 39.55 % mAP . Our model using ResNet - 50 produces the best performance 79.51 % in rank - 1 accuracy and 59.87 % in mAP , which outperforms other state - of - the - art algorithms . For CUHK03 , we evaluate our method in the single - shot setting as shown in Tab . [ reference ] . There is only one right image in the searching pool . In the evaluation , we randomly select 100 images from 100 identities under the other camera as gallery . The proposed model yields 83.4 % rank - 1 and 86.4 % mAP and outperforms the state - of - the - art performance . As shown in Tab . [ reference ] , we also report the results in the multi - shot setting , which uses all the images from the other camera as gallery and the number of the gallery images is about 500 . We think this setting is much closer to image retrieval and alleviate the unstable effect caused by the random searching pool under single - shot settings . Fig . [ reference ] presents some re - ID samples on CUHK03 dataset . The images in the first column are the query images . The retrieval images are sorted according to the similarity scores from left to right . Most ground - truth candidate images are correctly retrieved . Although the model retrieves some incorrect candidates on the third row , we find it is a reasonable prediction since the man with red hat and blue coat is similar to the query . The proposed model yields 88.3 % rank - 1 and 85.0 % mAP and also outperforms the state - of - the - art performance in the multi - shot setting . Results between camera pairs . CUHK03 only contains two camera views . So this experiment is evaluated on Market1501 since it contains six different cameras . We provide the re - identification results between all camera pairs in Fig . [ reference ] . Although camera - 6 is a low - resolution camera and captures distinct background with the other HD cameras , the re - ID accuracy between camera 6 and the others is relatively high . We also compute the cross - camera average mAP and average rank - 1 accuracy : 48.42 % and 54.42 % respectively . Comparing to the previous reported results , i.e. , 10.51 % and 13.72 % in , our method largely improves the performance and observes a smaller standard deviation between cameras . It suggests that the discriminatively learned embedding works under different viewpoints . Further , Fig . [ reference ] shows the Barnes - Hut t - SNE visualization on the learned embeddings of our model . By the clustering algorithm , the persons wearing the similar - color clothes are quit clustered together and are apart from other persons . The learned pedestrian descriptor pay more attention to the color and it is robust to some illusion and viewpoint variations . In realistic setting , we think color provides the most important information to figure out the person . Large - scale experiments . The Market1501 dataset also provides an additional distractor set with 500k images to enlarge the gallery . In general , more candidate images may confuse the image retrieval . The re - ID performance of our model ( ResNet ) on the large - scale dataset is presented in Tab . [ reference ] . As the searching pool gets larger , the accuracy drops . With the gallery size of , we still achieve 68.26 % rank1 accuracy and 45.24 % mAP . A relative drop 24.4 % from 59.87 % to 45.24 % on mAP is observed , compared to a relative drop 37.88 % from 13.94 % to 8.66 % in our previous work . Besides , we also compare our result with the performance of the ResNet Baseline . As shown in Fig . [ reference ] , it is interesting that the re - ID precision of our model decreases more quickly comparing to the baseline model . We speculate that the Market1501 training set is relatively small in covering the pedestrian variations encountered in a much larger test set . In fact , the 500k dataset was collected in a different time ( the same location ) with the Market1501 dataset , so the transfer effect is large enough that the learned embedding is inferior to the baseline on the scale of 500 k images . In the future , we will look into this interesting problem and design more robust descriptors for the transfer dataset . subsection : Instance Retrieval We apply the identification - verification model to the generic image retrieval task . Oxford5k is a testing dataset containing buildings in the Oxford University . We train the network on another scene dataset proposed in , which comprises of a number of buildings without overlapping with the Oxford5k . Similarly , the model is trained to not only tell which building the image depicts but also determine whether the two input images are from the same architecture . The training data is high - resolution . In order to obtain more information from the high - resolution building images , we modify the final pooling layer of our model to a MAC layer , which outputs the maximum value over the whole activation map . This layer helps us to handle large images without resizing them to a fixed size and output a fixed - dimension feature to retrieve the images . During training , the input image is randomly cropped to from and mirrored horizontally . During testing , we keep the original size of the images that are not cropped or resized and extract the feature . In Table [ reference ] , many previous works are based on CaffeNet or VGG16 . For fair comparison , we report the baseline results and the results of our model based on these two network structures , respectively . Our model which uses CaffeNet as pretrained model outperforms the state of the art . Meanwhile , the model using VGG16 is comparable to the state - of - the - arts methods . The proposed method show a 6.0 % and 6.6 % improvement over the baseline networks CaffeNet and VGG16 , respectively . We visualize some retrieval results in Fig . [ reference ] . The images in the first column are the query images . The retrieval images are sorted according to the similarity scores from left to right . The main difficulty in the image retrieval is various object sizes in the image . In the first row , we use the roof ( part of the building ) to retrieve the images and the top five images are correct candidate images . The other retrieval samples also show our model is robust to the scale variations . section : Conclusion In this work , we propose a siamese network that simultaneously considers the identification loss and the verification loss . The proposed model learns a discriminative embedding and a similarity measurement at the same time . It outperforms the state of the art on two popular person re - ID benchmarks and shows potential ability to apply on the generic instance retrieval task . Future work includes exploring more novel applications of the proposed method , such as car recognition and fine - grained classification . Besides , we will investigate how to learn a robust descriptor to further improve the performance of the person re - identification on large - scale testing set . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Market-1501"]]], "Method": [[["DLCE"]]], "Metric": [[["MAP"]]], "Task": [[["Person_Re-Identification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Market-1501"]]], "Method": [[["DLCE"]]], "Metric": [[["Rank-1"]]], "Task": [[["Person_Re-Identification"]]]}]}
{"docid": "TST3-SREX-0026", "doctext": "document : Learning to Adapt Structured Output Space for Semantic Segmentation Convolutional neural network - based approaches for semantic segmentation rely on supervision with pixel - level ground truth , but may not generalize well to unseen image domains . As the labeling process is tedious and labor intensive , developing algorithms that can adapt source ground truth labels to the target domain is of great interest . In this paper , we propose an adversarial learning method for domain adaptation in the context of semantic segmentation . Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains , we adopt adversarial learning in the output space . To further enhance the adapted model , we construct a multi - level adversarial network to effectively perform output space domain adaptation at different feature levels . Extensive experiments and ablation study are conducted under various domain adaptation settings , including synthetic - to - real and cross - city scenarios . We show that the proposed method performs favorably against the state - of - the - art methods in terms of accuracy and visual quality . section : Introduction Semantic segmentation aims to assign each pixel a semantic label , e.g. , person , car , road or tree , in an image . Recently , methods based on convolutional neural networks ( CNNs ) have achieved significant progress in semantic segmentation with applications for autonomous driving and image editing . The crux of CNN - based approaches is to annotate a large number of images that cover possible scene variations . However , this trained model may not generalize well to unseen images , especially when there is a domain gap between the training ( source ) and test ( target ) images . For instance , the distribution of appearance for objects and scenes may vary in different cities , and even weather and lighting conditions can change significantly in the same city . In such cases , relying only on the supervised model that requires re - annotating per - pixel ground truths in different scenarios would entail prohibitively high labor cost . To address this issue , knowledge transfer or domain adaptation techniques have been proposed to close the gap between source and target domains , where annotations are not available in the target domain . For image classification , one effective approach is to align features across two domains such that the adapted features can generalize to both domains . Similar efforts have been made for semantic segmentation via adversarial learning in the feature space . However , different from the image classification task , feature adaptation for semantic segmentation may suffer from the complexity of high - dimensional features that needs to encode diverse visual cues , including appearance , shape and context . This motivates us to develop an effective method for adapting pixel - level prediction tasks rather than using feature adaptation . In semantic segmentation , we note that the output space contains rich information , both spatially and locally . For instance , even if images from two domains are very different in appearance , their segmentation outputs share a significant amount of similarities , e.g. , spatial layout and local context ( see Figure [ reference ] ) . Based on this observation , we address the pixel - level domain adaptation problem in the output ( segmentation ) space . In this paper , we propose an end - to - end CNN - based domain adaptation algorithm for semantic segmentation . Our formulation is based on adversarial learning in the output space , where the intuition is to directly make the predicted label distributions close to each other across source and target domains . Based on the generative adversarial network ( GAN ) , the proposed model consists of two parts : 1 ) a segmentation model to predict output results , and 2 ) a discriminator to distinguish whether the input is from the source or target segmentation output . With an adversarial loss , the proposed segmentation model aims to fool the discriminator , with the goal of generating similar distributions in the output space for either source or target images . The proposed method also adapts features as the errors are back - propagated to the feature level from the output labels . However , one concern is that lower - level features may not be adapted well as they are far away from the high - level output labels . To address this issue , we develop a multi - level strategy by incorporating adversarial learning at different feature levels of the segmentation model . For instance , we can use both conv5 and conv4 features to predict segmentation results in the output space . Then two discriminators can be connected to each of the predicted output for multi - level adversarial learning . We perform one - stage end - to - end training for the segmentation model and discriminators jointly , without using any prior knowledge of the data in the target domain . In the testing phase , we can simply discard discriminators and use the adapted segmentation model on target images , with no extra computational requirements . Due to the high labor cost of annotating segmentation ground truth , there has been great interest in large - scale synthetic datasets with annotations , e.g. , GTA5 and SYNTHIA . As a result , one critical setting is to adapt the model trained on synthetic data to real - world datasets , such as Cityscapes . We follow this setting and conduct extensive experiments to validate the proposed domain adaptation method . First , we use a strong baseline model that is able to generalize to different domains . We note that a strong baseline facilitates real - world applications and can evaluate the limitation of the proposed adaptation approach . Based on this baseline model , we show comparisons using adversarial adaptation in the feature and output spaces . Furthermore , we show that the multi - level adversarial learning improves the results over single - level adaptation . In addition to the synthetic - to - real setting , we show experimental results on the Cross - City dataset , where annotations are provided in one city ( source ) , while testing the model on another unseen city ( target ) . Overall , our method performs favorably against state - of - the - art algorithms on numerous benchmark datasets under different settings . The contributions of this work are as follows . First , we propose a domain adaptation method for pixel - level semantic segmentation via adversarial learning . Second , we demonstrate that adaptation in the output ( segmentation ) space can effectively align scene layout and local context between source and target images . Third , a multi - level adversarial learning scheme is developed to adapt features at different levels of the segmentation model , which leads to improved performance . section : Related Work Semantic Segmentation . State - of - the - art semantic segmentation methods are mainly based on the recent advances of deep neural networks . As proposed by Long , one can transform a classification CNN ( e.g. , AlexNet , VGG , or ResNet ) to a fully - convolutional network ( FCN ) for semantic segmentation . Numerous methods have since been developed to improve this model by utilizing context information or enlarging receptive fields . To train these advanced networks , a substantial amount of dense pixel annotations must be collected in order to match the model capacity of deep CNNs . As a result , weakly and semi - supervised approaches are proposed in recent years to reduce the heavy labeling cost of collecting segmentation ground truths . However , in most real - world applications , it is difficult to obtain weak annotations and the trained model may not generalize well to unseen image domains . Another approach to tackle the annotation problem is to construct synthetic datasets based on rendering , e.g. , GTA5 and SYNTHIA . While the data collection is less costly since the pixel - level annotation can be done with a partially automated process , these datasets are usually used in conjunction with real - world datasets for joint learning to improve the performance . However , when training solely on the synthetic dataset , the model does not generalize well to real - world data , mainly due to the large domain shift between synthetic images and real - world images , i.e. , appearance differences are still significant with current rendering techniques . Although synthesizing more realistic images can decrease the domain shift , it is necessary to use domain adaptation to narrow the performance gap . Domain Adaptation . Domain adaptation methods for image classification have been developed to address the domain - shift problem between the source and target domains . Numerous methods are developed based on CNN classifiers due to performance gain . The main insight behind these approaches is to tackle the problem by aligning the feature distribution between source and target images . Ganin propose the Domain - Adversarial Neural Network ( DANN ) to transfer the feature distribution . A number of variants have since been proposed with different loss functions or classifiers . Recently , the PixelDA method addresses domain adaptation for image classification by transferring the source images to target domain , thereby obtaining a simulated training set for target images . We note that domain adaptation for pixel - level prediction tasks have not been explored widely . Hoffman introduce the task of domain adaptation on semantic segmentation by applying adversarial learning ( i.e. , DANN ) in a fully - convolutional way on feature representations and additional category constraints similar to the constrained CNN . Other methods focus on adapting synthetic - to - real or cross - city images by adopting class - wise adversarial learning or label transfer . Similar to the PixelDA method , one concurrent work , CyCADA uses the CycleGAN and transfers source domain images to the target domain with pixel alignment , thus generating extra training data combined with feature space adversarial learning . Although feature space adaptation has been successfully applied to image classification , pixel - level tasks such as semantic segmentation remains challenging based on feature adaptation - based approaches . In this paper , we use the property that pixel - level predictions are structured outputs that contain information spatially and locally , to propose an efficient domain adaptation algorithm through adversarial learning in the output space . section : Algorithmic Overview subsection : Overview of the Proposed Model Our domain adaptation algorithm consists of two modules : a segmentation network and the discriminator , where indicates the level of a discriminator in the multi - level adversarial learning . Two sets of images from source and target domains are denoted as and . We first forward the source image ( with annotations ) to the segmentation network for optimizing . Then we predict the segmentation softmax output for the target image ( without annotations ) . Since our goal is to make segmentation predictions of source and target images ( i.e. , and ) close to each other , we use these two predictions as the input to the discriminator to distinguish whether the input is from the source or target domain . With an adversarial loss on the target prediction , the network propagates gradients from to , which would encourage to generate similar segmentation distributions in the target domain to the source prediction . Figure [ reference ] shows the overview of the proposed algorithm . subsection : Objective Function for Domain Adaptation With the proposed network , we formulate the adaptation task containing two loss functions from both modules : where is the cross - entropy loss using ground truth annotations in the source domain , and is the adversarial loss that adapts predicted segmentations of target images to the distribution of source predictions ( see Section [ reference ] ) . In ( [ reference ] ) , is the weight used to balance the two losses . section : Output Space Adaptation Different from image classification based on features that describe the global visual information of the image , high - dimensional features learned for semantic segmentation encodes complex representations . As a result , adaptation in the feature space may not be the best choice for semantic segmentation . On the other hand , although segmentation outputs are in the low - dimensional space , they contain rich information , e.g. , scene layout and context . Our intuition is that no matter images are from the source or target domain , their segmentations should share strong similarities , spatially and locally . Thus , we utilize this property to adapt low - dimensional softmax outputs of segmentation predictions via an adversarial learning scheme . subsection : Single - level Adversarial Learning Discriminator Training . Before introducing how to adapt the segmentation network via adversarial learning , we first describe the training objective for the discriminator . Given the segmentation softmax output , where is the number of categories , we forward to a fully - convolutional discriminator using a cross - entropy loss for the two classes ( i.e. , source and target ) . The loss can be written as : where if the sample is drawn from the target domain , and for the sample from the source domain . Segmentation Network Training . First , we define the segmentation loss in ( [ reference ] ) as the cross - entropy loss for images from the source domain : where is the ground truth annotations for source images and is the segmentation output . Second , for images in the target domain , we forward them to and obtain the prediction . To make the distribution of closer to , we use an adversarial loss in ( [ reference ] ) as : This loss is designed to train the segmentation network and fool the discriminator by maximizing the probability of the target prediction being considered as the source prediction . subsection : Multi - level Adversarial Learning Although performing adversarial learning in the output space directly adapts predictions , low - level features may not be adapted well as they are far away from the output . Similar to the deep supervision method that uses auxiliary loss for semantic segmentation , we incorporate additional adversarial module in the low - level feature space to enhance the adaptation . The training objective for the segmentation network can be extended from ( [ reference ] ) as : where indicates the level used for predicting the segmentation output . We note that , the segmentation output is still predicted in each feature space , before passing through individual discriminators for adversarial learning . Hence , and remain in the same form as in ( [ reference ] ) and ( [ reference ] ) , respectively . Based on ( [ reference ] ) , we optimize the following min - max criterion : The ultimate goal is to minimize the segmentation loss in for source images , while maximizing the probability of target predictions being considered as source predictions . road sidewalk building wall fence pole light sign veg terrain sky person rider car truck bus train mbike bike section : Network Architecture and Training Discriminator . For the discriminator , we use an architecture similar to but utilize all fully - convolutional layers to retain the spatial information . The network consists of 5 convolution layers with kernel and stride of 2 , where the channel number is { 64 , 128 , 256 , 512 , 1 } , respectively . Except for the last layer , each convolution layer is followed by a leaky ReLU parameterized by . An up - sampling layer is added to the last convolution layer for re - scaling the output to the size of the input . We do not use any batch - normalization layers as we jointly train the discriminator with the segmentation network using a small batch size . Segmentation Network . It is essential to build upon a good baseline model to achieve high - quality segmentation results . We adopt the DeepLab - v2 framework with ResNet - 101 model pre - trained on ImageNet as our segmentation baseline network . However , we do not use the multi - scale fusion strategy due to the memory issue . Similar to the recent work on semantic segmentation , we remove the last classification layer and modify the stride of the last two convolution layers from 2 to 1 , making the resolution of the output feature maps effectively times the input image size . To enlarge the receptive field , we apply dilated convolution layers in conv4 and conv5 layers with a stride of 2 and 4 , respectively . After the last layer , we use the Atrous Spatial Pyramid Pooling ( ASPP ) as the final classifier . Finally , we apply an up - sampling layer along with the softmax output to match the size of the input image . Based on this architecture , our segmentation model achieves 65.1 % mean intersection - over - union ( IoU ) when trained on the Cityscapes training set and tested on the Cityscapes validation set . Multi - level Adaptation Model . We construct the above - mentioned discriminator and segmentation network as our single - level adaptation model . For the multi - level structure , we extract feature maps from the conv4 layer and add an ASPP module as the auxiliary classifier . Similarly , a discriminator with the same architecture is added for adversarial learning . Figure [ reference ] shows the proposed multi - level adaptation model . In this paper , we use two levels due to the balance of its efficiency and accuracy . Network Training . To train the proposed single / multi - level adaptation model , we find that jointly training the segmentation network and discriminators in one stage is effective . In each training batch , we first forward the source image to optimize the segmentation network for in ( [ reference ] ) and generate the output . For the target image , we obtain the segmentation output , and pass it along with to the discriminator for optimizing in ( [ reference ] ) . In addition , we compute the adversarial loss in ( [ reference ] ) for the target prediction . For the multi - level training objective in ( [ reference ] ) , we simply repeat the same procedure for each adaptation module . We implement our network using the PyTorch toolbox on a single Titan X GPU with 12 GB memory . To train the segmentation network , we use the Stochastic Gradient Descent ( SGD ) optimizer with Nesterov acceleration where the momentum is 0.9 and the weight decay is . The initial learning rate is set as and is decreased using the polynomial decay with power of 0.9 as mentioned in . For training the discriminator , we use the Adam optimizer with the learning rate as and the same polynomial decay as the segmentation network . The momentum is set as 0.9 and 0.99 . section : Experimental Results In this section , we present experimental results to validate the proposed domain adaptation method for semantic segmentation under different settings . First , we show evaluations of the model trained on synthetic datasets ( i.e. , GTA5 and SYNTHIA ) and test the adapted model on real - world images from the Cityscapes dataset . Extensive experiments including comparisons to the state - of - the - art methods and ablation study are also conducted , e.g. , adaptation in the feature / output spaces and single / multi - level adversarial learning . Second , we carry out experiments on the Cross - City dataset , where the model is trained on one city and adapted to another city without using annotations . In all the experiments , the IoU metric is used . The code and model are available at . subsection : GTA5 The GTA5 dataset consists of images with the resolution of synthesized from the video game based on the city of Los Angeles . The ground truth annotations are compatible with the Cityscapes dataset that contains 19 categories . Following , we use the full set of GTA5 and adapt the model to the Cityscapes training set with 2975 images . During testing , we evaluate on the Cityscapes validation set with 500 images . Overall Results . We present adaptation results in Table [ reference ] with comparisons to the state - of - the - art domain adaptation methods . For these approaches , the baseline model is trained using VGG - based architectures . To fairly evaluate our method , we first use the same baseline architecture ( VGG - 16 ) and train our model with the proposed single - level adaptation module . Table [ reference ] shows that our method performs favorably against the other algorithms . While these methods all have feature adaptation modules , our results show that adapting the model in the output space achieves better performance . We note that CyCADA has a pixel adaptation module by transforming source domain images to the target domain and hence obtains additional training samples . Although this strategy achieves a similar performance as ours , one can always apply pixel transformation combined with our output space adaptation to improve the results . On the other hand , we argue that utilizing a stronger baseline model is critical for understanding the importance of different adaptation components as well as for enhancing the performance to enable real - world applications . Thus , we use the ResNet - 101 based network introduced in Section [ reference ] and train the proposed adaptation model . Table [ reference ] shows the baseline results only trained on source images without adaptation , with comparisons to our adapted models under different settings , including feature adaptation and single / multi - level adversarial learning in the output space . Figure [ reference ] presents some example results for adapted segmentation . We note that for small objects such as poles and traffic signs , they are harder to adapt since they easily get merged with background classes . In addition , another factor to evaluate the adaptation performance is to measure how much gap is narrowed between the adaptation model and the fully - supervised model . Hence , we train the model using annotated ground truths in the Cityscapes dataset as the oracle results . Table [ reference ] shows the gap under different baseline models . We observe that , although the oracle result does not differ a lot between VGG - 16 and ResNet - 101 based models , the gap is larger for the VGG one . It suggests us that to narrow the gap , using a deeper model with larger capacity is more practical . Parameter Analysis . During optimizing the segmentation network , it is essential to balance the weight between segmentation and adversarial losses . We first consider the single - level case in ( [ reference ] ) and conduct experiments to observe the impact of changing . Table [ reference ] shows that a smaller may not facilitate the training process significantly , while a larger may propagate incorrect gradients to the network . We empirically choose as 0.001 in the single - level setting . Feature Level v.s . Output Space Adaptation . In the single - level setting in ( [ reference ] ) , we compare results by using feature - level or output space adaptation via adversarial learning . For feature - level adaptation , we adopt a similar strategy as used in and train our model accordingly . Table [ reference ] shows that the proposed adaptation method in the output space performs better than the one in the feature level . In addition , Table [ reference ] shows that adaptation in the feature space is more sensitive to , which causes the training process more difficult , while output space adaptation allows for a wider range of . One reason is that as feature adaptation is performed in the high - dimensional space , the problem for the discriminator becomes easier . Thus , such an adapted model can not effectively match distributions between source and target domains via adversarial learning . Single - level v.s . Multi - level Adversarial Learning . We have shown the merits of adopting adversarial learning in the output space . In addition , we present the results of using multi - level adversarial learning in Table [ reference ] . Here , we utilize an additional adversarial module ( see Figure [ reference ] ) and jointly optimize ( [ reference ] ) for two levels . To properly balance and , we use the same weight as in the single - level setting for the high - level output space ( i.e. , = 1 and = 0.001 ) . Since the low - level output carries less information to predict the segmentation , we use smaller weights for both the segmentation and adversarial loss ( i.e. , = 0.1 and = 0.0002 ) . Evaluation results show that our multi - level adversarial adaptation further improves the segmentation accuracy . More results and analysis are presented in the supplementary material . road sidewalk building light sign veg sky person rider car bus mbike bike subsection : SYNTHIA To adapt from the SYNTHIA to Cityscapes datasets , we use the SYNTHIA - RAND - CITYSCAPES set as the source domain which contains 9400 images compatible with the cityscapes annotated classes . Similar to , we evaluate images on the Cityscapes validation set with 13 classes . For the weight in ( [ reference ] ) and ( [ reference ] ) , we use the same ones as in the case of GTA5 dataset . Table [ reference ] shows evaluation results of the proposed algorithm against the state - of - the - art methods that use feature adaptation . Similar to the experiments with the GTA5 dataset , we first utilize the same VGG - based model and train our single - level adaptation model for fair comparisons . The experimental results suggest that adapting the model in the output space performs better . Second , we compare results using different components of the proposed method with the ResNet based model . We show that the multi - level adaptation module improves the results over the baseline , feature space adaptation and single - level adaptation models . In addition , we present comparisons of mean IoU gap between adapted and oracle results in Table [ reference ] . Our method achieves the smallest gap and is the only one that can minimize the gap below 30 % . road sidewalk building light sign veg sky person rider car bus mbike bike subsection : Cross - City Dataset In addition to the synthetic - to - real adaptation for a larger domain gap , we conduct experiment on the Cross - City dataset with smaller domain gaps between cities . The dataset contains four different cities : Rio , Rome , Tokyo and Taipei , in which each city has 3200 images without annotations and 100 images with pixel - level ground truths for 13 classes . Similar to , we use the Cityscapes training set as the source domain and adapt it to each target city using 3200 images , while 100 annotated images are used for evaluation . Since a smaller domain gap results in smaller output differences , we use smaller weights for the adversarial loss ( i.e. , ) when training our models , while the weights for segmentation remain the same as previous experiments . We show our results in Table [ reference ] with comparisons to and our baseline models under different settings . Again , our final multi - level model achieves consistent improvement for different cities , which demonstrates the advantages of the proposed adaptation method in the output space . Note that the state - of - the - art method uses a different baseline model , and we present it as a reference to analyze how much the proposed algorithm can improve . section : Concluding Remarks In this paper , we exploit the fact that segmentations are structured outputs and share many similarities between source and target domains . We tackle the domain adaptation problem for semantic segmentation via adversarial learning in the output space . To further enhance the adapted model , we construct a multi - level adversarial network to effectively perform output space domain adaptation at different feature levels . Experimental results show that the proposed method performs favorably against numerous baseline models and the state - of - the - art algorithms . We hope that our proposed method can be a generic adaptation model for a wide range of pixel - level prediction tasks . Acknowledgments . W. - C. Hung is supported in part by the NSF CAREER Grant # 1149783 , gifts from Adobe and NVIDIA . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["GTAV-to-Cityscapes_Labels"]]], "Method": [[["Single-level_Adaptation"]]], "Metric": [[["mIoU"]]], "Task": [[["Synthetic-to-Real_Translation"]]]}]}
{"docid": "TST3-SREX-0027", "doctext": "document : Pose - driven Deep Convolutional Model for Person Re - identification Feature extraction and matching are two crucial components in person Re - Identification ( ReID ) . The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images . To overcome these difficulties , in this work we propose a Pose - driven Deep Convolutional ( PDC ) model to learn improved feature extraction and matching models from end to end . Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts . To match the features from global human body and local body parts , a pose driven feature weighting sub - network is further designed to learn adaptive feature fusions . Extensive experimental analyses and results on three popular datasets demonstrate significant performance improvements of our model over all published state - of - the - art methods . section : Introduction Person Re - Identification ( ReID ) is an important component in a video surveillance system . Here person ReID refers to the process of identifying a probe person from a gallery captured by different cameras , and is generally deployed in the following scenario : given a probe image or video sequence containing a specific person under a certain camera , querying the images , locations , and time stamps of this person from other cameras . Despite decades of studies , the person ReID problem is still far from being solved . This is mainly because of challenging situations like complex view variations and large pose deformations on the captured person images . Most of traditional works try to address these challenges with the following two approaches : ( 1 ) representing the visual appearance of a person using customized local invariant features extracted from images or ( 2 ) learning a discriminative distance metric to reduce the distance among features of images containing the same person . Because the human poses and viewpoints are uncontrollable in real scenarios , hand - coded features may be not robust enough to pose and viewpoint variations . Distance metric is computed for each pair of cameras , making distance metric learning based person ReID suffers from the computational complexity . In recent years , deep learning has demonstrated strong model capabilities and obtains very promising performances in many computer vision tasks . Meanwhile , the release of person ReID datasets like CUHK 03 , Market - 1501 , and MARS , both of which contain many annotated person images , makes training deep models for person ReID feasible . Therefore , many researchers attempt to leverage deep models in person ReID . Most of these methods first learn a pedestrian feature and then compute Euclidean distance to measure the similarity between two samples . More specifically , existing deep learning based person ReID approaches can be summarized into two categories : 1 ) use Softmax Loss with person ID labels to learn a global representation , and 2 ) first learn local representations using predefined rigid body parts , then fuse the local and global representations to depict person images . Deep learning based methods have demonstrated significant performance improvements over the traditional methods . Although these approaches have achieved remarkable results on mainstream person ReID datasets , most of them do not consider pose variation of human body . Because pose variations may significantly change the appearance of a person , considering the human pose cues is potential to help person re - identification . Although there are several methods that segment the person images according to the predefined configuration , such simple segmentation can not capture the pose cues effectively . Some recent works attempt to use pose estimation algorithms to predict human pose and then train deep models for person ReID . However , they use manually cropped human body parts and their models are not trained from end to end . Therefore , the potential of pose information to boost the ReID performance has not been fully explored . To better alleviate the challenges from pose variations , we propose a Pose - driven Deep Convolutional ( PDC ) model for person ReID . The proposed PDC model learns the global representation depicting the whole body and local representations depicting body parts simultaneously . The global representation is learned using the Softmax Loss with person ID labels on the whole input image . For the learning of local representations , a novel Feature Embedding sub - Net ( FEN ) is proposed to learn and readjust human parts so that parts are affine transformed and re - located at more reasonable regions which can be easily recognizable through two different cameras . In Feature Embedding sub - Net , each body part region is first automatically cropped . The cropped part regions are hence transformed by a Pose Transformation Network ( PTN ) to eliminate the pose variations . The local representations are hence learned on the transformed regions . We further propose a Feature Weighting sub - Net ( FWN ) to learn the weights of global representations and local representations on different parts . Therefore , more reasonable feature fusion is conducted to facilitate feature similarity measurement . Some more detailed descriptions to our local representation generation are illustrated in Fig . [ reference ] . Our method first locates the key body joints from the input image , , illustrated in Fig . [ reference ] ( c ) . From the detected joints , six body parts are extracted , , shown in Fig . [ reference ] ( d ) . As shown in Fig . [ reference ] ( e ) , those parts are extracted and normalized into fixed sizes and orientations . Finally , they are fed into the Pose Transformation Network ( PTN ) to further eliminate the pose variations . With the normalized and transformed part regions , , Fig . [ reference ] ( f ) , local representations are learned by training the deep neural network . Different parts commonly convey different levels of discriminative cues to identify the person . We thus further learn weights for representations on different parts with a sub - network . Most of current deep learning based person ReID works do not consider the human pose cues and the weights of representation on different parts . This paper proposes a novel deep architecture that transforms body parts into normalized and homologous feature representations to better overcome the pose variations . Moreover , a sub - network is proposed to automatically learn weights for different parts to facilitate feature similarity measurement . Both the representation and weighting are learned jointly from end to end . Since pose estimation is not the focus of this paper , the used pose estimation algorithm , , Fully Convolutional Networks ( FCN ) based pose estimation method is simple and trained independently . Once the FCN is trained , it is incorporated in our framework , which is hence trained in an end - to - end manner , , using images as inputs and person ID labels as outputs . Experimental results on three popular datasets show that our algorithm significantly outperforms many state - of - the - art ones . section : Related Work Traditional algorithms perform person re - identification through two ways : ( a ) acquiring robust local features visually representing a person \u2019s appearance and then encoding them ; ( b ) closing the gap between a person \u2019s different features by learning a discriminative distance metric . Some recent works have started to apply deep learning in person ReID and achieved promising performance . In the following , we briefly review recent deep learning based person ReID methods . Deep learning is commonly used to either learn a person \u2019s representation or the distance metric . When handling a pair of person images , existing deep learning methods usually learn feature representations of each person by using a deep matching function from convolutional features or from the Fully Connected ( FC ) features . Apart from deep metric learning methods , some algorithms first learn image representations directly with the Triplet Loss or the Siamese Contrastive Loss , then utilize Euclidean distance for comparison . Wang use a joint learning framework to unify single - image representation and cross - image representation using a doublet or triplet CNN . Shi propose a moderate positive mining method to use deep distance metric learning for person ReID . Another novel method learns deep attributes feature for ReID with semi - supervised learning . Xiao train one network with several person ReID datasets using a Domain Guided Dropout algorithm . Predefined rigid body parts are also used by many deep learning based methods for the purpose of learning local pedestrian features . Different from these algorithms , our work and the ones in use more accurate human pose estimation algorithms to acquire human pose features . However , due to the limited accuracy of pose estimation algorithms as well as reasons like occlusion and lighting change , pose estimation might be not accurate enough . Moreover , different parts convey different levels of discriminative cues . Therefore , we normalize the part regions to get more robust feature representation using Feature Embedding sub - Net ( FEN ) and propose a Feature Weighting sub - Net ( FWN ) to learn the weight for each part feature . In this way , the part with high discriminative power can be identified and emphasized . This also makes our work different from existing ones , which do not consider the inaccuracy of human poses estimation and weighting on different parts features . section : Pose - driven Deep ReID Model In this section , we describe the overall framework of the proposed approach , where we mainly introduce the Feature Embedding sub - Net ( FEN ) and the Feature Weighting sub - Net ( FWN ) . Details about the training and test procedures of the proposed approach will also be presented . subsection : Framework Fig . [ reference ] shows the framework of our proposed deep ReID model . It can be seen that the global image and part images are simultaneously considered during each round of training . Given a training sample , we use an human pose estimation algorithm to acquire the locations of human pose joints . These pose joints are combined into different human body parts . The part regions are first transformed using our Feature Embedding sub - Net ( FEN ) and then are combined to form a new modified part image containing the normalized body parts . The global image and the new modified part image are then fed into our CNN together . The two images share the same weights for the first several layers , then have their own network weights in the subsequent layers . At last , we use Feature Weighting sub - Net ( FWN ) to learn the weights of part features before fusing them with the global features for final Softmax Loss computation . Considering that pedestrian images form different datasets have different sizes , it is not appropriate to directly use the CNN models pre - trained on the ImageNet dataset . We thus modify and design a network based on the GoogLeNet , as shown in the Table [ reference ] . Layers from data to inception ( 4e ) in Table [ reference ] corresponds to the blue CNN block in Fig . [ reference ] , CNNg and CNNp are inception ( 5a ) and inception ( 5b ) , respectively . The green CONV matches the subsequent 1 1 convolution . The loss layers are not shown in Table [ reference ] . The Batch Normalization Layers are inserted before every ReLU Layer to accelerate the convergence . We employ a Convolutional Layer and a Global Average Pooling Layer ( GAP ) at the end of network to let our network can fit different sizes of input images . In this work , we fix input image size as 512 256 . subsection : Feature Embedding sub - Net The Feature Embedding sub - Net ( FEN ) is divided into four steps , including locating the joint , generating the original part images , PTN , and outputting the final modified part images . With a given person image , FEN first locates the 14 joints of human body using human pose estimation algorithm . Fig . [ reference ] ( c ) shows an example of the 14 joints of human body . According to number , the 14 joints are { } . Then we propose six rectangles to cover six different parts of human body , including the head region , the upper body , two arms and two legs . For each human joint , we calculate a response feature map . The horizontal and vertical dimensions of the feature maps are denoted by and , respectively . With the feature maps , the fourteen body joints , can be located by finding the center of mass with the feature values : where in Eq . [ reference ] are the coordinates of joints , and is the value of pixels in response feature maps . Different from , we do not use complex pose estimation networks as the pre - trained network . Instead , we use a standard FCN trained on the LSP dataset and MPII human pose dataset . In the second step , the FEN uses the 14 human joints to further locate six sub - regions ( head , upper body , left arm , right arm , left leg , and right leg ) as human parts . These parts are normalized through cropping , rotating , and resizing to fixed size and orientation . As shown in Fig . [ reference ] ( d ) , the 14 located body joints are assigned to six rectangles indicating six parts . The head part , the upper body part , the left arm part , the right arm part , the left leg part , and the right leg part , respectively . For each body part set , The corresponding sub - region bounding box can be obtained based on the location coordinates of all body joints in each part set : An example of the extracted six body sub - regions are visualized in Fig . [ reference ] ( d ) . As shown in Fig . [ reference ] ( e ) , these body sub - regions are normalized through cropping , rotating , and resizing to fixed sizes and orientations . All body parts are rotated to fixed vertical direction . Arms and legs are resized to 256 64 , upper body is resized to 256 128 and head is resized to 128 128 . Those resized and rotated parts are combined to form the body part image . Because 6 body parts have different sizes , black area is unavoidable in body part image . Simply resizing and rotation can not overcome the complex pose variations , especially if the pose estimations are inaccurate . We thus design a PTN modified from Spatial Transformer Networks ( STN ) to learn the angles required for rotating the five body parts . STN is a spatial transformer module which can be inserted to a neural network to provide spatial transformation capabilities . It thus is potential to adjust the localizations and angles of parts . A STN is a small net which allows for end - to - end training with standard back - propagation , therefore , the introduction of STN does n\u2019t substantially increase the complexity of training procedure . The STN consist of three components : localisation network , parameterised sampling grid , and differentiable image sampling . The localisation network takes the input feature map and outputs the parameters of the transformation . For our net , we choose affine transformation so our transformation parameter is 6 - dimensional . The parameterized sampling grid computes each output pixel and the differentiable image sampling component produces the sampled output image . For more details about STN , please refer to . As discussed above , we use a 6 - dimensional parameter to complete affine transformation : where the are the scale and rotation parameters , while the are the translation parameters . The in Eq . [ reference ] are the target coordinates of the output image and the are the source coordinates of the input image . Usually the STN computes one affine transform for the whole image , considering a pedestrian \u2019s different parts have various orientations and sizes from each other , STN is not applicable to a part image . Inspired by STN , we design a Pose Transformer Network ( PTN ) which computes the affine transformation for each part in part image individually and combines 6 transformed parts together . Similar to STN , our PTN is also a small net and does n\u2019t substantially increase the complexity of our training procedure . As a consequence , PTN has potential to perform better than STN for person images . Fig . [ reference ] shows the detailed structure of PTN . Considering a pedestrian \u2019s head seldom has a large rotation angle , we do n\u2019t insert a PTN net for the pedestrian \u2019s head part . Therefore , we totally have 5 independent PTN , namely , , , , . Each PTN can generate a 6 - dimensional transformation parameter and use to adjust pedestrian \u2019s part , we can get modified body part . By combining the five transformed parts and a head part together , we obtain the modified part image . subsection : Feature Weighting sub - Net The generated part features are combined with the global feature to generate a robust feature representation for precise person re - identification . As the poses generated by the pose detector might be affected by factors like occlusions , pose changes , etc . Then inaccurate part detection results could be obtained . Examples are shown in Fig . [ reference ] . Therefore , the part features could be not reliable enough . This happens frequently in real applications with unconstrained video gathering environment . Simply fusing global feature and the part feature may introduces noises . This motivates us to introduce Feature Weighting sub - Net ( FWN ) to seek a more optimal feature fusion . FWN is consisted with a Weight Layer and a nonlinear transformation , which decides the importance of each dimension in the part feature vector . Considering that a single linear Weight Layer might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equalize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors . The and in Eq . [ reference ] are the weight and bias vectors which have the same dimensions with . The means the Hadamard product of two vectors , and the means concatenation of two vectors together . The imposes the hyperbolic tangent nonlinearity . is our final person feature generated by and . To allow back - propagation of the loss through the FWN , we give the gradient formula : where , , , , , and are the dimensions of and . subsection : ReID Feature Extraction The global feature and body - part features are learned by training the Pose - driven Deep Convolutional model . These two types of features are then fused under a unified framework for multi - class person identification . PDC extracts the global feature maps from the global body - based representation and learns a 1024 - dimensional feature embedding . Similarly , a 1024 - dimension feature is acquired from the modified part image after the FEN . The global body feature and the local body part features are compensated into a 2048 - dimensional feature as the final representation . After being weighted by FWN , the final representation is used for Person ReID with Euclidean distance . section : Experiment subsection : Datasets We select three widely used person ReID datasets as our evaluation protocols , including the CUHK 03 , Market 1501 , and VIPeR . Note that , because the amount of images in VIPeR is not enough for training a deep model , we combine the training sets of VIPeR , CUHK 03 and Market 1501 together to train the model for VIPeR . CUHK 03 : This dataset is made up of 14 , 096 images of 1 , 467 different persons taken by six campus cameras . Each person only appears in two views . This dataset provides two types of annotations , including manually labelled pedestrian bounding boxes and bounding boxes automatically detected by the Deformable - Part - Model ( DPM ) detector . We denote the two corresponding subsets as labeled dataset and detected dataset , respectively . The dataset also provides 20 test sets , each includes 100 identities . We select the first set and use 100 identities for testing and the rest 1 , 367 identities for training . We report the averaged performance after repeating the experiments for 20 times . Market 1501 : This dataset is made up of 32 , 368 pedestrian images taken by six manually configured cameras . It has 1 , 501 different persons in it . On average , there are 3.6 images for each person captured from each angle . The images can be classified into two types , , cropped images and images of pedestrians automatically detected by the DPM . Because Market 1501 has provided the training set and testing set , we use images in the training set for training our PDC network and follow the protocol to report the ReID performance . VIPeR : This dataset is made up of 632 person images captured from two views . Each pair of images depicting a person are collected by different cameras with varying viewpoints and illumination conditions . Because the amount of images in VIPeR is not enough to train the deep model , we also perform data augmentation with similar methods in existing deep learning based person ReID works . For each training image , we generate 5 augmented images around the image center by performing random 2D transformations . Finally , we combine the augmented training images of VIPeR , training images of CUHK 03 and Market 1501 together , as the final training set . subsection : Implementation Details The pedestrian representations are learned through multi - class classification CNN . We use the full body and body parts to learn the representations with Softmax Loss , respectively . We report rank1 , rank5 , rank10 and rank20 accuracy of cumulative match curve ( CMC ) on the three datasets to evaluate the ReID performance . As for Market - 1051 , mean Average Precision ( mAP ) is also reported as an additional criterion to evaluate the performance . Our model is trained and fine - tuned on Caffe . Stochastic Gradient Descent ( SGD ) is used to optimize our model . Images for training are randomly divided into several batches , each of which includes 16 images . The initial learning rate is set as 0.01 , and is gradually lowered after each iterations . It should be noted that , the learning rate in part localization network is only 0.1 % of that in feature learning network . For each dataset , we train a model on its corresponding training set as the pretrained body - based model . For the overall network training , the network is initialized using pretrained body - based model . Then , we adopt the same training strategy as described above . We implement our approach with GTX TITAN X GPU , Intel i7 CPU , and 128 GB memory . All images are resized to . The mean value is subtracted from each channel ( B , G , and R ) for training the network . The images of each dataset are randomized in the process of training stage . subsection : Evaluation of Individual Components We evaluate five variants of our approach to verify the validity of individual components in our PDC , , components like Feature Embedding sub - Net ( FEN ) and Feature Weighting sub - Net ( FWN ) . Comparisons on three datasets are summarized in Table [ reference ] . In the table , \u201c Global Only \u201d means we train our deep model without using any part information . \u201c Global + Part \u201d denotes CNN trained through two streams without FEN and FWN . Based on \u201c Global + Part \u201d , considering FEN is denoted as \u201c Global + Part + FEN \u201d . Similarly , \u201c Global + Part + FWN \u201d means considering FWN . In addition , \u201c Part Only \u201d denotes only using part features . PDC considers all of these components . From the experimental results , it can be observed that , fusing global features and part features achieves better performance than only using one of them . Compared with \u201c Global Only \u201d , considering extra part cues , , \u201c Global + Part \u201d , largely improves the ReID performance and achieves the rank1 accuracy of 85.07 % and 76.33 % on CUHK 03 labeled and detected datasets , respectively . Moreover , using FEN and FWN further boosts the rank1 identification rate . This shows that training our model using PTN and Weight Layer gets more competitive performance on three datasets . The above experiments shows that each of the components in our method is helpful for improving the performance . By considering all of these components , PDC exhibits the best performance . subsection : Comparison with Related Works CUHK 03 : For the CUHK 03 dataset , we compare our PDC with some recent methods , including distance metric learning methods : MLAPG , LOMO + XQDA , BoW + HS , WARCA , LDNS , feature extraction method : GOG and deep learning based methods : IDLA , PersonNet , DGDropout , SI + CI , Gate S - CNN , LSTM S - CNN , EDM , PIE and Spindle . We conduct experiments on both the detected dataset and the labeled dataset . Experimental results are presented in Table [ reference ] and Table [ reference ] . Experimental results show that our approach outperforms all distance metric learning methods by a large margin . It can be seen that PIE , Spindle and our PDC which all use the human pose cues achieve better performance than the other methods . This shows the advantages of considering extra pose cues in person ReID . It is also clear that , our PDC achieves the rank1 accuracy of 78.29 and 88.70 on detected and labeled datasets , respectively . This leads to 11.19 and 0.20 performance gains over the reported performance of PIE and Spindle , respectively . Market 1501 : On Market 1501 , the compared works that learn distance metrics for person ReID include LOMO + XQDA , BoW + Kissme , WARCA , LDNS , TMA and HVIL . Compared works based on deep learning are PersonNet , Gate S - CNN , LSTM S - CNN , PIE and Spindle . DGDropout does not report performance on Market1501 . So we implemented DGDroput and show experimental results in Table [ reference ] . It is clear that our method outperforms these compared works by a large margin . Specifically , PDC achieves rank1 accuracy of 84.14 % , and mAP of 63.41 % using the single query mode . They are higher than the rank1 accuracy and mAP of PIE , which performs best among the compared works . This is because our PDC not only learns pose invariant features with FEN but also learns better fusion strategy with FWN to emphasize the more discriminative features . VIPeR : We also evaluate our method by comparing it with several existing methods on VIPeR . The compared methods include distance metric learning ones : MLAPG , LOMO + XQDA , BoW , WARCA and LDNS , and deep learning based ones : IDLA , DGDropout , SI + CI , Gate S - CNN , LSTM S - CNN , MTL - LORAE and Spindle . From the results shown in Table [ reference ] , our PDC achieves the rank1 accuracy of 51.27 % . This outperforms most of compared methods except Spindle which also considers the human pose cues . We assume the reason might be because , Spindle involves more training sets to learn the model for VIPeR . Therefore , the training set of Spindle is larger than ours , , the combination of Market 1501 , CUHK03 and VIPeR . For the other two datasets , our PDC achieves better performance than Spindle . subsection : Evaluation of Feature Weighting sub - Net To test the effectiveness of Feature Weighting sub - Net ( FWN ) , we verify the performance of five variants of FWN , which are denoted as , = { 0 , 1 , 2 , 3 , 4 } , where is the number of Weight Layers in FWN with nonlinear transformation . For example , means we cascade two Weight Layers with nonlinear transformation , means we only have one Weight Layer without nonlinear transformation . The experimental results are shown in Table [ reference ] . As we can see that one Weight Layer with nonlinear transformation gets the best performance on the three datasets . The ReID performance starts to drop as we increase of the number of Weight Layers , despite more computations are being brought in . It also can be observed that , using one layer with nonlinear transformation gets better performance than one layer without nonlinear transformation , , . This means adding one nonlinear transformation after a Weight Layer learns more reliable weights for feature fusion and matching . Based on the above observations , we adopt as our final model in this paper . Examples of features before and after FWN are shown Fig . [ reference ] . section : Conclusions This paper presents a pose - driven deep convolutional model for the person ReID . The proposed deep architecture explicitly leverages the human part cues to learn effective feature representations and adaptive similarity measurements . For the feature representations , both global human body and local body parts are transformed to a normalized and homologous state for better feature embedding . For similarity measurements , weights of feature representations from human body and different body parts are learned to adaptively chase a more discriminative feature fusion . Experimental results on three benchmark datasets demonstrate the superiority of the proposed model over current state - of - the - art methods . Acknowledgments This work is partly supported by National Science Foundation of China under Grant No . 61572050 , 91538111 , 61620106009 , 61429201 , 61672519 , and the National 1000 Youth Talents Plan . Dr. Qi Tian is supported by ARO grant W911NF - 15 - 1 - 0290 and Faculty Research Gift Awards by NEC Laboratories of America and Blippar . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Market-1501"]]], "Method": [[["PDF"]]], "Metric": [[["MAP"]]], "Task": [[["Person_Re-Identification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Market-1501"]]], "Method": [[["PDF"]]], "Metric": [[["Rank-1"]]], "Task": [[["Person_Re-Identification"]]]}]}
{"docid": "TST3-SREX-0028", "doctext": "document : Distributed Prioritized Experience Replay We propose a distributed architecture for deep reinforcement learning at scale , that enables agents to learn effectively from orders of magnitude more data than previously possible . The algorithm decouples acting from learning : the actors interact with their own instances of the environment by selecting actions according to a shared neural network , and accumulate the resulting experience in a shared experience replay memory ; the learner replays samples of experience and updates the neural network . The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors . Our architecture substantially improves the state of the art on the Arcade Learning Environment , achieving better final performance in a fraction of the wall - clock training time . section : Introduction A broad trend in deep learning is that combining more computation Dean:2012:LSD:2999134.2999271 with more powerful models DBLP : journals / corr / KaiserGSVPJU17 and larger datasets imagenet_cvpr09 yields more impressive results . It is reasonable to hope that a similar principle holds for deep reinforcement learning . There are a growing number of examples to justify this optimism : effective use of greater computational resources has been a critical factor in the success of such algorithms as Gorila gorila , A3C a3c , GPU Advantage Actor Critic ga3c , Distributed PPO heess : dppo and AlphaGo alphago . Deep learning frameworks such as TensorFlow tensorflow support distributed training , making large scale machine learning systems easier to implement and deploy . Despite this , much current research in deep reinforcement learning concerns itself with improving performance within the computational budget of a single machine , and the question of how to best harness more resources is comparatively underexplored . In this paper we describe an approach to scaling up deep reinforcement learning by generating more data and selecting from it in a prioritized fashion prioritized - replay . Standard approaches to distributed training of neural networks focus on parallelizing the computation of gradients , to more rapidly optimize the parameters Dean:2012:LSD:2999134.2999271 . In contrast , we distribute the generation and selection of experience data , and find that this alone suffices to improve results . This is complementary to distributing gradient computation , and the two approaches can be combined , but in this work we focus purely on data - generation . We use this distributed architecture to scale up variants of Deep Q - Networks ( DQN ) and Deep Deterministic Policy Gradient ( DDPG ) , and we evaluate these on the Arcade Learning Environment benchmark bellemare2013arcade , and on a range of continuous control tasks . Our architecture achieves a new state of the art performance on Atari games , using a fraction of the wall - clock time compared to the previous state of the art , and without per - game hyperparameter tuning . We empirically investigate the scalability of our framework , analysing how prioritization affects performance as we increase the number of data - generating workers . Our experiments include an analysis of factors such as the replay capacity , the recency of the experience , and the use of different data - generating policies for different workers . Finally , we discuss implications for deep reinforcement learning agents that may apply beyond our distributed framework . section : Background paragraph : Distributed Stochastic Gradient Descent Distributed stochastic gradient descent is widely used in supervised learning to speed up training of deep neural networks , by parallelizing the computation of the gradients used to update their parameters . The resulting parameter updates may be applied synchronously krizhevsky2014one or asynchronously Dean:2012:LSD:2999134.2999271 . Both approaches have proven effective and are an increasingly standard part of the deep learning toolbox . Inspired by this , applied distributed asynchronous parameter updates and distributed data generation to deep reinforcement learning . Asynchronous parameter updates and parallel data generation have also been successfully used within a single - machine , in a multi - threaded rather than a distributed context a3c . GPU Asynchronous Actor - Critic [ GA3C; ][] ga3c and Parallel Advantage Actor - Critic [ PAAC; ][] paac adapt this approach to make efficient use of GPUs . paragraph : Distributed Importance Sampling A complementary family of techniques for speeding up training is based on variance reduction by means of importance sampling [ cf. ][] hastings1970monte . This has been shown to be useful in the context of neural networks Hinton2007 - vs . Sampling non - uniformly from a dataset and weighting updates according to the sampling probability in order to counteract the bias thereby introduced can increase the speed of convergence by reducing the variance of the gradients . One way of doing this is to select samples with probability proportional to the norm of the corresponding gradients . In supervised learning , this approach has been successfully extended to the distributed setting alain2015variance . An alternative is to rank samples according to their latest known loss value and make the sampling probability a function of the rank rather than of the loss itself loshchilov2015online . paragraph : Prioritized Experience Replay Experience replay experience - replay has long been used in reinforcement learning to improve data efficiency . It is particularly useful when training neural network function approximators with stochastic gradient descent algorithms , as in Neural Fitted Q - Iteration nfq and Deep Q - Learning dqn . Experience replay may also help to prevent overfitting by allowing the agent to learn from data generated by previous versions of the policy . Prioritized experience replay prioritized - replay extends classic prioritized sweeping ideas prioritized - sweeping to work with deep neural network function approximators . The approach is strongly related to the importance sampling techniques discussed in the previous section , but using a more general class of biased sampling procedures that focus learning on the most \u2018 surprising \u2019 experiences . Biased sampling can be particularly helpful in reinforcement learning , since the reward signal may be sparse and the data distribution depends on the agent \u2019s policy . As a result , prioritized experience replay is used in many agents , such as Prioritized Dueling DQN dueling , UNREAL unreal , DQfD dqfd , and Rainbow rainbow . In an ablation study conducted to investigate the relative importance of several algorithmic ingredients rainbow , prioritization was found to be the most important ingredient contributing to the agent \u2019s performance . section : Our Contribution : Distributed Prioritized Experience Replay In this paper we extend prioritized experience replay to the distributed setting and show that this is a highly scalable approach to deep reinforcement learning . We introduce a few key modifications that enable this scalability , and we refer to our approach as Ape - X . As in Gorila gorila , we decompose the standard deep reinforcement learning algorithm into two parts , which run concurrently with no high - level synchronization . The first part consists of stepping through an environment , evaluating a policy implemented as a deep neural network , and storing the observed data in a replay memory . We refer to this as acting . The second part consists of sampling batches of data from the memory to update the policy parameters . We term this learning . [ width = trim=0pt 400pt 0pt 0pt , clip ] images / apex_architecture.pdf [ t ! ] Actor { algorithmic} [ 1 ] T agent in environment instance , storing experiences . call to obtain latest network parameters . initial state from environment . to T an action using the current policy . the action in the environment . data to local buffer . a background thread , periodically send data to replay . buffered data ( e.g. batch of multi - step transitions ) . priorities for experience ( e.g. absolute TD error ) . call to add experience to replay memory . latest network parameters . [ t ! ] Learner { algorithmic} [ 1 ] network using batches sampled from memory . to T the parameters T times . a prioritized batch of transitions ( in a background thread ) . learning rule ; e.g. double Q - learning or DDPG priorities for experience , ( e.g. absolute TD error ) . call to update priorities . old experience from replay memory . In principle , both acting and learning may be distributed across multiple workers . In our experiments , hundreds of actors run on CPUs to generate data , and a single learner running on a GPU samples the most useful experiences ( Figure [ reference ] ) . Pseudocode for the actors and learners is shown in Algorithms [ reference ] and [ reference ] . Updated network parameters are periodically communicated to the actors from the learner . In contrast to gorila , we use a shared , centralized replay memory , and instead of sampling uniformly , we prioritize , to sample the most useful data more often . Since priorities are shared , high priority data discovered by any actor can benefit the whole system . Priorities can be defined in various ways , depending on the learning algorithm ; two instances are described in the next sections . In Prioritized DQN prioritized - replay priorities for new transitions were initialized to the maximum priority seen so far , and only updated once they were sampled . This does not scale well : due to the large number of actors in our architecture , waiting for the learner to update priorities would result in a myopic focus on the most recent data , which has maximum priority by construction . Instead , we take advantage of the computation the actors in Ape - X are already doing to evaluate their local copies of the policy , by making them also compute suitable priorities for new transitions online . This ensures that data entering the replay has more accurate priorities , at no extra cost . Sharing experiences has certain advantages compared to sharing gradients . Low latency communication is not as important as in distributed SGD , because experience data becomes outdated less rapidly than gradients , provided the learning algorithm is robust to off - policy data . Across the system , we take advantage of this by batching all communications with the centralized replay , increasing the efficiency and throughput at the cost of some latency . With this approach it is even possible for actors and learners to run in different data - centers without limiting performance . Finally , by learning off - policy [ cf. ][] SuttonBarto:1998 , SuttonBarto:2017 , we can further take advantage of Ape - X \u2019s ability to combine data from many distributed actors , by giving the different actors different exploration policies , broadening the diversity of the experience they jointly encounter . As we will see in the results , this can be sufficient to make progress on difficult exploration problems . subsection : Ape - X DQN The general framework we have described may be combined with different learning algorithms . First , we combined it with a variant of DQN dqn with some of the components of Rainbow rainbow . More specifically , we used double Q - learning doubleq , deepdoubleqlearning with multi - step bootstrap targets [ cf. ][] Sutton:1988 , SuttonBarto:1998 , SuttonBarto:2017 , a3c as the learning algorithm , and a dueling network architecture dueling as the function approximator . This results in computing for all elements in the batch the loss with where is a time index for an experience sampled from the replay starting with state and action , and denotes parameters of the target network dqn , a slow moving copy of the online parameters . Multi - step returns are truncated if the episode ends in fewer than steps . In principle , Q - learning variants are off - policy methods , so we are free to choose the policies we use to generate data . However , in practice , the choice of behaviour policy does affect both exploration and the quality of function approximation . Furthermore , we are using a multi - step return with no off - policy correction , which in theory could adversely affect the value estimation . Nonetheless , in Ape - X DQN , each actor executes a different policy , and this allows experience to be generated from a variety of strategies , relying on the prioritization mechanism to pick out the most effective experiences . In our experiments , the actors use - greedy policies with different values of . Low policies allow exploring deeper in the environment , while high policies prevent over - specialization . subsection : Ape - X DPG To test the generality of the framework we also combined it with a continuous - action policy gradient system based on DDPG ddpg , an implementation of deterministic policy gradients also similar to older methods adhdp , acd , and tested it on continuous control tasks from the DeepMind Control Suite tassa2018suite . The Ape - X DPG setup is similar to Ape - X DQN , but the actor \u2019s policy is now represented explicitly by a separate policy network , in addition to the Q - network . The two networks are optimized separately , by minimizing different losses on the sampled experience . We denote the policy and Q - network parameters by and respectively , and adopt the same convention as above to denote target networks . The Q - network outputs an action - value estimate for a given state , and multi - dimensional action . It is updated using temporal - difference learning with a multi - step bootstrap target . The Q - network loss can be written as , where The policy network outputs an action . The policy parameters are updated using policy gradient ascent on the estimated Q - value , using gradient \u2014 note that this depends on the policy parameters only through the action that is input to the critic network . Further details of the Ape - X DPG algorithm are available in the appendix . section : Experiments subsection : Atari [ width= ] images / apex_iclr_aggregate_scatter_jan_04 [ width= ] images / apex_iclr_selected_levels_jan_04 In our first set of experiments we evaluate Ape - X DQN on Atari , and show state of the art results on this standard reinforcement learning benchmark . We use 360 actor machines ( each using one CPU core ) to feed data into the replay memory as fast as they can generate it ; approximately 139 frames per second ( FPS ) each , for a total of 50 K FPS , which corresponds to 12.5 K transitions ( because of a fixed action repeat of 4 ) . The actors batch experience data locally before sending it to the replay : up to 100 transitions may be buffered at a time , which are then sent asynchronously in batches of . The learner asynchronously prefetches up to 16 batches of 512 transitions , and computes updates for 19 such batches each second , meaning that gradients are computed for 9.7 K transitions per second on average . To reduce memory and bandwidth requirements , observation data is compressed using a PNG codec when sent and when stored in the replay . The learner decompresses data as it prefetches it , in parallel with computing and applying gradients . The learner also asynchronously handles any requests for parameters from actors . Actors copy the network parameters from the learner every 400 frames ( 2.8 seconds ) . Each actor executes an - greedy policy where with , . Each is held constant throughout training . The episode length is limited to 50000 frames during training . The capacity of the shared experience replay memory is soft - limited to 2 million transitions : adding new data is always permitted , to not slow down the actors , but every 100 learning steps any excess data above this capacity threshold is removed en masse , in FIFO order . The median actual size of the memory is 2035050 . Data is sampled according to proportional prioritization , with a priority exponent of 0.6 and an importance sampling exponent set to 0.4 . In Figure [ reference ] , on the left , we compare the median human normalized score across all 57 games to several baselines : DQN , Prioritized DQN , Distributional DQN distributional , Rainbow , and Gorila . In all cases the performance is measured at the end of training under the no - op starts testing regime dqn . On the right , we show initial learning curves ( taken from the greediest actor ) for a selection of 6 games ( full learning curves for all games are in the appendix ) . Given that Ape - X can harness substantially more computation than most baselines , one might expect it to train faster . Figure [ reference ] shows that this was indeed the case . Perhaps more surprisingly , our agent achieved a substantially higher final performance . In Table [ reference ] we compare the median human - normalized performance of Ape - X DQN on the Atari benchmark to corresponding metrics as reported for other baseline agents in their respective publications . Whenever available we report results both for no - op starts and for human starts . The human - starts regime gorila corresponds to a more challenging generalization test , as the agent is initialized from random starts drawn from games played by human experts . Ape - X \u2019s performance is higher than the performance of any of the baselines according to both metrics . subsection : Continuous Control In a second set of experiments we evaluated Ape - X DPG on four continuous control tasks . In the manipulator domain the agent must learn to bring a ball to a specified location . In the humanoid domain the agent must learn to control a humanoid body to solve three distinct tasks of increasing complexity : Standing , Walking and Running . Since here we learn from features , rather than from pixels , the observation space is much smaller than it is in the Atari domain . We therefore use small , fully - connected networks ( details in the appendix ) . With 64 actors on this domain , we obtain 14 K total FPS ( the same number of transitions per second ; here we do not use action repeats ) . We process 86 batches of 256 transitions per second , or 22 K transitions processed per second . Figure [ reference ] shows that Ape - X DPG achieved very good performance on all four tasks . The figure shows the performance of Ape - X DPG for different numbers of actors : as the number of actors increases our agent becomes increasingly effective at solving these problems rapidly and reliably , outperforming a standard DDPG baseline trained for over 10 times longer . A parallel paper d4pg builds on this work by combining Ape - X DPG with distributional value functions , and the resulting algorithm is successfully applied to further continuous control tasks . [ width=0.8clip ] images / apex_iclr_continuous_control section : Analysis [ width=0.85trim=0pt 0 0pt 40pt , clip ] images / apex_iclr_num_actors_prioritized_only In this section we describe additional Ape - X DQN experiments on Atari that helped improve our understanding of the framework , and we investigate the contribution of different components . First , we investigated how the performance scales with the number of actors . We trained our agent with different numbers of actors ( 8 , 16 , 32 , 64 , 128 and 256 ) for 35 hours on a subset of 6 Atari games . In all experiments we kept the size of the shared experience replay memory fixed at 1 million transitions . Figure [ reference ] shows that the performance consistently improved as the number of actors increased . The appendix contains learning curves for additional games , and a comparison of the scalability of the algorithm with and without prioritized replay . It is perhaps surprising that performance improved so substantially purely by increasing the number of actors , without changing the rate at which the network parameters are updated , the structure of the network , or the update rule . We hypothesize that the proposed architecture helps with a common deep reinforcement learning failure mode , in which the policy discovered is a local optimum in the parameter space , but not a global one , e.g. , due to insufficient exploration . Using a large number of actors with varying amounts of exploration helps to discover promising new courses of action , and prioritized replay ensures that when this happens , the learning algorithm focuses its efforts on this important information . Next , we investigated varying the capacity of the replay memory ( see Figure [ reference ] ) . We used a setup with 256 actors , for a median of 37 K total environment frames per second ( approximately 9 K transitions ) . With such a large number of actors , the contents of the memory is replaced much faster than in most DQN - like agents . We observed a small benefit to using a larger replay capacity . We hypothesize this is due to the value of keeping some high priority experiences around for longer and replaying them . As above , a single learner machine trained the network with median 19 batches per second , each of 512 transitions , for a median of 9.7 K transitions processed per second . [ width = trim=0pt 0 0pt 40pt , clip ] images / apex_iclr_vary_replay_size.pdf Finally , we ran additional experiments to disentangle potential effects of two confounding factors in our scalability analysis : recency of the experience data in the replay memory , and diversity of the data - generating policies . The full description of these experiments is confined to the appendix ; to summarize , neither factor alone is sufficient to explain the performance we see . We therefore conclude that the results are due substantially to the positive effects of gathering more experience data ; namely better exploration of the environment and better avoidance of overfitting . section : Conclusion We have designed , implemented , and analyzed a distributed framework for prioritized replay in deep reinforcement learning . This architecture achieved state of the art results in a wide range of discrete and continuous tasks , both in terms of wall - clock learning speed and final performance . In this paper we focused on applying the Ape - X framework to DQN and DPG , but it could also be combined with any other off - policy reinforcement learning update . For methods that use temporally extended sequences [ e.g. , ] [ ] a3c , acer , the Ape - X framework may be adapted to prioritize sequences of past experiences instead of individual transitions . Ape - X is designed for regimes in which it is possible to generate large quantities of data in parallel . This includes simulated environments but also a variety of real - world applications , such as robotic arm farms , self - driving cars , online recommender systems , or other multi - user systems in which data is generated by many instances of the same environment [ c.f. ][] concurrent - rl . In applications where data is costly to obtain , our approach will not be directly applicable . With powerful function approximators , overfitting is an issue : generating more training data is the simplest way of addressing it , but may also provide guidance towards data - efficient solutions . Many deep reinforcement learning algorithms are fundamentally limited by their ability to explore effectively in large domains . Ape - X uses a naive yet effective mechanism to address this issue : generating a diverse set of experiences and then identifying and learning from the most useful events . The success of this approach suggests that simple and direct approaches to exploration may be feasible , even for synchronous agents . Our architecture illustrates that distributed systems are now practical both for research and , potentially , large - scale applications of deep reinforcement learning . We hope that the algorithms , architecture , and analysis we have presented will help to accelerate future efforts in this direction . subsubsection : Acknowledgments We would like to acknowledge the contributions of our colleagues at DeepMind , whose input and support has been vital to the success of this work . Thanks in particular to Tom Schaul , Joseph Modayil , Sriram Srinivasan , Georg Ostrovski , Josh Abramson , Todd Hester , Jean - Baptiste Lespiau , Alban Rrustemi and Dan Belov . bibliography : References appendix : Recency of Experience [ width= ] images / apex_iclr_virtual_actors [ width= ] images / apex_iclr_varying_the_pool_of_policies.pdf In our main experiments we do not change the size of the replay memory in proportion to the number of actors , so by changing the number of actors we also increased the rate at which the contents of the replay memory is replaced . This means that in the experiments with more actors , transitions in the replay memory are more recent : they are generated by following policies whose parameters are closer to version of the parameters being optimized by the learner , and in this sense they are more on - policy . Could this alone be sufficient to explain the improved performance ? If so , we might be able to recover the results without needing a large number of actor machines . To test this , we constructed an experiment wherein we replicate the rate at which the contents of the replay memory is replaced in the 256 - actor experiments , but instead of actually using 256 actors , we use 32 actors but add each transition they generate to the replay memory 8 times over . In this setup , the contents of the replay memory is similarly generated by policies with a recent version of the network parameters : the only difference is that the data is not as diverse as in the 256 - actor case . We observe ( see Figure [ reference ] ) that this does not recover the same performance , and therefore conclude that the recency of the experience alone is not sufficient to explain the performance of our method . Indeed , we see that adding the same data multiple times can sometimes harm performance , since although it increases recency this comes at the expense of diversity . Note : in principle , duplicating the added data in this fashion has a similar effect to reducing the capacity of the replay memory , and indeed , our results with a smaller replay memory in Figure [ reference ] do corroborate the finding . However , we test also by duplicating the data primarily in order to exclude any effects arising from the implementation . In particular , in contrast to simply reducing the replay capacity , duplicating each data point means that the computational demands on the replay server in these runs are the same as when we use the corresponding number of real actors . appendix : Varying the Data - Generating Policies Another factor that could conceivably contribute to the scalability of our algorithm is the fact that each actor has a different . To determine the extent to which this impacts upon the performance , we ran an experiment ( see Figure [ reference ] ) with some simple variations on the mechanism we use to choose the policies that generate the data we train on . The first alternative we tested is to choose a small fixed set of 6 values for , instead of the full range that we typically use . In this test , we use prioritized replay as normal , and we find that the results with the full range of are overall slightly better . However , it is not essential for achieving good results within our distributed framework . appendix : Atari : Additional Details The frames received from the environment are preprocessed on the actor side with the standard transformations introduced by DQN . This includes greyscaling , frame stacking , repeating actions 4 times , and clipping rewards to . The learner waits for at least 50000 transitions to be accumulated in the replay before starting learning . We use a Centered RMSProp optimizer with a learning rate of 0.00025 / 4 , decay of 0.95 , epsilon of 1.5e - 7 , and no momentum to minimize the multi - step loss ( with ) . Gradient norms are clipped to 40 . The target network used in the loss calculation is copied from the online network every 2500 training batches . We use the same network as in the Dueling DDQN agent . appendix : Continuous Control : Additional Details The critic network has a layer with 400 units , followed by a tanh activation , followed by another layer of 300 units . The actor network has a layer with 300 units , followed by a tanh activation , followed by another layer of 200 units . The gradient used to update the actor network is clipped to , element - wise . Training uses the Adam optimizer ( ) with learning rate of . The target network used in the loss calculation is copied from the online network every 100 training batches . Replay sampling priorities are set according to the absolute TD error as given by the critic , and are sampled by the learner using proportional prioritized sampling ( see appendix [ reference ] ) with priority exponent . To maintain a fixed replay capacity of , transitions are periodically evicted using proportional prioritized sampling , with priority exponent . This is a different strategy for removing data than in the Atari experiments , which simply removed the oldest data first - it remains to be seen which is superior . Unlike the original DPG algorithm which applies autocorrelated noise sampled from a Ornstein - Uhlenbeck process ( ) , we apply exploration noise to each action sampled from a normal distribution with . Evaluation is performed using the noiseless deterministic policy . Hyperparameters are otherwise as per DQN . Benchmarking was performed in two continuous control domains ( ( a ) Humanoid and ( b ) Manipulator , see Figure [ reference ] ) implemented in the MuJoCo physics simulator ( ) . Humanoid is a humanoid walker with action , state and observation dimensionalities , and respectively . Three Humanoid tasks were considered : walk ( reward for exceeding a minimum velocity ) , run ( reward proportional to movement speed ) and stand ( reward proportional to standing height ) . Manipulator is a 2 - dimensional planar arm with , and , which receives reward for catching a randomly - initialized moving ball . .5 [ width=.6 ] . / images / humanoid.png .5 [ width=.6 ] . / images / manipulator.png appendix : Tuning On Atari , we performed some limited tuning of the learning rate and batch size : we found that larger batch sizes contribute significantly to performance , when using many actors . We tried batch sizes from { 32 , 128 , 256 , 512 , 1024 } , seeing clear benefits up to 512 . We attempted increasing the learning rate to 0.00025 with the larger batch sizes but this destabilized training on some games . We also tried a lower learning rate of 0.00025 / 8 , but this did not reliably improve results . Likewise for continuous control , we experimented with batch sizes { 32 , 128 , 256 , 512 , 1024 } and learning rates from to . We also experimented with the prioritization exponents from to , with results proving essentially consistent within the range [ 0.3 , 0.7 ] ( beyond 0.7 , training would sometimes become unstable and diverge ) . For the experiments with many actors , we set the period for updating network parameters on the actors to be high enough that the learner was not overloaded with requests , and we set the number of transitions that are locally accumulated on each actor to be high enough that the replay server would not be overloaded with network traffic , but we did not otherwise tune those parameters and have not observed them to have significant impact on the learning dynamics . appendix : Implementation The following section makes explicit some of the more practical details that may be of interest to anyone wishing to implement a similar system . paragraph : Data Storage The algorithm is implemented using TensorFlow tensorflow . Replay data is kept in a distributed in - memory key - value store implemented using custom TensorFlow ops , similar to the lookup ops available in core TensorFlow . The ops allow adding , reading , and removing batches of Tensor data efficiently . paragraph : Sampling Data We also implemented ops for efficiently maintaining and sampling from a prioritized distribution over the keys , using the algorithm for proportional prioritization described in . The probability of sampling a transition is where is the priority of the transition with key . The exponent controls the amount of prioritization , and when uniform sampling is recovered . The proportional variant sets priority where is the TD error for transition . Whenever a batch of data is added to or removed from the store , or is processed by the learner , this distribution is correspondingly updated , recording any change to the set of valid keys and the priorities associated with them . A background thread on the learner fetches batches of sampled data from the remote replay and decompresses it using the learner \u2019s CPU , in parallel with the gradients being computed on the GPU . The fetched data is buffered in a TensorFlow queue , so that the GPU always has data available to train on . paragraph : Adding Data In order to efficiently construct - step transition data , each actor maintains a circular buffer of capacity containing tuples , where is the current size of the buffer . With each step , the new data is appended and the accumulated per - step discounts and partial returns for all entries in the buffer are updated . If the buffer has reached its capacity , , then its first element may be combined with the latest state and value estimates to produce a valid - step transition ( with accompanying Q - values ) . However , instead of being directly added to the remote replay memory on each step , the constructed transitions are first stored in a local TensorFlow queue , in order to reduce the number of requests to the replay server . The queue is periodically flushed , at which stage the absolute - step TD - errors ( and thus the initial priorities ) for the queued transitions are computed in batch , using the buffered Q - values to avoid recomputation . The Q - value estimates from which the initial priorities are derived are therefore based on the actor \u2019s copy of the network parameters at the time the corresponding state was obtained from the environment , rather than the latest version on the learner . These Q - values need not be stored after this , since the learner does not require them , although they can be helpful for debugging . A unique key is assigned to each transition , which records which actor and environment step it came from , and the dequeued transition tuples are stored in the remote replay memory . As mentioned in the previous section , the remote sampling distribution is immediately updated with the newly added keys and the corresponding initial priorities computed by the actor . Note that , since we store both the start and the end state with each transition , we are storing some data twice : this costs more RAM , but simplifies the code . paragraph : Contention It is important that the replay server be able to handle all requests in a timely fashion , in order to avoid slowing down the whole system . Possible bottlenecks include CPU , network bandwidth , and any locks protecting the shared data . In our experiments we found CPU to be the main bottleneck , but this was resolved by ensuring all requests and responses use sufficiently large batches . Nonetheless , it is advisable to consider all of these potential performance concerns when designing such systems . paragraph : Asynchronicity In our framework , since acting and learning proceed with no synchronization , and performance depends on both , it can be misleading to consider performance with reference to only one of these . For example , the results after a given total number of environment frames have been experienced are highly dependent on the number of updates the learner has performed in that time . For this reason it is important to monitor and report the speeds of all parts of the system and to consider them when analyzing results . paragraph : Failure Tolerance In distributed systems with many workers , it is inevitable that interruptions or failures will occur , either due to occasional hardware issues or because shared resources are needed by higher priority jobs . All stateful parts of the system therefore must periodically save their work and be able to resume where they left off when restarted . In our system , actors may be interrupted at any time and this will not prevent continued learning , albeit with a temporarily reduced rate of new data entering the replay memory . If the replay server is interrupted , the data it contains is discarded , and upon resuming , the memory is refilled quickly by the actors . In this event , to avoid overfitting , the learner will pause training briefly , until the minimum amount of data has once again been accumulated . If the learner is interrupted , progress will stall until it resumes . [ width=0.99 ] images / apex_iclr_full_sweep_oct_24 [ width=0.99 ] images / apex_iclr_full_sweep_oct_24_data_efficiency [ height=4cm ] images / apex_iclr_scalability_of_data_generation [ width=0.95 ] images / apex_iclr_num_actors", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Atari-57"]]], "Method": [[["Ape-X"]]], "Metric": [[["Medium_Human-Normalized_Score"]]], "Task": [[["Atari_Games"]]]}]}
{"docid": "TST3-SREX-0029", "doctext": "document : Graph2Seq : Graph to Sequence Learning with Attention - Based Neural Networks The celebrated Sequence to Sequence learning ( Seq2Seq ) technique and its numerous variants achieve excellent performance on many tasks . However , many machine learning tasks have inputs naturally represented as graphs ; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence . To address this challenge , we introduce a novel general end - to - end graph - to - sequence neural encoder - decoder model that maps an input graph to a sequence of vectors and uses an attention - based LSTM method to decode the target sequence from these vectors . Our method first generates the node and graph embeddings using an improved graph - based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings . We further introduce an attention mechanism that aligns node embeddings and the decoding sequence to better cope with large graphs . Experimental results on bAbI , Shortest Path , and Natural Language Generation tasks demonstrate that our model achieves state - of - the - art performance and significantly outperforms existing graph neural networks , Seq2Seq , and Tree2Seq models ; using the proposed bi - directional node embedding aggregation strategy , the model can converge rapidly to the optimal performance . section : Introduction The celebrated Sequence to Sequence learning ( Seq2Seq ) technique and its numerous variants achieve excellent performance on many tasks such as Neural Machine Translation bahdanau2014neural , gehring2017convolutional , Natural Language Generation ( NLG ) DBLP : conf / acl / SongPZWG17 and Speech Recognition zhang2017very . Most of the proposed Seq2Seq models can be viewed as a family of encoder - decoders DBLP : conf / nips / SutskeverVL14 , cho2014learning , bahdanau2014neural , where an encoder reads and encodes a source input in the form of sequences into a continuous vector representation of fixed dimension , and a decoder takes the encoded vectors and outputs a target sequence . Many other enhancements including Bidirectional Recurrent Neural Networks ( Bi - RNN ) schuster1997bidirectional or Bidirectional Long Short - Term Memory Networks ( Bi - LSTM ) graves2005framewise as encoder , and attention mechanism bahdanau2014neural , luong2015effective , have been proposed to further improve its practical performance for general or domain - specific applications . Despite their flexibility and expressive power , a significant limitation with the Seq2Seq models is that they can only be applied to problems whose inputs are represented as sequences . However , the sequences are probably the simplest structured data , and many important problems are best expressed with a more complex structure such as graphs that have more capacity to encode complicated pair - wise relationships in the data . For example , one task in NLG applications is to translate a graph - structured semantic representation such as Abstract Meaning Representation to a text expressing its meaning banarescu2013abstract . In addition , path planning for a mobile robot hu2004knowledge and path finding for question answering in bAbI task li2015gated can also be cast as graph - to - sequence problems . On the other hand , even if the raw inputs are originally expressed in a sequence form , it can still benefit from the enhanced inputs with additional information ( to formulate graph inputs ) . For example , for semantic parsing tasks ( text - to - AMR or text - to - SQL ) , they have been shown better performance by augmenting the original sentence sequences with other structural information such as dependency parsing trees pust2015parsing . Intuitively , the ideal solution for graph - to - sequence tasks is to build a more powerful encoder which is able to learn the input representation regardless of its inherent structure . To cope with graph - to - sequence problems , a simple and straightforward approach is to directly convert more complex structured graph data into sequences iyer2016summarizing , gomez2016automatic , liu2017retrosynthetic , and apply sequence models to the resulting sequences . However , the Seq2Seq model often fails to perform as well as hoped on these problems , in part because it inevitably suffers significant information loss due to the conversion of complex structured data into a sequence , especially when the input data is naturally represented as graphs . Recently , a line of research efforts have been devoted to incorporate additional information by extracting syntactic information such as the phrase structure of a source sentence ( Tree2seq ) eriguchi2016tree , by utilizing attention mechanisms for input sets ( Set2seq ) vinyals2015order , and by encoding sentences recursively as trees socher2010learning , tai2015improved . Although these methods achieve promising results on certain classes of problems , most of the presented techniques largely depend on the underlying application and may not be able to generalize to a broad class of problems in a general way . To address this issue , we propose Graph2Seq , a novel general attention - based neural network model for graph - to - sequence learning . The Graph2Seq model follows the conventional encoder - decoder approach with two main components , a graph encoder and a sequence decoder . The proposed graph encoder aims to learn expressive node embeddings and then to reassemble them into the corresponding graph embeddings . To this end , inspired by a recent graph representation learning method hamilton2017inductive , we propose an inductive graph - based neural network to learn node embeddings from node attributes through aggregation of neighborhood information for directed and undirected graphs , which explores two distinct aggregators on each node to yield two representations that are concatenated to form the final node embedding . In addition , we further design an attention - based RNN sequence decoder that takes the graph embedding as its initial hidden state and outputs a target prediction by learning to align and translate jointly based on the context vectors associated with the corresponding nodes and all previous predictions . Our code and data are available at . Graph2Seq is simple yet general and is highly extensible where its two building blocks , graph encoder and sequence decoder , can be replaced by other models such as Graph Convolutional ( Attention ) Networks kipf2016semi , velickovic2017graph or their extensions schlichtkrull2017modeling , and LSTM hochreiter1997long . We highlight three main contributions of this paper as follows : We propose a novel general attention - based neural networks model to elegantly address graph - to - sequence learning problems that learns a mapping between graph - structured inputs to sequence outputs , which current Seq2Seq and Tree2Seq may be inadequate to handle . We propose a novel graph encoder to learn a bi - directional node embeddings for directed and undirected graphs with node attributes by employing various aggregation strategies , and to learn graph - level embedding by exploiting two different graph embedding techniques . Equally importantly , we present an attention mechanism to learn the alignments between nodes and sequence elements to better cope with large graphs . Experimental results show that our model achieves state - of - the - art performance on three recently introduced graph - to - sequence tasks and significantly outperforms existing graph neural networks , Seq2Seq , and Tree2Seq models . section : Related Work Our model draws inspiration from the research fields of graph representation learning , neural networks on graphs , and neural encoder - decoder models . Graph Representation Learning . Graph representation learning has been proven extremely useful for a broad range of the graph - based analysis and prediction tasks hamilton2017representation , goyal2017graph . The main goal for graph representation learning is to learn a mapping that embeds nodes as points in a low - dimensional vector space . These representation learning approaches can be roughly categorized into two classes including matrix factorization - based algorithms and random - walk based methods . A line of research learn the embeddings of graph nodes through matrix factorization roweis2000nonlinear , belkin2002laplacian , ahmed2013distributed , cao2015grarep , ou2016asymmetric . These methods directly train embeddings for individual nodes of training and testing data jointly and thus inherently transductive . Another family of work is the use of random walk - based methods to learn low - dimensional embeddings of nodes by exploring neighborhood information for a single large - scale graph duran2017leanring , hamilton2017inductive , tang2015line , grover2016node2vec , perozzi2014deepwalk , velickovic2017graph . GraphSAGE hamilton2017inductive is such a technique that learns node embeddings through aggregation from a node local neighborhood using node attributes or degrees for inductive learning , which has better capability to generate node embeddings for previously unseen data . Our graph encoder is an extension to GraphSAGE with two major distinctions . First , we non - trivially generalize it to cope with both directed and undirected graphs by splitting original node into forward nodes ( a node directs to ) and backward nodes ( direct to a node ) according to edge direction and applying two distinct aggregation functions to these types of nodes . Second , we exploit two different schemes ( pooling - based and supernode - based ) to reassemble the learned node embeddings to generate graph embedding , which is not studied in GraphSAGE . We show the advantages of our graph encoder over GraphSAGE in our experiments . Neural Networks on Graphs . Over the past few years , there has been a surge of approaches that seek to learn the representations of graph nodes , or entire ( sub ) graphs , based on Graph Neural Networks ( GNN ) that extend well - known network architectures including RNN and CNN to graph data gori2005new , scarselli2009graph , li2015gated , bruna2013spectral , duvenaud2015convolutional , niepert2016learning , defferrard2016convolutional , yang2016revisiting , kipf2016semi , chen2018fastgcn . A line of research is the neural networks that operate on graphs as a form of RNN gori2005new , scarselli2009graph , and recently extended by Li et al . li2015gated by introducing modern practices of RNN ( using of GRU updates ) in the original GNN framework . Another important stream of work that has recently drawn fast increasing interest is graph convolutional networks ( GCN ) built on spectral graph theory , introduced by and then extended by with fast localized convolution . Most of these approaches can not scale to large graphs , which is improved by using a localized first - order approximation of spectral graph convolution kipf2016semi and further equipping with important sampling for deriving a fast GCN chen2018fastgcn . The closely relevant work to our graph encoder is GCN kipf2016semi , which is designed for semi - supervised learning in transductive setting that requires full graph Laplacian to be given during training and is typically applicable to a single large undirected graph . An extension of GCN can be shown to be mathematically related to one variant of our graph encoder on undirected graphs . We compare the difference between our graph encoder and GCN in our experiments . Another relevant work is gated graph sequence neural networks ( GGS - NNs ) li2015gated . Although it is also designed for outputting a sequence , it is essentially a prediction model that learns to predict a sequence embedded in graph while our approach is a generative model that learns a mapping between graph inputs and sequence outputs . A good analogy that can be drawn between our proposed Graph2Seq and GGS - NNs is the relationship between convolutional Seq2Seq and RNN . Neural Encoder - Decoder Models . One of the most successful encoder - decoder architectures is the sequence to sequence learning DBLP : conf / nips / SutskeverVL14 , cho2014learning , bahdanau2014neural , luong2015effective , gehring2017convolutional , which are originally proposed for machine translation . Recently , the classical Seq2Seq model and its variants have been applied to several applications in which these models can perform mappings from objects to sequences , including mapping from an image to a sentence vinyals2015show , models for computation map from problem statements of a python program to their solutions ( the answers to the program ) zaremba2014learning , the traveling salesman problem for the set of points vinyals2015pointer and deep generative model for molecules generation from existing known molecules in drug discovery . It is easy to see that the objects that are mapped to sequences in the listed examples are often naturally represented in graphs rather than sequences . Recently , many research efforts and the key contributions have been made to address the limitations of Seq2Seq when dealing with more complex data , that leverage external information using specialized neural models attached to underlying targeted applications , including Tree2Seq eriguchi2016tree , Set2Seq vinyals2015order , Recursive Neural Networks socher2010learning , and Tree - Structured LSTM tai2015improved . Due to more recent advances in graph representations and graph convolutional networks , a number of research has investigated to utilize various GNN to improve the performance over the Seq2Seq models in the domains of machine translation and graph generation bastings2017graph , beck2018graph , simonovsky2018graphvae , li2018learning . There are several distinctions between these work and ours . First , our model is the first general - purpose encoder - decoder model for graph - to - sequence learning that is applicable to different applications while the aforementioned research has to utilize domain - specific information . Second , we design our own graph embedding techniques for our graph decoder while most of other work directly apply existing GNN to their problems . section : Graph - to - Sequence Model As shown in Figure [ reference ] , our graph - to - sequence model includes a graph encoder , a sequence decoder , and a node attention mechanism . Following the conventional encoder - decoder architecture , the graph encoder first generates node embeddings , and then constructs graph embeddings based on the learned node embeddings . Finally , the sequence decoder takes both the graph embeddings and node embeddings as input and employs attention over the node embeddings whilst generating sequences . In this section , we first introduce the node - embedding generation algorithm which derives the bi - directional node embeddings by aggregating information from both forward and backward neighborhoods of a node in a graph . Upon these node embeddings , we propose two methods for generating graph embeddings capturing the whole - graph information . subsection : Node Embedding Generation Inspired by , we design a new inductive node embedding algorithm that generates bi - directional node embeddings by aggregating information from a node local forward and backward neighborhood within hops for both directed and undirected graphs . In order to make it more clear , we take the embedding generation process for node as an example to explain our node embedding generation algorithm : We first transform node \u2019s text attribute to a feature vector , av , by looking up the embedding matrix W . Note that for some tasks where \u2019s text attribute may be a word sequence , one neural network layer , such as an LSTM layer , could be additionally used to generate av . We categorize the neighbors of into forward neighbors , , and backward neighbors , , according to the edge direction . In particular , returns the nodes that directs to and returns the nodes that direct to ; We aggregate the forward representations of \u2019s forward neighbors { h\u22a2u - k1 , } into a single vector , h\u2062N\u22a2 ( v ) k , where is the iteration index . In our experiments , we find that the aggregator choice , , may heavily affect the overall performance and we will discuss it later . Notice that at iteration , this aggregator only uses the representations generated at . The initial forward representation of each node is its feature vector calculated in step ( 1 ) ; We concatenate \u2019s current forward representation , h\u22a2v - k1 , with the newly generated neighborhood vector , h\u2062N\u22a2 ( v ) k . This concatenated vector is fed into a fully connected layer with nonlinear activation function , which updates the forward representation of , h\u22a2vk , to be used at the next iteration ; We update the backward representation of , h\u22a3vk , using the similar procedure as introduced in step ( 3 ) and ( 4 ) except that operating on the backward representations instead of the forward representations ; We repeat steps ( 3 ) ( 5 ) times , and the concatenation of the final forward and backward representation is used as the final bi - directional representation of . Since the neighbor information from different hops may have different impact on the node embedding , we learn a distinct aggregator at each iteration . Aggregator Architectures . Since a node neighbors have no natural ordering , the aggregator function should be invariant to permutations of its inputs , ensuring that our neural network model can be trained and applied to arbitrarily ordered node - neighborhood feature sets . In practice , we examined the following three aggregator functions : Mean aggregator : This aggregator function takes the element - wise mean of the vectors in { h\u22a2u - k1 , } and { h\u22a3u - k1 , } . LSTM aggregator : Similar to hamilton2017inductive , we also examined a more complex aggregator based on an Long Short Term Memory ( LSTM ) architecture . Note that LSTMs are not inherently symmetric since they process their inputs sequentially . We use LSTMs to operate on unordered sets by simply applying them to a single random permutation of the node neighbors . Pooling aggregator : In this aggregator , each neighbor \u2019s vector is fed through a fully - connected neural network , and an element - wise max - pooling operation is applied : where max denotes the element - wise max operator , and is a nonlinear activation function . By applying max - pooling , the model can capture different information across the neighborhood set . subsection : Graph Embedding Generation Most existing works of graph convolution neural networks focus more on node embeddings rather than graph embeddings since their focus is on the node - wise classification task . However , graph embeddings that convey the entire graph information are essential to the downstream decoder . In this work , we introduce two approaches ( i.e. , Pooling - based and Node - based ) to generate these graph embeddings from the node embeddings . Pooling - based Graph Embedding . In this approach , we investigated three pooling techniques : max - pooling , min - pooling and average - pooling . In our experiments , we fed the node embeddings to a fully - connected neural network and applied each pooling method element - wise . We found no significant performance difference across the three different pooling approaches ; we thus adopt the max - pooling method as our default pooling approach . Node - based Graph Embedding . In this approach , we add one super node , , into the input graph , and all other nodes in the graph direct to . We use the aforementioned node embedding generation algorithm to generate the embedding of by aggregating the embeddings of the neighbor nodes . The embedding of that captures the information of all nodes is regarded as the graph embedding . subsection : Attention Based Decoder The sequence decoder is a Recurrent Neural Network ( RNN ) that predicts the next token , given all the previous words , the RNN hidden state for time , and a context vector that directs attention to the encoder side . In particular , the context vector depends on a set of node representations ( , \u2026 , ) which the graph encoder maps the input graph to . Each node representation contains information about the whole graph with a strong focus on the parts surrounding the - th node of the input graph . The context vector is computed as a weighted sum of these node representations and the weight of each node representation is computed by : where is an which scores how well the input node around position and the output at position match . The score is based on the RNN hidden state and the - th node representation of the input graph . We parameterize the alignment model as a feed - forward neural network which is jointly trained with other components of the proposed system . Our model is jointly trained to maximize the conditional log - probability of the correct description given a source graph . In the inference phase , we use the beam search to generate a sequence with the beam size = 5 . section : Experiments We conduct experiments to demonstrate the effectiveness and efficiency of the proposed method . Following the experimental settings in li2015gated , we firstly compare its performance with classical LSTM , GGS - NN , and GCN based methods on two selected tasks including bAbI Task 19 and the Shortest Path Task . We then compare Graph2Seq against other Seq2Seq based methods on a real - world application - Natural Language Generation Task . Note that the parameters of all baselines are set based on performance on the development set . Experimental Settings . Our proposed model is trained using the Adam optimizer DBLP : journals / corr / KingmaB14 , with mini - batch size 30 . The learning rate is set to 0.001 . We apply the dropout strategy DBLP : journals / jmlr / SrivastavaHKSS14 with a ratio of 0.5 at the decoder layer to avoid overfitting . Gradients are clipped when their norm is bigger than 20 . For the graph encoder , the default hop size is set to 6 , the size of node initial feature vector is set to 40 , the non - linearity function is ReLU DBLP : journals / jmlr / GlorotBB11 , the parameters of aggregators are randomly initialized . The decoder has 1 layer and hidden state size is 80 . Since Graph2Seq with mean aggregator and pooling - based graph embeddings generally performs better than other configurations ( we defer this discussion to Sec . [ reference ] ) , we use this setting as our default model in the following sections . subsection : bAbI Task 19 Setup . The bAbI artificial intelligence ( AI ) tasks DBLP : journals / corr / WestonBCM15 are designed to test reasoning capabilities that an AI system possesses . Among these tasks , Task 19 ( Path Finding ) is arguably the most challenging task ( see , e.g. , DBLP : conf / nips / SukhbaatarSWF15 which reports an accuracy of less than 20 % for all methods that do not use strong supervision ) . We apply the transformation procedure introduced in li2015gated to transform the description as a graph as shown in Figure [ reference ] . The left part shows an instance of bAbI task 19 : given a set of sentences describing the relative geographical positions for a pair of objects and , we aim to find the geographical path between and . The question is then treated as finding the shortest path between two nodes , and , which represent and in the graph . To tackle this problem with Graph2Seq , we annotate with text attribute START and with text attribute END . For other nodes , we assign their IDs in the graph as their text attributes . It is worth noting that , in our model , the START and END tokens are node features whose vector representations are first randomly initialized and then learned by the model later . In contrast , in GGS - NN , the vector representations of staring and end nodes are set as one - hot vectors , which is specially designed for the shortest path task . To aggregate the edge information into the node embedding , for each edge , we additionally add a node representing this edge into the graph and assign the edge \u2019s text as its text attribute . We generate 1000 training examples , 1000 development examples and 1000 test examples where each example is a graph - path pair . We use a standard LSTM model hochreiter1997long and GGS - NN li2015gated as our baselines . Since GCN kipf2016semi itself can not output a sequence , we also create a baseline that combines GCN with our sequence decoder . figurePath Finding Example . Results . From Table [ reference ] , we can see that the LSTM model fails on this task while our model makes perfect predictions , which underlines the importance of the use of graph encoder to directly encode a graph instead of using sequence model on the converted inputs from a graph . Comparing to GGS - NN that uses carefully designed initial embeddings for different types of nodes such as START and END , our model uses a purely end - to - end approach which generates the initial node feature vectors based on random initialization of the embeddings for words in text attributes . However , we still significantly outperform GGS - NN , demonstrating the expressive power of our graph encoder that considers information flows in both forward and backward directions . We observe similar results when comparing our whole Graph2Seq model to GCN with our decoder , which mainly because the current form of GCN kipf2016semi is designed for undirected graph and thus may have information loss when converting directed graph to undirected one as suggested in kipf2016semi . subsection : Shortest Path Task Setup . We further evaluate our model on the Shortest Path ( SP ) Task whose goal is to find the shortest directed path between two nodes in a graph , introduced in li2015gated . For this task , we created datasets by generating random graphs , and choosing pairs random nodes A and B which are connected by a unique shortest directed path . Since we can control the size of generated graphs , we can easily test the performance changes of each model when increasing the size of graphs as well . Two such datasets , SP - S and SP - L , were created , containing S mall ( node size=5 ) and L arge graphs ( node size=100 ) , respectively . We restricted the length of the generated shortest paths for SP - S to be at least 2 and at least 4 for SP - L. For each dataset , we used 1000 training examples and 1000 development examples for parameter tuning , and evaluated on 1000 test examples . We choose the same baselines as introduced in the previous section . Results . Table [ reference ] shows that the LSTM model still fails on both of these two datasets . Our Graph2Seq model achieves comparable performance with GGS - NN that both models could achieve 100 % accuracy on the SP - S dataset while achieves much better on larger graphs on the SP - L dataset . This is because our graph encoder is more expressive in learning the graph structural information with our dual - direction aggregators , which is the key to maintaining good performance when the graph size grows larger , while the performance of GGS - NN significantly degrades due to hardness of capturing the long - range dependence in a graph with large size . Compared to GCN , it achieves better performance than GGS - NN but still much lower than our Graph2Seq , in part because of both the poor effectiveness of graph encoder and incapability of handling with directed graph . subsection : Natural Language Generation Task Setup . We finally evaluate our model on a real - world application - Natural Language Generation ( NLG ) task where we translate a structured semantic representation \u2014 in this case a structured query language ( SQL ) query \u2014 to a natural language description expressing its meaning . As indicated in DBLP : journals / is / SpiliopoulouH92 , the structure of SQL query is essentially a graph . Thus we naturally cast this task as an application of the graph - to - sequence model which takes a graph representing the semantic structure as input and outputs a sequence . Figure [ reference ] illustrates the process of translation of an SQL query to a corresponding natural language description via our Graph2Seq model . We use the BLEU - 4 score to evaluate our model on the WikiSQL dataset zhongSeq2SQL2017 , a corpus of 87 , 726 hand - annotated instances of natural language questions , SQL queries , and SQL tables . WikiSQL was created as the benchmark dataset for the table - based question answering task ( for which the state - of - the - art performance is 82.6 % execution accuracy yu2018typesql ) ; here we reverse the use of the dataset , treating the SQL query as the input and having the goal of generating the correct English question . These WikiSQL SQL queries are split into training , development and test sets , which contain 61297 queries , 9145 queries and 17284 queries , respectively . Since the SQL - to - Text task can be cast as \u201d machine translation \u201d type of problems , we implemented several baselines to address this task . The first one is an attention - based sequence - to - sequence ( Seq2Seq ) model proposed by bahdanau2014neural ; the second one additionally introduces the copy mechanism in the decoder side gu2016incorporating ; the third one is a tree - to - sequence ( Tree2Seq ) model proposed by eriguchi2016tree as our baseline ; the fourth one is to combine a GCN kipf2016semi with our PGE graph embeddings with our sequence decoder ; the fifth one is to combine a GGS - NN li2015gated with our sequence decoder . To apply these baselines , we convert an SQL query to a sequence or a tree using some templates which we discuss in detail in the Appendix . Results . From Table [ reference ] , we can see that our Graph2Seq model performs significantly better than the Seq2Seq , Tree2Seq , and Graph2Seq baselines . This result is expected since the structure of SQL query is essentially a graph despite its expressions in sequence and a graph encoder is able to capture much more information directly in graph . Among all Graph2Seq models , our Graph2Seq model performed best , in part due to a more effective graph encoder . Tree2Seq achieves better performance compared to Seq2Seq since its tree - based encoder explicitly takes the syntactic structure of a SQL query into consideration . Two variants of the Graph2Seq models can substantially outperform Tree2Seq , which demonstrates that a general graph to sequence model that is independent of different structural information in complex data is very useful . Interestingly , we also observe that Graph2Seq - PGE ( pooling - based graph embedding ) performs better than Graph2Seq - NGE ( node - based graph embedding ) . One potential reason is that the node - based graph embedding method artificially added a super node in graph which changes the original graph topology and brings unnecessary noise into the graph . figureA running example of the NLG task . subsection : Impacts of Aggregator , Hop Size and Attention Mechanism on Garph2Seq Model Setup . We now investigate the impact of the aggregator and the hop size on the Graph2Seq model . Following the previous SP task , we further create three synthetic datasets : i ) SDP\u2062DAG whose graphs are directed acyclic graphs ( DAGs ) ; ii ) SDP\u2062DCG whose graphs are directed cyclic graphs ( DCGs ) that always contain cycles ; iii ) SDP\u2062SEQ whose graphs are essentially sequential lines . For each dataset , we randomly generated 10000 graphs with the graph size 100 and split them as 8000 / 1000 / 1000 for the training / development / test set . For each graph , we generated an SDP query by choosing two random nodes with the constraints that there should be a unique shortest path connecting these two nodes , and that its length should be at least 4 . We create six variants of the Graph2Seq model coupling with different aggregation strategies in the node embedding generation . The first three ( Graph2Seq - MA , - LA , - PA ) use the M ean A ggregator , L STM A ggregator and P ooling A ggregator to aggregate node neighbor information , respectively . Unlike these three models that aggregate the information of both forward and backward nodes , the other two models ( Graph2Seq - MA - F , - MA - B ) only consider one - way information aggregating the information from the forward nodes or the information from the backward nodes with the mean aggregator , respectively . We use the path accuracy to evaluate these models . The hop size is set to 10 . Impacts of the Aggregator . Table [ reference ] shows that on the SDP dataset , both Graph2Seq - MA and Graph2Seq - PA achieve the best performance . On more complicated structured data , such as SDP and SDP , Graph2Seq - MA ( our default model ) also performs better than other variants . We can also see that Graph2Seq - MA performs better than Graph2Seq - MA - F and Graph2Seq - MA - B on SDP and SDP since it captures more information from both directions to learn better node embeddings . However , Graph2Seq - MA - F and Graph2Seq - MA - B achieve comparable performance to Graph2Seq - MA on SDP . This is because in almost 95 % of the graphs , 90 % of the nodes could reach each other by traversing the graph for a given hop size , which dramatically restores its information loss . figureTest Results on SDP . Impact of Hop Size . To study the impact of the hop size , we create a SDP dataset , SDP and results are shown in Figure [ reference ] . We see that the performance of all variants of Graph2Seq converges to its optimal performance when increasing the number of hop size . Specifically , Graph2Seq - MA achieves significantly better performance than its counterparts considering only one direction propagation , especially when the hop size is small . As the hop size increases , the performance differences diminish . This is the desired property since Graph2Seq - MA can use much smaller hop size ( about the half ) to achieve the same performance of Graph2Seq - MA - F or Graph2Seq - MA - B with a larger size . This is particularly useful for large graphs where increasing hop size may need considerable computing resources and long run - time . We also compare Graph2Seq with GCN , where the hop size means the number of layers in the settings of GCN . Surprisingly , even Graph2Seq - MA - F or Graph2Seq - MA - B can significantly outperform GCN with the same hope size despite its rough equivalence between these two architectures . It again illustrates the importance of the methods that could take into account both directed and undirected graphs . For additional experimental results on the impact of hop size for graphs of different sizes , please refer to the Table 4 in Appendix C. Impact of Attention Mechanism . To investigate the impact of attention mechanism to the Graph2Seq model , we still evaluate our model on SDP , SDP and SDP datasets but without considering the attention strategy . As shown in Table 4 , we find that the attention strategy significantly improves the performance of all variants of Graph2Seq by at least 14.9 % . This result is expected since for larger graphs it is more difficult for the encoder to compress all necessary information into a fixed - length vector ; as intended , applying the attention mechanism in decoding enabled our proposed Graph2Seq model to successfully handle large graphs . section : Conclusion In this paper , we study the graph - to - sequence problem , introducing a new general and flexible Graph2Seq model that follows the encoder - decoder architecture . We showed that , using our proposed bi - directional node embedding aggregation strategy , the graph encoder could successfully learn representations for three representative classes of directed graph , i.e. , directed acyclic graphs , directed cyclic graphs and sequence - styled graphs . Experimental results on three tasks demonstrate that our model significantly outperforms existing graph neural networks , Seq2Seq , and Tree2Seq baselines on both synthetic and real application datasets . We also showed that introducing an attention mechanism over node representation into the decoding substantially enhances the ability of our model to produce correct target sequences from large graphs . Since much symbolic data is represented as graphs and many tasks express their desired outputs as sequences , we expect Graph2Seq to be broadly applicable to unify symbolic AI and beyond . bibliography : References appendix : Pseudo - code of the Graph - to - sequence Algorithm [ h ] Node embedding generation algorithm { algorithmic} [ 1 ] Graph \u2062G ( V , E ) ; node initial feature vector av , \u2208\u2200vV ; hops K ; weight matrices Wk , \u2208\u2200k{1 , \u2026 , K } ; non - linearity \u03c3 ; aggregator functions AGGREGATEk\u22a2 , AGGREGATEk\u22a3 , \u2208\u2200k{1 , \u2026 , K } ; neighborhood functions N\u22a2 , N\u22a3 Vector representations zv for all \u2208vV \u2190 av , \u2208\u2200vV \u2190 av , \u2208\u2200vV \u2190 AGGREGATEk\u22a2 ( {h\u22a2u - k1 , \u2208\u2200u\u2062N\u22a2 ( v ) } ) \u2190 \u03c3 ( W\u22c5k CONCAT ( h\u22a2v - k1 , h\u2062N\u22a2 ( v ) k ) ) h\u2062N\u22a3 ( v ) k \u2190 AGGREGATEk\u22a3 ( {h\u22a3u - k1 , \u2208\u2200u\u2062N\u22a3 ( v ) } ) \u2190 \u03c3 ( W\u22c5k CONCAT ( h\u22a3v - k1 , h\u2062N\u22a3 ( v ) k ) ) \u2190 CONCAT ( h\u22a2vK , h\u22a3vK ) , \u2208\u2200vV Algorithm [ reference ] describes the embedding generation process where the entire graph and initial feature vectors for all nodes av , , are provided as input . Here denotes the current hop in the outer loop . The h\u22a2vk denotes node \u2019s forward representation which aggregates the information of nodes in . Similarly , the h\u22a3vk denotes node \u2019s backward representation which is generated by aggregating the information of nodes in . Each step in the outer loop of Algorithm [ reference ] proceeds as follows . First , each node in a graph aggregates the forward representations of the nodes in its immediate neighborhood , { h\u22a2u - k1 , } , into a single vector , h\u2062N\u22a2 ( v ) k ( line 5 ) . Note that this aggregation step depends on the representations generated at the previous iteration of the outer loop , , and the forward representations are defined as the input node feature vector . After aggregating the neighboring feature vectors , we concatenate the node current forward representation , h\u22a2v - k1 , with the aggregated neighborhood vector , h\u2062N\u22a2 ( v ) k . Then this concatenated vector is fed through a fully connected layer with nonlinear activation function , which updates the forward representation of the current node to be used at the next step of the algorithm ( line 6 ) . We apply similar process to generate the backward representations of the nodes ( line 7 , 8 ) . Finally , the representation of each node zv is the concatenation of the forward representation ( i.e. , h\u22a2vK ) and the backward representation ( i.e. , h\u22a3vK ) at the last iteration . appendix : Structured Representation of the SQL Query To apply Graph2Seq , Seq2Seq and Tree2Seq models on the natural language generation task , we need to convert the SQL query to a graph , sequence and tree , respectively . In this section , we describe these representations of the SQL query . subsection : Sequence Representation We apply a simple template to construct the SQL query sequence : \u201c SELECT + aggregation function > + Split Symbol + selected column > + WHERE + condition0 > + Split Symbol + condition1 > + \u2026 \u201d . subsection : Tree Representation We apply the SQL Parser tool to convert an SQL query to a tree which is illustrated in Figure [ reference ] . Specifically , the root of this tree has two child nodes , namely SELECT LIST and WHERE CLAUSE . The child nodes of SELECT LIST node are the selected columns in the SQL query . The WHERE CLAUSE node has all occurred logical operators in the SQL query as its children . The children of a logical operator node are the columns on which this operator works . subsection : Graph Representation We use the following method to transform the SQL query to a graph : SELECT Clause . For the SELECT clause such as \u201c SELECT company \u201d , we first create a node assigned with text attribute select . This SELECT node connects with column nodes whose text attributes are the selected column names such as company . For the SQL queries that contain aggregation functions such as count or max , we add one aggregation node which is connected with the column node \u2014 their text attributes are the aggregation function names . WHERE Clause . The WHERE clause usually contains more than one condition . For each condition , we use the same process as for the SELECT clause to create nodes . For example , in Figure [ reference ] , we create node assets and for the first condition , the node sales and for the second condition . We then integrate the constraint nodes that have the same text attribute ( e.g. , in Figure [ reference ] ) . For a logical operator such as AND , OR and NOT , we create a node that connects with all column nodes that the operator works on ( e.g. , AND in Figure [ reference ] ) . These logical operator nodes then connect with SELECT node . appendix : More Results on the Impact of Hop Size In Algorithm [ reference ] , we can see that there are three key factors in the node embedding generation . The first factor is the aggregator choice which determines how information from neighborhood nodes is combined . The other two are the hop size ( ) and the neighborhood function ( , ) , which together determine which neighbor nodes should be aggregated to generate each node embedding . To study the impact of the hop size in our model , we create two SDP datasets , SDP and SDP , where each graph has 100 nodes or 1000 nodes , respectively . Both of these two datasets contain 8000 training examples , 1000 dev examples and 1000 test examples . We evaluated three models , Graph2Seq - MA - F , Graph2Seq - MA - B and Graph2Seq - MA , on these two datasets ; results are listed in Table [ reference ] . We see that Graph2Seq - MA - F and Graph2Seq - MA - B could show significant performance improvements with increasing the hop size . Specifically , on the SDP dataset , Graph2Seq - MA - F and Graph2Seq - MA - B achieve their best performance when the hop size reaches 7 ; further increases do not improve the overall performance . A similar situation is also observed on the SDP dataset ; performance converges at the hop size of 85 . Interestingly , the average diameters of the graphs in the two datasets are 6.8 and 80.2 , respectively , suggesting that the ideal hop size for best Graph2Seq - MA - F performance should be the graph diameter . This should not be surprising ; if the hop size equals the graph diameter , each node is guaranteed to aggregate the information of all reachable nodes on the graph within its embedding . Note that in the experiments on SDP , in the ( \u00bf 10 ) hop , we always use the aggregator in the 10 - th hop , because introducing too many aggregators ( i.e. , parameters ) may make the model over - fitting . Like Graph2Seq - MA - F , Graph2Seq - MA also benefited from increasing the hop size . However , on both datasets , Graph2Seq - MA could reach peak performance at a smaller hop size than Graph2Seq - MA - F. For example , on the SDP dataset , Graph2Seq - MA achieves 99.2 % accuracy once the hop size is greater than 4 while Graph2Seq - MA - F requires a hop size greater than 7 to achieve comparable accuracy ; similar observations hold for the SDP dataset . Moreover , we can see that the minimum required hop size that Graph2Seq - MA could achieve its best performance is approximately the average radii ( c.f . diameter ) of the graphs , which are 3.4 and 40.1 , respectively . Recall that the main difference between Graph2Seq - MA and Graph2Seq - MA - F ( or Graph2Seq - MA - B ) lies in whether the system aggregates information propagated from backward nodes ; the performance difference indicates that by incorporating forward and backward nodes \u2019 information , it is possible for the model to achieve the best performance by traversing less of the graph . This is useful in practice , especially for large graphs where increasing hop size may consume considerable computing resources and run - time . Table [ reference ] also makes clear the utility of the attention strategy ; the performance of both Graph2Seq - MA - F and Graph2Seq - MA decreases by at least 9.8 % on SDP and 14.9 % on SDP . This result is expected , since for larger graphs it is more difficult for the encoder to compress all necessary information into a fixed - length vector ; as intended , applying the attention mechanism in decoding enabled our proposed Graph2Seq model to handle large graphs successfully . As shown in Algorithm [ reference ] , the neighborhood function takes a given node as input and returns its directly connected neighbor nodes , which are then fed to the node embedding generator . Intuitively , to obtain a better representation of a node , this function should return all its neighbor nodes in the graph . However , this may result in high training times on large graphs . To address this , hamilton2017inductive proposes a sampling method which randomly selects a fixed number of neighbor nodes from which to aggregate information at each hop . We use this sampling method to manage the neighbor node size at each aggregation step .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["WikiSQL"]]], "Method": [[["Graph2Seq-PGE"]]], "Metric": [[["BLEU-4"]]], "Task": [[["SQL-to-Text"]]]}]}
{"docid": "TST3-SREX-0030", "doctext": "document : You Only Look Once : Unified , Real - Time Object Detection We present YOLO , a new approach to object detection . Prior work on object detection repurposes classifiers to perform detection . Instead , we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities . A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation . Since the whole detection pipeline is a single network , it can be optimized end - to - end directly on detection performance . Our unified architecture is extremely fast . Our base YOLO model processes images in real - time at 45 frames per second . A smaller version of the network , Fast YOLO , processes an astounding 155 frames per second while still achieving double the mAP of other real - time detectors . Compared to state - of - the - art detection systems , YOLO makes more localization errors but is less likely to predict false positives on background . Finally , YOLO learns very general representations of objects . It outperforms other detection methods , including DPM and R - CNN , when generalizing from natural images to other domains like artwork . section : Introduction Humans glance at an image and instantly know what objects are in the image , where they are , and how they interact . The human visual system is fast and accurate , allowing us to perform complex tasks like driving with little conscious thought . Fast , accurate algorithms for object detection would allow computers to drive cars without specialized sensors , enable assistive devices to convey real - time scene information to human users , and unlock the potential for general purpose , responsive robotic systems . Current detection systems repurpose classifiers to perform detection . To detect an object , these systems take a classifier for that object and evaluate it at various locations and scales in a test image . Systems like deformable parts models ( DPM ) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image . More recent approaches like R - CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes . After classification , post - processing is used to refine the bounding boxes , eliminate duplicate detections , and rescore the boxes based on other objects in the scene . These complex pipelines are slow and hard to optimize because each individual component must be trained separately . We reframe object detection as a single regression problem , straight from image pixels to bounding box coordinates and class probabilities . Using our system , you only look once ( YOLO ) at an image to predict what objects are present and where they are . YOLO is refreshingly simple : see Figure [ reference ] . A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes . YOLO trains on full images and directly optimizes detection performance . This unified model has several benefits over traditional methods of object detection . First , YOLO is extremely fast . Since we frame detection as a regression problem we do n\u2019t need a complex pipeline . We simply run our neural network on a new image at test time to predict detections . Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps . This means we can process streaming video in real - time with less than 25 milliseconds of latency . Furthermore , YOLO achieves more than twice the mean average precision of other real - time systems . For a demo of our system running in real - time on a webcam please see our project webpage : . Second , YOLO reasons globally about the image when making predictions . Unlike sliding window and region proposal - based techniques , YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance . Fast R - CNN , a top detection method , mistakes background patches in an image for objects because it ca n\u2019t see the larger context . YOLO makes less than half the number of background errors compared to Fast R - CNN . Third , YOLO learns generalizable representations of objects . When trained on natural images and tested on artwork , YOLO outperforms top detection methods like DPM and R - CNN by a wide margin . Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs . YOLO still lags behind state - of - the - art detection systems in accuracy . While it can quickly identify objects in images it struggles to precisely localize some objects , especially small ones . We examine these tradeoffs further in our experiments . All of our training and testing code is open source . A variety of pretrained models are also available to download . section : Unified Detection We unify the separate components of object detection into a single neural network . Our network uses features from the entire image to predict each bounding box . It also predicts all bounding boxes across all classes for an image simultaneously . This means our network reasons globally about the full image and all the objects in the image . The YOLO design enables end - to - end training and real - time speeds while maintaining high average precision . Our system divides the input image into an grid . If the center of an object falls into a grid cell , that grid cell is responsible for detecting that object . Each grid cell predicts bounding boxes and confidence scores for those boxes . These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts . Formally we define confidence as . If no object exists in that cell , the confidence scores should be zero . Otherwise we want the confidence score to equal the intersection over union ( IOU ) between the predicted box and the ground truth . Each bounding box consists of 5 predictions : , , , , and confidence . The coordinates represent the center of the box relative to the bounds of the grid cell . The width and height are predicted relative to the whole image . Finally the confidence prediction represents the IOU between the predicted box and any ground truth box . Each grid cell also predicts conditional class probabilities , . These probabilities are conditioned on the grid cell containing an object . We only predict one set of class probabilities per grid cell , regardless of the number of boxes . At test time we multiply the conditional class probabilities and the individual box confidence predictions , which gives us class - specific confidence scores for each box . These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object . For evaluating YOLO on Pascal VOC , we use , . Pascal VOC has 20 labelled classes so . Our final prediction is a tensor . subsection : Network Design We implement this model as a convolutional neural network and evaluate it on the Pascal VOC detection dataset . The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates . Our network architecture is inspired by the GoogLeNet model for image classification . Our network has 24 convolutional layers followed by 2 fully connected layers . Instead of the inception modules used by GoogLeNet , we simply use reduction layers followed by convolutional layers , similar to Lin et al . The full network is shown in Figure [ reference ] . We also train a fast version of YOLO designed to push the boundaries of fast object detection . Fast YOLO uses a neural network with fewer convolutional layers ( 9 instead of 24 ) and fewer filters in those layers . Other than the size of the network , all training and testing parameters are the same between YOLO and Fast YOLO . The final output of our network is the tensor of predictions . subsection : Training We pretrain our convolutional layers on the ImageNet 1000 - class competition dataset . For pretraining we use the first 20 convolutional layers from Figure [ reference ] followed by a average - pooling layer and a fully connected layer . We train this network for approximately a week and achieve a single crop top - 5 accuracy of 88 % on the ImageNet 2012 validation set , comparable to the GoogLeNet models in Caffe \u2019s Model Zoo . We use the Darknet framework for all training and inference . We then convert the model to perform detection . Ren et al . show that adding both convolutional and connected layers to pretrained networks can improve performance . Following their example , we add four convolutional layers and two fully connected layers with randomly initialized weights . Detection often requires fine - grained visual information so we increase the input resolution of the network from to . Our final layer predicts both class probabilities and bounding box coordinates . We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1 . We parametrize the bounding box and coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1 . We use a linear activation function for the final layer and all other layers use the following leaky rectified linear activation : We optimize for sum - squared error in the output of our model . We use sum - squared error because it is easy to optimize , however it does not perfectly align with our goal of maximizing average precision . It weights localization error equally with classification error which may not be ideal . Also , in every image many grid cells do not contain any object . This pushes the \u201c confidence \u201d scores of those cells towards zero , often overpowering the gradient from cells that do contain objects . This can lead to model instability , causing training to diverge early on . To remedy this , we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that do n\u2019t contain objects . We use two parameters , and to accomplish this . We set and . Sum - squared error also equally weights errors in large boxes and small boxes . Our error metric should reflect that small deviations in large boxes matter less than in small boxes . To partially address this we predict the square root of the bounding box width and height instead of the width and height directly . YOLO predicts multiple bounding boxes per grid cell . At training time we only want one bounding box predictor to be responsible for each object . We assign one predictor to be \u201c responsible \u201d for predicting an object based on which prediction has the highest current IOU with the ground truth . This leads to specialization between the bounding box predictors . Each predictor gets better at predicting certain sizes , aspect ratios , or classes of object , improving overall recall . During training we optimize the following , multi - part loss function : where denotes if object appears in cell and denotes that the th bounding box predictor in cell is \u201c responsible \u201d for that prediction . Note that the loss function only penalizes classification error if an object is present in that grid cell ( hence the conditional class probability discussed earlier ) . It also only penalizes bounding box coordinate error if that predictor is \u201c responsible \u201d for the ground truth box ( i.e. has the highest IOU of any predictor in that grid cell ) . We train the network for about 135 epochs on the training and validation data sets from Pascal VOC 2007 and 2012 . When testing on 2012 we also include the VOC 2007 test data for training . Throughout training we use a batch size of 64 , a momentum of and a decay of . Our learning rate schedule is as follows : For the first epochs we slowly raise the learning rate from to . If we start at a high learning rate our model often diverges due to unstable gradients . We continue training with for 75 epochs , then for 30 epochs , and finally for 30 epochs . To avoid overfitting we use dropout and extensive data augmentation . A dropout layer with rate = .5 after the first connected layer prevents co - adaptation between layers . For data augmentation we introduce random scaling and translations of up to 20 % of the original image size . We also randomly adjust the exposure and saturation of the image by up to a factor of in the HSV color space . subsection : Inference Just like in training , predicting detections for a test image only requires one network evaluation . On Pascal VOC the network predicts 98 bounding boxes per image and class probabilities for each box . YOLO is extremely fast at test time since it only requires a single network evaluation , unlike classifier - based methods . The grid design enforces spatial diversity in the bounding box predictions . Often it is clear which grid cell an object falls in to and the network only predicts one box for each object . However , some large objects or objects near the border of multiple cells can be well localized by multiple cells . Non - maximal suppression can be used to fix these multiple detections . While not critical to performance as it is for R - CNN or DPM , non - maximal suppression adds 2 - 3 % in mAP . subsection : Limitations of YOLO YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class . This spatial constraint limits the number of nearby objects that our model can predict . Our model struggles with small objects that appear in groups , such as flocks of birds . Since our model learns to predict bounding boxes from data , it struggles to generalize to objects in new or unusual aspect ratios or configurations . Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image . Finally , while we train on a loss function that approximates detection performance , our loss function treats errors the same in small bounding boxes versus large bounding boxes . A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU . Our main source of error is incorrect localizations . section : Comparison to Other Detection Systems Object detection is a core problem in computer vision . Detection pipelines generally start by extracting a set of robust features from input images ( Haar , SIFT , HOG , convolutional features ) . Then , classifiers or localizers are used to identify objects in the feature space . These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image . We compare the YOLO detection system to several top detection frameworks , highlighting key similarities and differences . Deformable parts models . Deformable parts models ( DPM ) use a sliding window approach to object detection . DPM uses a disjoint pipeline to extract static features , classify regions , predict bounding boxes for high scoring regions , etc . Our system replaces all of these disparate parts with a single convolutional neural network . The network performs feature extraction , bounding box prediction , non - maximal suppression , and contextual reasoning all concurrently . Instead of static features , the network trains the features in - line and optimizes them for the detection task . Our unified architecture leads to a faster , more accurate model than DPM . R - CNN . R - CNN and its variants use region proposals instead of sliding windows to find objects in images . Selective Search generates potential bounding boxes , a convolutional network extracts features , an SVM scores the boxes , a linear model adjusts the bounding boxes , and non - max suppression eliminates duplicate detections . Each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow , taking more than 40 seconds per image at test time . YOLO shares some similarities with R - CNN . Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features . However , our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object . Our system also proposes far fewer bounding boxes , only 98 per image compared to about 2000 from Selective Search . Finally , our system combines these individual components into a single , jointly optimized model . Other Fast Detectors Fast and Faster R - CNN focus on speeding up the R - CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search . While they offer speed and accuracy improvements over R - CNN , both still fall short of real - time performance . Many research efforts focus on speeding up the DPM pipeline . They speed up HOG computation , use cascades , and push computation to GPUs . However , only 30Hz DPM actually runs in real - time . Instead of trying to optimize individual components of a large detection pipeline , YOLO throws out the pipeline entirely and is fast by design . Detectors for single classes like faces or people can be highly optimized since they have to deal with much less variation . YOLO is a general purpose detector that learns to detect a variety of objects simultaneously . Deep MultiBox . Unlike R - CNN , Szegedy et al . train a convolutional neural network to predict regions of interest instead of using Selective Search . MultiBox can also perform single object detection by replacing the confidence prediction with a single class prediction . However , MultiBox can not perform general object detection and is still just a piece in a larger detection pipeline , requiring further image patch classification . Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system . OverFeat . Sermanet et al . train a convolutional neural network to perform localization and adapt that localizer to perform detection . OverFeat efficiently performs sliding window detection but it is still a disjoint system . OverFeat optimizes for localization , not detection performance . Like DPM , the localizer only sees local information when making a prediction . OverFeat can not reason about global context and thus requires significant post - processing to produce coherent detections . MultiGrasp . Our work is similar in design to work on grasp detection by Redmon et al . Our grid approach to bounding box prediction is based on the MultiGrasp system for regression to grasps . However , grasp detection is a much simpler task than object detection . MultiGrasp only needs to predict a single graspable region for an image containing one object . It does n\u2019t have to estimate the size , location , or boundaries of the object or predict it \u2019s class , only find a region suitable for grasping . YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image . section : Experiments First we compare YOLO with other real - time detection systems on Pascal VOC 2007 . To understand the differences between YOLO and R - CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R - CNN , one of the highest performing versions of R - CNN . Based on the different error profiles we show that YOLO can be used to rescore Fast R - CNN detections and reduce the errors from background false positives , giving a significant performance boost . We also present VOC 2012 results and compare mAP to current state - of - the - art methods . Finally , we show that YOLO generalizes to new domains better than other detectors on two artwork datasets . subsection : Comparison to Other Real - Time Systems Many research efforts in object detection focus on making standard detection pipelines fast . However , only Sadeghi et al . actually produce a detection system that runs in real - time ( 30 frames per second or better ) . We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz . While the other efforts do n\u2019t reach the real - time milestone we also compare their relative mAP and speed to examine the accuracy - performance tradeoffs available in object detection systems . Fast YOLO is the fastest object detection method on Pascal ; as far as we know , it is the fastest extant object detector . With mAP , it is more than twice as accurate as prior work on real - time detection . YOLO pushes mAP to while still maintaining real - time performance . We also train YOLO using VGG - 16 . This model is more accurate but also significantly slower than YOLO . It is useful for comparison to other detection systems that rely on VGG - 16 but since it is slower than real - time the rest of the paper focuses on our faster models . Fastest DPM effectively speeds up DPM without sacrificing much mAP but it still misses real - time performance by a factor of 2 . It also is limited by DPM \u2019s relatively low accuracy on detection compared to neural network approaches . R - CNN minus R replaces Selective Search with static bounding box proposals . While it is much faster than R - CNN , it still falls short of real - time and takes a significant accuracy hit from not having good proposals . Fast R - CNN speeds up the classification stage of R - CNN but it still relies on selective search which can take around 2 seconds per image to generate bounding box proposals . Thus it has high mAP but at fps it is still far from real - time . The recent Faster R - CNN replaces selective search with a neural network to propose bounding boxes , similar to Szegedy et al . In our tests , their most accurate model achieves 7 fps while a smaller , less accurate one runs at 18 fps . The VGG - 16 version of Faster R - CNN is 10 mAP higher but is also 6 times slower than YOLO . The Zeiler - Fergus Faster R - CNN is only 2.5 times slower than YOLO but is also less accurate . subsection : VOC 2007 Error Analysis To further examine the differences between YOLO and state - of - the - art detectors , we look at a detailed breakdown of results on VOC 2007 . We compare YOLO to Fast R - CNN since Fast R - CNN is one of the highest performing detectors on Pascal and it \u2019s detections are publicly available . We use the methodology and tools of Hoiem et al . For each category at test time we look at the top N predictions for that category . Each prediction is either correct or it is classified based on the type of error : Correct : correct class and Localization : correct class , Similar : class is similar , Other : class is wrong , Background : for any object Figure [ reference ] shows the breakdown of each error type averaged across all 20 classes . YOLO struggles to localize objects correctly . Localization errors account for more of YOLO \u2019s errors than all other sources combined . Fast R - CNN makes much fewer localization errors but far more background errors . 13.6 % of it \u2019s top detections are false positives that do n\u2019t contain any objects . Fast R - CNN is almost 3x more likely to predict background detections than YOLO . subsection : Combining Fast R - CNN and YOLO YOLO makes far fewer background mistakes than Fast R - CNN . By using YOLO to eliminate background detections from Fast R - CNN we get a significant boost in performance . For every bounding box that R - CNN predicts we check to see if YOLO predicts a similar box . If it does , we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes . The best Fast R - CNN model achieves a mAP of 71.8 % on the VOC 2007 test set . When combined with YOLO , its mAP increases by 3.2 % to 75.0 % . We also tried combining the top Fast R - CNN model with several other versions of Fast R - CNN . Those ensembles produced small increases in mAP between .3 and .6 % , see Table [ reference ] for details . The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R - CNN . Rather , it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R - CNN \u2019s performance . Unfortunately , this combination does n\u2019t benefit from the speed of YOLO since we run each model seperately and then combine the results . However , since YOLO is so fast it does n\u2019t add any significant computational time compared to Fast R - CNN . subsection : VOC 2012 Results On the VOC 2012 test set , YOLO scores 57.9 % mAP . This is lower than the current state of the art , closer to the original R - CNN using VGG - 16 , see Table [ reference ] . Our system struggles with small objects compared to its closest competitors . On categories like bottle , sheep , and tv / monitor YOLO scores 8 - 10 % lower than R - CNN or Feature Edit . However , on other categories like cat and train YOLO achieves higher performance . Our combined Fast R - CNN + YOLO model is one of the highest performing detection methods . Fast R - CNN gets a 2.3 % improvement from the combination with YOLO , boosting it 5 spots up on the public leaderboard . subsection : Generalizability : Person Detection in Artwork [ b ] .45 [ b ] .55 Academic datasets for object detection draw the training and testing data from the same distribution . In real - world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before . We compare YOLO to other detection systems on the Picasso Dataset and the People - Art Dataset , two datasets for testing person detection on artwork . Figure [ reference ] shows comparative performance between YOLO and other detection methods . For reference , we give VOC 2007 detection AP on person where all models are trained only on VOC 2007 data . On Picasso models are trained on VOC 2012 while on People - Art they are trained on VOC 2010 . R - CNN has high AP on VOC 2007 . However , R - CNN drops off considerably when applied to artwork . R - CNN uses Selective Search for bounding box proposals which is tuned for natural images . The classifier step in R - CNN only sees small regions and needs good proposals . DPM maintains its AP well when applied to artwork . Prior work theorizes that DPM performs well because it has strong spatial models of the shape and layout of objects . Though DPM does n\u2019t degrade as much as R - CNN , it starts from a lower AP . YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork . Like DPM , YOLO models the size and shape of objects , as well as relationships between objects and where objects commonly appear . Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects , thus YOLO can still predict good bounding boxes and detections . section : Real - Time Detection In The Wild YOLO is a fast , accurate object detector , making it ideal for computer vision applications . We connect YOLO to a webcam and verify that it maintains real - time performance , including the time to fetch images from the camera and display the detections . The resulting system is interactive and engaging . While YOLO processes images individually , when attached to a webcam it functions like a tracking system , detecting objects as they move around and change in appearance . A demo of the system and the source code can be found on our project website : . section : Conclusion We introduce YOLO , a unified model for object detection . Our model is simple to construct and can be trained directly on full images . Unlike classifier - based approaches , YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly . Fast YOLO is the fastest general - purpose object detector in the literature and YOLO pushes the state - of - the - art in real - time object detection . YOLO also generalizes well to new domains making it ideal for applications that rely on fast , robust object detection . Acknowledgements : This work is partially supported by ONR N00014 - 13 - 1 - 0720 , NSF IIS - 1338054 , and The Allen Distinguished Investigator Award . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["YOLO"]]], "Metric": [[["FPS"]]], "Task": [[["Real-Time_Object_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["YOLO"]]], "Metric": [[["MAP"]]], "Task": [[["Object_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["YOLO"]]], "Metric": [[["MAP"]]], "Task": [[["Real-Time_Object_Detection"]]]}]}
{"docid": "TST3-SREX-0031", "doctext": "In this work , we build on recent advances in distributional reinforcement learning to give a generally applicable , flexible , and state - of - the - art distributional variant of DQN . We achieve this by using quantile regression to approximate the full quantile function for the state - action return distribution . By reparameterizing a distribution over the sample space , this yields an implicitly defined return distribution and gives rise to a large class of risk - sensitive policies . We demonstrate improved performance on the 57 Atari 2600 games in the ALE , and use our algorithm \u2019s implicitly defined distributions to study the effects of risk - sensitive policies in Atari games . ImplicitQuantileNetworksforDistributionalReinforcementLearning section : Introduction Distributional reinforcement learning focuses on the intrinsic randomness of returns within the reinforcement learning ( RL ) framework . As the agent interacts with the environment , irreducible randomness seeps in through the stochasticity of these interactions , the approximations in the agent \u2019s representation , and even the inherently chaotic nature of physical interaction . Distributional RL aims to model the distribution over returns , whose mean is the traditional value function , and to use these distributions to evaluate and optimize a policy . Any distributional RL algorithm is characterized by two aspects : the parameterization of the return distribution , and the distance metric or loss function being optimized . Together , these choices control assumptions about the random returns and how approximations will be traded off . Categorical DQN [ C51 ] c51 combines a categorical distribution and the cross - entropy loss with the Cram\u00e9r - minimizing projection . For this , it assumes returns are bounded in a known range and trades off mean - preservation at the cost of overestimating variance . C51 outperformed all previous improvements to DQN on a set of 57 Atari 2600 games in the Arcade Learning Environment , which we refer to as the Atari - 57 benchmark . Subsequently , several papers have built upon this successful combination to achieve significant improvements to the state - of - the - art in Atari - 57 , and challenging continuous control tasks . These algorithms are restricted to assigning probabilities to an a priori fixed , discrete set of possible returns . dabney2017qr propose an alternate pair of choices , parameterizing the distribution by a uniform mixture of Diracs whose locations are adjusted using quantile regression . Their algorithm , QR - DQN , while restricted to a discrete set of quantiles , automatically adapts return quantiles to minimize the Wasserstein distance between the Bellman updated and current return distributions . This flexibility allows QR - DQN to significantly improve on C51 \u2019s Atari - 57 performance . In this paper , we extend the approach of dabney2017qr , from learning a discrete set of quantiles to learning the full quantile function , a continuous map from probabilities to returns . When combined with a base distribution , such as , this forms an implicit distribution capable of approximating any distribution over returns given sufficient network capacity . Our approach , implicit quantile networks ( IQN ) , is best viewed as a simple distributional generalization of the DQN algorithm , and provides several benefits over QR - DQN . First , the approximation error for the distribution is no longer controlled by the number of quantiles output by the network , but by the size of the network itself , and the amount of training . Second , IQN can be used with as few , or as many , samples per update as desired , providing improved data efficiency with increasing number of samples per training update . Third , the implicit representation of the return distribution allows us to expand the class of policies to more fully take advantage of the learned distribution . Specifically , by taking the base distribution to be non - uniform , we expand the class of policies to - greedy policies on arbitrary distortion risk measures . We begin by reviewing distributional reinforcement learning , related work , and introducing the concepts surrounding risk - sensitive RL . In subsequent sections , we introduce our proposed algorithm , IQN , and present a series of experiments using the Atari - 57 benchmark , investigating the robustness and performance of IQN . Despite being a simple distributional extension to DQN , and forgoing any other improvements , IQN significantly outperforms QR - DQN and nearly matches the performance of Rainbow , which combines many orthogonal advances . In fact , in human - starts as well as in the hardest Atari games ( where current RL agents still underperform human players ) IQN improves over Rainbow . section : Background / Related Work We consider the standard RL setting , in which the interaction of an agent and an environment is modeled as a Markov Decision Process , where and denote the state and action spaces , the ( state - and action - dependent ) reward function , the transition kernel , and a discount factor . A policy maps a state to a distribution over actions . For an agent following policy , the discounted sum of future rewards is denoted by the random variable , where , , , and . The action - value function is defined as , and can be characterized by the Bellman equation The objective in RL is to find an optimal policy , which maximizes , i.e. for all and all . One approach is to find the unique fixed point of the Bellman optimality operator : To this end , Q - learning iteratively improves an estimate , , of the optimal action - value function , , by repeatedly applying the Bellman update : The action - value function can be approximated by a parameterized function ( e.g. a neural network ) , and trained by minimizing the squared temporal difference ( TD ) error , over samples observed while following an - greedy policy over . This policy acts greedily with respect to with probability and uniformly at random otherwise . DQN uses a convolutional neural network to parameterize and the Q - learning algorithm to achieve human - level play on the Atari - 57 benchmark . subsection : Distributional RL In distributional RL , the distribution over returns ( the law of ) is considered instead of the scalar value function that is its expectation . This change in perspective has yielded new insights into the dynamics of RL , and been a useful tool for analysis . Empirically , distributional RL algorithms show improved sample complexity and final performance , as well as increased robustness to hyperparameter variation . An analogous distributional Bellman equation of the form can be derived , where denotes that two random variables and have equal probability laws , and the random variables and are distributed according to and , respectively . morimura10parametric defined the distributional Bellman operator explicitly in terms of conditional probabilities , parameterized by the mean and scale of a Gaussian or Laplace distribution , and minimized the Kullback - Leibler ( KL ) divergence between the Bellman target and the current estimated return distribution . However , the distributional Bellman operator is not a contraction in the KL . As with the scalar setting , a distributional Bellman optimality operator can be defined by with distributed according to . While the distributional Bellman operator for policy evaluation is a contraction in the - Wasserstein distance , this no longer holds for the control case . Convergence to the optimal policy can still be established , but requires a more involved argument . c51 parameterize the return distribution as a categorical distribution over a fixed set of equidistant points and minimize the KL divergence to the projected distributional Bellman target . Their algorithm , C51 , outperformed previous DQN variants on the Atari - 57 benchmark . Subsequently , hessel2018rainbow combined C51 with enhancements such as prioritized experience replay , - step updates , and the dueling architecture , leading to the Rainbow agent , current state - of - the - art in Atari - 57 . The categorical parameterization , using the projected KL loss , has also been used in recent work to improve the critic of a policy gradient algorithm , D4PG , achieving significantly improved robustness and state - of - the - art performance across a variety of continuous control tasks . subsection : - Wasserstein Metric The - Wasserstein metric , for , plays a key role in recent results in distributional RL . It has also been a topic of increasing interest in generative modeling , because unlike the KL divergence , the Wasserstein metric inherently trades off approximate solutions with likelihoods . The - Wasserstein distance is the metric on inverse cumulative distribution functions ( c.d.f . ) , also known as quantile functions . For random variables and with quantile functions and , respectively , the - Wasserstein distance is given by The class of optimal transport metrics express distances between distributions in terms of the minimal cost for transporting mass to make the two distributions identical . This cost is given in terms of some metric , , on the underlying space . The - Wasserstein metric corresponds to . We are particularly interested in the Wasserstein metrics due to the predominant use of spaces in mean - value reinforcement learning . subsection : Quantile Regression for Distributional RL c51 showed that the distributional Bellman operator is a contraction in the - Wasserstein metric , but as the proposed algorithm did not itself minimize the Wasserstein metric , this left a theory - practice gap for distributional RL . Recently , this gap was closed , in both directions . First and most relevant to this work , dabney2017qr proposed the use of quantile regression for distributional RL and showed that by choosing the quantile targets suitably the resulting projected distributional Bellman operator is a contraction in the - Wasserstein metric . Concurrently , rowland2018analysis showed the original class of categorical algorithms are a contraction in the Cram\u00e9r distance , the metric on cumulative distribution functions . By estimating the quantile function at precisely chosen points , QR - DQN minimizes the Wasserstein distance to the distributional Bellman target . This estimation uses quantile regression , which has been shown to converge to the true quantile function value when minimized using stochastic approximation . In QR - DQN , the random return is approximated by a uniform mixture of Diracs , with each assigned a fixed quantile target , for , where . These quantile estimates are trained using the huber1964robust quantile regression loss , with threshold , on the pairwise TD - errors At the time of this writing , QR - DQN achieves the best performance on Atari - 57 , human - normalized mean and median , of all agents that do not combine distributional RL , prioritized replay , and - step updates . subsection : Risk in Reinforcement Learning Distributional RL algorithms have been theoretically justified for the Wasserstein and Cram\u00e9r metrics , and learning the distribution over returns , in and of itself , empirically results in significant improvements to data efficiency , final performance , and stability . However , in each of these recent works the policy used was based entirely on the mean of the return distribution , just as in standard reinforcement learning . A natural question arises : can we expand the class of policies using information provided by the distribution over returns ( i.e. to the class of risk - sensitive policies ) ? Furthermore , when would this larger policy class be beneficial ? Here , \u2018 risk \u2019 refers to the uncertainty over possible outcomes , and risk - sensitive policies are those which depend upon more than the mean of the outcomes . At this point , it is important to highlight the difference between intrinsic uncertainty , captured by the distribution over returns , and parametric uncertainty , the uncertainty over the value estimate typically associated with Bayesian approaches such as PSRL and Kalman TD . Distributional RL seeks to capture the former , which classic approaches to risk are built upon . Expected utility theory states that if a decision policy is consistent with a particular set of four axioms regarding its choices then the decision policy behaves as though it is maximizing the expected value of some utility function , This is perhaps the most pervasive notion of risk - sensitivity . A policy maximizing a linear utility function is called risk - neutral , whereas concave or convex utility functions give rise to risk - averse or risk - seeking policies , respectively . Many previous studies on risk - sensitive RL adopt the utility function approach . A crucial axiom of expected utility is independence : given random variables , and , such that ( preferred over ) , any mixture between and is preferred to the same mixture between and . Stated in terms of the cumulative probability functions , . This axiom in particular has troubled many researchers because it is consistently violated by human behavior . The Allais paradox is a frequently used example of a decision problem where people violate the independence axiom of expected utility theory . However , as yaari1987dual showed , this axiom can be replaced by one in terms of convex combinations of outcome values , instead of mixtures of distributions . Specifically , if as before , then for any and random variable , . This leads to an alternate , dual , theory of choice than that of expected utility . Under these axioms the decision policy behaves as though it is maximizing a distorted expectation , for some continuous monotonic function : Such a function is known as a distortion risk measure , as it distorts the cumulative probabilities of the random variable . That is , we have two fundamentally equivalent approaches to risk - sensitivity . Either , we choose a utility function and follow the expectation of this utility . Or , we choose a reweighting of the distribution and compute expectation under this distortion measure . Indeed , yaari1987dual further showed that these two functions are inverses of each other . The choice between them amounts to a choice over whether the behavior should be invariant to mixing with random events or to convex combinations of outcomes . Distortion risk measures include , as special cases , cumulative probability weighting used in cumulative prospect theory , conditional value at risk , and many other methods . Recently majumdar2017should argued for the use of distortion risk measures in robotics . section : Implicit Quantile Networks We now introduce the implicit quantile network ( IQN ) , a deterministic parametric function trained to reparameterize samples from a base distribution , e.g. , to the respective quantile values of a target distribution . IQN provides an effective way to learn an implicit representation of the return distribution , yielding a powerful function approximator for a new DQN - like agent . Let be the quantile function at for the random variable . For notational simplicity we write , thus for the resulting state - action return distribution sample is . We propose to model the state - action quantile function as a mapping from state - actions and samples from some base distribution , typically , to , viewed as samples from the implicitly defined return distribution . Let be a distortion risk measure , with identity corresponding to risk - neutrality . Then , the distorted expectation of under is given by Notice that the distorted expectation is equal to the expected value of weighted by , that is , . The immediate implication of this is that for any , there exists a sampling distribution for such that the mean of is equal to the distorted expectation of under , that is , any distorted expectation can be represented as a weighted sum over the quantiles . Denote by the risk - sensitive greedy policy For two samples , and policy , the sampled temporal difference ( TD ) error at step is Then , the IQN loss function is given by where and denote the respective number of iid samples used to estimate the loss . A corresponding sample - based risk - sensitive policy is obtained by approximating in Equation [ reference ] by samples of : Implicit quantile networks differ from the approach of dabney2017qr in two ways . First , instead of approximating the quantile function at fixed values of we approximate it with for some differentiable functions , , and . If we ignore the distributional interpretation for a moment and view each as a separate action - value function , this highlights that implicit quantile networks are a type of universal value function approximator ( UVFA ) . There may be additional benefits to implicit quantile networks beyond the obvious increase in representational fidelity . As with UVFAs , we might hope that training over many different \u2019s ( goals in the case of the UVFA ) leads to better generalization between values and improved sample complexity than attempting to train each separately . Second , , , and are sampled from continuous , independent , distributions . Besides , we also explore risk - sentive policies , with non - linear . The independent sampling of each , results in the sample TD errors being decorrelated , and the estimated action - values go from being the true mean of a mixture of Diracs to a sample mean of the implicit distribution defined by reparameterizing the sampling distribution via the learned quantile function . subsection : Implementation Consider the neural network structure used by the DQN agent . Let be the function computed by the convolutional layers and the subsequent fully - connected layers mapping to the estimated action - values , such that . For our network we use the same functions and as in DQN , but include an additional function computing an embedding for the sample point . We combine these to form the approximation , where denotes the element - wise ( Hadamard ) product . As the network for is not particularly deep , we use the multiplicative form , , to force interaction between the convolutional features and the sample embedding . Alternative functional forms , e.g. concatenation or a \u2018 residual \u2019 function , are conceivable , and can be parameterized in different ways . To investigate these , we compared performance across a number of architectural variants on six Atari 2600 games ( Asterix , Assault , Breakout , Ms . Pacman , QBert , Space Invaders ) . Full results are given in the Appendix . Despite minor variation in performance , we found the general approach to be robust to the various choices . Based upon the results we used the following function in our later experiments , for embedding dimension : After settling on a network architecture , we study the effect of the number of samples , and , used in the estimate terms of Equation [ reference ] . We hypothesized that , the number of samples of , would affect the sample complexity of IQN , with larger values leading to faster learning , and that with one would potentially approach the performance of DQN . This would support the hypothesis that the improved performance of many distributional RL algorithms rests on their effect as auxiliary loss functions , which would vanish in the case of . Furthermore , we believed that , the number of samples of , would affect the variance of the gradient estimates much like a mini - batch size hyperparameter . Our prediction was that would have the greatest effect on variance of the long - term performance of the agent . We used the same set of six games as before , with our chosen architecture , and varied . In Figure [ reference ] we report the average human - normalized scores on the six games for each configuration . Figure [ reference ] ( left ) shows the average performance over the first ten million frames , while ( right ) shows the average performance over the last ten million ( from 190 M to 200 M ) . As expected , we found that has a dramatic effect on early performance , shown by the continual improvement in score as the value increases . Additionally , we observed that affected performance very differently than expected : it had a strong effect on early performance , but minimal impact on long - term performance past . Overall , while using more samples for both distributions is generally favorable , appears to be sufficient to achieve the majority of improvements offered by IQN for long - term performance , with variation past this point largely insignificant . To our surprise we found that even for , which is comparable to DQN in the number of loss components , the longer term performance is still quite strong ( DQN ) . In an informal evaluation , we did not find IQN to be sensitive to , the number of samples used for the policy , and have fixed it at for all experiments . section : Risk - Sensitive Reinforcement Learning In this section , we explore the effects of varying the distortion risk measure , , away from identity . This only affects the policy , , used both in Equation [ reference ] and for acting in the environment . As we have argued , evaluating under different distortion risk measures is equivalent to changing the sampling distribution for , allowing us to achieve various forms of risk - sensitive policies . We focus on a handful of sampling distributions and their corresponding distortion measures . The first one is the cumulative probability weighting parameterization proposed in cumulative prospect theory : In particular , we use the parameter value found by wu1996curvature to most closely match human subjects . This choice is interesting as , unlike the others we consider , it is neither globally convex nor concave . For small values of it is locally concave and for larger values of it becomes locally convex . Recall that concavity corresponds to risk - averse and convexity to risk - seeking policies . Second , we consider the distortion risk measure proposed by wang2000class , where and are taken to be the standard Normal cumulative distribution function and its inverse : For , this produces risk - averse policies and we include it due to its simple interpretation and ability to switch between risk - averse and risk - seeking distortions . Third , we consider a simple power formula for risk - averse ( ) or risk - seeking ( ) policies : Finally , we consider conditional value - at - risk ( CVaR ) : CVaR has been widely studied in and out of reinforcement learning . Its implementation as a modification to the sampling distribution of is particularly simple , as it changes to . Another interesting sampling distribution , not included in our experiments , is denoted and corresponds to sampled by averaging samples from . In Figure [ reference ] ( right ) we give an example of a distribution ( Neutral ) and how each of these distortion measures affects the implied distribution due to changing the sampling distribution of . and reduce the impact of the tails of the distribution , while and heavily shift the distribution mass towards the tails , creating a risk - averse or risk - seeking preference . Additionally , while CVaR entirely ignores all values corresponding to , gives these non - zero , but vanishingly small , probability . By using these sampling distributions we can induce various risk - sensitive policies in IQN . We evaluate these on the same set of six Atari 2600 games previously used . Our algorithm simply changes the policy to maximize the distorted expectations instead of the usual sample mean . Figure [ reference ] ( left ) shows our results in this experiment , with average scores reported under the usual , risk - neutral , evaluation criterion . Intuitively , we expected to see a qualitative effect from risk - sensitive training , e.g. strengthened exploration from a risk - seeking objective . Although we did see qualitative differences , these did not always match our expectations . For two of the games , Asterix and Assault , there is a very significant advantage to the risk - averse policies . Although tends to perform almost identically to the standard risk - neutral policy , and the risk - seeking performs as well or worse than risk - neutral , we find that both risk - averse policies improve performance over standard IQN . However , we also observe that the more risk - averse of the two , , suffers some loss in performance on two other games ( QBert and Space Invaders ) . Additionally , we note that the risk - seeking policy significantly underperforms the risk - neutral policy on three of the six games . It remains an open question as to exactly why we see improved performance for risk - averse policies . There are many possible explanations for this phenomenon , e.g. that risk - aversion encodes a heuristic to stay alive longer , which in many games is correlated with increased rewards . section : Full Atari - 57 Results Finally , we evaluate IQN on the full Atari - 57 benchmark , comparing with the state - of - the - art performance of Rainbow , a distributional RL agent that combines several advances in deep RL , the closely related algorithm QR - DQN , prioritized experience replay DQN , and the original DQN agent . Note that in this section we use the risk - neutral variant of the IQN , that is , the policy of the IQN agent is the regular - greedy policy with respect to the mean of the state - action return distribution . It is important to remember that Rainbow builds upon the distributional RL algorithm C51 , but also includes prioritized experience replay , Double DQN , Dueling Network architecture , Noisy Networks , and multi - step updates . In particular , besides the distributional update , - step updates and prioritized experience replay were found to have significant impact on the performance of Rainbow . Our other competitive baseline is QR - DQN , which is currently state - of - the - art for agents that do not combine distributional updates , - step updates , and prioritized replay . Thus , between QR - DQN and the much more complex Rainbow we compare to the two most closely related , and best performing , agents in published work . In particular , we would expect that IQN would benefit from the additional enhancements in Rainbow , just as Rainbow improved significantly over C51 . Figure [ reference ] shows the mean ( left ) and median ( right ) human - normalized scores during training over the Atari - 57 benchmark . IQN dramatically improves over QR - DQN , which itself improves on many previously published results . At 100 million frames IQN has reached the same level of performance as QR - DQN at 200 million frames . Table [ reference ] gives a comparison between the same methods in terms of their best , human - normalized , scores per game under the 30 random no - op start condition . These are averages over the given number of seeds . Additionally , using human - starts , IQN achieves median human - normalized score , whereas Rainbow reaches , see Table [ reference ] . Finally , we took a closer look at the games in which each algorithm continues to underperform humans , and computed , on average , how far below human - level they perform . We refer to this value as the human - gapThanks to Joseph Modayil for proposing this metric . metric and give results in Table [ reference ] . Interestingly , C51 outperforms QR - DQN in this metric , and IQN outperforms all others . This shows that the remaining gap between Rainbow and IQN is entirely from games on which both algorithms are already super - human . The games where the most progress in RL is needed happen to be the games where IQN shows the greatest improvement over QR - DQN and Rainbow . section : Discussion and Conclusions We have proposed a generalization of recent work based around using quantile regression to learn the distribution over returns of the current policy . Our generalization leads to a simple change to the DQN agent to enable distributional RL , the natural integration of risk - sensitive policies , and significantly improved performance over existing methods . The IQN algorithm provides , for the first time , a fully integrated distributional RL agent without prior assumptions on the parameterization of the return distribution . IQN can be trained with as little as a single sample from each state - action value distribution , or as many as computational limits allow to improve the algorithm \u2019s data efficiency . Furthermore , IQN allows us to expand the class of control policies to a large class of risk - sensitive policies connected to distortion risk measures . Finally , we show substantial gains on the Atari - 57 benchmark over QR - DQN , and even halving the distance between QR - DQN and Rainbow . Despite the significant empirical successes in this paper there are many areas in need of additional theoretical analysis . We highlight a few particularly relevant open questions we were unable to address in the present work . First , sample - based convergence results have been recently shown for a class of categorical distributional RL algorithms . Could existing sample - based RL convergence results be extended to the QR - based algorithms ? Second , can the contraction mapping results for a fixed grid of quantiles given by dabney2017qr be extended to the more general class of approximate quantile functions studied in this work ? Finally , and particularly salient to our experiments with distortion risk measures , theoretical guarantees for risk - sensitive RL have been building over recent years , but have been largely limited to special cases and restricted classes of risk - sensitive policies . Can the convergence of the distribution of returns under the Bellman operator be leveraged to show convergence to a fixed - point in distorted expectations ? In particular , can the control results of c51 be expanded to cover some class of risk - sensitive policies ? There remain many intriguing directions for future research into distributional RL , even on purely empirical fronts . hessel2018rainbow recently showed that distributional RL agents can be significantly improved , when combined with other techniques . Creating a Rainbow - IQN agent could yield even greater improvements on Atari - 57 . We also recall the surprisingly rich return distributions found by barthmaron2018d4pg , and hypothesize that the continuous control setting may be a particularly fruitful area for the application of distributional RL in general , and IQN in particular . bibliography : References section : Appendix subsection : Architecture and Hyperparameters We considered multiple architectural variants for parameterizing an IQN . All of these build on the Q - network of a regular DQN , which can be seen as the composition of a convolutional stack and an MLP , and extend it by an embedding of the sample point , , and a merging function , resulting in the function For the embedding , we considered a number of variants : a learned linear embedding , a learned MLP embedding with a single hidden layer of size , and a learned linear function of cosine basis functions of the form . Each of those was followed by either a ReLU or sigmoid nonlinearity . For the merging function , the simplest choice would be a simple vector concatenation of and . Note however , that the MLP which takes in the output of and outputs the action - value quantiles , only has a single hidden layer in the DQN network . Therefore , to force a sufficiently early interaction between the two representations , we also considered a multiplicative function , where denotes the element - wise ( Hadamard ) product of two vectors , as well as a \u2018 residual \u2019 function . Early experiments showed that a simple linear embedding of was insufficient to achieve good performance , and the residual version of did n\u2019t show any marked difference to the multiplicative variant , so we do not include results for these here . For the other configurations , Figure [ reference ] shows pairwise comparisons between 1 ) a cosine basis function embedding and a completely learned MLP embedding , 2 ) an embedding size ( hidden layer size or number of cosine basis elements ) 32 and 64 , 3 ) ReLU and sigmoid nonlinearity following the embedding , and 4 ) concatenation and a multiplicative interaction between and . Each comparison \u2018 violin plot \u2019 can be understood as a marginalization over the other variants of the architecture , with the human - normalized performance at the end of training , averaged across six Atari 2600 games , on the y - axis . Each white dot corresponds to a configuration ( each represented by two seeds ) , the black dots show the position of our preferred configuration . The width of the colored regions corresponds to a kernel density estimate of the number of configurations at each performance level . Our final choice is a multiplicative interaction with a linear function of a cosine embedding , with and a ReLU nonlinearity ( see Equation [ reference ] ) , as this configuration yielded the highest performance consistently over multiple seeds . Also noteworthy is the overall robustness of the approach to these variations : most of the configurations consistently outperform the QR - DQN baseline shown as a grey horizontal line for comparison . We give pseudo - code for the IQN loss in Algorithm [ reference ] . All other hyperparameters for this agent correspond to the ones used by dabney2017qr . In particular , the Bellman target is computed using a target network . Notice that IQN will generally be more computationally expensive per - sample than QR - DQN . However , in practice IQN requires many fewer samples per update than QR - DQN so that the actual running times are comparable . [ ht ] Implicit Quantile Network Loss and functions , # Compute greedy next action # Sample quantile thresholds # Compute distributional temporal differences # Compute Huber quantile loss subsection : Evaluation The human - normalized scores reported in this paper are given by the formula where , and are the per - game raw scores ( undiscounted returns ) for the given agent , a reference human player , and random agent baseline . The \u2018 human - gap \u2019 metric referred to at the end of Section [ reference ] builds on the human - normalized score , but emphasizes the remaining improvement for the agent to reach super - human performance . It is given by , with a value of corresponding to random play , and a value of corresponding to super - human level of performance . To avoid degeneracies in the case of , the quantity is being clipped above at .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Alien"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Amidar"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Assault"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Asterix"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Asteroids"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Atlantis"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Bank_Heist"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Battle_Zone"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Beam_Rider"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Berzerk"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Bowling"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Boxing"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Breakout"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Centipede"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Chopper_Command"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Crazy_Climber"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Defender"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Demon_Attack"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Double_Dunk"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Enduro"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Fishing_Derby"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Freeway"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Frostbite"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Gopher"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Gravitar"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_HERO"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Ice_Hockey"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_James_Bond"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Kangaroo"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Krull"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Kung-Fu_Master"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Montezuma_s_Revenge"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Ms__Pacman"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Name_This_Game"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Phoenix"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Pitfall_"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Pong"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Private_Eye"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Q_Bert"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_River_Raid"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Road_Runner"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Robotank"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Seaquest"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Skiing"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Solaris"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Space_Invaders"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Star_Gunner"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Surround"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Tennis"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Time_Pilot"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Tutankham"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Up_and_Down"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Venture"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Video_Pinball"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Wizard_of_Wor"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Yars_Revenge"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Atari_2600_Zaxxon"]]], "Method": [[["IQN"]]], "Metric": [[["Score"]]], "Task": [[["Atari_Games"]]]}]}
{"docid": "TST3-SREX-0032", "doctext": "document : SeqGAN : Sequence Generative Adversarial Nets with Policy Gradient As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data . However , it has limitations when the goal is for generating sequences of discrete tokens . A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model . Also , the discriminative model can only assess a complete sequence , while for a partially generated sequence , it is non - trivial to balance its current score and the future one once the entire sequence has been generated . In this paper , we propose a sequence generation framework , called SeqGAN , to solve the problems . Modeling the data generator as a stochastic policy in reinforcement learning ( RL ) , SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update . The RL reward signal comes from the GAN discriminator judged on a complete sequence , and is passed back to the intermediate state - action steps using Monte Carlo search . Extensive experiments on synthetic data and real - world tasks demonstrate significant improvements over strong baselines . section : Introduction Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning . Recently , recurrent neural networks ( RNNs ) with long short - term memory ( LSTM ) cells have shown excellent performance ranging from natural language generation to handwriting generation . The most common approach to training an RNN is to maximize the log predictive likelihood of each true token in the training sequence given the previous observed tokens . However , as argued in , the maximum likelihood approaches suffer from so - called exposure bias in the inference stage : the model generates a sequence iteratively and predicts next token conditioned on its previously predicted ones that may be never observed in the training data . Such a discrepancy between training and inference can incur accumulatively along with the sequence and will become prominent as the length of sequence increases . To address this problem , proposed a training strategy called scheduled sampling ( SS ) , where the generative model is partially fed with its own synthetic data as prefix ( observed tokens ) rather than the true data when deciding the next token in the training stage . Nevertheless , showed that SS is an inconsistent training strategy and fails to address the problem fundamentally . Another possible solution of the training / inference discrepancy problem is to build the loss function on the entire generated sequence instead of each transition . For instance , in the application of machine translation , a task specific sequence score / loss , bilingual evaluation understudy ( BLEU ) , can be adopted to guide the sequence generation . However , in many other practical applications , such as poem generation and chatbot , a task specific loss may not be directly available to score a generated sequence accurately . General adversarial net ( GAN ) proposed by is a promising framework for alleviating the above problem . Specifically , in GAN a discriminative net learns to distinguish whether a given data instance is real or not , and a generative net learns to confuse by generating high quality data . This approach has been successful and been mostly applied in computer vision tasks of generating samples of natural images . Unfortunately , applying GAN to generating sequences has two problems . Firstly , GAN is designed for generating real - valued , continuous data but has difficulties in directly generating sequences of discrete tokens , such as texts . The reason is that in GANs , the generator starts with random sampling first and then a determistic transform , govermented by the model parameters . As such , the gradient of the loss from w.r.t . the outputs by is used to guide the generative model ( paramters ) to slightly change the generated value to make it more realistic . If the generated data is based on discrete tokens , the \u201c slight change \u201d guidance from the discriminative net makes little sense because there is probably no corresponding token for such slight change in the limited dictionary space . Secondly , GAN can only give the score / loss for an entire sequence when it has been generated ; for a partially generated sequence , it is non - trivial to balance how good as it is now and the future score as the entire sequence . In this paper , to address the above two issues , we follow and consider the sequence generation procedure as a sequential decision making process . The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated . Unlike the work in that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model . To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy . In our policy gradient , we employ Monte Carlo ( MC ) search to approximate the state - action value . We directly train the policy ( generative model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional GAN . Extensive experiments based on synthetic and real data are conducted to investigate the efficacy and properties of the proposed SeqGAN . In our synthetic data environment , SeqGAN significantly outperforms the maximum likelihood methods , scheduled sampling and PG - BLEU . In three real - world tasks , i.e. poem generation , speech language generation and music generation , SeqGAN significantly outperforms the compared baselines in various metrics including human expert judgement . section : Related Work Deep generative models have recently drawn significant attention , and the ability of learning over large ( unlabeled ) data endows them with more potential and vitality . first proposed to use the contrastive divergence algorithm to efficiently training deep belief nets ( DBN ) . proposed denoising autoencoder ( DAE ) that learns the data distribution in a supervised learning fashion . Both DBN and DAE learn a low dimensional representation ( encoding ) for each data instance and generate it from a decoding network . Recently , variational autoencoder ( VAE ) that combines deep learning with statistical inference intended to represent a data instance in a latent hidden space , while still utilizing ( deep ) neural networks for non - linear mapping . The inference is done via variational methods . All these generative models are trained by maximizing ( the lower bound of ) training data likelihood , which , as mentioned by , suffers from the difficulty of approximating intractable probabilistic computations . proposed an alternative training methodology to generative models , i.e. GANs , where the training procedure is a minimax game between a generative model and a discriminative model . This framework bypasses the difficulty of maximum likelihood learning and has gained striking successes in natural image generation . However , little progress has been made in applying GANs to sequence discrete data generation problems , e.g. natural language generation . This is due to the generator network in GAN is designed to be able to adjust the output continuously , which does not work on discrete data generation . On the other hand , a lot of efforts have been made to generate structured sequences . Recurrent neural networks can be trained to produce sequences of tokens in many applications such as machine translation . The most popular way of training RNNs is to maximize the likelihood of each token in the training data whereas pointed out that the discrepancy between training and generating makes the maximum likelihood estimation suboptimal and proposed scheduled sampling strategy ( SS ) . Later theorized that the objective function underneath SS is improper and explained the reason why GANs tend to generate natural - looking samples in theory . Consequently , the GANs have great potential but are not practically feasible to discrete probabilistic models currently . As pointed out by , the sequence data generation can be formulated as a sequential decision making process , which can be potentially be solved by reinforcement learning techniques . Modeling the sequence generator as a policy of picking the next token , policy gradient methods can be adopted to optimize the generator once there is an ( implicit ) reward function to guide the policy . For most practical sequence generation tasks , e.g. machine translation , the reward signal is meaningful only for the entire sequence , for instance in the game of Go , the reward signal is only set at the end of the game . In those cases , state - action evaluation methods such as Monte Carlo ( tree ) search have been adopted . By contract , our proposed SeqGAN extends GANs with the RL - based generator to solve the sequence generation problem , where a reward signal is provided by the discriminator at the end of each episode via Monte Carlo approach , and the generator picks the action and learns the policy using estimated overall rewards . section : Sequence Generative Adversarial Nets The sequence generation problem is denoted as follows . Given a dataset of real - world structured sequences , train a - parameterized generative model to produce a sequence , where is the vocabulary of candidate tokens . We interpret this problem based on reinforcement learning . In timestep , the state is the current produced tokens and the action is the next token to select . Thus the policy model is stochastic , whereas the state transition is deterministic after an action has been chosen , i.e. for the next state if the current state and the action ; for other next states , . Additionally , we also train a - parameterized discriminative model to provide a guidance for improving generator . is a probability indicating how likely a sequence is from real sequence data or not . As illustrated in Figure [ reference ] , the discriminative model is trained by providing positive examples from the real sequence data and negative examples from the synthetic sequences generated from the generative model . At the same time , the generative model is updated by employing a policy gradient and MC search on the basis of the expected end reward received from the discriminative model . The reward is estimated by the likelihood that it would fool the discriminative model . The specific formulation is given in the next subsection . subsection : SeqGAN via Policy Gradient Following , when there is no intermediate reward , the objective of the generator model ( policy ) is to generate a sequence from the start state to maximize its expected end reward : where is the reward for a complete sequence . Note that the reward is from the discriminator , which we will discuss later . is the action - value function of a sequence , i.e. the expected accumulative reward starting from state , taking action , and then following policy . The rational of the objective function for a sequence is that starting from a given initial state , the goal of the generator is to generate a sequence which would make the discriminator consider it is real . The next question is how to estimate the action - value function . In this paper , we use the REINFORCE algorithm and consider the estimated probability of being real by the discriminator as the reward . Formally , we have : However , the discriminator only provides a reward value for a finished sequence . Since we actually care about the long - term reward , at every timestep , we should not only consider the fitness of previous tokens ( prefix ) but also the resulted future outcome . This is similar to playing the games such as Go or Chess where players sometimes would give up the immediate interests for the long - term victory . Thus , to evaluate the action - value for an intermediate state , we apply Monte Carlo search with a roll - out policy to sample the unknown last tokens . We represent an - time Monte Carlo search as where and is sampled based on the roll - out policy and the current state . In our experiment , is set the same as the generator , but one can use a simplified version if the speed is the priority . To reduce the variance and get more accurate assessment of the action value , we run the roll - out policy starting from current state till the end of the sequence for times to get a batch of output samples . Thus , we have : where , we see that when no intermediate reward , the function is iteratively defined as the next - state value starting from state and rolling out to the end . A benefit of using the discriminator as a reward function is that it can be dynamically updated to further improve the generative model iteratively . Once we have a set of more realistic generated sequences , we shall re - train the discriminator model as follows : Each time when a new discriminator model has been obtained , we are ready to update the generator . The proposed policy based method relies upon optimizing a parametrized policy to directly maximize the long - term reward . Following , the gradient of the objective function w.r.t . the generator \u2019s parameters can be derived as The above form is due to the deterministic state transition and zero intermediate rewards . The detailed derivation is provided in the appendix . Using likelihood ratios , we build an unbiased estimation for Eq . ( [ reference ] ) ( on one episode ) : where is the observed intermediate state sampled from . Since the expectation can be approximated by sampling methods , we then update the generator \u2019s parameters as : where denotes the corresponding learning rate at - th step . Also the advanced gradient algorithms such as Adam and RMSprop can be adopted here . In summary , Algorithm [ reference ] shows full details of the proposed SeqGAN . At the beginning of the training , we use the maximum likelihood estimation ( MLE ) to pre - train on training set . We found the supervised signal from the pre - trained discriminator is informative to help adjust the generator efficiently . After the pre - training , the generator and discriminator are trained alternatively . As the generator gets progressed via training on g - steps updates , the discriminator needs to be re - trained periodically to keeps a good pace with the generator . When training the discriminator , positive examples are from the given dataset , whereas negative examples are generated from our generator . In order to keep the balance , the number of negative examples we generate for each d - step is the same as the positive examples . And to reduce the variability of the estimation , we use different sets of negative samples combined with positive ones , which is similar to bootstrapping . [ t ] Sequence Generative Adversarial Nets [ 1 ] generator policy G\u03b8 ; roll - out policy G\u03b2 ; discriminator D\u03d5 ; a sequence dataset = S{X:1 T } Initialize G\u03b8 , D\u03d5 with random weights \u03b8 , \u03d5. G\u03b8 using MLE on S negative samples using G\u03b8 for training D\u03d5 D\u03d5 via minimizing the cross entropy a sequence Y:1T= ( y1 , \u2026 , yT ) \u223cG\u03b8 in :1 T Q ( a = yt;s = Y:1 - t1 ) by Eq . ( ) generator parameters via policy gradient Eq . ( ) current G\u03b8 to generate negative examples and combine with given positive examples S discriminator D\u03d5 for k epochs by Eq . ( ) converges subsection : The Generative Model for Sequences We use recurrent neural networks ( RNNs ) as the generative model . An RNN maps the input embedding representations of the sequence into a sequence of hidden states by using the update function recursively . Moreover , a softmax output layer maps the hidden states into the output token distribution where the parameters are a bias vector and a weight matrix . To deal with the common vanishing and exploding gradient problem of the backpropagation through time , we leverage the Long Short - Term Memory ( LSTM ) cells to implement the update function in Eq . ( [ reference ] ) . It is worth noticing that most of the RNN variants , such as the gated recurrent unit ( GRU ) and soft attention mechanism , can be used as a generator in SeqGAN . subsection : The Discriminative Model for Sequences Deep discriminative models such as deep neural network ( DNN ) , convolutional neural network ( CNN ) and recurrent convolutional neural network ( RCNN ) have shown a high performance in complicated sequence classification tasks . In this paper , we choose the CNN as our discriminator as CNN has recently been shown of great effectiveness in text ( token sequence ) classification . Most discriminative models can only perform classification well for an entire sequence rather than the unfinished one . In this paper , we also focus on the situation where the discriminator predicts the probability that a finished sequence is real . We first represent an input sequence as : where is the - dimensional token embedding and is the concatenation operator to build the matrix . Then a kernel applies a convolutional operation to a window size of words to produce a new feature map : where operator is the summation of elementwise production , is a bias term and is a non - linear function . We can use various numbers of kernels with different window sizes to extract different features . Finally we apply a max - over - time pooling operation over the feature maps . To enhance the performance , we also add the highway architecture based on the pooled feature maps . Finally , a fully connected layer with sigmoid activation is used to output the probability that the input sequence is real . The optimization target is to minimize the cross entropy between the ground truth label and the predicted probability as formulated in Eq . ( [ reference ] ) . Detailed implementations of the generative and discriminative models are provided in the appendix . section : Synthetic Data Experiments To test the efficacy and add our understanding of SeqGAN , we conduct a simulated test with synthetic data . To simulate the real - world structured sequences , we consider a language model to capture the dependency of the tokens . We use a randomly initialized LSTM as the true model , aka , the oracle , to generate the real data distribution for the following experiments . subsection : Evaluation Metric The benefit of having such oracle is that firstly , it provides the training dataset and secondly evaluates the exact performance of the generative models , which will not be possible with real data . We know that MLE is trying to minimize the cross - entropy between the true data distribution and our approximation , i.e. . However , the most accurate way of evaluating generative models is that we draw some samples from it and let human observers review them based on their prior knowledge . We assume that the human observer has learned an accurate model of the natural distribution . Then in order to increase the chance of passing Turing Test , we actually need to minimize the exact opposite average negative log - likelihood , with the role of and exchanged . In our synthetic data experiments , we can consider the oracle to be the human observer for real - world problems , thus a perfect evaluation metric should be where and denote our generative model and the oracle respectively . At the test stage , we use to generate 100 , 000 sequence samples and calculate for each sample by and their average score . Also significance tests are performed to compare the statistical properties of the generation performance between the baselines and SeqGAN . subsection : Training Setting To set up the synthetic data experiments , we first initialize the parameters of an LSTM network following the normal distribution as the oracle describing the real data distribution . Then we use it to generate 10 , 000 sequences of length 20 as the training set for the generative models . In SeqGAN algorithm , the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from with the label 1 . For different tasks , one should design specific structure for the convolutional layer and in our synthetic data experiments , the kernel size is from 1 to and the number of each kernel size is between 100 to 200 . Dropout and L2 regularization are used to avoid over - fitting . Four generative models are compared with SeqGAN . The first model is a random token generation . The second one is the MLE trained LSTM . The third one is scheduled sampling . The fourth one is the Policy Gradient with BLEU ( PG - BLEU ) . In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens . A curriculum rate is used to control the probability of replacing the true tokens with the generated ones . To get a good and stable performance , we decrease by 0.002 for every training epoch . In the PG - BLEU algorithm , we use BLEU , a metric measuring the similarity between a generated sequence and references ( training data ) , to score the finished samples from Monte Carlo search . subsection : Results The performance of generating sequences from the compared policies is provided in Table [ reference ] . Since the evaluation metric is fundamentally instructive , we can see the impact of SeqGAN , which outperforms other baselines significantly . A significance T - test on the score distribution of the generated sequences from the compared models is also performed , which demonstrates the significant improvement of SeqGAN over all compared models . The learning curves shown in Figure [ reference ] illustrate the superiority of SeqGAN explicitly . After about 150 training epochs , both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high score , whereas SeqGAN can improve the limit of the generator with the same structure as the baselines significantly . This indicates the prospect of applying adversarial training strategies to discrete sequence generative models to breakthrough the limitations of MLE . Additionally , SeqGAN outperforms PG - BLEU , which means the discriminative signal in GAN is more general and effective than a predefined score ( e.g. BLEU ) to guide the generative policy to capture the underlying distribution of the sequence data . subsection : Discussion In our synthetic data experiments , we find that the stability of SeqGAN depends on the training strategy . More specifically , the g - steps , d - steps and parameters in Algorithm [ reference ] have a large effect on the convergence and performance of SeqGAN . Figure [ reference ] shows the effect of these parameters . In Figure [ reference ] , the g - steps is much larger than the d - steps and epoch number , which means we train the generator for many times until we update the discriminator . This strategy leads to a fast convergence but as the generator improves quickly , the discriminator can not get fully trained and thus will provide a misleading signal gradually . In Figure [ reference ] , with more discriminator training epochs , the unstable training process is alleviated . In Figure [ reference ] , we train the generator for only one epoch and then before the discriminator gets fooled , we update it immediately based on the more realistic negative examples . In such a case , SeqGAN learns stably . The d - steps in all three training strategies described above is set to 1 , which means we only generate one set of negative examples with the same number as the given dataset , and then train the discriminator on it for various epochs . But actually we can utilize the potentially unlimited number of negative examples to improve the discriminator . This trick can be considered as a type of bootstrapping , where we combine the fixed positive examples with different negative examples to obtain multiple training sets . Figure [ reference ] shows this technique can improve the overall performance with good stability , since the discriminator is shown more negative examples and each time the positive examples are emphasized , which will lead to a more comprehensive guidance for training generator . This is in line with the theorem in . When analyzing the convergence of generative adversarial nets , an important assumption is that the discriminator is allowed to reach its optimum given . Only if the discriminator is capable of differentiating real data from unnatural data consistently , the supervised signal from it can be meaningful and the whole adversarial training process can be stable and effective . section : Real - world Scenarios To complement the previous experiments , we also test SeqGAN on several real - world tasks , i.e. poem composition , speech language generation and music generation . subsection : Text Generation For text generation scenarios , we apply the proposed SeqGAN to generate Chinese poems and Barack Obama political speeches . In the poem composition task , we use a corpus of 16 , 394 Chinese quatrains , each containing four lines of twenty characters in total . To focus on a fully automatic solution and stay general , we did not use any prior knowledge of special structure rules in Chinese poems such as specific phonological rules . In the Obama political speech generation task , we use a corpus , which is a collection of 11 , 092 paragraphs from Obama \u2019s political speeches . We use BLEU score as an evaluation metric to measure the similarity degree between the generated texts and the human - created texts . BLEU is originally designed to automatically judge the machine translation quality . The key point is to compare the similarity between the results created by machine and the references provided by human . Specifically , for poem evaluation , we set n - gram to be 2 ( BLEU - 2 ) since most words ( dependency ) in classical Chinese poems consist of one or two characters and for the similar reason , we use BLEU - 3 and BLEU - 4 to evaluate Obama speech generation performance . In our work , we use the whole test set as the references instead of trying to find some references for the following line given the previous line . The reason is in generation tasks we only provide some positive examples and then let the model catch the patterns of them and generate new ones . In addition to BLEU , we also choose poem generation as a case for human judgement since a poem is a creative text construction and human evaluation is ideal . Specifically , we mix the 20 real poems and 20 each generated from SeqGAN and MLE . Then 70 experts on Chinese poems are invited to judge whether each of the 60 poem is created by human or machines . Once regarded to be real , it gets + 1 score , otherwise 0 . Finally , the average score for each algorithm is calculated . The experiment results are shown in Tables [ reference ] and [ reference ] , from which we can see the significant advantage of SeqGAN over the MLE in text generation . Particularly , for poem composition , SeqGAN performs comparably to real human data . subsection : Music Generation For music composition , we use Nottingham dataset as our training data , which is a collection of 695 music of folk tunes in midi file format . We study the solo track of each music . In our work , we use 88 numbers to represent 88 pitches , which correspond to the 88 keys on the piano . With the pitch sampling for every 0.4s , we transform the midi files into sequences of numbers from 1 to 88 with the length 32 . To model the fitness of the discrete piano key patterns , BLEU is used as the evaluation metric . To model the fitness of the continuous pitch data patterns , the mean squared error ( MSE ) is used for evaluation . From Table [ reference ] , we see that SeqGAN outperforms the MLE significantly in both metrics in the music generation task . section : Conclusion In this paper , we proposed a sequence generation method , SeqGAN , to effectively train generative adversarial nets for structured sequences generation via policy gradient . To our best knowledge , this is the first work extending GANs to generate sequences of discrete tokens . In our synthetic data experiments , we used an oracle evaluation mechanism to explicitly illustrate the superiority of SeqGAN over strong baselines . For three real - world scenarios , i.e. , poems , speech language and music generation , SeqGAN showed excellent performance on generating the creative sequences . We also performed a set of experiments to investigate the robustness and stability of training SeqGAN . For future work , we plan to build Monte Carlo tree search and value network to improve action decision making for large scale data and in the case of longer - term planning . section : Acknowledgments We sincerely thank Tianxing He for many helpful discussions and comments on the manuscript . bibliography : References appendix : Appendix In Section 1 , we present the step - by - step derivation of Eq . ( 6 ) in the paper . In Section 2 , the detailed realization of the generative model and the discriminative model is discussed , including the model parameter settings . In Section 3 , an interesting ablation study is provided , which is a supplementary to the discussions of the synthetic data experiments . subsection : Proof for Eq . ( 6 ) For readability , we provide the detailed derivation of Eq . ( 6 ) here by following . As mentioned in Sequence Generative Adversarial Nets section , the state transition is deterministic after an action has been chosen , i.e. for the next state if the current state and the action ; for other next states , . In addition , the intermediate reward is 0 . We re - write the action value and state value as follows : For the start state , the value is calculated as which is the objective function to maximize in Eq . ( 1 ) of the paper . Then we can obtain the gradient of the objective function , defined in Eq . ( 1 ) , w.r.t . the generator \u2019s parameters : which is the result in Eq . ( 6 ) of the paper . subsection : Model Implementations In this section , we present a full version of the discussed generative model and discriminative model in our paper submission . subsubsection : The Generative Model for Sequences We use recurrent neural networks ( RNNs ) as the generative model . An RNN maps the input embedding representations of the sequence into a sequence of hidden states by using the update function recursively . Moreover , a softmax output layer maps the hidden states into the output token distribution where the parameters are a bias vector and a weight matrix . The vanishing and exploding gradient problem in backpropagation through time ( BPTT ) issues a challenge of learning long - term dependencies to recurrent neural network . To address such problems , gated RNNs have been designed based on the basic idea of creating paths through time that have derivatives that neither vanish nor explode . Among various gated RNNs , we choose the Long Short - Term Memory ( LSTM ) to be our generative networks with the update equations : where is the vector concatenation and is the elementwise product . For simplicity , we use the standard LSTM as the generator , while it is worth noticing that most of the RNN variants , such as the gated recurrent unit ( GRU ) and soft attention mechanism , can be used as a generator in SeqGAN . The standard way of training an RNN is the maximum likelihood estimation ( MLE ) , which involves minimizing the negative log - likelihood for a generated sequence given input . However , when applying MLE to generative models , there is a discrepancy between training and generating , which motivates our work . subsubsection : The Discriminative Model for Sequences Deep discriminative models such as deep neural network ( DNN ) , convolutional neural network ( CNN ) and recurrent convolutional neural network ( RCNN ) have shown a high performance in complicated sequence classification tasks . In this paper , we choose the CNN as our discriminator as CNN has recently been shown of great effectiveness in text ( token sequence ) classification . As far as we know , except for some specific tasks , most discriminative models can only perform classification well for a whole sequence rather than the unfinished one . In case of some specific tasks , one may design a classifier to provide intermediate reward signal to enhance the performance of our framework . But to make it more general , we focus on the situation where discriminator can only provide final reward , i.e. , the probability that a finished sequence was real . We first represent an input sequence as : where is the - dimensional token embedding and is the vertical concatenation operator to build the matrix . Then a kernel applies a convolutional operation to a window size of words to produce a new feature map : where operator is the summation of elementwise production , is a bias term and is a non - linear function . We can use various numbers of kernels with different window sizes to extract different features . Specifically , a kernel with window size applied to the concatenated embeddings of input sequence will produce a feature map Finally we apply a max - over - time pooling operation over the feature map and pass all pooled features from different kernels to a fully connected softmax layer to get the probability that a given sequence is real . We perform an empirical experiment to choose the kernel window sizes and numbers as shown in Table [ reference ] . For different tasks , one should design specific structures for the discriminator . To enhance the performance , we also add the highway architecture before the final fully connected layer : where , and are highway layer weights , denotes an affine transform followed by a non - linear activation function such as a rectified linear unit ( ReLU ) and is the \u201c transform gate \u201d with the same dimensionality as and . Finally , we apply a sigmoid transformation to get the probability that a given sequence is real : where and is the output layer weight and bias . When optimizing discriminative models , supervised training is applied to minimize the cross entropy , which is widely used as the objective function for classification and prediction tasks : where is the ground truth label of the input sequence and is the predicted probability from the discriminative models . subsection : More Ablation Study In the Discussion subsection of Synthetic Data Experiments section of our paper , we discussed the ablation study of three hyperparameters of SeqGAN , i.e. , g - steps , d - steps and epoch number . Here we provide another ablation study which is instructive for the better training of SeqGAN . As described in our paper , we start the adversarial training process after the convergence of MLE supervised pre - training . Here we further conduct experiments to investigate the performance of SeqGAN when the supervised pre - training is insufficient . As shown in Figure [ reference ] , if we pre - train the generative model with conventional MLE methods for only 20 epochs , which is far from convergence , then the adversarial training process improves the generator quite slowly and unstably . The reason is that in SeqGAN , the discriminative model provides reward guidance when training the generator and if the generator acts almost randomly , the discriminator will identify the generated sequence to be unreal with high confidence and almost every action the generator takes receives a low ( unified ) reward , which does not guide the generator towards a good improvement direction , resulting in an ineffective training procedure . This indicates that in order to apply adversarial training strategies to sequence generative models , a sufficient pre - training is necessary .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["COCO_Captions"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-2"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["COCO_Captions"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-3"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["COCO_Captions"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-4"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["COCO_Captions"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-5"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Chinese_Poems"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-2"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["EMNLP2017_WMT"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-2"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["EMNLP2017_WMT"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-3"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["EMNLP2017_WMT"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-4"]]], "Task": [[["Text_Generation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["EMNLP2017_WMT"]]], "Method": [[["SeqGAN"]]], "Metric": [[["BLEU-5"]]], "Task": [[["Text_Generation"]]]}]}
{"docid": "TST3-SREX-0033", "doctext": "Joint Maximum Purity Forest with Application to Image Super - Resolution section : Abstract - In this paper , we propose a novel random - forest scheme , namely Joint Maximum Purity Forest ( JMPF ) , for classification , clustering , and regression tasks . In the JMPF scheme , the original feature space is transformed into a compactly pre - clustered feature space , via a trained rotation matrix . The rotation matrix is obtained through an iterative quantization process , where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity . In the new feature space , orthogonal hyperplanes , which are employed at the split - nodes of decision trees in random forests , can tackle the clustering problems effectively . We evaluated our proposed method on public benchmark datasets for regression and classification tasks , and experiments showed that JMPF remarkably outperforms other state - of - the - art random - forest - based approaches . Furthermore , we applied JMPF to image super - resolution , because the transformed , compact features are more discriminative to the clustering - regression scheme . Experiment results on several public benchmark datasets also showed that the JMPF - based image super - resolution scheme is consistently superior to recent state - of - the - art image super - resolution algorithms . section : from achieving the optimal hyperplanes as SVM does ( i.e. , there is no orthogonal constraint in SVM ) in some original feature space , as shown in Fig . 1 ( a ) . In this paper , we aim to solve this orthogonalconstraint limitation . With the fixed orthogonal hyperplanes , we propose to rotate the feature space , this is equivalent to rotating the hyperplanes , in such a way that global maximum purity on the clustered data can be achieved , as illustrated in Fig . 2 . This strategy can achieve a joint maximum purity for all the split - nodes when training a random forest . Image super - resolution can be performed based on clustering / classification , according to the recent emerging clustering - regression stream [ reference ][ reference ][ reference ] , and the JMPF scheme can achieve remarkable performance on both the classification and regression tasks . Therefore , JMPF is applied to single - image super - resolution in this paper . In our algorithm , principal component analysis ( PCA ) is applied to the features for dimensionality reduction . The projected feature space is then rotated to a compact , preclustered feature space via a learned rotation matrix . Finally , for all the split - nodes trained for a random forest , their thresholds are directly set to the inherent zero - center orthogonal hyperplanes in the rotated feature space to meet the maximum - purity criterion . Experiment results show that JMPF can achieve more accurate clustering / classification performance on random forests , and applying JMPF to image super - resolution can achieve superior quality , compared to state - of - the - art methods . Having introduced the main idea of our proposed algorithm , the remainder of this paper is organized as follows . In Section II , we will describe our proposed scheme , the joint maximum purity forest scheme , and present in detail how to compute the rotation matrix via clustering data into the feature - space vertices . Section III will evaluate our proposed method and compare its performance with recent state - of - the - art random - forest - based approaches on regression and classification tasks . In Section IV , we will validate the performance of JMPF scheme on single - image super - resolution . Conclusions are given in Section V. section : II . JOINT MAXIMUM PURITY FOREST SCHEME section : II.1 Random Forest and Our Insights A random forest is an ensemble of binary decision trees ( ) : \u2192 \u211d , where ( = 1 , 2 , \u2026 , ) is the index of the trees , \u2208 \u211d is the m - dimension feature space , and \u211d = [ 0 , 1 ] represents the space of class probability distributions over the label space = { 1 , . . . , } . As shown in Fig . 1 ( b ) , the vertical dotted line forms a hyperplane , = 0 , chosen in the first split - node for separating training samples , and the horizontal dotted line is the hyperplane , = 0 , for the second split - node to cluster all the feature data assigned to this node . This results in separating the three data samples ( Red , Green and Blue ) into three leaf - nodes . It can be seen from Fig . 1 ( b ) that , for each split - node , the optimal hyperplane with more generalization capability is the one which can achieve maximum purity in clustering samples into two groups . For example , the vertical dotted line is the first optimal hyperplane because it clusters all the red training samples into the right node , while all the blue and green samples are clustered into the left node . Furthermore , the left margin and the right margin are equal . Although there is no guarantee that optimal hyperplanes can be determined for all the split - nodes in a random forest , approximated optimal hyperplanes can be obtained through a random bagging strategy . The training of a whole random forest is to train all of its decision trees , by choosing the candidate features and thresholds for each of the split - nodes , where the feature dimensions and thresholds are determined using a random bagging strategy . In the prediction stage , each decision tree returns a class probability ( | ) for a given query sample \u2208 \u211d , and the final class label y * is then obtained via averaging , as follows : * = arg max \u2211 ( | ) . The splitting function for a split - node is denoted as ( ; \u0398 ) , where is a sample and \u0398 is typically parameterized by two values : ( i ) a feature dimension \u0398 \u00ce{1 , . . . , } , and ( ii ) a threshold \u0398 \u00ce\u211d. The splitting function is defined as follows : where the outcome defines to which child node the sample is routed , and 0 and 1 are the two labels for the left and right child nodes , respectively . Each node chooses the best splitting function \u0398 * out of a randomly sampled set { \u0398 } by optimizing the following function : where and are the sets of samples that are routed to the left and the right child nodes , and | | represents the number of samples in the set . During the training of a random forest , the decision trees are provided with a random subset of the training data ( i.e. bagging ) , and are trained independently of each other . Therefore , the decision trees are working as independent experts . Taking random - forest - based classification as an example , training a single decision tree involves recursively splitting each node , such that the training data in each newly created child node is clustered according to their corresponding class labels , so the purity at each node is increasing along a tree . Each tree is grown until a stopping criterion is reached ( e.g. the number of samples in a node is less than a threshold or the tree depth reaches a maximum value ) and the class probability distributions are estimated in the leaf - nodes . After fulfilling one of these criteria , a density model ( ) in the leaf - node is estimated by all samples falling into this leaf - node for predicting the target value in the testing stage . A simple way to estimate the probability distribution ( ) is averaging all the samples in the leaf - node , while there are also variant methods , such as fitting a Gaussian distribution or kernel density estimation , ridge regression [ reference ][ reference ][ reference ] , and so on . ( ) is the local score for a set of samples ( is either or ) , which normally is calculated using entropy as in Eqn . ( 4 ) , but it can be replaced by variance [ reference ][ reference ][ reference ] or the Gini index [ reference ] . where K is the number of classes , and ( | ) is the probability for class , given the set . For the regression problem , the differential entropy : over continuous outputs can be employed , where ( | ) denotes the conditional probability of a target variable given the input sample . Assuming ( . , . ) to be a Gaussian distribution and having only a finite set of samples , the differential entropy can be written in closed form as where det ( \u03a3 ) is the determinant of the estimated covariance matrix of the target variables in . For training each decision tree in a random forest , the goal on each split - node is to maximize the information gain ( IG ) by reducing the entropy after splitting . IG is defined as follows : Since each decision tree is a binary tree and each step is to split a current node ( a parent set ) into two children nodes ( and sets ) , IG can be described as follows : where \u210b is the optimal hyperplane of the split - node , and Eqn . ( 8 ) is the target function of each splitnode when training each decision tree of a random forest . As we can see from Fig . 1 ( b ) , all the optimal hyperplanes from split - nodes are achieved independently and locally . Since each optimal hyperplane is obtained from a subset of feature - dimension candidates with the randomly bagging strategy , there is no guarantee of obtaining a global optimum with respect to all the hyperplanes in all the split - nodes . An intuitive thinking , which was inspired by the data distribution in Fig . 1 ( b ) , is to achieve a global optimum by jointly considering all the hyperplanes of all the split - nodes , in the form as follows : where is the total number of split - nodes that a training sample has routed through a decision tree . As there is no mathematical solution to the problem described in Eqn . [ reference ] , an alternative way ( i.e. , an approximate method ) to numerically solving Eqn . ( 9 ) is to jointly maximize the purity of the clustered data groups at each of the split - nodes . This also means that all the data is clustered into the corners ( feature - space vertices ) of the feature space , as shown in Fig . 2 . section : II.2 The Joint Maximum Purity Forest Scheme To calculate the threshold for each split - node in each decision tree when training a random forest , we are attempting to determine an orthogonal hyperplane for a three - category classification problem , as shown in Fig . 1 . Since the hyperplanes for the split - nodes of a decision tree are required to be orthogonal to each other , seeking an optimal orthogonal hyperplane locally can not guarantee obtaining maximum purity for the whole tree globally . As shown in Fig . 2 , it is easy to determine the vertical hyperplane for maximum purity , but it is hard to obtain the horizontal hyperplane for maximum purity in the original feature space . To achieve an optimal classification performance for the whole decision tree , all the split - nodes should be considered globally or simultaneously . As shown in Fig . 2 , a number of split - nodes , which have their hyperplanes orthogonal to each other , are required to separate the samples into different nodes . However , if we can transform the samples ( zerocentered feature data ) to locate them at the respective corners of the feature space , i.e. { \u22121 , 1 } for mdimensional features , the feature data can be easily and accurately separated by the orthogonal ( either vertical or horizontal ) hyperplanes , which contain the space center { 0 } , as illustrated in Fig . 1 ( b ) . The insight behind this is that the data is clustered into the feature - space vertices ( the corners in a 2 - D feature space means that the data points belong to { \u22121 , 1 } as the coordinate range is set to [ \u22121 , 1 ] ) . To tackle the original feature data , which is not ideally clustered in the vertices or corners of the feature space or close to them , as shown in Fig . 1 ( a ) , an intuitive idea is to rotate the feature space ( this is equivalent to rotating the hyperplanes ) . This transformation clusters the feature data compactly into feature - space vertices { \u22121 , 1 } with a total of 2 vertices . Therefore , a possible solution to the problem described in Eqn . ( 10 ) is to rotate the data features by a rotation matrix \u211b \u00d7 , as shown in Fig . 2 , through which the original feature space is transformed into a more compact clustered feature space , where all the feature data is clustered close to the feature - space vertices . This solution can be mathematically defined as follows : where \u2208 \u211d \u00d7 contains n samples , each of which is a - dimensional feature vector arranged in a row , and is zero - centered , i.e. all the feature vectors are demeaned by subtracting the mean vector from each feature vector . This idea of clustering data into the feature - space vertices can also be found in locality - sensitive hashing ( LSH ) [ reference ] and image representation [ reference ] . In [ reference ] , a simple and efficient alternating minimization scheme was proposed to find a rotation matrix for zero - centered feature data , which minimizes the quantization errors by mapping the feature data to the vertices of a zero - centered binary hypercube . The method is termed as iterative quantization ( ITQ ) , which can work on multi - class spectral clustering and orthogonal Procrustes problem . Yu et al . [ reference ] proposed using a circulant matrix to speed up the computation , because the circulant structure enables the use of Fast Fourier Transformation ( FFT ) . As the computation of the rotation matrix in the training and testing stage is ignorable , we choose a similar scheme to ITQ [ reference ] to determine the rotation matrix ( we throw away the final quantization matrix described in Eqn . [ reference ] , which is used for hashing in [ reference ] ) , through which the original feature space can be transformed into a new compact clustered feature space : = \u211b , where the data is located at the respective vertices in the new feature space . After this transformation , a random forest with globally joint maximum purity of all the clustered data can be trained , through all the hyperplanes in the split - nodes of each decision tree . Based on this idea , our proposed scheme is called joint maximum purity forest ( JMPF ) . section : II.3 Learning the Rotation Matrix via Clustering Data into Feature - Space Vertices Assuming that \u2208 \u211d is one point in the - dimensional feature space ( zero - centered data ) , the respective vertices in the zero - centered binary hypercube space can be denoted as ( ) \u2208 { \u22121 , 1 } , and there is a total of 2 vertices in the - dimensional feature space . It is easy to see from We denote a binary code matrix \u2208 { \u22121 , 1 } \u00d7 , whose rows = ( ) \u2208 . For a matrix or a vector , ( . ) applies the sign operation to it element - wise . Our objective is to minimize the error between the feature and the feature - space vertices , i.e. , min\u2016 \u2212 \u2016 . As we can see in Fig . 2 , when the feature space is rotated , the feature points will be more concentrated around their nearest vertices , which means that the quantization error will become smaller . Therefore , the minimization problem of min\u2016 \u2212 \u2016 is equivalent to minimizing the error of the zerocentered data with respect to the Frobenius norm , as in the following formulation : Therefore , the task of this minimization problem is to determine an optimal rotation matrix \u211b to satisfy Eqn . [ reference ] . Since there are two variables in Eqn . ( 11 ) , the expectation - maximization ( E - M ) algorithm is applied to cluster data into the feature - space vertices , such that a local minimum of the binary code matrix and the rotation matrix \u211b are computed simultaneously . The idea of rotating feature data to minimize the error between the transformed data and the featurespace vertices can also be found in [ reference ] , which showed that the rotation matrix \u211b can be initialized randomly , and then iterated to converge to the required rotation matrix . Two iteration steps will be performed : in every iteration , each feature vector in the feature space is firstly quantized to the nearest vertex of the binary hypercube , i.e. to a vertex in , and then the rotation matrix \u211b is updated to minimize the quantization error by fixing . These two alternating steps are described in detail below : ( 1 ) Fix \u211b and update : Because the zero - centered data matrix is fixed , minimizing Eqn . ( 12 ) is equivalent to maximizing the following term : where is an element of = \u211b. To maximize Eqn . ( 13 ) with respect to , = 1 whenever \u2265 0 and = \u22121 otherwise , i.e. = ( \u211b ) \u2208 { \u22121 , 1 } . ( 2 ) Fix and update \u211b : The problem of fixing to obtain a rotation matrix based on the objective function Eqn . ( 11 ) is relative to the classic orthogonal Procrustes problem [ reference ][ reference ][ reference ] , in which a rotation matrix is determined to align one point set with another . In our algorithm , these two point sets are the zero - centered data set and the quantized matrix . Therefore , a closed - form solution for \u211b is available , by applying SVD on the \u00d7 matrix to obtain \u03a9 ( \u03a9 is a diagonal matrix ) , then set \u211b = to update \u211b. section : II.4 Proof of the Orthogonal Procrustes Problem : For completeness , we prove the orthogonal Procrustes problem , for which the solution can be found in [ reference ][ reference ][ reference ] : Proof : Thus , min \u211b \u2016 \u2212 \u211b \u2016 equals to maximizing : The last inequality holds because Z is also an orthonormal matrix , and \u2211 , = 1 , , \u2264 1 . The objective function can be maximized if Z = , i.e. section : \u211b = \u220e section : III . JOINT MAXIMUM PURITY FOREST FOR REGRESSION AND CLASSIFICATION section : III.1 The Workflow of Joint Maximum Purity Forest Random forest is a machine - learning method using an ensemble of randomized decision trees for classification . Each tree in a random forest consists of split - nodes and leaf - nodes , which can be trained recursively . A random forest is constructed recursively , where each node attempts to find a splitting function or a hyperplane to separate its samples into two leaf - nodes , such that the information gain is optimized . A tree stops growing if the maximum depth is reached or if a node has achieved maximum purity , i.e. it contains only samples from one class . Then , each leaf - node collects the statistics of the samples falling in it . In the evaluation phase , the probability of a query sample x belonging to class k is given by averaging all the trees , or by other methods . Most random - forest - based models [ reference ][ reference ][ reference ][ reference ] share a similar workflow , as shown in Fig . 3 , in which the main task on training a tree in a random forest is to decide thresholds in the split - nodes and learn the regressors or classes in the leaf - nodes . Rigid regression or linear regression is often employed in the leaf - nodes for the prediction task , because rigid regression has a closed - form solution , while linear regression is an efficient optimization tool , and the LibLinear package [ reference ] can be used to fine - tune its configurations . Compared to conventional random forests , our JMPF scheme has one more step , as shown in the left of Fig . 3 , the rotation matrix . The JMPF scheme transforms the original feature space by rotating it into a more compact , pre - clustered feature space , using a trained rotation matrix learned through clustering feature vectors iteratively into the vertices of a new feature space . The whole workflow of our proposed algorithm , the JMPF scheme , is outlined in Fig . 3 . The source code of our algorithm is available to download at : https: // github.com / HarleyHK / JMPF . a section : III.2 The inherent zero - center hyperplanes as thresholds for split - nodes In training a random forest , the two main operations for training ( splitting ) each split - node are to choose splitting feature ( s ) , and to determine the threshold , using a random bagging strategy , which can avoid over - fitting in training classifiers . In the rotated compact pre - clustered feature space , the inherent zerocenter hyperplanes are inherently the optimal thresholds ( to meet the max - purity criterion on two clustered data groups ) after training the rotation matrix . Therefore , these inherent zero - center hyperplanes can directly be set as the thresholds to achieve optimal classification performance on training a random forest . Compared to conventional random forests , our proposed JMPF only needs to choose which feature ( s ) to split data at split - nodes . This can speed up the training process for a random forest . Experimental results in the next subsection will validate this performance . section : III.3 : Experimental results on JMPF regression and classification To evaluate the performances of the proposed JMPF , we test it with 15 standard machine - learning tasks , 7 for classification and 8 for regression . The datasets used in the experiments are summarized in Table - 1 . We use standard performance evaluation metrics : error rate for classification and root mean squared error ( RMSE ) for regression , unless otherwise specified . We firstly evaluate the proposed approach on two real applications , one for classification ( Table - 2 ) and one for regression ( Table - 3 ) . Our proposed JMPF is compared with the original random forest before refinement ( denoted as RF ) , and two state - of - the - art variants : alternating decision forests ( ADF ) [ reference ] and alternating regression forests ( ARF ) [ reference ] , for classification and regression , respectively . Furthermore , we compare with JMPF + ADF / ARF , for demonstrating that our algorithm can be combined with other methods . We follow the experiment settings in [ reference ][ reference ] . We set the maximum tree depth D at 15 , and the minimum sample number in a splitting node is set at 5 . The experiments were repeated five times , and the average error and standard deviation were measured . The results are presented in Table - 2 and Table - 3 , for the classification and regression tasks , respectively . In terms of accuracy , our proposed JMPF significantly outperforms the standard random forest on all classification and regression tasks . Compared to RF , JMPF achieves an average of 23.57 % improvement on the classification tasks , and an average of 23.13 % improvement on the regression tasks . Our method also consistently outperforms the state - of - theart variants : ADF / ARF . Moreover , the performance of our JMPF algorithm can be further improved by integrating with ADF and ARF , denoted as JMPF + ADF / ARF . As shown in Table - 2 and Table - [ reference ] , JMPF : proposed algorithm , JMPF + ARF : our proposed algorithm embedded into ARF . is the error scale . The number of randomly chosen hyperplanes # \u210b is 3 . The percentages in brackets for JMPF and JMPF + ARF are the reduction rates in RMSE compared with the RF algorithm . section : III.4 : Discussions on Experimental Results The computational complexity of JMPF is similar to that of the standard random forest . As illustrated in the workflow of JMPF in Fig . 3 , only one additional step , which computes the rotation matrix , is required , when compared to the standard random forest . For a small dataset ( e.g. , feature dimension size less than 500 and data size less than 10 , 000 ) , the computation required to compute the rotation matrix for clustering data into the feature - space vertices is acceptable in the training stage ( about 10 seconds per level , using MatLab ) and negligible in the testing stage . When the dimension size becomes larger , PCA dimensionality reduction can be employed . If the size of the dataset increases , such that using PCA still involves heavy computation , bagging can be used to achieve comparable accuracy and the whole extra computation will be insignificant . , the number hyperplane ( s ) # \u210b on training the random forest is 3 ) . To study the stability of JMPF , we choose the letterorig dataset for classification and the kin8 nm dataset for regression , and the respective results are shown in Fig . 4 ( a ) and Fig . 4 ( b ) , respectively . In the experiments , the number of trees , i.e. , the number of weak classifiers in the random forest , varies from 10 to 200 , and we have three observations . Firstly , as shown in Fig . 4 , when the number of trees increases , the performance of all the algorithms improves . For classification , as shown in Fig . 4 ( a ) , when the number of trees is larger than 100 , the errors are converged to become steady . On the contrary , for the regression task as shown in Fig . 4 ( b ) , the errors are almost stable , ranged from 10 to 200 . Secondly , the results show that JMPF consistently outperforms ADF and RF , irrespective of the number of trees used . Finally , Fig . 4 clearly shows that JMPF can integrate with ADF or ARF to further improve its performance . section : IV . IMAGE SUPER - RESOLUTION BASED ON JOINT MAXIMUM PURITY FOREST section : IV.1 Overview of Image Super - resolution and Related Works Image super - resolution ( SR ) , which recovers a high - resolution ( HR ) image from one single image or a number of low - resolution ( LR ) images , has been a hot research topic in the field of image processing for decades . SR is a well - known ill - posed problem , which needs artistic skills from mathematics and machine learning . Prior methods on SR are mainly based on edge preserving , such as New Edge - directed Interpolation ( NEDI ) [ reference ] , Soft - decision Adaptive Interpolation ( SAI ) [ reference ] , Directional Filtering and Data - Fusion ( DFDF ) [ reference ] , Modified Edge - Directed Interpolation ( MEDI ) [ reference ] , etc . The neighbor - embedding ( NE ) methods [ reference ][ reference ] set the milestone on the patch - learning - based superresolution approach . In this approach , each LR patch is approximated as a linear regression of its nearest LR neighbors in a collected dataset , while its HR counterpart can be reconstructed with the same coefficients of corresponding HR neighbors , based on the non - linear manifold structure . Although the NE method is simple and practical , it requires a huge dataset ( millions of patches ) to achieve good reconstruction quality and it is computationally intensive , because k - NN is used in searching neighboring patches in the huge dataset . Instead of using the patches extracted directly from natural images , Yang et al . [ reference ] employed sparse coding [ reference ][ reference ] to represent patch images , of large size , efficiently , which opens the era for sparse coding in the image inverse problems . The sparse - coding super - resolution ( ScSR ) approach is a framework that the HR counterpart of an LR patch can be reconstructed aided by two learned dictionaries , with the sparse constraint on the coefficients via the following formulations : The compact LR and HR dictionaries can be jointly learned with a sparsity constraint , using the following sparse representation : where and are the LR patch and the corresponding HR patch , respectively ; and D and D are the LR and HR dictionaries learned from the LR and the corresponding HR patch samples , respectively . The value of in \u2016 \u2016 is the sparsity factor of the coefficients . \u2016 \u2016 is - norm , which means the non - zero count of the coefficients in . For each LR patch of an input LR image , the problem of finding the sparse coefficients can be formulated as follows : where is a linear or non - linear feature - extraction operator on the LR patches , which makes the LR patches more discriminative from each other . Typically , can be chosen as a high - pass filter , and a simple high - pass filter can be obtained by subtracting the input from the output of a low - pass filter , as in an early work [ reference ] . In The ideal regularization term for the sparse constraint on the coefficients \u03b1 is the - norm ( nonconvex ) , but , based on greedy matching , it leads to an NP - hard problem . Alternatively , Yang et al . [ reference ] relaxed it to - norm , as shown in the following formulation : The Lagrange multiplier provides an equivalent formulation as follows : where the parameter balances the sparsity of the solution and the fidelity of the approximation to . However , the effectiveness of sparsity was challenged in [ reference ][ reference ] , as to whether real sparsity can help image classification and restoration , or locality property can achieve the same effect . Timofte et al . [ reference ] proposed an anchored neighborhood regression ( ANR ) framework , which relaxes the sparse decomposition optimization ( - norm ) of [ reference ][ reference ] to a ridge regression ( - norm ) problem . An important step in the ANR model is the relaxation of the - norm in Eqn . ( 23 ) to the - norm least - squares minimization constraint , as follows : where D and D are the LR and HR patch - based dictionaries , respectively . This - norm constraint problem can be solved with a closed - form solution from the ridge regression [ reference ] theory . Based on the Tikhonov regularization / ridge - regression theory , the closed - form solution of the coefficients is given : We assume that the HR patches share the same coefficient \u03b1 from their counterpart LR patches , i.e. , = D . From Eqn . ( 25 ) , we have : Therefore , the HR patches can be reconstructed by : = y , where can be considered a projection matrix , which can be calculated offline , as follows : Ridge regression allows the coefficients to be calculated by multiplying the constant projection matrix with the new extracted feature , as described in Eqn . ( 26 ) and Eqn . [ reference ] . More importantly , the projection matrix can be pre - computed , and this offline learning enables significant speed - up at the prediction stage . Timofte et al . [ reference ] further extended the ANR approach to the A + approach , which learns regressors from all the training samples , rather than from a small quantity of neighbors of the anchor atoms as ANR does . Later , there are numerous variants and extended approaches , based on ANR and A + [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] . By investigating the ANR model , Li et al . [ reference ] found that the weights of the supporting atoms can be of different values to represent their similarities to the anchor atom . Based on this idea , the normal collaborative representation ( CR ) model in ANR is generalized to a weighted model , named as weighted collaborative representation ( WCR ) model , as follows : where is a diagonal matrix . The weights on the diagonal atoms are proportional to their similarities to the anchor atom . Similarly , the new closed - form solution for the coefficients can be calculated offline , as follows : and the new projection matrix is given as follows : The WCR model can further improve the ANR or A + model in terms of image quality , but it is still a time - consuming problem to find the most similar anchor atoms in a dictionary , and this always hinders its applications where fast speed is greatly required . Schulter et al . [ reference ] adopted the random forest as a classifier , and the regressors are learned from the patches in the leaf - nodes . With the same number of regressors , these random - forest - based methods [ reference ][ reference ][ reference ][ reference ] can perform on a par with the A + method in terms of accuracy . However , they achieve an increase in speed , because the sublinear search property of random forest can remarkably reduce the regressors ' search complexity . Recently , deep learning has become a hot research topic , which has been successfully applied to image super - resolution [ reference ][ reference ][ reference ][ reference ] and achieved promising performance , particularly in terms of image quality . In [ reference ][ reference ] , a convolutional neural - network - based image super - resolution ( SRCNN ) was proposed , in which an end - to - end mapping between LR and HR images is learned through a deep convolutional neural network ( CNN ) . The recent emerging stream [ reference ][ reference ] on single - image SR is to formulate the problem as a clusteringregression problem , which can be solved with machine - learning tools . These approaches are learningbased methods , which attempt to reconstruct an HR image from patches with the help of an external database . These methods first decompose an image into patches , then classify them into clusters . Regressors are then trained for each of the clusters , which generate mappings from an input LR patch 's feature to its corresponding HR patch ( see Fig . 5 ) . In the testing stage , an LR query image follows the same procedures to cut into patches and to extract features , which are then assigned to their corresponding clusters using the k - NN algorithm [ reference ][ reference ] or random forest [ reference ][ reference ][ reference ] . The respective HR patches are constructed through regressors learned for the clusters ( see Fig . 6 ) . This kind of clustering - regression algorithms , based on random forest [ reference ][ reference ][ reference ] , has achieved state - of - the - art performance in single image super - resolution , both in terms of accuracy and efficiency , because of the use of ensemble learning and sublinear search . As JMPF achieves promising results on both classification and regression tasks , it can be employed for image super - resolution for better performances . An overview of the training and testing processes of the proposed JMPF - based image SR method is illustrated in Fig . 5 and Fig . 6 , respectively . In our method , the first and second - order gradients are extracted as features from each patch , followed by PCA for dimensionality reduction . These features are then rotated into a more compact , pre - clustered feature space . Finally , all the thresholds are directly set to the inherent zero - center hyperplanes when training the random forest , and similar to other algorithms , the regressors at the leaf - nodes are computed using the rigid regression algorithms . This approach is named as JMPF - based image super - resolution method . section : IV.3 The Working Processes of JMPF - based Image Super - resolution JMPF has been shown to achieve a better performance for clustering and classification than other random forest methods . Since image super - resolution can be considered as a clustering / classification problem , using JMPF is likely to result in better performance . This is mainly due to the features transformed to the vertices in the new feature space , so the features become more discriminative . The image super - resolution training and testing processes of our proposed JMPF - based method are described in Algorithm 1 and Algorithm 2 , respectively . section : IV.4 Experimental Results on JMPF - based Image Super - Resolution In this section , we evaluate our image SR algorithm on some standard super - resolution datasets , including Set 5 , Set14 , and B100 [ reference ] , and compare it with a number of classical or state - of - the - art methods . These include bicubic interpolation , sparse representation SR ( Zeyde ) [ reference ] , anchored neighborhood regression ( ANR ) [ reference ] , A + [ reference ] , standard random forest ( RF ) [ reference ] , and alternating regression forests ( ARF ) [ reference ] . We set the same parameters for all the random - forest - based algorithms : the number of trees in the random forest is 10 , and the maximum depth of each tree is 15 . Experiment results are tabulated in Tables - 4 and Tables - 5 , where JMPF is our proposed JMPF - based image super - resolution method , and JMPF \uf02d is a trimmed version , such that the thresholds for the splitnodes are not the inherent zero - center hyperplanes , but set by the standard random - forest bagging strategy . We use the same training images ( 91 images ) for all the algorithms as previous works [ reference ][ reference ][ reference ][ reference ] do . However , for JMPF + , 100 more images from the General - 100 dataset [ reference ] are used , so as to check whether or not more training samples can further improve our proposed algorithm . Table - 5 : Detailed results of the proposed method , compared with state - of - the - art methods on the dataset Set5 , in terms of PSNR ( dB ) using three different magnification factors ( \u00d72 , \u00d73 , \u00d74 ) . Table - 4 tabulates the performances , in terms of the average peak signal to noise ratio ( PSNR ) scores , of our proposed algorithm and other image SR methods , on the 3 datasets with different magnification factors . For the Set5 and Set14 datasets , with different magnification factors , our proposed JMPF - based algorithm can achieve a comparable performance to other recent state - of - the - art methods , such as A + and ARF . As those random - forest - based algorithms may not be stable on small datasets , when evaluation works on extensive datasets , such as B100 , our proposed algorithm JMPF can stably outperform A + and ARF for all magnification factors ( \u00d72 , \u00d73 , \u00d74 ) . Moreover , the objective quality metrics on PSNR also show that the JMPF algorithm can achieve a better performance when more samples are used for training , as shown from JMPF + in Table - 4 . Table - 5 provides more details of the performances in datasets Set5 . To compare the visual quality of our proposed JMPF - based SR algorithm to other methods , Fig . 7 , shows the reconstructed HR images using different methods . Some regions in the reconstructed images are also enlarged , so as to show the details in the images . In general , our proposed method can produce better quality images , particularly in areas with rich texture , which verifies the feature discrimination of the proposed JMPF scheme . section : V. CONCLUSIONS In this paper , we have proposed a novel random - forest scheme , namely the Joint Maximum Purity Forest ( JMPF ) scheme , which rotates the feature space into a compact , clustered feature space , by jointly maximizing the purity of all the feature - space vertices . In the new pre - clustered feature space , orthogonal hyperplanes can be effectively used in the split - nodes of a decision tree , which can improve the performance of the trained random forest . Compared to the standard random forests and the recent state - of - the - art variants , such as alternating decision forests ( ADF ) [ reference ] and alternating regression forests ( ARF ) [ reference ] , our proposed random - forest method inherits the merits of random forests ( fast training and testing , multi - class capability , etc . ) , and also yields promising results on both classification and regression tasks . Experiments have shown that our method achieves an average improvement of about 20 % for classification and regression on publicly benchmarked datasets . Furthermore , our proposed scheme can integrate with other methods , such as ADF and ARF , to further improve the performance . We have also applied JMPF to single - image super - resolution . We tackle image super - resolution as a clustering - regression problem , and focus on the clustering stage , which happens at the split - nodes of each decision tree . By employing the JMPF strategy , we rotate the feature space into a pre - clustered feature space , which can cluster samples into different sub - spaces more compactly in an unsupervised problem . The compact pre - clustered feature space can provide the optimal thresholds for split - nodes in decision trees , which are the zero - centered orthogonal hyperplanes . Our experiment results on intensive image benchmark datasets , such as B100 , show that the proposed JMPF - based image super - resolution approach can consistently outperform recent state - of - the - art algorithms , in terms of PSNR and visual quality . Our method also inherits the advantages of random forests , which have fast speed on both the training and inference processes . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["BSD100_-_4x_upscaling"]]], "Method": [[["JMPF_"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set14_-_4x_upscaling"]]], "Method": [[["JMPF_"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Set5_-_4x_upscaling"]]], "Method": [[["JMPF_"]]], "Metric": [[["PSNR"]]], "Task": [[["Image_Super-Resolution"]]]}]}
{"docid": "TST3-SREX-0034", "doctext": "document : Cell - aware Stacked LSTMs for Modeling Sentences We propose a method of stacking multiple long short - term memory ( LSTM ) layers for modeling sentences . In contrast to the conventional stacked LSTMs where only hidden states are fed as input to the next layer , our architecture accepts both hidden and memory cell states of the preceding layer and fuses information from the left and the lower context using the soft gating mechanism of LSTMs . Thus the proposed stacked LSTM architecture modulates the amount of information to be delivered not only in horizontal recurrence but also in vertical connections , from which useful features extracted from lower layers are effectively conveyed to upper layers . We dub this architecture Cell - aware Stacked LSTM ( CAS - LSTM ) and show from experiments that our models achieve state - of - the - art results on benchmark datasets for natural language inference , paraphrase detection , and sentiment classification . section : Introduction In the field of natural language processing ( NLP ) , the most prevalent neural approach to obtaining sentence representations is to use recurrent neural networks ( RNNs ) , where words in a sentence are processed in a sequential and recurrent manner . Along with their intuitive design , RNNs have shown outstanding performance across various NLP tasks e.g. language modeling , machine translation , text classification , and parsing . Among several variants of the original RNN , gated recurrent architectures such as long short - term memory ( LSTM ) and gated recurrent unit ( GRU ) have been accepted as de - facto standard choices for RNNs due to their capability of addressing the vanishing and exploding gradient problem and considering long - term dependencies . Gated RNNs achieve these properties by introducing additional gating units that learn to control the amount of information to be transferred or forgotten , and are proven to work well without relying on complex optimization algorithms or careful initialization . Meanwhile , the common practice for further enhancing the expressiveness of RNNs is to stack multiple RNN layers , each of which has distinct parameter sets ( stacked RNN ) . In stacked RNNs , the hidden states of a layer are fed as input to the subsequent layer , and they are shown to work well due to increased depth or their ability to capture hierarchical time series which are inherent to the nature of the problem being modeled . 0.25 0.25 However this setting of stacking RNNs might hinder the possibility of more sophisticated recurrence - based structures since the information from lower layers is simply treated as input to the next layer , rather than as another class of state that participates in core RNN computations . Especially for gated RNNs such as LSTMs and GRUs , this means that layer - to - layer connections can not fully benefit from the carefully constructed gating mechanism used in temporal transitions . Some recent work on stacking RNNs suggests alternative methods that encourage direct and effective interaction between RNN layers by adding residual connections , by shortcut connections , or by using cell states of LSTMs . In this paper , we propose a method of constructing multi - layer LSTMs where cell states are used in controlling the vertical information flow . This system utilizes states from the left and the lower context equally in computation of the new state , thus the information from lower layers is elaborately filtered and reflected through a soft gating mechanism . Our method is easy - to - implement , effective , and can replace conventional stacked LSTMs without much modification of the overall architecture . We call the proposed architecture Cell - aware Stacked LSTM , or CAS - LSTM , and evaluate our method on multiple benchmark datasets : SNLI , MultiNLI , Quora Question Pairs , and SST . From experiments we show that the CAS - LSTMs consistently outperform typical stacked LSTMs , opening the possibility of performance improvement of architectures that use stacked LSTMs . Our contribution is summarized as follows . We bring the idea of utilizing states coming from multiple directions to construction of stacked LSTM and apply the idea to the research of sentence representation learning . There is some prior work addressing the idea of incorporating more than one type of state , however to the best of our knowledge there is little work on applying the idea to sentence encoding and text classification . We conduct extensive evaluation of the proposed method and empirically prove its effectiveness on encoding sentences . Our models achieve new state - of - the - art results on SNLI and Quora Question Pairs datasets , and are on par with the best performing models on MultiNLI and SST datasets . This paper is organized as follows . We give a detailed description about the proposed method in \u00a7 [ reference ] . Experimental results are given in \u00a7 [ reference ] . We study prior work related to our objective in \u00a7 [ reference ] and conclude in \u00a7 [ reference ] . section : Model Description In this section , we give a detailed formulation of the architectures used in experiments . subsection : Notation Throughout this paper , we denote matrices as boldface capital letters ( ) , vectors as boldface lowercase letters ( ) , and scalars as normal italic letters ( ) . For LSTM states , we denote a hidden state as and a cell state as . Also , a layer index of or is denoted by superscript and a time index is denoted by a subscript , i.e. indicates the hidden state at time and layer . means the element - wise multiplication between two vectors . We write - th component of vector as . All vectors are assumed to be column vectors . subsection : Stacked LSTMs While there exist various versions of LSTM formulation , in this work we use the following , one of the most common versions : where , , and , , are trainable parameters . and are the sigmoid activation and the hyperbolic tangent activation function respectively . Also we assume that where is the - th input to the network . The input gate and the forget gate control the amount of information transmitted from and , the candidate cell state and the previous cell state , to the new cell state . Similarly the output gate soft - selects which portion of the cell state is to be used in the final hidden state . We can clearly see that cell states ( , , ) play a crucial role in forming horizontal recurrence . However the current formulation does not consider , the cell state from - th layer , in computation and thus the lower context is reflected only through the rudimentary way , hindering the possibility of controlling vertical information flow . subsection : Cell - aware Stacked LSTMs Now we extend the stacked LSTM formulation defined above to address the problem noted in the previous subsection . To enhance the interaction between layers in a way similar to how LSTMs keep and forget the information from the previous time step , we introduce the additional forget gate that determines whether to accept or ignore the signals coming from the previous layer . Therefore the proposed Cell - aware Stacked LSTM is formulated as follows : where and . can either be a vector of constants or parameters . When , the equations defined in the previous subsection are used . Therefore , it can be said that each non - bottom layer of CAS - LSTM accepts two sets of hidden and cell states \u2014 one from the left context and the other from the below context . The left and the below context participate in computation with the equivalent procedure so that the information from lower layers can be efficiently propagated . Fig . [ reference ] compares CAS - LSTM to the conventional stacked LSTM architecture , and Fig . [ reference ] depicts the computation flow of the CAS - LSTM . We argue that considering in computation is beneficial for the following reasons . First , contains additional information compared to since it is not filtered by . Thus a model that directly uses does not rely solely on for extracting information , due to the fact that it has access to the raw information , as in temporal connections . In other words , no longer has to take all responsibility for selecting useful features for both horizontal and vertical transitions , and the burden of selecting information is shared with . Another advantage of using the lies in the fact that it directly connects and . This direct connection helps and stabilizes training , since the terminal error signals can be easily backpropagated to model parameters . Fig . [ reference ] illustrates paths between the two cell states . We find experimentally that there is little difference between letting be constant and letting it be trainable parameters , thus we set in all experiments . We also experimented with the architecture without i.e. two cell states are combined by unweighted summation similar to multidimensional RNNs , and found that it leads to performance degradation and unstable convergence , likely due to mismatch in the range of cell state values between layers ( for the first layer and for the others ) . Experimental results on various are presented in \u00a7 [ reference ] . paragraph : Connection to tree - structured RNNs . The idea of having multiple states is also related to tree - structured RNNs . Among them , tree - structured LSTMs ( Tree - LSTMs ) are similar to ours in that they use both hidden and cell states from children nodes . In Tree - LSTMs , states for all children nodes are regarded as input , and they participate in the computation equally through weight - shared ( in Child - Sum Tree - LSTMs ) or weight - unshared ( in - ary Tree - LSTMs ) projection . From this perspective , each CAS - LSTM layer ( where ) can be seen as a binary Tree - LSTM where the structures it operates on are fixed to right - branching trees . The use of cell state in computation could be one reason that Tree - LSTMs perform better than sequential LSTMs even when trivial trees ( strictly left - or right - branching ) are given . paragraph : Connection to multidimensional RNNs . Multidimensional RNNs ( MDRNN ) are an extension of 1D sequential RNNs that can accept multidimensional input e.g. images , and have been successfully applied to image segmentation and handwriting recognition . Notably multidimensional LSTMs ( MDLSTM ) have an analogous formulation to ours except the term and the fact that we use distinct weights per column ( or \u2018 layer \u2019 in our case ) . From this view , CAS - LSTM can be seen as a certain kind of MDLSTM that accepts a 2D input . Grid LSTMs also take inputs but emit outputs , which is different from our case where a single set of hidden and cell states is produced . subsection : Sentence Encoders The sentence encoder network we use in our experiments takes words ( assumed to be one - hot vectors ) as input . The words are projected to corresponding word representations : where . Then is fed to a - layer CAS - LSTM model , resulting in the representations . The sentence representation , , is computed by max - pooling over time as in the work of conneau2017infersent conneau2017infersent . Similar to their results , from preliminary experiments we found that the max - pooling performs consistently better than mean - and last - pooling . To make models more expressive , a bidirectional CAS - LSTM network may also be used . In the bidirectional case , the forward representations and the backward representations are concatenated and max - pooled to yield the sentence representation . We call this bidirectional architecture Bi - CAS - LSTM in experiments . subsection : Top - layer Classifiers For the natural language inference experiments , we use the following heuristic function proposed by mou2016snli mou2016snli in feature extraction : where means vector concatenation , and and are applied element - wise . And we use the following function in paraphrase identification experiments : as in the work of ji2013discriminative ji2013discriminative . For sentiment classification , we use the sentence representation itself . We feed the feature extracted from as input to the MLP classifier with ReLU activation followed by the fully - connected softmax layer to predict the label distribution : where , is the number of label classes , and the dimension of the MLP output , section : Experiments We evaluate our method on natural language inference ( NLI ) , paraphrase identification ( PI ) , and sentiment classification . We also conduct analysis on gate values and experiments on model variants . For detailed experimental settings , we refer readers to the supplemental material . For the NLI and PI tasks , there exists recent work specializing in sentence pair classification . However in this work we confine our model to the architecture that encodes each sentence using a shared encoder without any inter - sentence interaction , in order to focus on the effectiveness of the models in extracting semantics . But note that the applicability of CAS - LSTM is not limited to sentence encoding based approaches . subsection : Natural Language Inference For the evaluation of performance of the proposed method on the NLI task , SNLI and MultiNLI datasets are used . The objective of both datasets is to predict the relationship between a premise and a hypothesis sentence : entailment , contradiction , and neutral . SNLI and MultiNLI datasets are composed of about 570k and 430k premise - hypothesis pairs respectively . GloVe pretrained word embeddings are used and remain fixed during training . The dimension of encoder states ( ) is set to 300 and a 1024D MLP with one or two hidden layers is used . We apply dropout to the word embeddings and the MLP layers . The features used as input to the MLP classifier are extracted following Eq . [ reference ] . Table [ reference ] and [ reference ] contain results of the models on SNLI and MultiNLI datasets . In SNLI , our best model achieves the new state - of - the - art accuracy of 87.0 % with relatively fewer parameters . Similarly in MultiNLI , our models match the accuracy of state - of - the - art models in both in - domain ( matched ) and cross - domain ( mismatched ) test sets . Note that only the GloVe word vectors are used as word representations , as opposed to some models that introduce character - level features . It is also notable that our proposed architecture does not restrict the selection of pooling method ; the performance could further be improved by replacing max - pooling with other advanced algorithms e.g. intra - sentence attention and generalized pooling . subsection : Paraphrase Identification We use Quora Question Pairs dataset in evaluating the performance of our method on the PI task . The dataset consists of over 400k question pairs , and each pair is annotated with whether the two sentences are paraphrase of each other or not . Similar to the NLI experiments , GloVe pretrained vectors , 300D encoders , and 1024D MLP are used . The number of CAS - LSTM layers is fixed to 2 in PI experiments . Two sentence vectors are aggregated using Eq . [ reference ] and fed as input to the MLP . The results on the Quora Question Pairs dataset are summarized in Table [ reference ] . Again we can see that our models outperform other models by large margin , achieving the new state of the art . 0.22 0.22 0.22 0.22 0.22 0.22 subsection : Sentiment Classification In evaluating sentiment classification performance , the Stanford Sentiment Treebank ( SST ) is used . It consists of about 12 , 000 binary - parsed sentences where constituents ( phrases ) of each parse tree are annotated with a sentiment label ( very positive , positive , neutral , negative , very negative ) . Following the convention of prior work , all phrases and their labels are used in training but only the sentence - level data are used in evaluation . In evaluation we consider two settings , namely SST - 2 and SST - 5 , the two differing only in their level of granularity with regard to labels . In SST - 2 , data samples annotated with \u2018 neutral \u2019 are ignored from training and evaluation . The two positive labels ( very positive , positive ) are considered as the same label , and similarly for the two negative labels . As a result 98 , 794 / 872 / 1 , 821 data samples are used in training / validation / test , and the task is considered as a binary classification problem . In SST - 5 , data are used as - is and thus the task is a 5 - class classification problem . All 318 , 582 / 1 , 101 / 2 , 210 data samples for training / validation / test are used in the SST - 5 setting . We use 300D GloVe vectors , 2 - layer 150D or 300D encoders , and a 300D MLP classifier for the models , however unlike previous experiments we tune the word embeddings during training . The results on SST are listed in Table [ reference ] . Our models achieve the new state - of - the - art accuracy on SST - 2 and competitive accuracy on SST - 5 , without utilizing parse tree information . subsection : Forget Gate Analysis To inspect the effect of the additional forget gate , we investigate how the values of vertical forget gates are distributed . We sample 1 , 000 random sentences from the development set of the SNLI dataset , and use the 3 - layer CAS - LSTM model trained on the SNLI dataset to compute gate values . If all values from a vertical forget gate were to be 0 , this would mean that the introduction of the additional forget gate is meaningless and the model would reduce to a plain stacked LSTM . On the contrary if all values were 1 , meaning that the vertical forget gates were always open , it would be impossible to say that the information is modulated effectively . Fig . [ reference ] and [ reference ] represent histograms of the vertical forget gate values from the second and the third layer . From the figures we can validate that the trained model does not fall into the degenerate case where vertical forget gates are ignored . Also the figures show that the values are right - skewed , which we conjecture to be a result of focusing more on a strong interaction between adjacent layers . To further verify that the gate values are diverse enough within each time step , we compute the distribution of the range of values per time step , , where . We plot the histograms in Fig . [ reference ] and [ reference ] . From the figure we see that a vertical forget gate controls the amount of information flow effectively , making the decision of retaining or discarding signals . Finally , to investigate the argument presented in \u00a7 [ reference ] that the additional forget gate helps the previous output gate with reducing the burden of extracting all needed information , we inspect the distribution of the values from . This distribution indicates how differently the vertical forget gate and the previous output gate select information from . From Fig . [ reference ] and [ reference ] we can see that the two gates make fairly different decisions , from which we demonstrate that the direct path between and enables a model to utilize signals overlooked by . subsection : Model Variations In this subsection , we see the influence of each component of a model on performance by removing or replacing its components . the SNLI dataset is used for experiments , and the best performing configuration is used as a baseline for modifications . We consider the following variants : ( i ) models that use plain stacked LSTMs , ( ii ) models with different , ( iii ) models without , and ( iv ) models that integrate lower contexts via peephole connections . Variant ( iv ) integrates lower contexts via the following equations : where represent peephole weights that take cell states into account . Among the above equations , those that use the lower cell state are Eq . [ reference ] and [ reference ] . We can see that affects the value of only via peephole connections , which makes independent of . Table [ reference ] summarizes the results of model variants . We can again see that the use of cell states clearly improves sentence modeling performance ( baseline vs. ( i ) and ( iv ) vs. ( i ) ) . Also from the results of baseline and ( ii ) , we validate that the selection of does not significantly affect performance but introducing is beneficial ( baseline vs. ( iii ) ) possibly due to its effect on normalizing information from multiple sources , as mentioned in \u00a7 [ reference ] . Finally , from the comparison between baseline and ( iv ) , we show that the proposed way of combining the left and the lower contexts leads to better modeling of sentence representations than that of zhang2016highway zhang2016highway in encoding sentences . section : Related Work paragraph : Stacked recurrent neural networks . There is some prior work on methods of stacking RNNs beyond the plain stacked RNNs . Residual LSTMs add residual connections between the hidden states computed at each LSTM layer , and shortcut - stacked LSTMs concatenate hidden states from all previous layers to make the backpropagation path short . In our method , the lower context is aggregated via a gating mechanism , and we believe it modulates the amount of information to be transmitted in a more efficient and effective way than vector addition or concatenation . Also , compared to concatenation , our method does not significantly increase the number of parameters . Highway LSTMs and depth - gated LSTMs are similar to our proposed models in that they use cell states from the previous layer , and they are successfully applied to the field of automatic speech recognition and language modeling . However in contrast to CAS - LSTM , where the additional forget gate aggregates the previous layer states , and thus contexts from the left and below participate in computation equitably , in Highway LSTMs and depth - gated LSTMs the previous layer states are considered only through peephole connections . The comparison of our models and this architecture is presented in \u00a7 [ reference ] . paragraph : Multidimensional recurrent neural networks . There is another line of research that aims to extend RNNs to operate on multidimensional inputs . Grid LSTMs are a general - dimensional LSTM architecture that accepts sets of hidden and cell states as input and yields sets of states as output , in contrast to our architecture , which emits a single set of states . 2D and 3D Grid LSTMs bring a performance gain on character - level language modeling and machine translation respectively . Multidimensional RNNs have a similar formulation as ours , except that they do not normalize cell states and weights for all columns ( layers ) are tied . However they are often employed to model multidimensional data such as images of handwritten text with RNNs , rather than stacking RNN layers for modeling sequential data . paragraph : Deep recurrent transitions . Rather than stacking recurrent layers , some work focuses on increasing the depth of horizontal recurrence . pascanu2014construct pascanu2014construct have investigated various architectures to increase the depth of RNNs , inter alia Deep Transition RNNs address the problem of deep hidden - to - hidden transitions . graves2016adaptive graves2016adaptive proposed an adaptive computation time algorithm that learns how many micro time steps to take between receiving an input and emitting an output . Fast - Slow RNNs process data on different timescales by letting a fast cell iterate for a fixed number of time steps before a slow cell receives the next input . Multiscale RNNs e.g. Clockwork RNNs and Hierarchical Multiscale RNNs can be also regarded as architectures with increased recurrence depth . However as noted by zilly2017rhn zilly2017rhn , increase in recurrent depth results in a longer maximum path than stacking recurrent layers and makes training difficult without careful initialization or architectural choice . section : Conclusion In this paper , we proposed a method of stacking multiple LSTM layers for modeling sentences , dubbed CAS - LSTM . It uses not only hidden states but also cell states from the previous layer , for the purpose of controlling the vertical information flow in a more elaborate way . We evaluated the proposed method on various benchmark tasks : natural language inference , paraphrase identification , and sentiment classification . Our models achieve the new state - of - the - art accuracy on SNLI and Quora Question Pairs datasets and obtain comparable results on MultiNLI and SST datasets . The proposed architecture can replace any stacked LSTM under one weak restriction \u2014 the size of states should be identical across all layers . For future work we plan to apply the CAS - LSTM architecture beyond sentence modeling tasks . Various problems e.g. sequence labeling , sequence generation , and language modeling might benefit from sophisticated modulation on context integration . Aggregating diverse contexts from sequential data , e.g. those from forward and backward reading of text , could also be an intriguing research direction . section : Acknowledgments We thank Dan Edmiston for the review of the manuscript . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Quora_Question_Pairs"]]], "Method": [[["Bi-CAS-LSTM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Paraphrase_Identification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_2-layer_Bi-CAS-LSTM"]]], "Metric": [[["__Test_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SST-2_Binary_classification"]]], "Method": [[["Bi-CAS-LSTM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SST-5_Fine-grained_classification"]]], "Method": [[["Bi-CAS-LSTM"]]], "Metric": [[["Accuracy"]]], "Task": [[["Sentiment_Analysis"]]]}]}
{"docid": "TST3-SREX-0035", "doctext": "document : Robust Face Detection via Learning Small Faces on Hard Images Recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially occluded faces . A reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images . In this paper , we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images . Our intuitions are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking . We build an anchor - based deep face detector , which only output a single feature map with small anchors , to specifically learn small faces and train it by a novel hard image mining strategy . Extensive experiments have been conducted on WIDER FACE , FDDB , Pascal Faces , and AFW datasets to show the effectiveness of our method . Our method achieves APs of 95.7 , 94.9 and 89.7 on easy , medium and hard WIDER FACE val dataset respectively , which surpass the previous state - of - the - arts , especially on the hard subset . Code and model are available at . section : Introduction Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition . Stem from the recent successful development of deep neural networks , massive CNN - based face detection approaches have been proposed and achieved the state - of - the - art performance . However , face detection remains a challenging task due to occlusion , illumination , makeup , as well as pose and scale variance , as shown in the benchmark dataset WIDER FACE . Current state - of - the - art CNN - based face detectors attempt to address these challenges by employing more powerful backbone models , exploiting feature pyramid - style architectures to combine features from multiple detection feature maps , designing denser anchors and utilizing larger contextual information . These methods and techniques have been shown to be successful to build a robust face detector , and improve the performance towards human - level for most images . In spite of their success for most images , an evident performance gap still exists especially for those hard images which contain small , blurred and partially occluded faces . We realize that these hard images have become the main barriers for face detectors to achieve human - level detection performance . In Figure [ reference ] , we show that , even on the train set of WIDER FACE , the official pre - trained SSH still fails on some of the images with extremely hard faces . We show two such hard training images in the upper right corner in Figure [ reference ] . On the other hand , most training images with easy faces can be almost perfectly detected ( see the illustration in the right lower corner of Figure [ reference ] ) . As shown in left part of Figure [ reference ] , over two thirds of the training images already obtained perfect detection accuracy , which indicates that those easy images are less useful towards training a robust face detector . To address this issue , in this paper , we propose a robust face detector by putting more training focus on those hard images . This issue is most related to anchor - level hard example mining discussed in OHEM . However , due to the sparsity of ground - truth faces and positive anchors , traditional anchor - level hard example mining mainly focuses on mining hard negative anchors , and mining hard anchors on well - detected images exhibits less effectiveness since there is no useful information that can be further exploited in these easy images . To address this issue , we propose to mine hard examples at image level in parallel with anchor level . More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training . This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process . We show this strategy can make our detector more robust towards hard faces , without involving more complex network architecture and computation overhead . Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces . Small faces are typically hard and have attracted extensive research attention . Existing methods aim at building a scale - invariant face detector to learn and infer on both small and big faces , with multiple levels of detection features and anchors of different sizes . Compared with these methods , our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training . More specifically , large faces are automatically ignored during training due to our anchor design , so that the model can fully focus on the small hard faces . Additionally , experiments demonstrate that this design effectively achieves improvements on detecting all faces in spite of its simple and shallow architecture . To conclude , in this paper , we propose a novel face detector with the following contributions : We propose a hard image mining strategy , to improve the robustness of our detector to those extremely hard faces . This is done without any extra modules , parameters or computation overhead added on the existing detector . We design a single shot detector with only one detection feature map , which focuses on small faces with a specific range of sizes . This allows our model to be simple and focus on difficult small faces without struggling with scale variance . Our face detector establishes state - of - the - art performance on all popular face detection datasets , including WIDER FACE , FDDB , Pascal Faces , and AFW . We achieve 95.7 , 94.9 and 89.7 on easy , medium and hard WIDER FACE val dataset . Our method also achieves APs of 99.00 and 99.60 on Pascal Faces and AFW respectively , as well as a TPR of 98.7 on FDDB . The remainder of this paper is organized as follows . In Section [ reference ] , we discuss some studies have been done which are related to our paper . In Section [ reference ] , we dive into details of our proposed method , and we discuss experiment results and ablation experiments in Section [ reference ] . Finally , conclusions are drawn in Section [ reference ] . section : Related work Face detection has received extensive research attention . With the emergence of modern CNN and object detector , there are many face detectors proposed to achieve promising performances , by adapting general object detection framework into face detection domain . We briefly review hard example mining , face detection architecture , and anchor design & matching . subsection : Hard example mining Hard example mining is an important strategy to improve model quality , and has been studied extensively in image classification and general object detection . The main idea is to find some hard positive and hard negative examples at each step , and put more effort into training on those hard examples . Recently , with modern detection frameworks proposed to boost the performance , OHEM and Focal loss have been proposed to select hard examples . OHEM computed the gradients of the networks by selecting the proposals with highest losses in every minibatch ; while Focal loss aimed at naturally putting more focus on hard and misclassified examples by adding a factor to the standard cross entropy criterion . However , these algorithms mainly focused on anchor - level or proposal - level mining . It can not handle the imbalance of easy and hard images in the dataset . In our paper , we propose to exploit hard example mining on image level , hard image mining , to improve the quality of face detector on extremely hard faces . More specifically , we assign difficulty scores to training images while training with an SGD mechanism , and re - sample the training images to build a new training subset at the next epoch . subsection : Face Detection Architecture Recent state - of - the - art face detectors are generally built based on Faster - RCNN , R - FCN or SSD . SSH exploited the RPN ( Region Proposal Network ) from Faster - RCNN to detect faces , by building three detection feature maps and designing six anchors with different sizes attached to the detection feature maps . S FD and PyramidBox , on the other hand , adopted SSD as their detection architecture with six different detection feature maps . Different from S FD , PyramidBox exploited a feature pyramid - style structure to combine features from different detection feature maps . Our proposed method , on the other hand , only builds single level detection feature map , based on VGG16 , for classification and bounding - box regression , which is both simple and effective . subsection : Anchor design and matching Usually , anchors are designed to have different sizes to detect objects with different scales , in order to build a scale - invariant detector . SSD as well as its follow - up detectors S FD and PyramidBox , had six sets of anchors with different sizes , ranging from ( ) to ( ) , and their network architectures had six levels of detection feature maps , with resolutions ranging from to , respectively . Similarly , SSH had the same anchor setting , and those anchors were attached to three levels of detection feature maps with resolutions ranging from to . The difference between SSH and S DF is that in SSH , anchors with two neighboring sizes shared the same detection feature map , while in S DF , anchors with different sizes are attached to different detection feature maps . SNIP discussed an alternative approach to handle scales . It showed that CNNs are not robust to changes in scale , so training and testing on the same scales of an image pyramid can be a more optimal strategy . In our paper , we exploit this idea by limiting the anchor sizes to be ( ) , ( ) and ( ) . Then those faces with either too small or too big sizes will not be matched to any of the anchors , thus will be ignored during the training and testing . By removing those large anchors with sizes larger than ( ) , our network focuses more on small faces which are potentially more difficult . To deal with large faces , we use multiscale training and testing to resize them to match our anchors . Experiments show this design performs well on both small and big faces , although it has fewer detection feature maps and anchor sizes . section : Proposed method In this section , we introduce our proposed method for effective face detection . We first discuss the architecture of our detector in Section [ reference ] , then we elaborate our hard image mining strategy in Section [ reference ] , as well as some other useful training techniques in Section [ reference ] . subsection : Single - level small face detection framework The framework of our face detector is illustrated in Figure [ reference ] . We use VGG16 network as our backbone CNN , and combine conv4_3 and conv5_3 features , to build the detection feature map with both low - level and high - level semantic information . Similar to SSH , we apply 1 1 convolution layers after conv4_3 and conv5_3 to reduce dimension , and then apply a 3 3 convolution layer on the concatenation of these two dimension reduced features . The output feature of the 3 3 convolution layer is the final detection feature map , which will be fed into the detection head for classification and bounding - box regression . The detection feature map has a resolution of of the original image ( of size ) . We attach three anchors at each point in the grid as default face detection boxes . Then we do classification and bounding - box regression on those anchors . Unlike many other face detectors which build multiple feature maps to detect face with a variant range of scales , inspired by SNIP , faces are trained and inferred with roughly the same scales . We only have one detection feature map , with three sets of anchors attached to it . The anchors have sizes of ( ) , ( ) and ( ) , and the aspect ratio is set to be 1 . By making this configuration , our network only trains and infers on small and medium size of faces ; and we propose to handle large faces by shrinking the images in the test phase . We argue that there is no speed or accuracy degradation for large faces , since inferring on a tiny image ( with short side containing 100 or 300 pixels ) is very fast , and the shrinked large face will still have enough information to be recognized . To handle the difference of anchor sizes attached to the same detection feature map , we propose a detection head which uses different dilation rates for anchors with different sizes , as shown in Figure [ reference ] . The intuition is that in order to detect faces with different sizes , different effective receptive fields are required . This naturally requires the backbone feature map to be invariant to scales . To this end , we adopt different dilation rates for anchors with different sizes . For anchors with size ( ) , ( ) and ( ) , we use a convolution with kernel size of and dilation rate of , and to gather context features at different scales . These three convolution layers share weights to reduce the model size . With this design , the input of the convolution , will be aligned to the same location of faces , regardless of the size of faces and anchors . Ablation experiments show the effectiveness of this multi - dilation design . subsection : Hard image mining Different from OHEM discussed in Section [ reference ] , which selects proposals or anchors with the highest losses , we propose a novel hard image mining strategy at image level . The intuition is that most images in the dataset are very easy , and we can achieve a very high AP even on the hard subset of the WIDER FACE val dataset with our baseline model . We believe not all training images should be treated equally , and well - recognized images will not help towards training a more robust face detector . To put more attention on training hard images instead of easy ones , we use a subset of all training images , to contain hard ones for training . At the beginning of each epoch , we build based on the difficulty scores obtained in the previous epoch . We initially use all training images to train our model ( ) . This is due to the fact that our initial ImageNet pre - trained model will only give random guess towards face detection . In this case , there is no easy image . In other words , every image is considered as hard image and fed to the network for training at the first epoch . During the training procedure , we dynamically assign different difficulty scores to training images , which is defined by the metric Worst Positive Anchor Score ( WPAS ) : where is the set of positive anchors for image , with IoU over 0.5 against ground - truth boxes , is the classification logit and , are the logits of anchor for image to be foreground face and background . All images are initially marked as hard , and any image with WPAS greater than a threshold will be marked as easy image . At the beginning of each epoch , we first randomly shuffle the training dataset to generate the complete training list for the following epoch of training . Then given an image marked as easy , we remove it from with a probability of . The remaining training list , which focuses more on hard images , will be used for training at this epoch . Note that for multi - GPU training , each GPU will maintain its training list independently . In our experiments , we set the probability to be 0.7 , and the threshold to be 0.85 . subsection : Training strategy subsubsection : Multi - scale training and anchor matching Since we only have anchors covering a limited range of face scales , we train our model by varying the sizes of training images . During the training phase , we resize the training images so that the short side of the image contains pixels , where is randomly selected from . We also set an upper bound of 2000 pixels to the long side of the image considering the GPU memory limitation . For each anchor , we assign a label based on how well it matches with any ground - truth face bounding box . If an anchor has an IoU ( Intersection over Union ) over 0.5 against a ground - truth face bounding box , we assign to that anchor . On the other hand , if the IoU against any ground - truth face bounding box is lower than 0.3 , we assign to that anchor . All other anchors will be given as the label , and thus will be ignored in the classification loss . By doing so , we only train on faces with designated scales . Those faces with no anchor matching will be simply ignored , since we do not assign the anchor with largest IoU to it ( thus assign the corresponding anchor label ) as Faster - RCNN does . This anchor matching strategy will ignore the large faces , and our model can put more capacity on learning different face patterns on hard small faces instead of memorizing the change in scales . For the regression loss , all anchors with IoU greater than 0.3 against ground - truth faces will be taken into account and contribute to the smooth loss . We use a smaller threshold ( 0.3 ) because ( 1 ) this will allow imperfectly matched anchors to be able to localize the face , which may be useful during the testing and ( 2 ) the regression task has less supervision since unlike classification , there are no negative anchors for computing loss and the positive anchors are usually sparse . subsubsection : Anchor - level hard example mining OHEM has been proven to be useful for object detection and face detection in . During our training , in parallel with our newly proposed hard image mining , we also exploit the traditional hard anchor mining method to focus more on the hard and misclassificed anchors . Given a training image with size , there are anchors at the detection head , and we only select 256 of them to be involved in computing the classification loss . For all positive anchors with IoU greater than against ground - truth boxes , we select the top 64 of them with lowest confidences to be recognized as face . After selecting positive anchors , ( ) negative anchors with highest face confidence are selected to compute the classification loss as the hard negative anchors . Note that we only perform OHEM for classification loss , and we keep all anchors with IoU greater than 0.3 for computing regression loss , without selecting a subset based on either classification loss or bounding - box regression loss . subsubsection : Data augmentation Data augmentation is extremely useful to make the model robust to light , scale changes and small shifts . In our proposed method , we exploit cropping and photometric distortion as data augmentation . Given a training image after resizing , we crop a patch of it with a probability of . The patch has a height of and a width of which are independently drawn from and , where is the uniform distribution and , are the height and width of the resized training image . All ground - truth boxes whose centers are located inside the patch are kept . After the random cropping , we apply photometric distortion following SSD by randomly modifying the brightness , contrast , saturation and hue of the cropped image randomly . [ b ] 0.33 easy subset [ b ] 0.33 medium subset [ b ] 0.33 hard subset [ b ] 0.33 [ b ] 0.33 [ b ] 0.33 section : Experiments To verify the effectiveness of our model and proposed method , we conduct extensive experiments on popular face detection datasets , including WIDER FACE , FDDB , Pascal Faces and AFW . It is worth noting that the training is only performed on the train set of WIDER FACE , and we use the same model for evaluation on all these datasets without further fine - tuning . subsection : Experimental settings We train our model on the train set of WIDER FACE , which has 12880 images with 159k faces annotated . We flip all images horizontally , to double the size of our training dataset to 25760 . For each training image , we first randomly resize it , and then we use the cropping and photometric distortion data augmentation methods discussed in Section [ reference ] to pre - process the resized image . We use an ImageNet pre - trained VGG16 model to initialize our network backbone , and our newly introduced layers are randomly initialized with Gaussian initialization . We train the model with the itersize to be 2 , for 46k iterations , with a learning rate of , and then for another 14k iterations with a smaller learning rate of . During training , we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum . The first two blocks of VGG16 are frozen during the training , and the rest layers of VGG16 are set to have a double learning rate . Since our model is designed and trained on only small faces , we use a multiscale image pyramid for testing to deal with faces larger than our anchors . Specifically , we resize the testing image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels for evaluation on WIDER FACE dataset . We also follow the testing strategies used in PyramidBox such as horizontal flip and bounding - box voting . subsection : Experiment results WIDER FACE dataset includes 3226 images and 39708 faces labelled in the val dataset , with three subsets \u2013 easy , medium and hard . In Figure [ reference ] , we show the precision - recall ( PR ) curve and average precision ( AP ) for our model compared with many other state - of - the - arts on these three subsets . As we can see , our method achieves the best performance on the hard subset , and outperforms the current state - of - the - art by a large margin . Since the hard set is a super set of small and medium , which contains all faces taller than 10 pixels , the performance on hard set can represent the performance on the full testing dataset more accurately . Our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - the - arts . There is also a WIDER FACE test dataset with no annotations provided publicly . It contains 16097 images , and is evaluated by WIDER FACE author team . We report the performance of our method at Figure [ reference ] for the hard subset . FDDB dataset includes 5171 faces on a set of 2845 images , and we use our model trained on WIDER FACE train set to infer on the FDDB dataset . We use the raw bounding - box result without fitting it into ellipse to compute ROC . We show the discontinuous ROC curve at Figure [ reference ] compared with , and our method achieves the state - of - the - art performance of TPR=98.7 % given 1000 false positives . Pascal Faces dataset includes 1335 labeled faces on a set of 851 images extracted for the Pascal VOC dataset . We show the PR curve at Figure [ reference ] compared with , and our method achieves a new the state - of - the - art performance of AP=99.0 . AFW dataset includes 473 faces labelled in a set of 205 images . As shown in Figure [ reference ] compared with , our method achieves state - of - the - art and almost perfect performance , with an AP of 99.60 . subsection : Ablation study and diagnosis subsubsection : Ablation experiments In order to verify the performance of our single level face detector , as well as the effectiveness of our proposed hard image mining , the dilated - head classification and regression structure , we conduct various ablation experiments on the WIDER FACE val dataset . All results are summarized in Table [ reference ] . From Table [ reference ] , we can see that our single level baseline model can achieve performance comparable to the current state - of - the - art face detector , especially on the hard subset . Our model with single detection feature map performs better than the one with three detection feature maps , despite its shallower structure , fewer parameters and anchors . This confirms the effectiveness of our simple face detector with single detection feature map focusing on small faces . We also separately verify our newly proposed hard image mining ( HIM ) and dilated head architecture ( DH ) described in Subsection [ reference ] and Figure [ reference ] respectively . HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead . DH itself can also boost the performance , which shows the effectiveness of designing larger convolution for larger anchors . Combining HIM and DH together can improve further towards the state - of - the - art performance . subsubsection : Diagnosis of hard image mining We investigate the effects of our hard image mining mechanism . We show the ratio of and ( the ratio of the number of selected training images to the number of ignored training images ) in Figure [ reference ] for each epoch . We can see that at the first epoch , all training images are used to train the model . Meanwhile , as the training process continues , more and more training images will be ignored . At the last epoch , over a half images will be ignored and thus will not be included in . subsubsection : Diagnosis of data augmentation We investigate the effectiveness of the photometric distortion as well as the cropping mechanisms as discussed in Subsection [ reference ] . The ablation results evaluated on WIDER FACE val dataset are shown in Table [ reference ] . Both photometric distortion and cropping can contribute to a more robust face detector . subsubsection : Diagnosis of multi - scale testing Our face detector with one detection feature map is design for small face detection , and our anchors are only capable of capturing faces with sizes ranging from ( ) to ( ) . As a result , it is critical to adopt multi - scale testing to deal with large faces . Different from SSH , S FD and PyramidBox , our testing pyramid includes some extreme small scales ( short side contains only 100 or 300 pixels ) . In Table [ reference ] , we show the effectiveness of these extreme small scales to deal with easy and large images . Our full evaluation resizes the image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels respectively , to build an image pyramid . We diagnose the impact of the extra small scales ( 100 and 300 ) by removing them from the image pyramid . As shown in Table [ reference ] , the extra small scales are crucial to detect easy faces . Without resizing the short side to contain 100 and 300 pixels , the performance on easy subset is only , which is even lower than the performance on medium and hard which contain much harder faces . We will show in the next subsection that these extra small scales ( and ) lead to negligible computation overhead , due to the lower resolution . subsubsection : Diagnosis of accuracy / speed trade - off We evaluate the speed of our method as well as some other popular face detectors in Table [ reference ] . For fair comparison , we run all methods on the same machine , with one Titan X ( Maxwell ) GPU , and Intel Core i7 - 4770 K 3.50GHz . All methods except for PyramidBox are based on Caffe1 implementation , which is compiled with CUDA 9.0 and CUDNN 7 . For PyramidBox , we follow the official fluid code and the default configurations . We use the officially built PaddlePaddle with CUDA 9.0 and CUDNN 7 . For SSH , S FD and Pyramid , we use the official inference code and configurations . For SSH , we use multi - scale testing with the short side containing 500 , 800 , 1200 and 1600 pixels , and for S FD , we execute the official evaluation code with both multi - scale testing and horizontal flip . PyramidBox takes a similar testing configuration as S FD . As shown in Table [ reference ] , our detector can outperform SSH , S FD and PyramidBox significantly with a smaller inference time . Based on that , using horizontal flip can further improve the performance slightly . In terms of GPU memory usage , our method uses only a half of what PyramidBox occupies , while achieving better performance . Ours in Table [ reference ] indicates our method without extra small scales in inference , , evaluated with scales [ 600 , 1000 , 1400 ] . It is only faster than evaluation with [ 100 , 300 , 600 , 1000 , 1400 ] ( 1.59 compared with 1.70 ) . This proves that although our face detector is only trained on small faces , it can perform well on large faces , by simply shrinking the testing image with negligible computation overhead . section : Conclusion To conclude , we propose a novel face detector to focus on learning small faces on hard images , which achieves the state - of - the - art performance on all popular face detection datasets . We propose a hard image mining strategy by dynamically assigning difficulty scores to training images , and re - sampling subsets with hard images for training before each epoch . We also design a single shot face detector with only one detection feature map , to train and test on small faces . With these designs , our model can put more attention on learning small hard faces instead of memorizing change of scales . Extensive experiments and ablations have been done to show the effectiveness of our method , and our face detector achieves the state - of - the - art performance on all popular face detection datasets , including WIDER FACE , FDDB , Pascal Faces and AFW . Our face detector also enjoys faster multi - scale inference speed and less GPU memory usage . Our proposed method are flexible and can be applied to other backbones and tasks , which we remain as future work . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Annotated_Faces_in_the_Wild"]]], "Method": [[["Anchor-based"]]], "Metric": [[["AP"]]], "Task": [[["Face_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["FDDB"]]], "Method": [[["Anchor-based"]]], "Metric": [[["AP"]]], "Task": [[["Face_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["PASCAL_Face"]]], "Method": [[["Anchor-based"]]], "Metric": [[["AP"]]], "Task": [[["Face_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WIDER_Face__Hard_"]]], "Method": [[["Anchor-based"]]], "Metric": [[["AP"]]], "Task": [[["Face_Detection"]]]}]}
{"docid": "TST3-SREX-0036", "doctext": "Convolutional Pose Machines section : Abstract Pose Machines provide a sequential prediction framework for learning rich implicit spatial models . In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image - dependent spatial models for the task of pose estimation . The contribution of this paper is to implicitly model long - range dependencies between variables in structured prediction tasks such as articulated pose estimation . We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages , producing increasingly refined estimates for part locations , without the need for explicit graphical model - style inference . Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision , thereby replenishing back - propagated gradients and conditioning the learning procedure . We demonstrate state - of - the - art performance and outperform competing methods on standard benchmarks including the MPII , LSP , and FLIC datasets . section : Introduction We introduce Convolutional Pose Machines ( CPMs ) for the task of articulated pose estimation . CPMs inherit the benefits of the pose machine [ reference ] architecture - the implicit learning of long - range dependencies between image and multi - part cues , tight integration between learning and inference , a modular sequential design - and combine them with the advantages afforded by convolutional architectures : the ability to learn feature representations for both image and spatial context directly from data ; a differentiable architecture that allows for globally joint training with backpropagation ; and the ability to efficiently handle large training datasets . CPMs consist of a sequence of convolutional networks that repeatedly produce 2D belief maps [ reference ] for the location [ reference ] We use the term belief in a slightly loose sense , however the belief of each part . At each stage in a CPM , image features and the belief maps produced by the previous stage are used as input . The belief maps provide the subsequent stage an expressive non - parametric encoding of the spatial uncertainty of location for each part , allowing the CPM to learn rich image - dependent spatial models of the relationships between parts . Instead of explicitly parsing such belief maps either using graphical models [ reference ][ reference ][ reference ] or specialized post - processing steps [ reference ][ reference ] , we learn convolutional networks that directly operate on intermediate belief maps and learn implicit image - dependent spatial models of the relationships between parts . The overall proposed multistage architecture is fully differentiable and therefore can be trained in an end - to - end fashion using backpropagation . At a particular stage in the CPM , the spatial context of part beliefs provide strong disambiguating cues to a subsequent stage . As a result , each stage of a CPM produces belief maps with increasingly refined estimates for the locations of each part ( see Figure 1 ) . In order to capture longrange interactions between parts , the design of the network in each stage of our sequential prediction framework is motivated by the goal of achieving a large receptive field on both the image and the belief maps . We find , through experiments , that large receptive fields on the belief maps are crucial for learning long range spatial relationships and remaps described are closely related to beliefs produced in message passing inference in graphical models . The overall architecture can be viewed as an unrolled mean - field message passing inference algorithm [ reference ] that is learned end - to - end using backpropagation . sult in improved accuracy . Composing multiple convolutional networks in a CPM results in an overall network with many layers that is at risk of the problem of vanishing gradients [ reference ][ reference ][ reference ][ reference ] during learning . This problem can occur because backpropagated gradients diminish in strength as they are propagated through the many layers of the network . While there exists recent work 2 which shows that supervising very deep networks at intermediate layers aids in learning [ reference ][ reference ] , they have mostly been restricted to classification problems . In this work , we show how for a structured prediction problem such as pose estimation , CPMs naturally suggest a systematic framework that replenishes gradients and guides the network to produce increasingly accurate belief maps by enforcing intermediate supervision periodically through the network . We also discuss different training schemes of such a sequential prediction architecture . Our main contributions are ( a ) learning implicit spatial models via a sequential composition of convolutional architectures and ( b ) a systematic approach to designing and training such an architecture to learn both image features and image - dependent spatial models for structured prediction tasks , without the need for any graphical model style inference . We achieve state - of - the - art results on standard benchmarks including the MPII , LSP , and FLIC datasets , and analyze the effects of jointly training a multi - staged architecture with repeated intermediate supervision . section : Related Work The classical approach to articulated pose estimation is the pictorial structures model [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] in which spatial correlations between parts of the body are expressed as a tree - structured graphical model with kinematic priors that couple connected limbs . These methods have been successful on images where all the limbs of the person are visible , but are prone to characteristic errors such as double - counting image evidence , which occur because of correlations between variables that are not captured by a tree - structured model . The work of Kiefel et al . [ reference ] is based on the pictorial structures model but differs in the underlying graph representation . Hierarchical models [ reference ][ reference ] represent the relationships between parts at different scales and sizes in a hierarchical tree structure . The underlying assumption of these models is that larger parts ( that correspond to full limbs instead of joints ) can often have discriminative image structure that can be easier to detect and consequently help reason about the location of smaller , harder - to - detect parts . Non - tree models [ reference ][ reference ][ reference ][ reference ][ reference ] incorporate interactions that introduce loops to augment the tree structure with additional edges that capture symmetry , occlusion and long - range relation - ships . These methods usually have to rely on approximate inference during both learning and at test time , and therefore have to trade off accurate modeling of spatial relationships with models that allow efficient inference , often with a simple parametric form to allow for fast inference . In contrast , methods based on a sequential prediction framework [ reference ] learn an implicit spatial model with potentially complex interactions between variables by directly training an inference procedure , as in [ reference ][ reference ][ reference ][ reference ] . There has been a recent surge of interest in models that employ convolutional architectures for the task of articulated pose estimation [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] . Toshev et al . [ reference ] take the approach of directly regressing the Cartesian coordinates using a standard convolutional architecture [ reference ] . Recent work regresses image to confidence maps , and resort to graphical models , which require hand - designed energy functions or heuristic initialization of spatial probability priors , to remove outliers on the regressed confidence maps . Some of them also utilize a dedicated network module for precision refinement [ reference ][ reference ] . In this work , we show the regressed confidence maps are suitable to be inputted to further convolutional networks with large receptive fields to learn implicit spatial dependencies without the use of hand designed priors , and achieve state - of - the - art performance over all precision region without careful initialization and dedicated precision refinement . Pfister et al . [ reference ] also used a network module with large receptive field to capture implicit spatial models . Due to the differentiable nature of convolutions , our model can be globally trained , where Tompson et al . [ reference ] and Steward et al . [ reference ] also discussed the benefit of joint training . Carreira et al . [ reference ] train a deep network that iteratively improves part detections using error feedback but use a cartesian representation as in [ reference ] which does not preserve spatial uncertainty and results in lower accuracy in the highprecision regime . In this work , we show how the sequential prediction framework takes advantage of the preserved uncertainty in the confidence maps to encode the rich spatial context , with enforcing the intermediate local supervisions to address the problem of vanishing gradients . section : Method section : Pose Machines We denote the pixel location of the p - th anatomical landmark ( which we refer to as a part ) , Y p \u2208 Z \u2282 R 2 , where Z is the set of all ( u , v ) locations in an image . Our goal is to predict the image locations Y = ( Y 1 , . . . , Y P ) for all P parts . A pose machine [ reference ] ( see Figure 2a and 2b ) consists of a sequence of multi - class predictors , g t ( \u00b7 ) , that are trained to predict the location of each part in each level of the hierarchy . In each stage t \u2208 { 1 . . . T } , the classifiers g t predict beliefs for assigning a location to each part Y p = z , \u2200z \u2208 Z , based on features extracted from the image at the location z denoted by x z \u2208 R d and contextual information from the preceding classifier in the neighbor - 26 \u21e5 26 60 \u21e5 60 96 \u21e5 96 160 \u21e5 160 240 \u21e5 240 320 \u21e5 320 400 \u21e5 400 hood around each Y p in stage t. A classifier in the first stage t = 1 , therefore produces the following belief values : where is the score predicted by the classifier g 1 for assigning the p th part in the first stage at image location z. We represent all the beliefs of part p evaluated at every location z = ( u , v ) T in the image as b p t \u2208 R w\u00d7h , where w and h are the width and height of the image , respectively . That is , b For convenience , we denote the collection of belief maps for all the parts as b t \u2208 R w\u00d7h\u00d7 ( P + 1 ) ( P parts plus one for background ) . In subsequent stages , the classifier predicts a belief for assigning a location to each part Y p = z , \u2200z \u2208 Z , based on ( 1 ) features of the image data x t z \u2208 R d again , and ( 2 ) contextual information from the preceeding classifier in the neighborhood around each Y p : where \u03c8 t>1 ( \u00b7 ) is a mapping from the beliefs b t\u22121 to context features . In each stage , the computed beliefs provide an increasingly refined estimate for the location of each part . Note that we allow image features x z for subsequent stage to be different from the image feature used in the first stage x. The pose machine proposed in [ reference ] used boosted random forests for prediction ( { g t } ) , fixed hand - crafted image features across all stages ( x = x ) , and fixed hand - crafted context feature maps ( \u03c8 t ( \u00b7 ) ) to capture spatial context across all stages . section : Convolutional Pose Machines We show how the prediction and image feature computation modules of a pose machine can be replaced by a deep convolutional architecture allowing for both image and contextual feature representations to be learned directly from data . Convolutional architectures also have the advantage of being completely differentiable , thereby enabling endto - end joint training of all stages of a CPM . We describe our design for a CPM that combines the advantages of deep convolutional architectures with the implicit spatial modeling afforded by the pose machine framework . section : Keypoint Localization Using Local Image Evidence The first stage of a convolutional pose machine predicts part beliefs from only local image evidence . Figure 2c shows the network structure used for part detection from local image evidence using a deep convolutional network . The evidence is local because the receptive field of the first stage of the network is constrained to a small patch around the output pixel location . We use a network structure composed of five convolutional layers followed by two 1 \u00d7 1 convolutional layers which results in a fully convolutional archi - Figure 3 : Spatial context from belief maps of easier - to - detect parts can provide strong cues for localizing difficult - to - detect parts . The spatial contexts from shoulder , neck and head can help eliminate wrong ( red ) and strengthen correct ( green ) estimations on the belief map of right elbow in the subsequent stages . tecture [ reference ] . In practice , to achieve certain precision , we normalize input cropped images to size 368 \u00d7 368 ( see Section 4.2 for details ) , and the receptive field of the network shown above is 160 \u00d7 160 pixels . The network can effectively be viewed as sliding a deep network across an image and regressing from the local image evidence in each 160 \u00d7 160 image patch to a P + 1 sized output vector that represents a score for each part at that image location . section : Sequential Prediction with Learned Spatial Context Features While the detection rate on landmarks with consistent appearance , such as the head and shoulders , can be favorable , the accuracies are often much lower for landmarks lower down the kinematic chain of the human skeleton due to their large variance in configuration and appearance . The landscape of the belief maps around a part location , albeit noisy , can , however , be very informative . Illustrated in Figure 3 , when detecting challenging parts such as right elbow , the belief map for right shoulder with a sharp peak can be used as a strong cue . A predictor in subsequent stages ( g t>1 ) can use the spatial context ( \u03c8 t>1 ( \u00b7 ) ) of the noisy belief maps in a region around the image location z and improve its predictions by leveraging the fact that parts occur in consistent geometric configurations . In the second stage of a pose machine , the classifier g 2 accepts as input the image features x 2 z and features computed on the beliefs via the feature function \u03c8 for each of the parts in the previous stage . The feature function \u03c8 serves to encode the landscape of the belief maps from the previous stage in a spatial region around the location z of the different parts . For a convolutional pose machine , we do not have an explicit function that computes context features . Instead , we define \u03c8 as being the receptive field of the predictor on the beliefs from the previous stage . The design of the network is guided by achieving a receptive field at the output layer of the second stage network that is large enough to allow the learning of potentially complex and long - range correlations between parts . By simply supplying features on the outputs of the previous stage ( as opposed to specifying potential functions in a graphical model ) , the convolutional layers in the subsequent stage allow the classifier to freely combine contextual information by picking the most predictive features . The belief maps from the first stage are generated from a network that examined the image locally with a small receptive field . In the second stage , we design a network that drastically increases the equivalent receptive field . Large receptive fields can be achieved either by pooling at the expense of precision , increasing the kernel size of the convolutional filters at the expense of increasing the number of parameters , or by increasing the number of convolutional layers at the risk of encountering vanishing gradients during training . Our network design and corresponding receptive field for the subsequent stages ( t \u2265 2 ) is shown in Figure 2d . We choose to use multiple convolutional layers to achieve large receptive field on the 8\u00d7 downscaled heatmaps , as it allows us to be parsimonious with respect to the number of parameters of the model . We found that our stride - 8 network performs as well as a stride - 4 one even at high precision region , while it makes us easier to achieve larger receptive fields . We also repeat similar structure for image feature maps to make the spatial context be image - dependent and allow error correction , following the structure of pose machine . We find that accuracy improves with the size of the receptive field . In Figure 4 we show the improvement in accuracy on the FLIC dataset [ reference ] as the size of the receptive field on the original image is varied by varying the architecture without significantly changing the number of parameters , through a series of experimental trials on input images normalized to a size of 304 \u00d7 304 . We see that the accuracy improves as the effective receptive field increases , and starts to saturate around 250 pixels , which also happens to be roughly the size of the normalized object . This improvement in accuracy with receptive field size suggests that the network does indeed encode long range interactions between parts and that doing so is beneficial . In our best performing setting in Figure 2 , we normalize cropped images into a larger size of 368 \u00d7 368 pixels for better precision , and the receptive field of the second stage output on the belief maps of the first stage is set to 31 \u00d7 31 , which is equivalently 400 \u00d7 400 pixels on the original image , where the radius can usually cover any pair of the parts . With more stages , the effective receptive field is even larger . In the following section we show our results from up to 6 stages . section : Learning in Convolutional Pose Machines The design described above for a pose machine results in a deep architecture that can have a large number of layers . Training such a network with many layers can be prone to the problem of vanishing gradients [ reference ][ reference ][ reference ] where , as observed by Bradley [ reference ] and Bengio et al . [ reference ] , the magnitude of back - propagated gradients decreases in strength with the number of intermediate layers between the output layer and the input layer . Fortunately , the sequential prediction framework of the pose machine provides a natural approach to training our deep architecture that addresses this problem . Each stage of the pose machine is trained to repeatedly produce the belief maps for the locations of each of the parts . We encourage the network to repeatedly arrive at such a representation by defining a loss function at the output of each stage t that minimizes the l 2 distance between the predicted and ideal belief maps for each part . The ideal belief map for a part p is written as b p * ( Y p = z ) , which are created by putting Gaussian peaks at ground truth locations of each body part p. The cost function we aim to minimize at the output of each stage at each level is therefore given by : The overall objective for the full architecture is obtained by adding the losses at each stage and is given by : We use standard stochastic gradient descend to jointly train all the T stages in the network . To share the image feature x across all subsequent stages , we share the weights of corresponding convolutional layers ( see Figure 2 ) across stages t \u2265 2 . section : Evaluation section : Analysis Addressing vanishing gradients . The objective in Equation 5 describes a decomposable loss function that operates on different parts of the network ( see Figure 2 ) . Specifically , each term in the summation is applied to the network after each stage t effectively enforcing supervision in intermediate stages through the network . Intermediate supervision has the advantage that , even though the full architecture can have many layers , it does not fall prey to the vanishing gradient problem as the intermediate loss functions replenish the gradients at each stage . We verify this claim by observing histograms of gradient magnitude ( see Figure 5 ) at different depths in the architecture across training epochs for models with and without intermediate supervision . In early epochs , as we move from the output layer to the input layer , we observe on the model larger variance across all layers , suggesting that learning is indeed occurring in all the layers thanks to intermediate supervision . We also notice that as training progresses , the variance in the gradient magnitude distributions decreases pointing to model convergence . Benefit of end - to - end learning . We see in Figure 6a that replacing the modules of a pose machine with the appropriately designed convolutional architecture provides a large boost of 42.4 percentage points over the previous approach of [ reference ] in the high precision regime ( PCK@0.1 ) and 30.9 percentage points in the low precision regime ( PCK@0.2 ) . Comparison on training schemes . We compare different variants of training the network in Figure 6b on the LSP dataset with person - centric ( PC ) annotations . To demonstrate the benefit of intermediate supervision with joint training across stages , we train the model in four ways : ( i ) training from scratch using a global loss function that enforces intermediate supervision ( ii ) stage - wise ; where each stage is trained in a feed - forward fashion and stacked ( iii ) as same as ( i ) but initialized with weights from ( ii ) , and ( iv ) as same as ( i ) but with no intermediate supervision . We find that network ( i ) outperforms all other training methods , showing that intermediate supervision and joint training across stage is indeed crucial in achieving good performance . The stagewise training in ( ii ) saturate at suboptimal , and the jointly fine - tuning in ( iii ) improves from this sub - optimal to the accuracy level closed to ( i ) , however with effectively longer training iterations . Performance across stages . We show a comparison of performance across each stage on the LSP dataset ( PC ) in Figure 6c . We show that the performance increases monotonically until 5 stages , as the predictors in subsequent stages make use of contextual information in a large receptive field on the previous stage beliefs maps to resolve confusions between parts and background . We see diminishing returns at the 6th stage , which is the number we choose for reporting our best results in this paper for LSP and MPII datasets . section : Datasets and Quantitative Analysis In this section we present our numerical results in various standard benchmarks including the MPII , LSP , and FLIC datasets . To have normalized input samples of 368 \u00d7 368 for training , we first resize the images to roughly make the samples into the same scale , and then crop or pad the image according to the center positions and rough scale estimations provided in the datasets if available . In datasets such as LSP without these information , we estimate them according to joint positions or image sizes . For testing , we perform similar resizing and cropping ( or padding ) , but estimate center position and scale only from image sizes when necessary . In addition , we merge the belief maps from different scales ( perturbed around the given one ) for final predictions , to handle the inaccuracy of the given scale estimation . We define and implement our model using the Caffe [ 13 ] libraries for deep learning . We publicly release the source code and details on the architecture , learning parameters , design decisions and data augmentation to ensure full reproducibility . [ reference ] MPII Human Pose Dataset . We show in Figure 8 our results on the MPII Human Pose dataset [ reference ] which consists more than 28000 training samples . We choose to randomly augment the data with rotation degrees in [ \u221240 \u2022 , 40 \u2022 ] , scaling with factors in [ 0.7 , 1.3 ] , and horizonal flipping . The evaluation is based on PCKh metric [ reference ] where the error tolerance is normalized with respect to head size of the target . Because there often are multiple people in the proximity of the interested person ( rough center position is given in the dataset ) , we made two sets of ideal belief maps for training : one includes all the peaks for every person appearing in the proximity of the primary subject and the second type where we only place peaks for the primary subject . We supply the first set of belief maps to the loss layers in the first stage as the initial stage only relies on local image evidence to make predictions . We supply the second type of belief maps to the Wrists Elbows ( a ) ( b ) loss layers of all subsequent stages . We also find that supplying to all subsequent stages an additional heat - map with a Gaussian peak indicating center of the primary subject is beneficial . Our total PCKh - 0.5 score achieves state of the art at 87.95 % ( 88.52 % when adding LSP training data ) , which is 6.11 % higher than the closest competitor , and it is noteworthy that on the ankle ( the most challenging part ) , our PCKh - 0.5 score is 78.28 % ( 79.41 % when adding LSP training data ) , which is 10.76 % higher than the closest competitor . This result shows the capability of our model to capture long distance context given ankles are the farthest parts from head and other more recognizable parts . Figure 11 shows our accuracy is also consistently significantly higher than other methods across various view angles defined in [ reference ] , especially in those challenging non - frontal views . In summary , our method improves the accuracy in all parts , over all precisions , across all view angles , and is the first one achieving such high accuracy without any pre - training from other data , or post - inference parsing with hand - design priors or initialization of such a structured prediction task as in [ reference ][ reference ] . Our methods also does not need another module dedicated to location refinement as in [ reference ] to achieve great high - precision accuracy with a stride - 8 network . Leeds Sports Pose ( LSP ) Dataset . We evaluate our method on the Extended Leeds Sports Dataset [ reference ] that consists of 11000 images for training and 1000 images for testing . We trained on person - centric ( PC ) annotations and evaluate our method using the Percentage Correct Keypoints ( PCK ) metric [ reference ] . Using the same augmentation scheme as for the MPI dataset , our model again achieves state of the art at 84.32 % ( 90.5 % when adding MPII train - MPII FLIC LSP Figure 10 : Qualitative results of our method on the MPII , LSP and FLIC datasets respectively . We see that the method is able to handle non - standard poses and resolve ambiguities between symmetric parts for a variety of different relative camera views . ing data ) . Note that adding MPII data here significantly boosts our performance , due to its labeling quality being much better than LSP . Because of the noisy label in the LSP dataset , Pishchulin et al . [ reference ] reproduced the dataset with original high resolution images and better labeling quality . FLIC Dataset . We evaluate our method on the FLIC Dataset [ reference ] which consists of 3987 images for training and 1016 images for testing . We report accuracy as per the metric introduced in Sapp et al . [ reference ] for the elbow and wrist joints in Figure 12 . Again , we outperform all prior art at PCK@0.2 with 97.59 % on elbows and 95.03 % on wrists . In higher precision region our advantage is even more significant : 14.8 percentage points on wrists and 12.7 percentage points on elbows at PCK@0.05 , and 8.9 percentage points on wrists and 9.3 percentage points on elbows at PCK@0.1 . section : Discussion Convolutional pose machines provide an end - to - end architecture for tackling structured prediction problems in computer vision without the need for graphical - model style inference . We showed that a sequential architecture composed of convolutional networks is capable of implicitly learning a spatial models for pose by communicating increasingly refined uncertainty - preserving beliefs between stages . Problems with spatial dependencies between variables arise in multiple domains of computer vision such as semantic image labeling , single image depth prediction and object detection and future work will involve extending our architecture to these problems . Our approach achieves state of the art accuracy on all primary benchmarks , however we do observe failure cases mainly when multiple people are in close proximity . Handling multiple people in a single end - to - end architecture is also a challenging problem and an interesting avenue for future work . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["FLIC_Elbows"]]], "Method": [[["Convolutional_Pose_Machines"]]], "Metric": [[["PCK_0_2"]]], "Task": [[["Pose_Estimation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["FLIC_Wrists"]]], "Method": [[["Convolutional_Pose_Machines"]]], "Metric": [[["PCK_0_2"]]], "Task": [[["Pose_Estimation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Leeds_Sports_Poses"]]], "Method": [[["Convolutional_Pose_Machines"]]], "Metric": [[["PCK"]]], "Task": [[["Pose_Estimation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["MPII_Human_Pose"]]], "Method": [[["Convolutional_Pose_Machines"]]], "Metric": [[["PCKh-0_5"]]], "Task": [[["Pose_Estimation"]]]}]}
{"docid": "TST3-SREX-0037", "doctext": "Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING section : ABSTRACT In this paper , we study the problem of question answering when reasoning over multiple facts is required . We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts . QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time . Our experiments show that QRN produces the state - of - the - art results in bAbI QA and dialog tasks , and in a real goal - oriented dialog dataset . In addition , QRN formulation allows parallelization on RNN 's time axis , saving an order of magnitude in time complexity for training and inference . section : INTRODUCTION In this paper , we address the problem of question answering ( QA ) when reasoning over multiple facts is required . For example , consider we know that Frogs eat insects and Flies are insects . Then answering Do frogs eat flies ? requires reasoning over both of the above facts . Question answering , more specifically context - based QA , has been extensively studied in machine comprehension tasks [ reference ][ reference ][ reference ][ reference ] . However , most of the datasets are primarily focused on lexical and syntactic understanding , and hardly concentrate on inference over multiple facts . Recently , several datasets aimed for testing multi - hop reasoning have emerged ; among them are story - based QA and the dialog task . Recurrent Neural Network ( RNN ) and its variants , such as Long Short - Term Memory ( LSTM ) [ reference ] and Gated Recurrent Unit ( GRU ) [ reference ] , are popular choices for modeling natural language . However , when used for multi - hop reasoning in question answering , purely RNN - based models have shown to perform poorly . This is largely due to the fact that RNN 's internal memory is inherently unstable over a long term . For this reason , most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory [ reference ][ reference ][ reference ][ reference ] . The attention mechanism allows these models to focus on a single sentence in each layer . They can sequentially read multiple relevant sentences from the memory with multiple layers to perform multi - hop reasoning . However , one major drawback of these standard attention mechanisms is that they are insensitive to the time step ( memory address ) of the sentences when accessing them . Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ( Figure 1 ) . QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time . For instance in Figure 1b , the original question , Where is the apple ? , can not be directly answered by any single sentence from the story . After observing the first sentence , Sandra got the apple there , QRN transforms the original question to a reduced query Where is Sandra ? , which is presumably 1 Code is publicly available at : seominjoon.github.io / qrn / Figure 1 : ( 1a ) QRN unit , ( 1b ) 2 - layer QRN on 5 - sentence story , and ( 1c ) entire QA system ( QRN and input / output modules ) . x , q , \u0177 are the story , question and predicted answer in natural language , respectively . x = x1 , . . . , xT , q , \u0177 are their corresponding vector representations ( upright font ) . \u03b1 and \u03c1 are update gate and reduce functions , respectively.\u0177 is assigned to be h 2 5 , the local query at the last time step in the last layer . Also , red - colored text is the inferred meanings of the vectors ( see ' Interpretations ' of Section 5.3 ) . easier to answer than the original question given the context provided by the first sentence . 2 Unlike RNN - based models , QRN 's candidate state ( h t in Figure 1a ) does not depend on the previous hidden state ( h t\u22121 ) . Compared to memory - based approaches [ reference ][ reference ][ reference ][ reference ] , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in Figure 2 ) , and the query updates are performed locally . In short , the main contribution of QRN is threefold . First , QRN is a simple variant of RNN that reduces the query given the context sentences in a differentiable manner . Second , QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively . Hence it is well - suited for sequential data with both local and global interactions ( note that QRN is not the replacement of RNN , which is arguably better for modeling complex local interactions ) . Third , unlike most RNN - based models , QRN can be parallelized over time by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) . In fact , the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN , hence effectively addressing the long - term dependency . We experimentally demonstrate these contributions by achieving the state - of - the - art results on story - based QA and interactive dialog datasets . section : MODEL In story - based QA ( or dialog dataset ) , the input is the context as a sequence of sentences ( story or past conversations ) and a question in natural language ( equivalent to the user 's last utterance in the dialog ) . The output is the predicted answer to the question in natural language ( the system 's next utterance in the dialog ) . The only supervision provided during training is the answer to the question . In this paper we particularly focus on end - to - end solutions , i.e. , the only supervision comes from questions and answers , and we restrain from using manually defined rules or external language resources , such as lexicon or dependency parser . Let x 1 , . . . , x T denote the sequence of sentences , where T is the number of sentences in the story , and let q denote the question . Let\u0177 denote the predicted answer , and y denote the true answer . Our proposed system for end - to - end QA task is divided into three modules ( Figure 1c ) : input module , QRN layers , and output module . Input module . Input module maps each sentence x t and the question q to d - dimensional vector space , x t \u2208 R d and q t \u2208 R d . We adopt a previous solution for the input module ( details in Section 5 ) . QRN layers . QRN layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space , \u0177 \u2208 R d . A QRN layer refers to the recurrent application of a QRN unit , which can be considered as a variant of RNN with two inputs , two outputs , and a hidden state ( reduced query ) , all of which operate in vector space . The details of the QRN module is explained throughout this section ( 2.1 , 2.2 ) . Output module . Output module maps\u0177 obtained from QRN to a natural language answer\u0177 . Similar to the input module , we adopt a standard solution for the output module ( details in Section 5 ) . We first formally define the base model of a QRN unit , and then we explain how we connect the input and output modules to it ( Section 2.1 ) . We also present a few extensions to the network that can improve QRN 's performance ( Section 2.2 ) . Finally , we show that QRN can be parallelized over time , giving computational advantage over most RNN - based models by one order of magnitude ( Section 3 ) . section : QRN UNIT As an RNN - based model , QRN is a single recurrent unit that updates its hidden state ( reduced query ) through time and layers . Figure 1a depicts the schematic structure of a QRN unit , and Figure 1b demonstrates how layers are stacked . A QRN unit accepts two inputs ( local query vector q t \u2208 R d and sentence vector x t \u2208 R d ) , and two outputs ( reduced query vector h t \u2208 R d , which is similar to the hidden state in RNN , and the sentence vector x t from the input without modification ) . The local query vector is not necessarily identical to the original query ( question ) vector q. In order to compute the outputs , we use update gate function \u03b1 : Intuitively , the update gate function measures the relevance between the sentence and the local query and is used to update the hidden state . The reduce function transforms the local query input to a candidate state which is a new reduced ( easier ) query given the sentence . The outputs are calculated with the following equations : where z t is the scalar update gate , h t is the candidate reduced query , and h t is the final reduced query at time step t , \u03c3 ( \u00b7 ) is sigmoid activation , tanh ( \u00b7 ) is hyperboolic tangent activation ( applied element - wise ) , \u2022 is element - wise vector multiplication , and [ ; ] is vector concatenation along the row . As a base case , h 0 = 0 . Here we have explicitly defined \u03b1 and \u03c1 , but they can be any reasonable differentiable functions . The update gate is similar to the global attention mechanism [ reference ][ reference ] in that it measures the similarity between the sentence ( a memory slot ) and the query . However , a significant difference is that the update gate is computed using sigmoid ( \u03c3 ) function on the current memory slot only ( hence internally embedded within the unit ) , whereas the global attention is computed using softmax function over the entire memory ( hence globally defined ) . The update gate can be rather considered as local sigmoid attention . section : Stacking layers We just showed the single - layer case of QRN , but QRN with multiple layers is able to perform reasoning over multiple facts more effectively , as shown in the example of Figure 1b . In order to stack several layers of QRN , the outputs of the current layer are used as the inputs to the next layer . That is , using superscript k to denote the current layer 's index ( assuming 1 - based indexing ) , we let q k + 1 t = h k t . Note that x t is passed to the next layer without any modification , so we do not put a layer index on it . Bi - direction . So far we have assumed that QRN only needs to look at past sentences , whereas often times , query answers can depend on future sentences . For instance , consider a sentence \" John dropped the football . \" at time t. Then , even if there is no mention about the \" football \" in the past ( at time i < t ) , it can be implied that \" John \" has the \" football \" at the current time t. In order to incorporate the future dependency , we obtain \u2212 \u2192 h t and \u2190 \u2212 h t in both forward and backward directions , respectively , using Equation 3 . We then add them together to get q t for the next layer . That is , [ reference ] are shared between the two directions . Connecting input and output modules . Figure 1c depicts how QRN is connected with the input and output modules . In the first layer of QRN , q 1 t = q for all t , where q is obtained from the input module by processing the natural language question input q. x t is also obtained from x t by the same input module . The output at the last time step in the last layer is passed to the output module . That is , y = h K t where K represent the number of layers in the network . Then the output module gives the predicted answer\u0177 in natural language . section : EXTENSIONS Here we introduce a few extensions of QRN , and later in our experiments , we test QRN 's performance with and without each of these extensions . Reset gate . Inspired by GRU [ reference ] , we found that it is useful to allow the QRN unit to reset ( nullify ) the candidate reduced query ( i.e. , h t ) when necessary . For this we use a reset gate function \u03b2 : , which can be defined similarly to the update gate function : where W ( r ) \u2208 R 1\u00d7d is a weight matrix , and b ( r ) \u2208 R is a bias term . Equation 3 is rewritten as Note that we do not use the reset gate in the last layer . Vector gates . As in LSTM and GRU , update and reset gates can be vectors instead of scalar values for fine - controlled gating . For vector gates , we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d. Then we obtain z t , r t \u2208 R d ( instead of z t , r t \u2208 R ) , and these can be element - wise multiplied ( \u2022 ) instead of being broadcasted in Equation 3 and 6 . section : PARALLELIZATION An important advantage of QRN is that the recurrent updates in Equation 3 and 5 can be computed in parallel across time . This is in contrast with most RNN - based models that can not be parallelized , where computing the candidate hidden state at time t explicitly requires the previous hidden state . In QRN , the final reduced queries ( h t ) can be decomposed into computing over candidate reduced queries ( h t ) , without looking at the previous reduced query . Here we primarily show that the query update in Equation 3 can be parallelized by rewriting the equation with matrix operations . The extension to Equation 5 is straightforward . The proof for QRN with vector gates is shown in Appendix B. The recursive definition of Equation 3 can be explicitly written as Let b i = log ( 1 \u2212 z i ) for brevity . Then we can rewrite Equation 7 as the following equation : [ reference ] Figure 2 : The schematics of QRN and the two state - of - the - art models , End - to - End Memory Networks ( N2N ) and Improved Dynamic Memory Networks ( DMN + ) , simplified to emphasize the differences among the models . AGRU is a variant of GRU where the update gate is replaced with soft attention , proposed by [ reference ] . where L , L \u2208 R T \u00d7T are lower and strictly lower triangular matrices of 1 's , respectively , \u2022 is elementwise multiplication , and B is a matrix where T b 's are tiled across the column , i. section : RELATED WORK QRN is inspired by RNN - based models with gating mechanism , such as LSTM [ reference ] and GRU [ reference ] . While GRU and LSTM use the previous hidden state and the current input to obtain the candidate hidden state , QRN only uses the current two inputs to obtain the candidate reduced query ( equivalent to candidate hidden state ) . We conjecture that this not only gives computational advantage via parallelization , but also makes training easier , i.e. , avoiding vanishing gradient ( which is critical for long - term dependency ) , overfitting ( by simplifying the model ) , and converging to local minima . The idea of structurally simplifying ( constraining ) RNNs for learning longer - term patterns has been explored in recent previous work , such as Structurally Constrained Recurrent Network [ reference ] and Strongly - Typed Recurrent Neural Network ( STRNN ) [ reference ] . QRN is similar to STRNN in that both architectures use gating mechanism , and the gates and the candidate hidden states do not depend on the previous hidden states , which simplifies the recurrent relation . However , QRN can be distinguished from STRNN in three ways . First , QRN 's update gate simulates attention mechanism , measuring the relevance between the input sentence and query . On the other hand , the gates in STRNN can be considered as the simplification of LSTM / GRU by removing their dependency on previous hidden state . Second , QRN is an RNN that is natively compatible with context - based QA tasks , where the QRN unit accepts two inputs , i.e. each context sentence and query . This is distinct from STRNN which has only one input . Third , we show that QRN is timewise - parallelizable on GPUs . Our parallelization algorithm is also applicable to STRNN . End - to - end Memory Network ( N2N ) [ reference ] uses external memory with multi - layer attention mechanism to focus on sentences that are relevant to the question . There are two key differences between N2N and our QRN . First , N2N summarizes the entire memory in each layer to control the attention in the next layer ( circle nodes in Figure 2b ) . Instead , QRN does not have any controller node ( Figure 2a ) and is able to focus on relevant sentences through the update gate that is internally embodied within its unit . Second , N2N adds time - dependent trainable weights to the sentence representations to model the time dependency of the sentences ( as discussed in Section 1 ) . QRN does not need such additional weights as its inherent RNN architecture allows QRN to effectively model the time dependency . Neural Reasoner [ reference ] and Gated End - toend Memory Network [ reference ] ) are variants of MemN2N that share its fundamental characteristics . Improved Dynamic Memory Network ( DMN + ) [ reference ] uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences . It consists of two distinct GRUs , one for the time axis ( rectangle nodes in Figure 2c ) and one for the layer axis ( circle nodes in Figure 2c ) . Note that the update gate of the GRU for the time axis is replaced with external softmax attention weights . DMN + uses the time - axis GRU to summarizes the entire memory in each layer , and then the layer - axis GRU controls the attention weights in each layer . In contrast , QRN is simply a single recurrent unit without any controller node . section : EXPERIMENTS 5.1 DATA bAbI story - based QA dataset bAbI story - based QA dataset is composed of 20 different tasks ( Appendix A ) , each of which has 1 , 000 ( 1k ) synthetically - generated story - question pair . A story can be as short as two sentences and as long as 200 + sentences . A system is evaluated on the accuracy of getting the correct answers to the questions . The answers are single words or lists ( e.g. \" football , apple \" ) . Answering questions in each task requires selecting a set of relevant sentences and applying different kinds of logical reasoning over them . The dataset also includes 10k training data ( for each task ) , which allows training more complex models . Note that DMN + [ reference ] only reports on the 10k dataset . bAbI dialog dataset bAbI dialog dataset consists of 5 different tasks ( Table 3 ) , each of which has 1k synthetically - generated goal - oriented dialogs between a user and the system in the domain of restaurant reservation . Each dialog is as long as 96 utterances and comes with external knowledge base ( KB ) providing information of each restaurant . The authors also provide Out - Of - Vocabulary ( OOV ) version of the dataset , where many of the words and KB keywords in test data are not seen during training . A system is evaluated on the accuracy of its response to each utterance of the user , choosing from up to 2500 possible candidate responses . A system is required not only to understand the user 's request but also refer to previous conversations in order to obtain the context information of the current conversation . transformed the Second Dialog State Tracking Challenge ( DSTC2 ) dataset [ reference ] into the same format as the bAbI dialog dataset , for the measurement of performance on a real dataset . Each dialog can be as long as 800 + utterances , and a system needs to choose from 2407 possible candidate responses for each utterance of the user . Note that the evaluation metric of the original DSTC2 is different from that of the transformed DSTC2 , so previous work on the original DSTC2 should not be directly compared to our work . We will refer to this transformed DSTC2 dataset by \" Task 6 \" of dialog dataset . section : DSTC2 ( Task 6 ) dialog dataset section : MODEL DETAILS Input Module . In the input module , we are given sentences ( previous conversations in dialog ) x t and a question ( most recent user utterance ) q , and we want to obtain their vector representations , x t , q \u2208 R d . We use a trainable embedding matrix A \u2208 R d\u00d7V to encode the one - hot vector of each word x tj in each sentence x t into a d - dimensional vector x tj \u2208 R d . Then the sentence representation x t is obtained by Position Encoder . The same encoder with the same embedding matrix is also used to obtain the question vector q from q. Output Module for story - based QA . In the output module , we are given the vector representation of the predicted answer\u0177 and we want to obtain the natural language form of the answer , \u0177 . We use a V - way single - layer softmax classifier to map\u0177 to a V - dimensional sparse vector , v = softmax W ( y ) \u0177 \u2208 R V , where W ( y ) \u2208 R V \u00d7d is a weight matrix . Then the final answer\u0177 is simply the argmax word inv . To handle questions with multiple - word answers , we consider each of them as a single word that contains punctuations such as space and comma , and put it in the vocabulary . Output Module for dialog . We use a fixed number single - layer softmax classifiers , each of which is similar to that of the sotry - based QA model , to sequentially output each word of the system 's response . While it is similar in spirit to the RNN decoder [ reference ] , our output module does not have a recurrent hidden state or gating mechanism . Instead , it solely uses the final ouptut of the QRN , \u0177 , and the current word output to influence the prediction of the next word among possible candidates . Training . We withhold 10 % of the training for development . We use the hidden state size of 50 by deafult . Batch sizes of 32 for bAbI story - based QA 1k , bAbI dialog and DSTC2 dialog , and 128 for bAbI QA 10k are used . The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / \u221a d. Weights in the QRN unit are initialized using techniques by [ reference ] , and are tied across the layers . Forget bias of 2.5 is used for update gates ( no bias for reset gates ) . L2 weight decay of 0.001 ( 0.0005 for QA 10k ) is used for all weights . The loss function is the cross entropy betweenv and the one - hot vector of the true answer . The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs . The learning rate is controlled by AdaGrad [ reference ] with the initial learning rate of 0.5 ( 0.1 for QA 10k ) . Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data . section : RESULTS . We compare our model with baselines and previous state - of - the - art models on story - based and dialog tasks ( Table 1 ) . These include LSTM [ reference ] , End - to - end Memory Networks ( N2N ) [ reference ] , Dynamic Memory Networks ( DMN + ) [ reference ] , Gated End - to - end Memory Networks ( GMemN2N ) [ reference ] , and Differentiable Neural Computer ( DNC ) [ reference ] . Story - based QA . Table 1 ( top ) reports the summary of results of our model ( QRN ) and previous work on bAbI QA ( task - wise results are shown in Table 2 in Appendix ) . In 1k data , QRN 's ' 2r ' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) . In 10k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % . Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in Table 3 in Appendix ) . As done in previous work [ reference ] , we also report results when we use ' Match ' for dialogs . ' Match ' is the extension to the model which additionally takes as input whether each answer candidate matches with context ( more details on Appendix ) . QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison . Ablations . We test four types of ablations ( also discussed in Section 2.2 ) : number of layers ( 1 , 2 , 3 , or 6 ) , reset gate ( r ) , and gate vectorization ( v ) and the dimension of the hidden vector ( 50 , 100 ) . We show a subset of combinations of the ablations for bAbI QA in Table 1 and Table 2 ; other combinations performed poorly and / or did not give interesting observations . According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability . In the case of 1k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult . In the case of 10k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) Parallelization . We implement QRN with and without parallelization in TensorFlow [ reference ] ) on a single Titan X GPU to qunaitify the computational gain of the parallelization . For QRN without parallelization , we use the RNN library provided by TensorFlow . QRN with parallelization gives 6.2 times faster training and inference than QRN without parallelization on average . We expect that the speedup can be even higher for datasets with larger context . Interpretations . An advantage of QRN is that the intermediate query updates are interpretable . Figure 1 shows intermediate local queries ( q k t ) interpreted in natural language , such as \" Where is Sandra ? \" . In order to obtain these , we place a decoder on the input question embedding q and add its loss for recovering the question to the classification loss ( similarly to [ reference ] ) . We then use the same decoder to decode the intermediate queries . This helps us understand the flow of information in the networks . In Figure 1 , the question Where is apple ? is transformed into Where is Sandra ? at t = 1 . At t = 2 , as Sandra dropped the apple , the apple is no more relevant to Sandra . We obtain Where is Daniel ? at time t = 3 , and it is propagated until t = 5 , where we observe a sentence ( fact ) that can be used to answer the query . Visualization . Figure 3 shows vizualization of the ( scalar ) magnitudes of update and reset gates on story sentences and dialog utterances . More visualizations are shown in Appendices : Figure 4 and Figure 5 . In Figure 3 , we observe high values on facts that provide information to answer question ( the system 's next utterance for dialog ) . In QA Task 2 example ( top left ) , we observe high update gate values in the first layer on facts that state who has the apple , and in the second layer , the high update gate values are on those that inform where that person went to . We also observe that the forward reset gate at t = 2 in the first layer ( \u2212 \u2192 r 1 2 ) is low , which is signifying that apple no more belongs to Sandra . In dialog Task 3 ( bottom left ) , the model is able to infer that three restaurants are already recommended so that it can recommend another one . In dialog Task 6 ( bottom ) , the model focuses on the sentences containing Spanish , and does not concentrate much on other facts such as I do n't care . section : CONCLUSION In this paper , we introduce Query - Reduction Network ( QRN ) to answer context - based questions and carry out conversations with users that require multi - hop reasoning . We show the state - of - theart results in the three datasets of story - based QA and dialog . We model a story or a dialog as a sequence of state - changing triggers and compute the final answer to the question or the system 's next utterance by recurrently updating ( or reducing ) the query . QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively . It addresses the long - term dependency problem of most RNNs by simplifying the recurrent update , in which the candidate hidden state ( reduced query ) does not depend on the previous state . Moreover , QRN can be parallelized and can address the well - known problem of RNN 's vanishing gradients . section : A TASK - WISE RESULTS Here we provide detailed per - task breakdown of our results in QA ( Table 2 ) and dialog datasets ( Table 3 ) . Table 2 : bAbI QA dataset error rates ( % ) of QRN and previous work : LSTM , End - to - end Memory Networks ( N2N ) [ reference ] , Dynamic Memory Networks ( DMN + ) [ reference ] , Gated End - to - end Memory Networks ( GMemN2N ) [ reference ] . Results within each task of Differentiable Neural Computer ( DNC ) were not provided in its paper [ reference ] Table 3 : bAbI dialog and DSTC2 dialog dataset average error rates ( % ) of QRN and previous work : End - to - end Memory Networks ( N2N ) and Gated End - to - end Memory Networks ( GMemN2N [ reference ] ) . For QRN , a number in the front [ reference ][ reference ][ reference ][ reference ] indicates the number of layers and a number in the back ( 100 ) indicates the dimension of hidden vector , while the default value is 50 . ' r ' indicates that the reset gate is used , ' v ' indicates that the gates were vectorized , and ' + ' indicates that ' match ' was used . section : B VECTOR GATE PARALLELIZATION For vector gates , we have z t \u2208 R d instead of z t \u2208 R. Therefore the following equation replaces Equation 7 : where z j k is the k - th column vector of z j . Let b ij = log ( 1 \u2212 z i j ) for brevity . Then , we can rewrite Equation 8 as following : . . . where L , L \u2208 R T \u00d7T are lower and strictly lower triangular matrices of 1 's are tiled across the column . section : C MODEL DETAILS Match . While similar in spirit , our ' Match ' model is slightly different from previous work ( Bordes and [ reference ] . We use answer candidate embedding matrix , and add 2 dimension of 0 - 1 matrix which expresses whether the answer candidate matches with any word in the paragraph and the question . In other words , the softmax is computed b\u0177 are trainable weight matrices , and M ( y ) \u2208 R V \u00d72 is the 0 - 1 match matrix . section : D VISUALIZATIONS Visualization of Story - based QA . Figure 4 shows visualization of models for story - based QA tasks . In the task 3 ( left ) , the model focuses on the facts that contain ' football ' in the first layer , and found out where Mary journeyed to before the bathroom in the second layer . In task 7 ( right ) , the model focuses on the facts that provide information about the location of Sandra . Can you make a restaurant reservation for eight in a cheap price range in madrid 0.00 1.00 0.93 1.00 I ' m on it . 0.00 1.00 0.74 0.00 Any preference on a type of cuisine . 0.00 0.11 1.00 0.01 I love british food . 0.00 0.99 0.99 0.57 Okay let me look into some options for you . Here it is : resto - paris - expen - spanish - 8stars - address Figure 5 : Visualization of update and reset gates in QRN ' 2r ' model for on several tasks of bAbI dialog and DSTC2 dialog ( Table 3 ) . We do not put reset gate in the last layer . Note that we only show some of recent sentences here , even the dialog has more sentences . Visualization of Dialog . Figure 5 shows visualization of models for dialog tasks . In the first dialog of task 1 , the model focuses on the user utterance that mentions the user 's desired cuisine and location , and the current query ( user 's last utterance ) informs the system of the number of people , so the system is able to learn that it now needs to ask the user about the desired price range . In the second dialog of task 1 , the model focuses on the facts that provide information about the requests of the user . In task 4 ( third ) , the model focuses on what restaurant a user is talking about and the information about the restaurant . section : section : ACKNOWLEDGMENTS This research was supported by the NSF ( IIS 1616112 ) , Allen Institute for AI ( 66 - 9175 ) , Allen Distinguished Investigator Award , Google Research Faculty Award , and Samsung GRO Award . We thank the anonymous reviewers for their helpful comments . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["bAbi"]]], "Method": [[["QRN"]]], "Metric": [[["Accuracy__trained_on_10k_"]]], "Task": [[["Question_Answering"]]]}, {"incident_type": "SciREX_incident", "Material": [[["bAbi"]]], "Method": [[["QRN"]]], "Metric": [[["Accuracy__trained_on_1k_"]]], "Task": [[["Question_Answering"]]]}, {"incident_type": "SciREX_incident", "Material": [[["bAbi"]]], "Method": [[["QRN"]]], "Metric": [[["Mean_Error_Rate"]]], "Task": [[["Question_Answering"]]]}]}
{"docid": "TST3-SREX-0038", "doctext": "document : Neural Tree Indexers for Text Understanding Recurrent neural networks ( RNNs ) process input text sequentially and model the conditional transition between word tokens . In contrast , the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language . However , the current recursive architecture is limited by its dependence on syntactic tree . In this paper , we introduce a robust syntactic parsing - independent tree structured model , Neural Tree Indexers ( NTI ) that provides a middle ground between the sequential RNNs and the syntactic tree - based recursive models . NTI constructs a full n - ary tree by processing the input text with its node function in a bottom - up fashion . Attention mechanism can then be applied to both structure and node function . We implemented and evaluated a binary - tree model of NTI , showing the model achieved the state - of - the - art performance on three different NLP tasks : natural language inference , answer sentence selection , and sentence classification , outperforming state - of - the - art recurrent and recursive neural networks . section : Introduction Recurrent neural networks ( RNNs ) have been successful for modeling sequence data . RNNs equipped with gated hidden units and internal short - term memories , such as long short - term memories ( LSTM ) have achieved a notable success in several NLP tasks including named entity recognition , constituency parsing , textual entailment recognition , question answering , and machine translation . However , most LSTM models explored so far are sequential . It encodes text sequentially from left to right or vice versa and do not naturally support compositionality of language . Sequential LSTM models seem to learn syntactic structure from the natural language however their generalization on unseen text is relatively poor comparing with models that exploit syntactic tree structure . Unlike sequential models , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis . However its dependence on a syntactic tree architecture limits practical NLP applications . In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks . NTI takes a sequence of tokens and produces its representation by constructing a full n - ary tree in a bottom - up fashion . Each node in NTI is associated with one of the node transformation functions : leaf node mapping and non - leaf node composition functions . Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling . Furthermore , we propose different variants of node composition function and attention over tree for our NTI models . When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . Figure shows a binary - tree model of NTI . Although the model does not follow the syntactic tree structure , we empirically show that it achieved the state - of - the - art performance on three different NLP applications : natural language inference , answer sentence selection , and sentence classification . section : Related Work subsection : Recurrent Neural Networks and Attention Mechanism RNNs model input text sequentially by taking a single token at each time step and producing a corresponding hidden state . The hidden state is then passed along through the next time step to provide historical sequence information . Although a great success in a variety of tasks , RNNs have limitations . Among them , it is not efficient at memorizing long or distant sequence . This is frequently called as information flow bottleneck . Approaches have therefore been developed to overcome the limitations . For example , to mitigate the information flow bottleneck , bahdanau:15 extended RNNs with a soft attention mechanism in the context of neural machine translation , leading to improved the results in translating longer sentences . RNNs are linear chain - structured ; this limits its potential for natural language which can be represented by complex structures including syntactic structure . In this study , we propose models to mitigate this limitation . subsection : Recursive Neural Networks Unlike RNNs , recursive neural networks explicitly model the compositionality and the recursive structure of natural language over tree . The tree structure can be predefined by a syntactic parser . Each non - leaf tree node is associated with a node composition function which combines its children nodes and produces its own representation . The model is then trained by back - propagating error through structures . The node composition function can be varied . A single layer network with non - linearity was adopted in recursive auto - associate memories and recursive autoencoders . socher2012semantic extended this network with an additional matrix representation for each node to augment the expressive power of the model . Tensor networks have also been used as composition function for sentence - level sentiment analysis task . Recently , zhu2015long introduced S - LSTM which extends LSTM units to compose tree nodes in a recursive fashion . In this paper , we introduce a novel attentive node composition function that is based on S - LSTM . Our NTI model does not rely on either a parser output or a fine - grained supervision of non - leaf nodes , both required in previous work . In NTI , the supervision from the target labels is provided at the root node . As such , our NTI model is robust and applicable to a wide range of NLP tasks . We introduce attention over tree in NTI to overcome the vanishing / explode gradients challenges as shown in RNNs . section : Methods Our training set consists of examples , where the input is a sequence of word tokens and the output can be either a single target or a sequence . Each input word token is represented by its word embedding . NTI is a full n - ary tree ( and the sub - trees can be overlapped ) . It has two types of transformation function : non - leaf node function and leaf node function . computes a ( possibly non - linear ) transformation of the input word embedding . is a function of its child nodes representation , where is the total number of child nodes of this non - leaf node . NTI can be implemented with different tree structures . In this study we implemented and evaluated a binary tree form of NTI : a non - leaf node can take in only two direct child nodes ( i.e. , ) . Therefore , the function composes its left child node and right child node . Figure illustrates our NTI model that is applied to question answering ( a ) and natural language inference tasks ( b ) . Note that the node and leaf node functions are neural networks and are the only training parameters in NTI . We explored two different approaches to compose node representations : an extended LSTM and attentive node composition functions , to be described below . subsection : Non - Leaf Node Composition Functions We define two different methods for non - leaf node function . LSTM - based Non - leaf Node Function ( S - LSTM ) : We initiate with LSTM . For non - leaf node , we adopt S - LSTM zhu2015long , an extension of LSTM to tree structures , to learn a node representation by its children nodes . Let , , and be vector representations and cell states for the left and right children . An S - LSTM computes a parent node representation and a node cell state as where and biases ( for brevity we eliminated the bias terms ) are the training parameters . and denote the element - wise function and the element - wise vector multiplication . Extension of S - LSTM non - leaf node function to compose more children is straightforward . However , the number of parameters increases quadratically in S - LSTM as we add more child nodes . Attentive Non - leaf Node Function ( ANF ) : Some NLP applications ( e.g. , QA and machine translation ) would benefit from a dynamic query dependent composition function . We introduce ANF as a new non - leaf node function . Unlike S - LSTM , ANF composes the child nodes attentively in respect to another relevant input vector . The input vector can be a learnable representation from a sequence representation . Given a matrix resulted by concatenating the child node representations , and the third input vector , ANF is defined as where is a learnable matrix , the attention score and the attention weight vector for each child . is an attention scoring function , which can be implemented as a multi - layer perceptron ( MLP ) or a matrix - vector product . The matrices and and the vector are training parameters . is a vector of ones and the outer product . We use function for non - linear transformation . subsection : Attention Over Tree Comparing with sequential LSTM models , NTI has less recurrence , which is defined by the tree depth , for binary tree where is the length of the input sequence . However , NTI still needs to compress all the input information into a single representation vector of the root . This imposes practical difficulties when processing long sequences . We address this issue with attention mechanism over tree . In addition , the attention mechanism can be used for matching trees ( described in Section 4 as Tree matching NTI ) that carry different sequence information . We first define a global attention and then introduce a tree attention which considers the parent - child dependency for calculation of the attention weights . Global Attention : An attention neural network for the global attention takes all node representations as input and produces an attentively blended vector for the whole tree . This neural net is similar to ANF . Particularly , given a matrix resulted by concatenating the node representations , \u2026 , and the relevant input representation , the global attention is defined as where and are training parameters and the attention weight vector for each node . This attention mechanism is robust as it globally normalizes the attention score with to obtain the weights . However , it does not consider the tree structure when producing the final representation . Tree Attention : We modify the global attention network to the tree attention mechanism . The resulting tree attention network performs almost the same computation as ANF for each node . It compares the parent and children nodes to produce a new representation assuming that all node representations are constructed . Given a matrix resulted by concatenating the parent node representation , the left child and the right child and the relevant input representation , every non - leaf node simply updates its own representation by using the following equation in a bottom - up manner . and this equation is similarity to the global attention . However , now each non - leaf node attentively collects its own and children representations and passes towards the root which finally constructs the attentively blended tree representation . Note that unlike the global attention , the tree attention locally normalizes the attention scores with . section : Experiments We describe in this section experiments on three different NLP tasks , natural language inference , question answering and sentence classification to demonstrate the flexibility and the effectiveness of NTI in the different settings . We trained NTI using Adam with hyperparameters selected on development set . The pre - trained 300 - D Glove 840B vectors were obtained for the word embeddings . The word embeddings are fixed during training . The embeddings for out - of - vocabulary words were set to zero vector . We pad the input sequence to form a full binary tree . A padding vector was inserted when padding . We analyzed the effects of the padding size and found out that it has no influence on the performance ( see Appendix [ reference ] ) . The size of hidden units of the NTI modules were set to 300 . The models were regularized by using dropouts and an weight decay . subsection : Natural Language Inference We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset , which consists of 549 , 367 / 9 , 842 / 9 , 824 premise - hypothesis pairs for train / dev / test sets and target label indicating their relation . Unless otherwise noted , we follow the setting in the previous work and use an MLP for classification which takes in NTI outputs and computes the concatenation , absolute difference and elementwise product of the two sentence representations . The MLP has also an input layer with 1024 units with activation and a output layer . We explored nine different task - oriented NTI models with varying complexity , to be described below . For each model , we set the batch size to 32 . The initial learning , the regularization strength and the number of epoch to be trained are varied for each model . NTI - SLSTM : this model does not rely on transformer but uses the S - LSTM units for the non - leaf node function . We set the initial learning rate to 1e - 3 and regularizer strength to 3e - 5 , and train the model for 90 epochs . The neural net was regularized by 10 % input dropouts and the 20 % output dropouts . NTI - SLSTM - LSTM : we use LSTM for the leaf node function . Concretely , the LSTM output vectors are given to NTI - SLSTM and the memory cells of the lowest level S - LSTM were initialized with the LSTM memory states . The hyper - parameters are the same as the previous model . NTI - SLSTM node - by - node global attention : This model learns inter - sentence relation with the global attention over premise - indexed tree , which is similar to word - by - word attention model of rocktaschel:16 in that it attends over the premise tree nodes at every time step of hypothesis encoding . We tie the weight parameters of the two NTI - SLSTMs for premise and hypothesis and no transformer used . We set the initial learning rate to 3e - 4 and regularizer strength to 1e - 5 , and train the model for 40 epochs . The neural net was regularized by 15 % input dropouts and the 15 % output dropouts . NTI - SLSTM node - by - node tree attention : this is a variation of the previous model with the tree attention . The hyper - parameters are the same as the previous model . NTI - SLSTM - LSTM node - by - node global attention : in this model we include LSTM as the leaf node function . Here we initialize the memory cell of S - LSTM with LSTM memory and hidden / memory state of hypothesis LSTM with premise LSTM ( the later follows the work of ) . We set the initial learning rate to 3e - 4 and regularizer strength to 1e - 5 , and train the model for 10 epochs . The neural net was regularized by 10 % input dropouts and the 15 % output dropouts . NTI - SLSTM - LSTM node - by - node tree attention : this is a variation of the previous model with the tree attention . The hyper - parameters are the same as the previous model . Tree matching NTI - SLSTM - LSTM global attention : this model first constructs the premise and hypothesis trees simultaneously with the NTI - SLSTM - LSTM model and then computes their matching vector by using the global attention and an additional LSTM . The attention vectors are produced at each hypothesis tree node and then are given to the LSTM model sequentially . The LSTM model compress the attention vectors and outputs a single matching vector , which is passed to an MLP for classification . The MLP for this tree matching setting has an input layer with 1024 units with activation and a output layer . Unlike WangJ15b \u2019s matching LSTM model which is specific to matching sequences , we use the standard LSTM units and match trees . We set the initial learning rate to 3e - 4 and regularizer strength to 3e - 5 , and train the model for 20 epochs . The neural net was regularized by 20 % input dropouts and the 20 % output dropouts . Tree matching NTI - SLSTM - LSTM tree attention : we replace the global attention with the tree attention . The hyper - parameters are the same as the previous model . Full tree matching NTI - SLSTM - LSTM global attention : this model produces two sets of the attention vectors , one by attending over the premise tree regarding each hypothesis tree node and another by attending over the hypothesis tree regarding each premise tree node . Each set of the attention vectors is given to a LSTM model to achieve full tree matching . The last hidden states of the two LSTM models ( i.e. one for each attention vector set ) are concatenated for classification . The training weights are shared among the LSTM models The hyper - parameters are the same as the previous model . Table [ reference ] shows the results of our models . For comparison , we include the results from the published state - of - the - art systems . While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output ; which present strong baseline systems . The last set of methods designs inter - sentence relation with soft attention . Our best score on this task is 87.3 % accuracy obtained with the full tree matching NTI model . The previous best performing model on the task performs phrase matching by using the attention mechanism . Our results show that NTI - SLSTM improved the performance of the sequential LSTM encoder by approximately 2 % . Not surprisingly , using LSTM as leaf node function helps in learning better representations . Our NTI - SLSTM - LSTM is a hybrid model which encodes a sequence sequentially through its leaf node function and then hierarchically composes the output representations . The node - by - node attention models improve the performance , indicating that modeling inter - sentence interaction is an important element in NLI . Aggregating matching vector between trees or sequences with a separate LSTM model is effective . The global attention seems to be robust on this task . The tree attention were not helpful as it normalizes the attention scores locally in parent - child relationship . subsection : Answer Sentence Selection For this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences . We experiment on WikiQA dataset constructed from Wikipedia . The dataset contains 20 , 360 / 2 , 733 / 6 , 165 QA pairs for train / dev / test sets . We used the same setup in the language inference task except that we replace the layer with a layer and model the following conditional probability distribution . where and are the question and the answer encoded vectors and denotes the output of the hidden layer of the MLP . For this task , we use NTI - SLSTM - LSTM to encode answer candidate sentences and NTI - ANF - LSTM to encode the question sentences . Note that NTI - ANF - LSTM is relied on ANF as the non - leaf node function . vector for NTI - ANF - LSTM is the answer representation produced by the answer encoding NTI - SLSTM - LSTM model . We set the batch size to 4 and the initial learning rate to 1e - 3 , and train the model for 10 epochs . We used 20 % input dropouts and no weight decay . Following previous work , we adopt MAP and MRR as the evaluation metrics for this task . Table [ reference ] presents the results of our model and the previous models for the task . The classifier with handcrafted features is a SVM model trained with a set of features . The Bigram - CNN model is a simple convolutional neural net . The Deep LSTM and LSTM attention models outperform the previous best result by a large margin , nearly 5 - 6 % . NASM improves the result further and sets a strong baseline by combining variational auto - encoder with the soft attention . In NASM , they adopt a deep three - layer LSTM and introduced a latent stochastic attention mechanism over the answer sentence . Our NTI model exceeds NASM by approximately 0.4 % on MAP for this task . subsection : Sentence Classification Lastly , we evaluated NTI on the Stanford Sentiment Treebank ( SST ) . This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes . We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences . We use NTI - SLSTM and NTI - SLSTM - LSTM models to learn sentence representations for the task . The sentence representations were passed to a two - layer MLP for classification . We set the batch size to 64 , the initial learning rate to 1e - 3 and regularizer strength to 3e - 5 , and train each model for 10 epochs . The NTI - SLSTM model was regularized by 10% / 20 % of input / output and 20% / 30 % of input / output dropouts and the NTI - SLSTM - LSTM model 20 % of input and 20% / 30 % of input / output dropouts for binary and fine - grained settings . NTI - SLSTM - LSTM ( as shown in Table [ reference ] ) set the state - of - the - art results on both subtasks . Our NTI - SLSTM model performed slightly worse than its constituency tree - based counter part , CT - LSTM model . The CT - LSTM model composes phrases according to the output of a sentence parser and uses a node composition function similar to S - LSTM . After we transformed the input with the LSTM leaf node function , we achieved the best performance on this task . section : Qualitative Analysis subsection : Attention and Compositionality To help analyzing the results , we output attention weights by our NTI - SLSTM node - by - node global attention model . Figure [ reference ] shows the attention heatmaps for two sentences in the SNLI test set . It shows that our model semantically aligns single or multiword expressions ( \u201d little child \u201d and \u201d toddler \u201d ; \u201d rock wall \u201d and \u201d stone \u201d ) . In addition , our model is able to re - orient its attention over different parts of the hypothesis when the expression is more complex . For example , for ( c ) \u201d rock wall in autumn \u201d , NTI mostly focuses on the nodes in depth 1 , 2 and 3 representing contexts related to \u201d a stone \u201d , \u201d leaves . \u201d and \u201d a stone wall surrounded \u201d . Surprisingly , attention degree for the single word expression like \u201d stone \u201d , \u201d wall \u201d and \u201d leaves \u201d is lower to compare with multiword phrases . Sequence models lack this property as they have no explicit composition module to produce such mutiword phrases . Finally , the most interesting pattern is that the model attends over higher level ( low depth ) tree nodes with rich semantics when considering a ( c ) longer phrase or ( d ) full sentence . As shown in ( d ) , the NTI model aligns the root node representing the whole hypothesis sentence to the higher level tree nodes covering larger sub - trees in the premise . It certainly ignores the lower level single word expressions and only starts to attend when the words are collectively to form rich semantics . subsection : Learned Representations of Phrases and Sentences Using cosine similarity between their representations produced by the NTI - SLSTM model , we show that NTI is able to capture paraphrases on SNLI test data . As shown in Table [ reference ] , NTI seems to distinguish plural from singular forms ( similar phrases to \u201d a person \u201d ) . In addition , NTI captures non - surface knowledge . For example , the phrases similar to \u201d park for fun \u201d tend to align to the semantic content of fun and park , including \u201d people play frisbee outdoors \u201d . The NTI model was able to relate \u201d Santa Claus \u201d to christmas and snow . Interestingly , the learned representations were also able to connect implicit semantics . For example , NTI found that \u201d sad , depressed , and hatred \u201d is close to the phrases like \u201d an Obama supporter is upset \u201d . Overall the NTI model is robust to the length of the phrases being matched . Given a short phrase , NTI can retrieve longer yet semantically coherent sequences from the SNLI test set . In Table [ reference ] , we show nearest - neighbor sentences from SNLI test set . Note that the sentences listed in the first two columns sound semantically coherent but not the ones in the last column . The query sentence \u201d A dog sells a women a hat \u201d does not actually represent a common - sense knowledge and this sentence now seem to confuse the NTI model . As a result , the retrieved sentence are arbitrary and not coherent . subsection : Effects of Padding Size We introduced a special padding character in order to construct full binary tree . Does this padding character influence the performance of the NTI models ? In Figure [ reference ] , we show relationship between the padding size and the accuracy on Stanford sentiment analysis data . Each sentence was padded to form a full binary tree . The x - axis represents the number of padding characters introduced . When the padding size is less ( up to 10 ) , the NTI - SLSTM - LSTM model performs better . However , this model tends to perform poorly or equally when the padding size is large . Overall we do not observe any significant performance drop for both models as the padding size increases . This suggests that NTI learns to ignore the special padding character while processing padded sentences . The same scenario was also observed while analyzing attention weights . The attention over the padded nodes was nearly zero . section : Discussion and Conclusion We introduced Neural Tree Indexers , a class of tree structured recursive neural network . The NTI models achieved state - of - the - art performance on different NLP tasks . Most of the NTI models form deep neural networks and we think this is one reason that NTI works well even if it lacks direct linguistic motivations followed by other syntactic - tree - structured recursive models . CNN and NTI are topologically related . Both NTI and CNNs are hierarchical . However , current implementation of NTI only operates on non - overlapping sub - trees while CNNs can slide over the input to produce higher - level representations . NTI is flexible in selecting the node function and the attention mechanism . Like CNN , the computation in the same tree - depth can be parallelized effectively ; and therefore NTI is scalable and suitable for large - scale sequence processing . Note that NTI can be seen as a generalization of LSTM . If we construct left - branching trees in a bottom - up fashion , the model acts just like sequential LSTM . Different branching factors for the underlying tree structure have yet to be explored . NTI can be extended so it learns to select and compose dynamic number of nodes for efficiency , essentially discovering intrinsic hierarchical structure in the input . section : Acknowledgments We would like to thank the anonymous reviewers for their insightful comments and suggestions . This work was supported in part by the grant HL125089 from the National Institutes of Health ( NIH ) . Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_Full_tree_matching_NTI-SLSTM-LSTM_w__global_attention"]]], "Metric": [[["__Test_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_NTI-SLSTM-LSTM_encoders"]]], "Metric": [[["__Test_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_Full_tree_matching_NTI-SLSTM-LSTM_w__global_attention"]]], "Metric": [[["__Train_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_NTI-SLSTM-LSTM_encoders"]]], "Metric": [[["__Train_Accuracy"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_Full_tree_matching_NTI-SLSTM-LSTM_w__global_attention"]]], "Metric": [[["Parameters"]]], "Task": [[["Natural_Language_Inference"]]]}, {"incident_type": "SciREX_incident", "Material": [[["SNLI"]]], "Method": [[["300D_NTI-SLSTM-LSTM_encoders"]]], "Metric": [[["Parameters"]]], "Task": [[["Natural_Language_Inference"]]]}]}
{"docid": "TST3-SREX-0039", "doctext": "document : Understanding Humans in Crowded Scenes : Deep Nested Adversarial Learning and A New Benchmark for Multi - Human Parsing Despite the noticeable progress in perceptual tasks like detection , instance segmentation and human parsing , computers still perform unsatisfactorily on visually understanding humans in crowded scenes , such as group behavior analysis , person re - identification and autonomous driving , etc . To this end , models need to comprehensively perceive the semantic information and the differences between instances in a multi - human image , which is recently defined as the multi - human parsing task . In this paper , we present a new large - scale database \u201c M ulti - H uman P arsing ( MHP ) \u201d for algorithm development and evaluation , and advances the state - of - the - art in understanding humans in crowded scenes . MHP contains 25 , 403 elaborately annotated images with 58 fine - grained semantic category labels , involving 2 - 26 persons per image and captured in real - world scenes from various viewpoints , poses , occlusion , interactions and background . We further propose a novel deep N ested A dversarial N etwork ( NAN ) model for multi - human parsing . NAN consists of three G enerative A dversarial N etwork ( GAN )- like sub - nets , respectively performing semantic saliency prediction , instance - agnostic parsing and instance - aware clustering . These sub - nets form a nested structure and are carefully designed to learn jointly in an end - to - end way . NAN consistently outperforms existing state - of - the - art solutions on our MHP and several other datasets , and serves as a strong baseline to drive the future research for multi - human parsing . section : Introduction One of the primary goals of intelligent human - computer interaction is understanding the humans in visual scenes . It involves several perceptual tasks including detection , i.e. localizing different persons at a coarse , bounding box level ( Fig . [ reference ] ( a ) ) , instance segmentation , i.e. labelling each pixel of each person uniquely ( Fig . [ reference ] ( b ) ) , and human parsing , i.e. decomposing persons into their semantic categories ( Fig . [ reference ] ( c ) ) . Recently , deep learning based methods have achieved remarkable sucess in these perceptual tasks thanks to the availability of plentiful annotated images for training and evaluation purposes . Though exciting , current progress is still far from the utimate goal of visually understanding humans . As Fig . [ reference ] shows , previous efforts on understanding humans in visual scenes either only consider coarse information or are agnostic to different instances . In the real - world scenarios , it is more likely that there simutaneously exist multiple persons , with various human interactions , poses and occlusion . Thus , it is more practically demanded to parse human body parts and fashion items at the instance level , which is recently defined as the multi - human parsing task . Multi - human parsing enables more detailed understanding of humans in crowded scenes and aligns better with many real - world applications , such as group behavior analysis , person re - identification , e - commerce , image editing , video surveillance , autonomous driving and virtual reality . However , the existing benchmark datasets are not suitable for such a new task . Even though Li et al . proposed a preliminary M ulti - H uman P arsing ( MHP v1.0 ) dataset , it only contains 4 , 980 images annotated with 18 semantic labels . In this work , we propose a new large - scale benchmark \u201c M ulti - H uman P arsing ( MHP v2.0 ) \u201d , aiming to push the frontiers of multi - human parsing research towards holistically understanding humans in crowded scenes . The data in MHP v2.0 cover wide variability and complexity w.r.t . viewpoints , poses , occlusion , human interactions and background . It in total includes 25 , 403 human images with pixel - wise annotations of 58 semantic categories . We further propose a novel deep N ested A dversarial N etwork ( NAN ) model for solving the challenging multi - human parsing problem . Unlike most existing methods which rely on separate stages of instance localization , human parsing and result refinement , the proposed NAN parses semantic categories and differentiates different person instances simultaneously in an effective and time - efficient manner . NAN consists of three G enerative A dversarial N etwork ( GAN )- like sub - nets , respectively performing semantic saliency prediction , instance - agnostic parsing and instance - aware clustering . Each sub - task is simpler than the original multi - human parsing task , and is more easily addressed by the corresponding sub - net . Unlike many multi - task learning applications , in our method the sub - nets depend on each other , forming a causal nest by dynamically boosting each other through an adversarial strategy ( See Fig . [ reference ] ) , which is hence called a \u201c nested adversarial learning \u201d structure . Such a structure enables effortless gradient B ackpro P agation ( BP ) in NAN such that it can be trained in a holistic , end - to - end way , which is favorable to both accuracy and speed . We conduct qualitative and quantitative experiments on the MHP v2.0 dataset proposed in this work , as well as the MHP v1.0 , PASCAL - Person - Part and Buffy benchmark datasets . The results demonstrate the superiority of NAN on multi - human parsing over the state - of - the - arts . Our contributions are summarized as follows . We propose a new large - scale benchmark and evaluation server to advance understanding of humans in crowded scenes , which contains 25 , 403 images annotated pixel - wisely with 58 semantic category labels . We propose a novel deep N ested A dversarial N etwork ( NAN ) model for multi - human parsing , which serves as a strong baseline to inspire more future research efforts on this task . Comprehensive evaluations on the MHP v2.0 dataset proposed in this work , as well as the MHP v1.0 , PASCAL - Person - Part and Buffy benchmark datasets verify the superiority of NAN on understanding humans in crowded scenes over the state - of - the - arts . section : Related Work paragraph : Human Parsing Datasets The statistics of popular publicly available datasets for human parsing are summarized in Tab . [ reference ] . The Buffy dataset was released in 2011 for human parsing and instance segmentation . It contains only 748 images annotated with 13 semantic categories . The Fashionista dataset was released in 2012 for human parsing , containing limited images annotated with 56 fashion categories . The PASCAL - Person - Part dataset was initially annotated by Chen et al . from the PASCAL - VOC - 2010 dataset . Chen et al . extended it for human parsing with 7 coarse body part labels . The ATR dataset was released in 2015 for human parsing with a large number of images annotated with 18 semantic categories . The LIP dataset further extended ATR by cropping person instances from Microsoft COCO . It is a large - scale human parsing dataset with densely pixel - wise annotations of 20 semantic categories . But it has two limitations . 1 ) Despite the large data size , it contains limited semantic category annotations , which restricts the fine - grained understanding of humans in visual scenes . 2 ) In LIP , only a small proportion of images involve multiple persons with interactions . Such an instance - agnostic setting severely deviates from reality . Even in the MHP v1.0 dataset proposed by Li et al . for multi - human parsing , only 4 , 980 images are included and annotated with 18 semantic labels . Comparatively , our MHP v2.0 dataset contains 25 , 403 elaborately annotated images with 58 fine - grained semantic part labels . It is the largest and most comprehensive multi - human parsing dataset to date , to our best knowledge . Visual comparisons between LIP , MHP v1.0 and our MHP v2.0 are provided in Fig . [ reference ] . paragraph : Human Parsing Approaches Recently , many research efforts have been devoted to human parsing due to its wide range of potential applications . For example , Liang et al . proposed a proposal - free network for instance segmentation by directly predicting the instance numbers of different categories and the pixel - level information . Gong et al . proposed a self - supervised structure - sensitive learning approach , which imposes human pose structures to parsing results without resorting to extra supervision . Liu et al . proposed a single frame video parsing method which integrates frame parsing , optical flow estimation and temporal fusion into a unified network . Zhao et al . proposed a self - supervised neural aggregation network , which learns to aggregate the multi - scale features and incorporates a self - supervised joint loss to ensure the consistency between parsing and pose . He et al . proposed the Mask R - CNN , which is extended from Faster R - CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition . Brabandere et al . proposed to tackle instance segmentation with a discriminative loss function , operating at the pixel level , which encourages a convolutional network to produce a representation of the image that can be easily clustered into instances with a simple post - processing step . However , these methods either only consider coarse semantic information or are agnostic to different instances . To enable more detailed human - centric analysis , Li et al . initially proposed the multi - human parsing task , which aligns better with the realistic scenarios . They also proposed a novel MH - Parser model as a reference method which generates parsing maps and instance masks simutaneously in a bottom - up fashion . Jiang et al . proposed a new approach to segment human instances and label their body parts using region assembly . Li et al . proposed a framework with a human detector and a category - level segmentation module to segment the parts of objects at the instance level . These methods involve mutiple separate stages for instance localization , human parsing and result refinement . In comparison , the proposed NAN produces accurate multi - human parsing results through a single forward - pass in a time - efficient manner without tedious pre - or post - processing . section : Multi - Human Parsing Benchmark In this section , we introduce the \u201c M ulti - H uman P arsing ( MHP v2.0 ) \u201d , a new large - scale dataset focusing on semantic understanding of humans in crowded scenes with several appealing properties . 1 ) It contains 25 , 403 elaborately annotated images with 58 fine - grained labels on body parts , fashion items and one background label , which is larger and more comprehensive than previous similar attempts . 2 ) The images within MHP v2.0 are collected from real - world scenarios , involving humans with various viewpoints , poses , occlusion , interactions and resolution . 3 ) The background of images in MHP v2.0 is more complex and diverse than previous datasets . Some examples are showed in Fig . [ reference ] . The MHP v2.0 dataset is expected to provide a new benchmark suitable for multi - human parsing together with a standard evaluation server where the test set will be kept secret to avoid overfitting . subsection : Image Collection and Annotation We manually specify some underlying relationships ( such as family , couple , team , etc . ) and possible scenes ( such as sports , conferences , banquets , etc . ) to ensure the diversity of returned results . Based on any one of these specifications , corresponding multi - human images are located by performing Internet searches over Creative Commons licensed imagery . For each identified image , the contained human number and the corresponding URL are stored in a spreadsheet . Automated scrapping software is used to download the multi - human imagery and stores all relevant information in a relational database . Moreover , a pool of images containing clearly visible persons with interactions and rich fashion items is also constructed from the existing human - centric datasets to augment and complement Internet scraping results . After curating the imagery , manual annotation is conducted by professional data annotators , which includes two distinct tasks . The first task is manually counting the number of foreground persons and duplicating each image to several copies according to the count number . Each duplicated image is marked with the image ID , the contained person number and a self - index . The second is assigning the fine - grained pixel - wise label to each semantic category for each person instance . We implement an annotation tool and generate multi - scale superpixels of images based on to speed up the annotation . See Fig . [ reference ] for an example . Each multi - human image contains at least two instances . The annotation for each instance is done in a left - to - right order , corresponding to the duplicated image with the self - index from beginning to end . For each instance , 58 semantic categories are defined and annotated , including cap / hat , helmet , face , hair , left - arm , right - arm , left - hand , right - hand , protector , bikini / bra , jacket / windbreaker / hoodie , t - shirt , polo - shirt , sweater , singlet , torso - skin , pants , shorts / swim - shorts , skirt , stockings , socks , left - boot , right - boot , left - shoe , right - shoe , left - highheel , right - highheel , left - sandal , right - sandal , left - leg , right - leg , left - foot , right - foot , coat , dress , robe , jumpsuits , other - full - body - clothes , headwear , backpack , ball , bats , belt , bottle , carrybag , cases , sunglasses , eyewear , gloves , scarf , umbrella , wallet / purse , watch , wristband , tie , other - accessaries , other - upper - body - clothes and other - lower - body - clothes . Each instance has a complete set of annotations whenever the corresponding category appears in the current image . When annotating one instance , others are regarded as background . Thus , the resulting annotation set for each image consists of instance - level parsing masks , where is the number of persons in the image . After annotation , manual inspection is performed on all images and corresponding annotations to verify the correctness . In cases where annotations are erroneous , the information is manually rectified by 5 well informed analysts . The whole work took around three months to accomplish by 25 professional data annotators . subsection : Dataset Splits and Statistics In total , there are 25 , 403 images in the MHP v2.0 dataset . Each image contains 2 - 26 person instances , with 3 on average . The resolution of the images ranges from 85 100 to 4 , 511 6 , 919 , with 644 718 on average . We spit the images into training , validation and testing sets . Following random selection , we arrive at a unique split consisting of 15 , 403 training and 5 , 000 validation images with publicly available annotations , as well as 5 , 000 testing images with annotations withheld for benchmarking purpose . The statistics w.r.t . data distribution on 59 semantic categories , the average semantic category number per image and the average instance number per image in the MHP v2.0 dataset are illustrated in Fig . [ reference ] ( a ) , ( b ) and ( c ) , respectively . In general , face , arms and legs are the most remarkable parts of a human body . However , understanding humans in crowded scenes needs to analyze fine - grained details of each person of interest , including different body parts , clothes and accessaries . We therefore define 11 body parts , and 47 clothes and accessaries . Among these 11 body parts , we divide arms , hands , legs and feet into left and right side for more precise analysis , which also increases the difficulty of the task . We define hair , face and torso - skin as the remaining three body parts , which can be used as auxiliary guidance for more comprehensive instance - level analysis . As for clothing categories , we have common clothes like coat , jacket / windbreaker / hoodie , sweater , singlet , pants , shorts / swim - shorts and shoes , confusing categories such as t - shirt v.s . polo - shirt , stockings v.s . socks , skirt v.s . dress and robe , and boots v.s . sandals and highheels , and infrequent categories such as cap / hat , helmet , protector , bikini / bra , jumpsuits , gloves and scarf . Furthermore , accessaries like sunglasses , belt , tie , watch and bags are also taken into account , which are common but hard to predict , especially for the small - scale ones . To summarize , the pre - defined semantic categories of MHP v2.0 involve most body parts , clothes and accessaries of different styles for men , women and children in all seasons . The images in the MHP v2.0 dataset contain diverse instance numbers , viewpoints , poses , occlusion , interactions and background complexities . MHP v2.0 aligns better with real - world scenarios and serves as a more realistic benchmark for human - centric analysis , which pushes the frontiers of fine - grained multi - human parsing research . section : Deep Nested Adversarial Networks As shown in Fig . [ reference ] , the proposed deep N ested A dversarial N etwork ( NAN ) model consists of three GAN - like sub - nets that jointly perform semantic saliency prediction , instance - agnostic parsing and instance - aware clustering end - to - end . NAN produces accurate multi - human parsing results through a single forward - pass in a time - efficient manner without tedious pre - or post - processing . We now present each component in details . subsection : Semantic Saliency Prediction Large modality and interaction variations are the main challenge to multi - human parsing and also the key obstacle to learning a well - performing human - centric analysis model . To address this problem , we propose to decompose the original task into three granularities and adaptively impose a prior on the specific process , each with the aid of a GAN - based sub - net . This reduces the training complexity and leads to better empirical performance with limited data . The first sub - net estimates semantic saliency maps to locate the most noticeable and eye - attracting human regions in images , which serves as a basic prior to facilitate further processing on humans , as illustrated in Fig . [ reference ] left . We formulate semantic saliency prediction as a binary pixel - wise labelling problem to segment out foreground v.s . background . Inspired by the recent success of F ully C onvolutional N etwork s ( FCNs ) based methods on image - to - image applications , we leverage an FCN backbone ( FCN - 8s ) as the generator of NAN for semantic saliency prediction , where denotes the network parameters , and , , and denote the image height , width , channel number and semantic category ( i.e. , foreground plus background ) number , repectively . Formally , let the input RGB image be denoted by and the semantic saliency map be denoted by , then The key requirements for are that the semantic saliency map should present indistinguishable properities compared with a real one ( i.e. , ground truth ) in appearance while preserving the intrinsic contextually remarkable information . To this end , we propose to learn by minimizing a combination of two losses : where is the adv ersarial loss for refining realism and alleviating artifacts , is the s emantic s aliency loss for pixel - wise image labelling , are weighting parameters among different losses . is a pixel - wise cross - entropy loss calculated based on the binary pixel - wise annotations to learn : is proposed to narrow the gap between the distributions of generated and real results . To facilitate this process , we leverage a C onvolutional N eural N etwork ( CNN ) backbone as the discriminator to be as simple as possible to avoid typical GAN tricks . We alternatively optimize and to learn and : where denotes the binary real v.s . fake indicator . subsection : Instance - Agnostic Parsing The second sub - net concatenates the information from the original RGB image with semantic saliency prior as input and estimates a fine - grained instance - agnostic parsing map , which further serves as stronger semantic guidance from the global perspective to facilitate instance - aware clustering , as illustrated in Fig . [ reference ] middle . We formulate instance - agnostic parsing as a multi - class dense classification problem to mask semantically consistent regions of body parts and fashion items . Inspired by the leading performance of the skip - net on recognition tasks , we modify a skip - net ( WS - ResNet ) into an FCN - based architecture as the generator of NAN to learn a highly non - linear transformation for instance - agnostic parsing , where denotes the network parameters for the generator and denotes the semantic category number . The prediction is downsampled by for accuracy v.s . speed trade - off . Contextual information from global and local regions compensates each other and naturally benefits human parsing . The hierarchical features within a skip - net are multi - scale in nature due to the increasing receptive field sizes , which are combined together via skip connections . Such a combined representation comprehensively maintains the contextual information , which is crucial for generating smooth and accurate parsing results . Formally , let the instance - agnostic parsing map be denoted by , then Similar to the first sub - net , we propose to learn by minimizing : where is the g lobal p arsing loss for semantic part labelling . is a standard pixel - wise cross - entropy loss calculated based on the multi - class pixel - wise annotations to learn . is also slightly finetuned due to the hinged gradient backpropagation route within the nested structure : is proposed to ensure the correctness and realism of the current phase and also the previous one for information flow consistency . To facilitate this process , we leverage a same CNN backbone with as the discriminator , which are learned separately . We alternatively optimize and to learn , and slightly finetune : subsection : Instance - Aware Clustering The third sub - net concatenates the information from the original RGB image with semantic saliency and instance - agnostic parsing priors as input and estimates an instance - aware clustering map by associating each semantic parsing mask to one of the person instances in the scene , as illustrated in Fig . [ reference ] right . Inspired by the observation that a human glances at an image and instantly knows how many and where the objects are in the image , we formulate instance - aware clustering by parallelly inferring the instance number and pixel - wise instance location , discarding the requirement of time - consuming region proposal generation . We modify a same backbone architecture to incorporate two sibling branches as the generator of NAN for location - sensitive learning , where denotes the network parameters for the generator and denotes the pre - defined instance location coordinate number . As multi - scale features integrating both global and local contextual information are crucial for increasing location prediction accuracy , we further augment the pixel - wise instance location prediction branch with a M ulti - S cale F usion U nit ( MSFU ) to fuse shallow - , middle - and deep - level features , while using the feature maps downsampled by concatenated with feature maps from the first branch for instance number regression . Formally , let the pixel - wise instance location map be denoted by and the instance number be denoted by , then We propose to learn by minimizing : where is the p ixel - wise i nstance l ocation loss for pixel - wise instance location regression and is the i nstance n umber loss for instance number regression . is a standard smooth - loss calculated based on the foreground pixel - wise instance location annotations to learn . Since a person instance can be identified by its top - left corner and bottom - right corner of the surrounding bounding box , for each pixel belonging to the person instance , the pixel - wise instance location vector is defined as , where and are the width and height of the person instance for normalization , respectively . is a standard loss calculated based on the instance number annotations to learn . and are also slightly finetuned due to the chained schema within the nest : Given these information , instance - aware clustering maps can be effortlessly generated with little computational overhead , which are denoted by . Similar to , is proposed to ensure the correctness and realism of all phases for the information flow consistency . To facilitate this process , we leverage a same CNN backbone with as the discriminator , which are learned separately . We alternatively optimize and to learn , and slightly finetune and : subsection : Training and Inference The goal of NAN is to use sets of real targets to learn three GAN - like sub - nets that mutually boost and jointly accomplish multi - human parsing . Each separate loss serves as a deep supervision within the nested structure benefitting network convergence . The overall objective function for NAN is Clearly , the NAN is end - to - end trainable and can be optimized with the proposed nested adversarial learning strategy and BP algorithm . During testing , we simply feed the input image into NAN to get the instance - agnostic parsing map from , pixel - wise instance location map and instance number from . Then we employ an off - the - shelf clustering method to obtain the instance - aware clustering map . Example results are visualized in Fig . [ reference ] . section : Experiments We evaluate NAN qualitatively and quantitatively under various settings and granularities for understanding humans in crowded scenes . In particular , we evaluate multi - human parsing performance on the MHP v2.0 dataset proposed in this work , as well as the MHP v1.0 and PASCAL - Person - Part benchmark datasets . We also evaluate instance - agnostic parsing and instance - aware clustering results on the Buffy benchmark dataset , which are byproducts of NAN . subsection : Experimental Settings subsubsection : Implementation Details Throughout the experiments , the sizes of the RGB image , the semantic saliency prediction , inputs to the discriminator and inputs to the generator are fixed as ; the sizes of the instance - agnostic parsing prediction , instance - aware clustering prediction , inputs to the discriminator , inputs to the generator , inputs to the discriminator and instance location map are fixed as ; the channel number of the pixel - wise instance location map is fixed as , incorporating two corner points of the associated bounding box ; the constraint factors are empirically fixed as and , respectively ; the generator is initialized with FCN - 8s by replacing the last layer with a new convolutional layer with kernel size , pretrained on PASCAL - VOC - 2011 and finetuned on the target dataset ; the generator is initialized with WS - ResNet by eliminating the spatial pooling layers , increasing the strides of the first convolutional layers up to 2 in B , eliminating the top - most global pooling layer and the linear classifier , and adding two new convolutional layers with kernel sizes and , pretrained on ImageNet and PASCAL - VOC - 2012 , and finetuned on the target dataset ; the generator is initialized with the same backbone architecture and pre - trained weights with ( which are learned separately ) , by further augmenting it with two sibling branches for pixel - wise instance location map prediction and instance number prediction , where the first branch utilizes a MSFU ( three convolutional layers with kernal sizes for specific scale adaption ) ended with a convolutional layer with kernel size for multi - scale feature aggregation and a final convolutional layer with kernel size for location regression and the second branch utilizes the feature maps downsampled by 8 concatenated with the feature maps from the first branch ended with a global pooling layer , a hidden 512 - way fully - connected layer and a final 1 - way fully - connected layer for instance number regression ; the three discriminators ( which are learned separately ) are all initialized with a VGG - 16 by adding a new convolutional layer at the very begining with kernel size for input adaption , and replacing the last layer with a new 1 - way fully - connected layer activated by sigmoid , pre - trained on ImageNet and finetuned on the target dataset ; the newly added layers are randomly initialized by drawing weights from a zero - mean Gaussian distribution with standard deviation ; we employ an off - the - shelf clustering method to obtain the instance - aware clustering map ; the dropout ratio is empirically fixed as ; the weight decay and batch size are fixed as and , respectively ; We use an initial learning rate of for pre - trained layers , and for newly added layers in all our experiments ; we decrease the learning rate to of the previous one after 20 epochs and train the network for roughly 60 epochs one after the other ; the proposed network is implemented based on the publicly available TensorFlow platform , which is trained using Adam ( ) on four NVIDIA GeForce GTX TITAN X GPUs with 12 G memory ; the same training setting is utilized for all our compared network variants ; we evaluate the testing time by averaging the running time for images on the target set on NVIDIA GeForce GTX TITAN X GPU and Intel Core i7 - 4930 K CPU@3.40GHZ ; our NAN can rapidly process one image in about 1 second , which compares much favorably to other state - of - the - art approaches , as the current state - of - the - art methods rely on region proposal preprocessing and complex processing steps . subsubsection : Evaluation Metrics Following , we use the A verage P recision based on p art ( ) and P ercentage of C orrectly parsed semantic P arts ( PCP ) metrics for multi - human parsing evaluation . Different from the A verage P recision based on r egion ( ) used in instance segmentation , uses part - level pixel I ntersection o ver U nion ( IoU ) of different semantic part categories within a person instance to determine if one instance is a true positive . We prefer over as we focus on human - centric analysis and we aim to investigate to how well a person instance as a whole is parsed . Additionally , we also report the , which is the mean of the at IoU thresholds ranging from to , in increments of 0.1 . As averages the IoU of each semantic part category , it fails to reflect how many semantic parts are correctly parsed . We further incorporate the PCP , originally used in human pose estimation , to evaluate the parsing quality within person instances . For each true - positive person instance , we find all the semantic categories ( excluding background ) with pixel IoU larger than a threshold , which are regarded as correctly parsed . The PCP of one person instance is the ratio between the correctly parsed semantic category number and the total semantic category number of that person . Missed person instances are assigned with PCP . The overall PCP is the average PCP for all person instances . Note that PCP is also a human - centric evaluation metric . subsection : Evaluations on the MHP v2.0 Benchmark The MHP v2.0 dataset proposed in this paper is the largest and most comprehensive multi - human parsing benchmark to date , which extends MHP v1.0 to push the frontiers of understanding humans in crowded scenes by containing 25 , 403 elaborately annotated images with 58 fine - grained semantic category labels . Annotation examples are visualized in Fig . [ reference ] ( c ) . The data are randomly organized into 3 splits , consisting of 15 , 403 training and 5 , 000 validation images with publicly available annotations , as well as 5 , 000 testing images with annotations withheld for benchmarking purpose . Evaluation systems report the and PCP over the validation and testing sets . subsubsection : Component Analysis We first investigate different architectures and loss function combinations of NAN to see their respective roles in multi - human parsing . We compare 16 variants from four aspects , i.e. , different baselines ( Mask R - CNN and MH - Parser ) , different network structures ( w / o , w / o concatenated input ( RGB only ) , w / o concatenated input ( RGB only ) , w / o , w / o , w / o concatenated input , w / o , w / o concatenated input , w / o MSFU ) , our proposed NAN , and upperbounds ( : use the ground truth semantic saliency maps instead of prediction while keeping other settings the same ; : use the ground truth instance - agnostic parsing maps instead of prediction while keeping other settings the same ; : use the ground truth instance number instead of prediction while keeping other settings the same ; : use the ground truth pixel - wise instance location maps instead of prediction while keeping other settings the same ) . The performance comparison in terms of @IoU=0.5 , and PCP@IoU=0.5 on the MHP v2.0 validation set is reported in Tab . [ reference ] . By comaring the results from the v.s . panels , we observe that our proposed NAN consistently outperforms the baselines Mask R - CNN and MH - Parser by a large margin , i.e. , and in terms of , and in terms of , and and in terms of PCP . Mask R - CNN suffers difficulties to differentiate entangled humans . MH - Parser involves multiple stages for instance localization , human parsing and result refinement with high complexity , yielding sub - optimal results , whereas NAN parses semantic categories , differentiates different person instances and refines results simultaneously through deep nested adversarial learning in an effective yet time - efficient manner . By comaring the results from the v.s . panels , we observe that NAN consistently outperforms the 9 variants in terms of network structure . In particular , w / o refers to truncating the semantic saliency prediction sub - net from NAN , leading to , and performance drop in terms of all metrics . This verifies the necessity of semantic saliency prediction that locates the most noticeable human regions in images to serve as a basic prior to facilitate further human - centic processing . The superiority of incorporating adaptive prior information to specific process can be verified by comparing w / o concatenated input with NAN , i.e. , , and ; , and differences in terms of all metrics . The superiority of incorporating adversarial learning to specific process can be verified by comparing w / o with NAN , i.e. , , and ; , and ; , and decrease in terms of all metrics . Nested adversarial learning strategy ensures the correctness and realism of all phases for information flow consistency , the superiority of which is verified by comparing w / o concatenated input with NAN , i.e. , , and ; , and decline in terms of all metrics . MSFU dynamically fuses multi - scale features for enhancing instance - aware clustering accuracy , the superiority of which is verified by comparing w / o MSFU with NAN , i.e. , , and drop in terms of all metrics . Finally , we also evaluate the limitations of our current algorithm . By comparing with NAN , only , and improvement in term of all metrics are obtained , which shows that the errors from semantic saliency prediction are already small and have only little effect on the final results . A large gap between , and of and , and of NAN shows that a better instance - agnostic parsing network architecture can definitely help improve the performance of multi - human parsing under our NAN framework . By comparing and with NAN , , and ; , and improvement in term of all metrics are obtained , which shows that accurate instance - aware clustering results are critical for superior multi - human parsing . subsubsection : Quantitative Comparison The performance comparison of the proposed NAN with two state - of - the - art methods in terms of @IoU=0.5 , and PCP@IoU=0.5 on the MHP v2.0 testing set is reported in Tab . [ reference ] . Following , we conduct experiments under three settings : All reports the evaluation over the whole testing set ; Inter%20 reports the evaluation over the sub - set containing the images with top 20 % interaction intensity ; Inter%10 reports the evaluation over the sub - set containing the images with top 10 % interaction intensity . Our NAN is significantly superior over other state - of - the - arts on setting - 1 . In particular , NAN improves the - best by , and in terms of all metrics . For the more challenging scenarios with intensive interactions ( setting - 2 , 3 ) , NAN also consistently achieves the best performance . In particular , for Inter%20 and Inter%10 , NAN improves the - best by , and ; , and in terms of all metrics . This verifies the effectiveness of our NAN for multi - human parsing and understanding humans in crowded scenes . Moreover , NAN can rapidly process one 512 512 image in about 1 second with acceptable resource consumption , which is attractive to real applications . This compares much favorably to MH - Parser ( 14.94 img / s ) , which relies on separate and complex post - processing ( including CRF ) steps . subsubsection : Qualitative Comparison Fig . [ reference ] visualizes the qualitative comparison of the proposed NAN with two state - of - the - art methods and corresponding ground truths on the MHP v2.0 dataset . Note that Mask R - CNN only offers silhouettes of different person instances , we only compare our instance - aware clustering results with it while comparing our holistic results with MH - Parser . It can be observed that the proposed NAN performs well in multi - human parsing with a wide range of viewpoints , poses , occlusion , interactions and background complexity . The instance - agnostic parsing and instance - aware clustering predictions of NAN present high consistency with corresponding ground truths , thanks to the novel network structure and effective training strategy . In contrast , Mask R - CNN suffers difficulties to differentiate entangled humans , while MH - Parser struggles to generate fine - grained parsing results and clearly segmented instance masks . This further desmonstrates the effectiveness of the proposed NAN . We also show some failure cases of our NAN in Fig . [ reference ] . As can be observed , humans in crowded scenes with heavy occlusion , extreme poses and intensive interactions are difficult to identify and segment . Some small - scale semantic categories within person instances are difficult to parse . This confirms that MHP v2.0 aligns with real - world situations and deserves more furture attention and research efforts . subsection : Evaluations on the MHP v1.0 Benchmark The MHP v1.0 dataset is the first multi - human parsing benchmark , originally proposed by Li et al . , which contains 4 , 980 images annotated with 18 semantic labels . Annotation examples are visualized in Fig . [ reference ] ( b ) . The data are randomly organized into 3 splits , consisting of 3 , 000 training , 1 , 000 validation and 1 , 000 testing images with publicly available annotations . Evaluation systems report the and PCP over the testing set . Refer to for more details . The performance comparison of the proposed NAN with three state - of - the - art methods in terms of @IoU=0.5 , and PCP@IoU=0.5 on the MHP v1.0 testing set is reported in Tab . [ reference ] . With the nested adversarial learning of semantic saliency prediction , instance - agnostic parsing and instance - aware clustering , our method outperforms the - best by for , for and for PCP . Visual comparison of multi - human parsing results by NAN and three state - of - the - art methods is provided in Fig . [ reference ] , which further validates the advantages of our NAN over existing solutions . subsection : Evaluations on the PASCAL - Person - Part Benchmark The PASCAL - Person - Part dataset is a set of additional annotations for PASCAL - VOC - 2010 . It goes beyond the original PASCAL object detection task by providing pixel - wise labels for six human body parts , i.e. , head , torso , upper -/ lower - arms , and upper -/ lower - legs . The rest of each image is considered as background . There are 3 , 535 images in the PASCAL - Person - Part dataset , which is split into separate training set containing 1 , 717 images and testing set containing 1 , 818 images . For fair comparison , we report the over the testing set for multi - human parsing . Refer to for more details . The performance comparison of the proposed NAN with two state - of - the - art methods in terms of @IoU= and on the PASCAL - Person - Part testing set is reported in Tab . [ reference ] . Our method dramatically surpasses the - best by for and for . Qualitative multi - human parsing results by NAN are visualized in Fig . [ reference ] , which possess a high concordance with corresponding ground truths . This again verifies the effectiveness of our method for human - centric analysis . subsection : Evaluations on the Buffy Benchmark The Buffy dataset was released in 2011 for human parsing and instance segmentation , which contains 748 images annotated with 12 semantic labels . The data are randomly organized into 2 splits , consisting of 452 training and 296 testing images with publicly available annotations . For fair comparison , we report the F orward ( F ) and B ackward ( B ) scores over the episode 4 , 5 and 6 for instance segmentation evaluation . Refer to for more details . The performance comparison of the proposed NAN with three state - of - the - art methods in terms of F and B scores on the Buffy dataset episode 4 , 5 and 6 is reported in Tab . [ reference ] . Our NAN consistently achieves the best performance for all metrics . In particualr , NAN significantly improves the - best by for F score and for B score , with an average boost of . Qualitative instance - agnostic parsing and instance - aware clustering results by NAN are visualized in Fig . [ reference ] , which well shows the promising potential of our method for fine - grained understanding humans in crowded scenes . section : Conclusions In this work , we presented \u201c M ulti - H uman P arsing ( MHP v2.0 ) \u201d , a large - scale multi - human parsing dataset and a carefully designed benchmark to spark progress in understanding humans in crowded scenes . MHP v2.0 contains 25 , 403 images , which are richly labelled with 59 semantic categories . We also proposed a novel deep N ested A dversarial N etwork ( NAN ) model to address this challenging problem and performed detailed evaluations of the proposed method with current state - of - the - arts on MHP v2.0 and several other datasets . We envision the proposed MHP v2.0 dataset and the baseline method would drive the human parsing research towards real - world application scenario with simultaneous presence of multiple persons and complex interactions among them . In future , we will continue to take efforts to construct a more comprehensive multi - human parsing benchmark dataset with more images and more detailed semantic category annotations to further push the frontiers of multi - human parsing research . section : Acknowledgement The work of Jian Zhao was partially supported by C hina S cholarship C ouncil ( CSC ) grant 201503170248 . The work of Jiashi Feng was partially supported by NUS startup R - 263 - 000 - C08 - 133 , MOE Tier - I R - 263 - 000 - C21 - 112 , NUS IDS R - 263 - 000 - C67 - 646 and ECRA R - 263 - 000 - C87 - 133 . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["MHP_v1_0"]]], "Method": [[["NAN"]]], "Metric": [[["AP_0_5"]]], "Task": [[["Multi-Human_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["MHP_v2_0"]]], "Method": [[["NAN"]]], "Metric": [[["AP_0_5"]]], "Task": [[["Multi-Human_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["PASCAL-Person-Part"]]], "Method": [[["NAN"]]], "Metric": [[["AP_0_5"]]], "Task": [[["Multi-Human_Parsing"]]]}]}
{"docid": "TST3-SREX-0040", "doctext": "document : Deep Fried Convnets The fully - connected layers of deep convolutional neural networks typically contain over 90 % of the network parameters . Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices . In this paper , we introduce a novel Adaptive Fastfood transform to reparameterize the matrix - vector multiplication of fully connected layers . Reparameterizing a fully connected layer with inputs and outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from to and respectively . Using the Adaptive Fastfood transform in convolutional networks results in what we call a deep fried convnet . These convnets are end - to - end trainable , and enable us to attain substantial reductions in the number of parameters without affecting prediction accuracy on the MNIST and ImageNet datasets . section : Introduction In recent years we have witnessed an explosion of applications of convolutional neural networks with millions and billions of parameters . Reducing this vast number of parameters would improve the efficiency of training in distributed architectures . It would also allow for the deployment of state - of - the - art convolutional neural networks on embedded mobile applications . These train and test time considerations are both of great importance . A standard convolutional network is composed of two types of layers , each with very different properties . Convolutional layers , which contain a small fraction of the network parameters , represent most of the computational effort . In contrast , fully connected layers contain the vast majority of the parameters but are comparatively cheap to evaluate . This imbalance between memory and computation suggests that the efficiency of these two types of layers should be addressed in different ways . and both describe methods for minimizing computational cost of evaluating a network at test time by approximating the learned convolutional filters with separable approximations . These approaches realize speed gains at test time but do not address the issue of training , since the approximations are made after the network has been fully trained . Additionally , neither approach achieves a substantial reduction in the number of parameters , since they both work with approximations of the convolutional layers , which represent only a small portion of the total number of parameters . Many other works have addressed the computational efficiency of convolutional networks in more specialized settings . In contrast to the above approaches , demonstrates that there is significant redundancy in the parameterization of several deep learning models , and exploits this to reduce the number of parameters . More specifically , their method represents the parameter matrix as a product of two low rank factors , and the training algorithm fixes one factor ( called static parameters ) and only updates the other factor ( called dynamic parameters ) . uses low - rank matrix factorization to reduce the size of the fully connected layers at train time . They demonstrate large improvements in reducing the number of parameters of the output softmax layer , but only modest improvements for the hidden fully connected layers . implements low - rank factorizations using the SVD after training the full model . In contrast , the methods advanced in and this paper apply both at train and test time . In this paper we show how the number of parameters required to represent a deep convolutional neural network can be substantially reduced without sacrificing predictive performance . Our approach works by replacing the fully connected layers of the network with an Adaptive Fastfood transform , which is a generalization of the Fastfood transform for approximating kernels . Convolutional neural networks with Adaptive Fastfood transforms , which we refer to as deep fried convnets , are end - to - end trainable and achieve the same predictive performance as standard convolutional networks on ImageNet using approximately half the number of parameters . Several works have considered kernel methods in deep learning . The Doubly Stochastic Gradients method of showed that effective use of randomization can allow kernel methods to scale to extremely large data sets . However , the approach used fixed convolutional features , and can not jointly learn the kernel classifier and convolutional filters . showed how to learn a kernel function in an unsupervised manner . There have been other attempts to replace the fully connected layers . The Network in Network architecture of achieves state of the art results on several deep learning benchmarks by replacing the fully connected layers with global average pooling . A similar approach was used by to win the ILSVRC 2014 object detection competition . Although the global average pooling approach achieves impressive results , it has two significant drawbacks . First , feature transfer is more difficult with this approach . It is very common in practice to take a convolutional network trained on ImageNet and re - train the top layer on a different data set , re - using the features learned from ImageNet for the new task ( potentially with fine - tuning ) , and this is difficult with global average pooling . This deficiency is noted by , and motivates them to add an extra linear layer to the top of their network to enable them to more easily adapt and fine tune their network to other label sets . The second drawback of global average pooling is computation . Convolutional layers are much more expensive to evaluate than fully connected layers , so replacing fully connected layers with more convolutions can decrease model size but comes at the cost of increased evaluation time . In parallel or after the first ( technical report ) version of this work , several researchers have attempted to create sparse networks by applying pruning or sparsity regularizers . These approaches however require training the original full model and , consequently , do not enjoy the efficient training time benefits of the techniques proposed in this paper . Since then , hashing methods have also been advanced to reduce the number of parameters . Hashes have irregular memory access patterns and , consequently , good performance on large GPU - based platforms is yet to be demonstrated . Finally , distillation also offers a way of compressing neural networks , as a post - processing step . section : The Adaptive Fastfood Transform Large dense matrices are the main building block of fully connected neural network layers . In propagating the signal from the - th layer with activations to the - th layer with activations , we have to compute The storage and computational costs of this matrix multiplication step are both . The storage cost in particular can be prohibitive for many applications . Our proposed solution is to reparameterize the matrix of parameters with an Adaptive Fastfood transform , as follows In Section [ reference ] , we will provide background and intuitions behind this design . For now it suffices to state that the storage requirements of this reparameterization are and the computational cost is . We will also show in the experimental section that these theoretical savings are mirrored in practice by significant reductions in the number of parameters without increased prediction errors . To understand these claims , we need to describe the component modules of the Adaptive Fastfood transform . For simplicity of presentation , let us first assume that . Adaptive Fastfood has three types of module : and are diagonal matrices of parameters . In the original non - adaptive Fastfood formulation they are random matrices , as described further in Section [ reference ] . The computational and storage costs are trivially . is a random permutation matrix . It can be implemented as a lookup table , so the storage and computational costs are also . denotes the Walsh - Hadamard matrix , which is defined recursively as The Fast Hadamard Transform , a variant of Fast Fourier Transform , enables us to compute in time . In summary , the overall storage cost of the Adaptive Fastfood transform is , while the computational cost is . These are substantial theoretical improvements over the costs of ordinary fully connected layers . When the number of output units is larger than the number of inputs , we can perform Adaptive Fastfood transforms and stack them to attain the desired size . In doing so , the computational and storage costs become and respectively , as opposed to the more substantial costs for linear modules . The number of outputs can also be refined with pruning . subsection : Learning Fastfood by backpropagation The parameters of the Adaptive Fastfood transform ( and ) can be learned by standard error derivative backpropagation . Moreover , the backward pass can also be computed efficiently using the Fast Hadamard Transform . In particular , let us consider learning the - th layer of the network , . For simplicity , let us again assume that and that . Using backpropagation , assume we already have , where is the objective function , then Since is a diagonal matrix , we only need to calculate the derivative with respect to the diagonal entries and this step requires only operations . Proceeding in this way , denote the partial products by Then the gradients with respect to different parameters in the Fastfood layer can be computed recursively as follows : Note that the operations in and are simply applications of the Hadamard transform , since , and consequently can be computed in time . The operation in is an application of a permutation ( the transpose of permutation matrix is a permutation matrix ) and can be computed in time . All other operations are diagonal matrix multiplications . section : Intuitions behind Adaptive Fastfood The proposed Adaptive Fastfood transform may be understood either as a trainable type of structured random projection or as an approximation to the feature space of a learned kernel . Both views not only shed light on Adaptive Fastfood and competing techniques , but also open up room to innovate new techniques to reduce computation and memory in neural networks . subsection : A view from structured random projections Adaptive Fastfood is based on the Fastfood transform , in which the diagonal matrices , and have random entries . In the experiments , we will compare the performance of the existing random and proposed adaptive versions of Fastfood when used to replace fully connected layers in convolutional neural networks . The intriguing idea of constructing neural networks with random weights has been reasonably explored in the neural networks field . This idea is related to random projections , which have been deeply studied in theoretical computer science . In a random projection , the basic operation is of the form where is a random matrix , either Gaussian or binary . Importantly , the embeddings generated by these random projections approximately preserve metric information , as formalized by many variants of the celebrated Johnson - Lindenstrauss Lemma . The one shortcoming of random projections is that the cost of storing the matrix is . Using a sparse random matrix by itself to reduce this cost is often not a viable option because the variance of the estimates of can be very high for some inputs , for example when is also sparse . To see this , consider the extreme case of a very sparse input , then many of the products with will be zero and hence not help improve the estimates of metric properties of the embedding space . One popular option for reducing the storage and computational costs of random projections is to adopt random hash functions to replace the random matrix multiplication . For example , the count - sketch algorithm uses pairwise independent hash functions to carry this job very effectively in many applications . This technique is often referred to as the hashing trick in the machine learning literature . Hashes have irregular memory access patterns , so it is not clear how to get good performance on GPUs when following this approach , as pointed out in . Ailon and Chazelle introduced an alternative approach that is not only very efficient , but also preserves most of the desirable theoretical properties of random projections . Their idea was to replace the random matrix by a transform that mimics the properties of random matrices , but which can be stored efficiently . In particular , they proposed the following PHD transform : where is a sparse random matrix with Gaussian entries , is a Hadamard matrix and is a diagonal matrix with entries drawn independently with probability . The inclusion of the Hadamard transform avoids the problems of using a sparse random matrix by itself , but it is still efficient to compute . We can think of the original Fastfood transform as an alternative to this . Fastfood reduces the computation and storage of random projections to and respectively . In the original formulation and are diagonal random matrices , which are computed once and then stored . In contrast , in our proposed Adaptive Fastfood transform , the diagonal matrices are learned by backpropagation . By adapting , we are effectively implementing Automatic Relevance Determination on features . The matrix controls the bandwidth of the kernel and its spectral incoherence . Finally , represents different kernel types . For example , for the RBF kernel follows Chi - squared distribution . By adapting , we learn the correct kernel type . While we have introduced Fastfood in this section , it was originally proposed as a fast way of computing random features to approximate kernels . We expand on this perspective in the following section . subsection : A view from kernels There is a nice duality between inner products of features and kernels . This duality can be used to design neural network modules using kernels and vice - versa . For computational reasons , we often want to determine the features associated with a kernel . Working with features is preferable when the kernel matrix is dense and large . ( Storing this matrix requires space , and computing it takes operations , where is the number of data points and is the dimension . ) We might also want to design statistical methods using kernels and then map these designs to features that can be used as modules in neural networks . Unfortunately , one of the difficulties with this line of attack is that deriving features from kernels is far from trivial in general . An important fact , noted in , is that infinite kernel expansions can be approximated in an unbiased manner using randomly drawn features . For shift - invariant kernels this relies on a classical result from harmonic analysis , known as Bochner \u2019s Lemma , which states that a continuous shift - invariant kernel on is positive definite if and only if is the Fourier transform of a non - negative measure . This measure , known as the spectral density , in turn implies the existence of a probability density such that where the imaginary part is dropped since both the kernel and distribution are real . We can apply Monte Carlo methods to approximate the above expectation , and hence approximate the kernel with an inner product of stacked cosine and sine features . Specifically , suppose we sample vectors from and collect them in a matrix . The kernel can then be approximated as the inner - product of the following random features : That is , is the neural network module , consisting of a linear layer and entry - wise nonlinearities ( cosine and sine in the above equation ) , that corresponds to a particular implicit kernel function . Approximating a given kernel function with random features requires the specification of a sampling distribution . Such distributions have been derived for many popular kernels . For example , if we want the implicit kernel to be a squared exponential kernel , we know that the distribution must be Gaussian : . In other words , if we draw the rows of from this Gaussian distribution and use equation ( [ reference ] ) to implement a neural module , we are implicitly approximating a squared exponential kernel . As another example of the mapping between kernels and random features , introduced the rotationally invariant arc - cosine kernel where is the angle between and . Then by choosing to be a random Gaussian matrix , they showed that this kernel can be approximated with Rectified Linear Unit ( ReLU ) features : The Fastfood transform was introduced to replace in Equation [ reference ] with , thus decreasing the computational and storage costs . section : Deep Fried Convolutional Networks We propose to greatly reduce the number of parameters of the fully connected layers by replacing them with an Adaptive Fastfood transform followed by a nonlinearity . We call this new architecture a deep fried convolutional network . An illustration of this architecture is shown in Figure [ reference ] . In principle , we could also apply the Adaptive Fastfood transform to the softmax classifier . However , reducing the memory cost of this layer is already well studied ; for example , show that low - rank matrix factorization can be applied during training to reduce the size of the softmax layer substantially . Importantly , they also show that training a low rank factorization for the internal layers performs poorly , which agrees with the results of . For this reason , we focus our attention on reducing the size of the internal layers . section : MNIST Experiment The first problem we study is the classical MNIST optical character recognition task . This simple task serves as an easy proof of concept for our method , and contrasting the results in this section with our later experiments gives insights into the behavior of the Adaptive Fastfood transform at different scales . As a reference model we use the Caffe implementation of the LeNet convolutional network . It achieves an error rate of on the MNIST dataset . We jointly train all layers of the deep fried network ( including convolutional layers ) from scratch . We compare both the adaptive and non - adaptive Fastfood transforms using 1024 and 2048 features . For the non - adaptive transforms we report the best performance achieved by varying the standard deviation of the random Gaussian matrix over the set , and for the adaptive variant we learn these parameters by backpropagation as described in Section [ reference ] . The results of the MNIST experiment are shown in Table [ reference ] . Because the width of the deep fried network is substantially larger than the reference model , we also experimented with adding dropout in the model , which increased performance in the deep fried case . Deep fried networks are able to obtain high accuracy using only a small fraction of of parameters of the original network ( 11 times reduction in the best case ) . Interestingly , we see no benefit from adaptation in this experiment , with the more powerful adaptive models performing equivalently or worse than their non - adaptive counterparts ; however , this should be contrasted with the ImageNet results reported in the following sections . section : Imagenet Experiments We now examine how deep fried networks behave in a more realistic setting with a much larger dataset and many more classes . Specifically , we use the ImageNet ILSVRC - 2012 dataset which has 1.2 M training examples and 50 K validation examples distributed across 1000 classes . We use the the Caffe ImageNet model as the reference model in these experiments . This model is a modified version of AlexNet , and achieves top - 1 error on the ILSVRC - 2012 validation set . The initial layers of this model are a cascade of convolution and pooling layers with interspersed normalization . The last several layers of the network take the form of an MLP and follow a 9216\u20134096\u20134096\u20131000 architecture . The final layer is a logistic regression layer with 1000 output classes . All layers of this network use the ReLU nonlinearity , and dropout is used in the fully connected layers to prevent overfitting . There are total of 58 , 649 , 184 parameters in the reference model , of which 58 , 621 , 952 are in the fully connected layers and only 27 , 232 are in the convolutional layers . The parameters of fully connected layer take up of the total number of parameters . We show that the Adaptive Fastfood transform can be used to substantially reduce the number of parameters in this model . subsection : Fixed feature extractor Previous work on applying kernel methods to ImageNet has focused on building models on features extracted from the convolutional layers of a pre - trained network . This setting is less general than training a network from scratch but does mirror the common use case where a convolutional network is first trained on ImageNet and used as a feature extractor for a different task . In order to compare our Adaptive Fastfood transform directly to this previous work , we extract features from the final convolutional layer of a pre - trained reference model and train an Adaptive Fastfood transform classifier using these features . Although the reference model uses two fully connected layers , we investigate replacing these with only a single Fastfood transform . We experiment with two sizes for this transform : Fastfood 16 and Fastfood 32 using 16 , 384 and 32 , 768 Fastfood features respectively . Since the Fastfood transform is a composite module , we can apply dropout between any of its layers . In the experiments reported here , we applied dropout after the matrix and after the matrix . We also applied dropout to the last convolutional layer ( that is , before the matrix ) . We also train an MLP with the same structure as the top layers of the reference model for comparison . In this setting it is important to compare against the re - trained MLP rather than the jointly trained reference model , as training on features extracted from fixed convolutional layers typically leads to lower performance than joint training . The results of the fixed feature experiment are shown in Table [ reference ] . Following and we observe that training on ImageNet activations produces significantly lower performance than of the original , jointly trained network . Nonetheless , deep fried networks are able to outperform both the re - trained MLP model as well as the results in while using fewer parameters . In contrast with our MNIST experiment , here we find that the Adaptive Fastfood transform provides a significant performance boost over the non - adaptive version , improving top - 1 performance by 4.5 - 6.5 % . subsection : Jointly trained model Finally , we train a deep fried network from scratch on ImageNet . With 16 , 384 features in the Fastfood layer we lose less than 0.3 % top - 1 validation performance , but the number of parameters in the network is reduced from 58.7 M to 16.4 M which corresponds to a factor of 3.6x . By further increasing the number of features to 32 , 768 , we are able to perform 0.6 % better than the reference model while using approximately half as many parameters . Results from this experiment are shown in Table [ reference ] . Nearly all of the parameters of the deep fried network reside in the final softmax regression layer , which still uses a dense linear transformation , and accounts for more than 99 % of the parameters of the network . This is a side effect of the large number of classes in ImageNet . For a data set with fewer classes the advantage of deep fried convolutional networks would be even greater . Moreover , as shown by , the last layer often contains considerable redundancy . We also note that any of the techniques from could be applied to the final layer of a deep fried network to further reduce memory consumption at test time . We illustrate this with low - rank matrix factorization in the following section . section : Comparison with Post Processing In this section we provide a comparison to some existing works on reducing the number of parameters in a convolutional neural network . The techniques we compare against here are post - processing techniques , which start from a full trained model and attempt to compress it , whereas our method trains the compressed network from scratch . Matrix factorization is the most common method for compressing neural networks , and has proven to be very effective . Given the weight matrix of fully connected layers , we factorize it as where and and is a diagonal matrix . In order to reduce the parameters , we truncate all but the largest singular values , leading to the approximation where and and has been absorbed into the other two factors . If is sufficiently small then storing and is less expensive than storing directly , and this parameterization is still learnable . It has been shown that training a factorized representation directly leads to poor performance ( although it does work when applied only to the final logistic regression layer ) . However , first training a full model , then preforming an SVD of the weight matrices followed by a fine tuning phase preserves much of the performance of the original model . We compare our deep fried approach to SVD followed by fine tuning , and show that our approach achieves better performance per parameter in spite of training a compressed parameterization from scratch . We also compare against a post - processed version of our model , where we train a deep fried convnet and then apply SVD plus fine - tuning to the final softmax layer , which further reduces the number of parameters . Results of these post - processing experiments are shown in Table [ reference ] . For the SVD decomposition of each of the three fully connected layers in the reference model we set in SVD - half and in SVD - quarter . SVD - half - F and SVD - quarter - F mean that the model has been fine tuned after the decomposition . There is 1 % drop in accuracy for SVD - half and 3.5 % drop for SVD - quarter . Even though the increase in the error for the SVD can be mitigated by finetuning ( the drop decreases to 0.1 % for SVD - half - F and 1.3 % for SVD - quarter - F ) , deep fried convnets still perform better both in terms of the accuracy and the number of parameters . Applying a rank 600 SVD followed by fine tuning to the final softmax layer of the Adaptive Fastfood 32 model removes an additional 12.5 M parameters at the expense of 0.7 % top - 1 error . For reference , we also include the results of Collins and Kohli , who pre - train a full network and use a sparsity regularizer during fine - tuning to encourage connections in the fully connected layers to be zero . They are able to achieve a significant reduction in the number of parameters this way , however the performance of their compressed network suffers when compared to the reference model . Another drawback of this method is that using sparse weight matrices requires additional overhead to store the indexes of the non - zero values . The index storage takes up space and using sparse representation is better than using a dense matrix only when number of nonzero entries is small . section : Conclusion Many methods have been advanced to reduce the size of convolutional networks at test time . In contrast to this trend , the Adaptive Fastfood transform introduced in this paper is end - to - end differentiable and hence it enables us to attain reductions in the number of parameters even at train time . Deep fried convnets capitalize on the proposed Adaptive Fastfood transform to achieve a substantial reduction in the number of parameters without sacrificing predictive performance on MNIST and ImageNet . They also compare favorably against simple test - time low - rank matrix factorization schemes . Our experiments have also cast some light on the issue of random versus adaptive weights . The structured random transformations developed in the kernel literature perform very well on MNIST without any learning ; however , when moving to ImageNet , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance . This is an important point which illustrates the importance of learning which would not have been visible from experiments only on small data sets . The Fastfood transform allows for a theoretical reduction in computation from to . However , the computation in convolutional neural networks is dominated by the convolutions , and hence deep fried convnets are not necessarily faster in practice . It is clear looking at out results on ImageNet in Table 2 that the remaining parameters are mostly in the output softmax layer . The comparative experiment in Section 7 showed that the matrix of parameters in the softmax can be easily compressed using the SVD , but many other methods could be used to achieve this . One avenue for future research involves replacing the softmax matrix , at train and test times , using the abundant set of techniques that have been proposed to solve this problem , including low - rank decomposition , Adaptive Fastfood , and pruning . The development of GPU optimized Fastfood transforms that can be used to replace linear layers in arbitrary neural models would also be of great value to the entire research community given the ubiquity of fully connected layers layers . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["MNIST"]]], "Method": [[["Deep_Fried_Convnets"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Image_Classification"]]]}]}
{"docid": "TST3-SREX-0041", "doctext": "Strong Baselines for Neural Semi - supervised Learning under Domain Shift section : Abstract Novel neural models have been proposed in recent years for learning under domain shift . Most models , however , only evaluate on a single task , on proprietary datasets , or compare to weak baselines , which makes comparison of models difficult . In this paper , we re - evaluate classic general - purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi - task tri - training method that reduces the time and space complexity of classic tri - training . Extensive experiments on two benchmarks are negative : while our novel method establishes a new state - of - the - art for sentiment analysis , it does not fare consistently the best . More importantly , we arrive at the somewhat surprising conclusion that classic tri - training , with some additions , outperforms the state of the art . We conclude that classic approaches constitute an important and strong baseline . section : Introduction Deep neural networks ( DNNs ) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing [ reference ] , named entity recognition [ reference ] , and semantic role labeling [ reference ] . In contrast , learning from unlabeled data , especially under domain shift , remains a challenge . This is common in many real - world applications where the distribution of the training and test data differs . Many state - of - the - art domain adaptation approaches leverage task - specific characteristics such as sentiment words [ reference ][ reference ] or distributional features [ reference ][ reference ] which do not generalize to other tasks . Other approaches that are in theory more general only evaluate on proprietary datasets [ reference ] or on a single benchmark [ reference ] , which carries the risk of overfitting to the task . In addition , most models only compare against weak baselines and , strikingly , almost none considers evaluating against approaches from the extensive semi - supervised learning ( SSL ) literature [ reference ] . In this work , we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usefulness of classic approaches [ reference ][ reference ] . We re - evaluate bootstrapping algorithms in the context of DNNs . These are general - purpose semi - supervised algorithms that treat the model as a black box and can thus be used easily - with a few additions - with the current generation of NLP models . Many of these methods , though , were originally developed with in - domain performance in mind , so their effectiveness in a domain adaptation setting remains unexplored . In particular , we re - evaluate three traditional bootstrapping methods , self - training [ reference ] , tri - training [ reference ] , and tritraining with disagreement [ reference ] for neural network - based approaches on two NLP tasks with different characteristics , namely , a sequence prediction and a classification task ( POS tagging and sentiment analysis ) . We evaluate the methods across multiple domains on two wellestablished benchmarks , without taking any further task - specific measures , and compare to the best results published in the literature . We make the somewhat surprising observation that classic tri - training outperforms task - agnostic state - of - the - art semi - supervised learning [ reference ] and recent neural adaptation approaches [ reference ][ reference ] . In addition , we propose multi - task tri - training , which reduces the main deficiency of tri - training , namely its time and space complexity . It establishes a new state of the art on unsupervised domain adaptation for sentiment analysis but it is outperformed by classic tri - training for POS tagging . Contributions Our contributions are : a ) We propose a novel multi - task tri - training method . b ) We show that tri - training can serve as a strong and robust semi - supervised learning baseline for the current generation of NLP models . c ) We perform an extensive evaluation of bootstrapping 1 algorithms compared to state - of - the - art approaches on two benchmark datasets . d ) We shed light on the task and data characteristics that yield the best performance for each model . section : Neural bootstrapping methods We first introduce three classic bootstrapping methods , self - training , tri - training , and tri - training with disagreement and detail how they can be used with neural networks . For in - depth details we refer the reader to [ reference ][ reference ][ reference ] ) . We introduce our novel multitask tri - training method in \u00a7 2.3 . section : Self - training Self - training [ reference ][ reference ] ) is one of the earliest and simplest bootstrapping approaches . In essence , it leverages the model 's own predictions on unlabeled data to obtain additional information that can be used during training . Typically the most confident predictions are taken at face value , as detailed next . Self - training trains a model m on a labeled training set L and an unlabeled data set U . At each iteration , the model provides predictions m ( x ) in the form of a probability distribution over classes for all unlabeled examples x in U . If the probability assigned to the most likely class is higher than a predetermined threshold \u03c4 , x is added to the labeled examples with p ( x ) = arg max m ( x ) as pseudo - label . This instantiation is the most widely used and shown in Algorithm 1 . Calibration It is well - known that output probabilities in neural networks are poorly calibrated [ reference ] . Using a fixed threshold \u03c4 is thus Algorithm 1 Self - training [ reference ] L \u2190 L \u222a { ( x , p ( x ) ) } 6 : until no more predictions are confident not the best choice . While the absolute confidence value is inaccurate , we can expect that the relative order of confidences is more robust . For this reason , we select the top n unlabeled examples that have been predicted with the highest confidence after every epoch and add them to the labeled data . This is one of the many variants for self - training , called throttling [ reference ] . We empirically confirm that this outperforms the classic selection in our experiments . Online learning In contrast to many classic algorithms , DNNs are trained online by default . We compare training setups and find that training until convergence on labeled data and then training until convergence using self - training performs best . Classic self - training has shown mixed success . In parsing it proved successful only with small datasets [ reference ] or when a generative component is used together with a reranker in high - data conditions [ reference ][ reference ] . Some success was achieved with careful task - specific data selection [ reference ] , while others report limited success on a variety of NLP tasks [ reference ][ reference ][ reference ] . Its main downside is that the model is not able to correct its own mistakes and errors are amplified , an effect that is increased under domain shift . section : Tri - training Tri - training [ reference ] ) is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models . Tri - training ( cf . Algorithm 2 ) first trains three models m 1 , m 2 , and m 3 on bootstrap samples of the labeled data L. An unlabeled data point is added to the training set of a model m i if the other two models m j and m k agree on its label . Training stops when the classifiers do not change anymore . Tri - training with disagreement [ reference ] Algorithm 2 Tri - training [ reference ] 1 : for i \u2208 { 1 .. 3 } do 2 : for i \u2208 { 1 .. 3 } do 6 : for x \u2208 U do 8 : 10 : until none of m i changes 11 : apply majority vote over m i is based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be skewed by easy data points . In order to achieve this , it adds a simple modification to the original algorithm ( altering line 8 in Algorithm 2 ) , requiring that for an unlabeled data point on which m j and m k agree , the other model m i disagrees on the prediction . Tri - training with disagreement is more data - efficient than tritraining and has achieved competitive results on part - of - speech tagging [ reference ] . Sampling unlabeled data Both tri - training and tri - training with disagreement can be very expensive in their original formulation as they require to produce predictions for each of the three models on all unlabeled data samples , which can be in the millions in realistic applications . We thus propose to sample a number of unlabeled examples at every epoch . For all traditional bootstrapping approaches we sample 10k candidate instances in each epoch . For the neural approaches we use a linearly growing candidate sampling scheme proposed by [ reference ] , increasing the candidate pool size as the models become more accurate . Confidence thresholding Similar to selftraining , we can introduce an additional requirement that pseudo - labeled examples are only added if the probability of the prediction of at least one model is higher than some threshold \u03c4 . We did not find this to outperform prediction without threshold for traditional tri - training , but thresholding proved essential for our method ( \u00a7 2.3 ) . The most important condition for tri - training and tri - training with disagreement is that the models are diverse . Typically , bootstrap samples are used to create this diversity [ reference ][ reference ] . However , training separate models on bootstrap samples of a potentially large amount of training data is expensive and takes a lot of time . This drawback motivates our approach . section : Multi - task tri - training In order to reduce both the time and space complexity of tri - training , we propose Multi - task Tritraining ( MT - Tri ) . MT - Tri leverages insights from multi - task learning ( MTL ) [ reference ] to share knowledge across models and accelerate training . Rather than storing and training each model separately , we propose to share the parameters of the models and train them jointly using MTL . 2 All models thus collaborate on learning a joint representation , which improves convergence . The output softmax layers are model - specific and are only updated for the input of the respective model . We show the model in Figure 1 ( as instantiated for POS tagging ) . As the models leverage a joint representation , we need to ensure that the features used for prediction in the softmax layers of the different models are as diverse as possible , so that the models can still learn from each other 's predictions . In contrast , if the parameters in all output softmax layers were the same , the method would degenerate to self - training . To guarantee diversity , we introduce an orthogonality constraint [ reference ] as an additional loss term , which we define as follows : where | \u00b7 2 F is the squared Frobenius norm and W m 1 and W m 2 are the softmax output parameters of the two source and pseudo - labeled output layers m 1 and m 2 , respectively . The orthogonality constraint encourages the models not to rely on the same features for prediction . As enforcing pairwise orthogonality between three matrices is not possible , we only enforce orthogonality between the softmax output layers of m 1 and m 2 , 3 while m 3 is gradually trained to be more target - specific . We parameterize L orth by \u03b3=0.01 following . We do not further tune \u03b3 . More formally , let us illustrate the model by taking the sequence prediction task ( Figure 1 ) as illustration . Given an utterance with labels y 1 , .. , y n , our Multi - task Tri - training loss consists of three task - specific ( m 1 , m 2 , m 3 ) tagging loss functions ( where h is the uppermost Bi - LSTM encoding ) : In contrast to classic tri - training , we can train the multi - task model with its three model - specific outputs jointly and without bootstrap sampling on the labeled source domain data until convergence , as the orthogonality constraint enforces different representations between models m 1 and m 2 . From this point , we can leverage the pair - wise agreement of two output layers to add pseudo - labeled examples as training data to the third model . We train the third output layer m 3 only on pseudo - labeled target instances in order to make tri - training more robust to a domain shift . For the final prediction , majority voting of all three output layers is used , which resulted in the best instantiation , together with confidence thresholding ( \u03c4 = 0.9 , except for highresource POS where \u03c4 = 0.8 performed slightly better ) . We also experimented with using a domainadversarial loss [ reference ] on the jointly learned representation , but found this not to help . The full pseudo - code is given in Algorithm 3 . Computational complexity The motivation for MT - Tri was to reduce the space and time complexity of tri - training . We thus give an estimate of its efficiency gains . MT - Tri is~3\u00d7 more spaceefficient than regular tri - training ; tri - training stores one set of parameters for each of the three models , while MT - Tri only stores one set of parameters ( we use three output layers , but these make up a comparatively small part of the total parameter budget ) . In terms of time efficiency , tri - training first [ reference ] We also tried enforcing orthogonality on a hidden layer rather than the output layer , but this did not help . 10 : until end condition is met 11 : apply majority vote over m i requires to train each of the models from scratch . The actual tri - training takes about the same time as training from scratch and requires a separate forward pass for each model , effectively training three independent models simultaneously . In contrast , MT - Tri only necessitates one forward pass as well as the evaluation of the two additional output layers ( which takes a negligible amount of time ) and requires about as many epochs as tri - training until convergence ( see Table 3 , second column ) while adding fewer unlabeled examples per epoch ( see Section 3.4 ) . In our experiments , MT - Tri trained about 5 - 6\u00d7 faster than traditional tri - training . MT - Tri can be seen as a self - ensembling technique , where different variations of a model are used to create a stronger ensemble prediction . Recent approaches in this line are snapshot ensembling ) that ensembles models converged to different minima during a training run , asymmetric tri - training [ reference ] ) ( ASYM ) that leverages agreement on two models as information for the third , and temporal ensembling [ reference ] , which ensembles predictions of a model at different epochs . We tried to compare to temporal ensembling in our experiments , but were not able to obtain consistent results . [ reference ] We compare to the closest most recent method , asymmetric tritraining [ reference ] . It differs from ours in two aspects : a ) ASYM leverages only pseudolabels from data points on which m 1 and m 2 agree , and b ) it uses only one task ( m 3 ) as final predictor . In essence , our formulation of MT - Tri is closer to the original tri - training formulation ( agreements on two provide pseudo - labels to the third ) thereby incorporating more diversity . [ reference ] for POS tagging ( above ) and the Amazon Reviews dataset [ reference ] for sentiment analysis ( below ) . section : Experiments In order to ascertain which methods are robust across different domains , we evaluate on two widely used unsupervised domain adaptation datasets for two tasks , a sequence labeling and a classification task , cf . Table 1 for data statistics . section : POS tagging For POS tagging we use the SANCL 2012 shared task dataset [ reference ] and compare to the top results in both low and high - data conditions [ reference ][ reference ] . Both are strong baselines , as the FLORS tagger has been developed for this challenging dataset and it is based on contextual distributional features ( excluding the word 's identity ) , and hand - crafted suffix and shape features ( including some languagespecific morphological features ) . We want to gauge to what extent we can adopt a nowadays fairly standard ( but more lexicalized ) general neural tagger . Our POS tagging model is a state - of - the - art Bi - LSTM tagger [ reference ] with word and 100 - dim character embeddings . Word embeddings are initialized with the 100 - dim Glove embeddings [ reference ] . The BiLSTM has one hidden layer with 100 dimensions . The base POS model is trained on WSJ with early stopping on the WSJ development set , using patience 2 , Gaussian noise with \u03c3 = 0.2 and word dropout with p = 0.25 [ reference ] . Regarding data , the source domain is the Ontonotes 4.0 release of the Penn treebank Wall Street Journal ( WSJ ) annotated for 48 fine - grained POS tags . This amounts to 30 , 060 labeled sentences . We use 100 , 000 WSJ sentences from 1988 as unlabeled data , following [ reference ] . [ reference ] As target data , we use the five SANCL domains ( answers , emails , newsgroups , reviews , weblogs ) . We restrict the amount of unlabeled data for each SANCL domain to the first 100k sentences , and do not do any pre - processing . We consider the development set of ANSWERS as our only target dev set to set hyperparameters . This may result in suboptimal per - domain settings but better resembles an unsupervised adaptation scenario . section : Sentiment analysis For sentiment analysis , we evaluate on the Amazon reviews dataset [ reference ] . Reviews with 1 to 3 stars are ranked as negative , while reviews with 4 or 5 stars are ranked as positive . The dataset consists of four domains , yielding 12 adaptation scenarios . We use the same pre - processing and architecture as used in [ reference ][ reference ] : 5 , 000 - dimensional tf - idf weighted unigram and bigram features as input ; 2k labeled source samples and 2k unlabeled target samples for training , 200 labeled target samples for validation , and between 3k - 6k samples for testing . The model is an MLP with one hidden layer with 50 dimensions , sigmoid activations , and a softmax output . We compare against the Variational Fair Autoencoder ( VFAE ) [ reference ] model and domain - adversarial neural networks ( DANN ) [ reference ] . section : Baselines Besides comparing to the top results published on both datasets , we include the following baselines : a ) the task model trained on the source domain ; b ) self - training ( Self ) ; c ) tri - training ( Tri ) ; d ) tri - training with disagreement ( Tri - D ) ; and e ) asymmetric tri - training [ reference ] ) . Our proposed model is multi - task tri - training ( MTTri ) . We implement our models in DyNet . Reporting single evaluation scores might result in biased results [ reference ] . Throughout the paper , we report mean accuracy and standard deviation over five runs for POS tagging and over ten runs for sentiment analysis . Significance is computed using bootstrap test . The code for all experiments is released at : https: // github.com / bplank / semi - supervised - baselines . section : Results section : Sentiment analysis We show results for sentiment analysis for all 12 domain adaptation scenarios in Figure 2 . For clarity , we also show the accuracy scores averaged across each target domain as well as a global macro average in Table 2 Self - training achieves surprisingly good results but is not able to compete with tri - training . Tritraining with disagreement is only slightly better than self - training , showing that the disagreement component might not be useful when there is a strong domain shift . Tri - training achieves the best average results on two target domains and clearly outperforms the state of the art on average . MT - Tri finally outperforms the state of the art on 3 / 4 domains , and even slightly traditional tritraining , resulting in the overall best method . This improvement is mainly due to the B - >E and D - >E scenarios , on which tri - training struggles . These domain pairs are among those with the highest Adistance [ reference ] , which highlights that tri - training has difficulty dealing with a strong shift in domain . Our method is able to mitigate this deficiency by training one of the three output layers only on pseudo - labeled target domain examples . In addition , MT - Tri is more efficient as it adds a smaller number of pseudo - labeled examples than tri - training at every epoch . For sentiment analysis , tri - training adds around 1800 - 1950 / 2000 unlabeled examples at every epoch , while MT - Tri only adds around 100 - 300 in early epochs . This shows that the orthogonality constraint is useful for inducing diversity . In addition , adding fewer examples poses a smaller risk of swamping the learned representations with useless signals and is more akin to fine - tuning , the standard method for supervised domain adaptation [ reference ] . We observe an asymmetry in the results between some of the domain pairs , e.g. B - >D and D - >B. We hypothesize that the asymmetry may be due to properties of the data and that the domains are relatively far apart e.g. , in terms of A - distance . In fact , asymmetry in these domains is already reflected Table 4 : Accuracy for POS tagging on the dev and test sets of the SANCL domains , models trained on full source data setup . Values for methods with * are from [ reference ] . in the results of [ reference ] and is corroborated in the results for asymmetric tri - training [ reference ] and our method . We note a weakness of this dataset is high variance . Existing approaches only report the mean , which makes an objective comparison difficult . For this reason , we believe it is essential to evaluate proposed approaches also on other tasks . POS tagging Results for tagging in the low - data regime ( 10 % of WSJ ) are given in Table 3 . Self - training does not work for the sequence prediction task . We report only the best instantiation ( throttling with n=800 ) . Our results contribute to negative findings regarding self - training [ reference ][ reference ] . In the low - data setup , tri - training with disagreement works best , reaching an overall average accuracy of 89.70 , closely followed by classic tritraining , and significantly outperforming the baseline on 4 / 5 domains . The exception is newsgroups , a difficult domain with high OOV rate where none of the approches beats the baseline ( see \u00a7 3.4 ) . Our proposed MT - Tri is better than asymmetric tritraining , but falls below classic tri - training . It beats Table 5 : Accuracy scores on dev sets for OOV and unknown word - tag ( UWT ) tokens . the baseline significantly on only 2 / 5 domains ( answers and emails ) . The FLORS tagger [ reference ] fares better . Its contextual distributional features are particularly helpful on unknown word - tag combinations ( see \u00a7 3.4 ) , which is a limitation of the lexicalized generic bi - LSTM tagger . For the high - data setup ( Table 4 ) results are similar . Disagreement , however , is only favorable in the low - data setups ; the effect of avoiding easy points no longer holds in the full data setup . Classic tritraining is the best method . In particular , traditional tri - training is complementary to word embedding initialization , pushing the non - pre - trained baseline to the level of SRC with Glove initalization . Tritraining pushes performance even further and results in the best model , significantly outperforming the baseline again in 4 / 5 cases , and reaching FLORS performance on weblogs . Multi - task tritraining is often slightly more effective than asymmetric tri - training [ reference ] ; however , improvements for both are not robust across domains , sometimes performance even drops . The model likely is too simplistic for such a high - data POS setup , and exploring shared - private models might prove more fruitful . On the test sets , tri - training performs consistently the best . section : POS analysis We analyze POS tagging accuracy with respect to word frequency 6 and unseen word - tag combinations ( UWT ) on the dev sets . known tags , OOVs and unknown word - tag ( UWT ) rate . The SANCL dataset is overall very challenging : OOV rates are high ( 6.8 - 11 % compared to 2.3 % in WSJ ) , so is the unknown word - tag ( UWT ) rate ( answers and emails contain 2.91 % and 3.47 % UWT compared to 0.61 % on WSJ ) and almost all target domains even contain unknown tags [ reference ] ) ( unknown tags : ADD , GW , NFP , XX ) , except for weblogs . Email is the domain with the highest OOV rate and highest unknown - tag - for - known - words rate . We plot accuracy with respect to word frequency on email in Figure 3 , analyzing how the three methods fare in comparison to the baseline on this difficult domain . Regarding OOVs , the results in Table 5 ( second part ) show that classic tri - training outperforms the source model ( trained on only source data ) on 3 / 5 domains in terms of OOV accuracy , except on two domains with high OOV rate ( newsgroups and weblogs ) . In general , we note that tri - training works best on OOVs and on low - frequency tokens , which is also shown in Figure 3 ( leftmost bins ) . Both other methods fall typically below the baseline in terms of OOV accuracy , but MT - Tri still outperforms Asym in 4 / 5 cases . Table 5 ( last part ) also shows that no bootstrapping method works well on unknown word - tag combinations . UWT tokens are very difficult to predict correctly using an unsupervised approach ; the less lexicalized and more context - driven approach taken by FLORS is clearly superior for these cases , resulting in higher UWT accuracies for 4 / 5 domains . section : Related work Learning under Domain Shift There is a large body of work on domain adaptation . Studies on unsupervised domain adaptation include early work on bootstrapping [ reference ][ reference ] , shared feature representations [ reference ][ reference ] and instance weighting [ reference ] . Recent approaches include adversarial learning [ reference ] and fine - tuning [ reference ] . There is almost no work on bootstrapping approaches for recent neural NLP , in particular under domain shift . Tri - training is less studied , and only recently re - emerged in the vision community [ reference ] , albeit is not compared to classic tri - training . Neural network ensembling Related work on self - ensembling approaches includes snapshot ensembling or temporal ensembling [ reference ] . In general , the line between \" explicit \" and \" implicit \" ensembling , like dropout [ reference ] or temporal ensembling [ reference ] , is more fuzzy . As we noted earlier our multi - task learning setup can be seen as a form of self - ensembling . Multi - task learning in NLP Neural networks are particularly well - suited for MTL allowing for parameter sharing [ reference ] . Recent NLP conferences witnessed a \" tsunami \" of deep learning papers [ reference ] , followed by what we call a multi - task learning \" wave \" : MTL has been successfully applied to a wide range of NLP tasks [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] . Related to it is the pioneering work on adversarial learning ( DANN ) [ reference ] . For sentiment analysis we found tri - training and our MT - Tri model to outperform DANN . Our MT - Tri model lends itself well to shared - private models such as those proposed recently [ reference ] , which extend upon [ reference ] by having separate source and target - specific encoders . section : Conclusions We re - evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural network approaches to semi - supervised learning under domain shift . For the two examined NLP tasks classic tri - training works the best and even outperforms a recent state - of - the - art method . The drawback of tri - training it its time and space complexity . We therefore propose a more efficient multi - task tri - training model , which outperforms both traditional tri - training and recent alternatives in the case of sentiment analysis . For POS tagging , classic tri - training is superior , performing especially well on OOVs and low frequency tokens , which suggests it is less affected by error propagation . Overall we emphasize the importance of comparing neural approaches to strong baselines and reporting results across several runs . section : section : Acknowledgments We thank the anonymous reviewers for their valuable feedback . Sebastian is supported by Irish Research Council Grant Number EBPPG / 2014 / 30 and Science Foundation Ireland Grant Number SFI / 12 / RC / 2289 . Barbara is supported by NVIDIA corporation and thanks the Computing Center of the University of Groningen for HPC support . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["Multi-task_tri-training"]]], "Metric": [[["Average"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["Multi-task_tri-training"]]], "Metric": [[["Books"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["Multi-task_tri-training"]]], "Metric": [[["DVD"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["Multi-task_tri-training"]]], "Metric": [[["Electronics"]]], "Task": [[["Sentiment_Analysis"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Multi-Domain_Sentiment_Dataset"]]], "Method": [[["Multi-task_tri-training"]]], "Metric": [[["Kitchen"]]], "Task": [[["Sentiment_Analysis"]]]}]}
{"docid": "TST3-SREX-0042", "doctext": "document : PCL : Proposal Cluster Learning for Weakly Supervised Object Detection Weakly Supervised Object Detection ( WSOD ) , using only image - level annotations to train object detectors , is of growing importance in object recognition . In this paper , we propose a novel deep network for WSOD . Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning ( MIL ) , our strategy generates proposal clusters to learn refined instance classifiers by an iterative process . The proposals in the same cluster are spatially adjacent and associated with the same object . This prevents the network from concentrating too much on parts of objects instead of whole objects . We first show that instances can be assigned object or background labels directly based on proposal clusters for instance classifier refinement , and then show that treating each cluster as a small new bag yields fewer ambiguities than the directly assigning label method . The iterative instance classifier refinement is implemented online using multiple streams in convolutional neural networks , where the first is an MIL network and the others are for instance classifier refinement supervised by the preceding one . Experiments are conducted on the PASCAL VOC , ImageNet detection , and MS - COCO benchmarks for WSOD . Results show that our method outperforms the previous state of the art significantly . Object detection , weakly supervised learning , convolutional neural network , multiple instance learning , proposal cluster . section : Introduction Object detection is one of the most important problems in computer vision with many applications . Recently , due to the development of Convolutional Neural Network ( CNN ) and the availability of large scale datasets with detailed boundingbox - level annotations , there have been great leap forwards in object detection . However , it is very labor - intensive and time - consuming to collect detailed annotations , whereas acquiring images with only image - level annotations ( i.e . , image tags ) indicating whether an object class exists in an image or not is much easier . For example , we can use image search queries to search on the Internet ( e.g . , Google and Flickr ) to obtain a mass of images with such image - level annotations . This fact inspires us to explore methods for the Weakly Supervised Object Detection ( WSOD ) problem , i.e . , training object detectors with only image tag supervisions . Many previous methods follow the Multiple Instance Learning ( MIL ) pipeline for WSOD . They treat images as bags and proposals as instances ; then instance classifiers ( object detectors ) are trained under MIL constraints ( i.e . , a positive bag contains at least one positive instance and all instances in negative bags are negative ) . In addition , inspired by the great success of CNN , recent efforts often combine MIL and CNN to obtain better WSOD performance . Some researches have shown that treating CNNs pre - trained on large scale datasets as off - the - shelf proposal feature extractors can obtain much better performance than traditional hand - designed features . Moreover , many recent works have achieved even better results for WSOD by an MIL network using standard end - to - end training or a variant of end - to - end training . See Section [ reference ] for this variant of end - to - end and how it differs from the standard one . We use the same strategy of training a variant of end - to - end MIL network inspired by . Although some promising results have been obtained by MIL networks for WSOD , they do not perform as well as fully supervised ones . As shown in Fig . [ reference ] ( a ) , previous MIL networks integrate the MIL constraints into the network training by transferring the instance classification ( object detection ) problem to a bag classification ( image classification ) problem , where the final image scores are the aggregation of the proposal scores . However , there is a big gap between image classification and object detection . For classification , even parts of objects can contribute to correct results ( e.g . , the red boxes in Fig . [ reference ] ) , because important parts include many characteristics of the objects . Many proposals only cover parts of objects , and \u201c seeing \u201d proposals only of parts may be enough to roughly localize the objects . But this may not localize objects well enough considering the performance requirement of high Intersection - over - Union ( IoU ) between the resulting boxes and groundtruth boundingboxes : the top ranking proposals may only localize parts of objects instead of whole objects . Recall that for detection , the resulting boxes should not only give correct classification , but also localize objects and have enough overlap with groundtruth boundingboxes ( e.g . , the green boxes in Fig . [ reference ] ) . Before presenting our solution of the problem referred above , we first introduce the concept of proposal cluster . Object detection requires algorithms to generate multiple overlapping proposals closely surrounding objects to ensure high proposal recall ( e.g . , for each object , there are tens of proposals on average from Selective Search which have IoU 0.5 with the groundtruth boundingbox on the PASCAL VOC dataset ) . Object proposals in an image can be grouped into different spatial clusters . Except for one cluster for background proposals , each object cluster is associated with a single object and proposals in each cluster are spatially adjacent , as shown in Fig . [ reference ] . For fully supervised object detection ( i.e . , training object detectors using boundingbox - level annotations ) , proposal clusters can be generated by treating the groundtruth boundingboxes as cluster centers . Then object detectors are trained according to the proposal clusters ( e.g . , assigning all proposals the label of the corresponding object class for each cluster ) . This alleviates the problem that detectors may only focus on parts . But in the weakly supervised scenario , it is difficult to generate proposal clusters because groundtruth boundingboxes that can be used as cluster centers are not provided . To cope with this difficulty , we suggest to find proposal clusters as follows . First we generate proposal cluster centers from those proposals which have high classification scores during training , because these top ranking proposals can always detect at least parts of objects . That is , for each image , after obtaining proposal scores , we select some proposals with high scores as cluster centers , and then proposal clusters are generated based on spatial overlaps with the cluster centers . Then the problem reduces to how to select proposals as centers , because many high scoring proposals may correspond to the same object . The most straightforward way is to choose the proposal with the highest score for each positive object class ( i.e . , the object class exists in the image ) as the center . But such a method ignores the fact that there may exist more than one object with the same object category in natural images ( e.g . , the two motorbikes in Fig . [ reference ] ) . Therefore , we propose a graph - based method to find cluster centers . More specifically , we build a graph of top ranking proposals according to the spatial similarity for each positive object class . In the graph , two proposals are connected if they have enough spatial overlaps . Then we greedily and iteratively choose the proposals which have most connections with others to estimate the centers . Although a cluster center proposal may only capture an object partially , its adjacent proposals ( i.e . , other proposals in the cluster ) can cover the whole object , or at worst contain larger parts of the object . Based on these proposal clusters , we propose two methods to refine instance classifiers ( object detectors ) during training . We first propose to assign proposals object labels directly . That is , for each cluster , we assign its proposals the label of its corresponding object class , as in Fig . [ reference ] ( b ) . Compared with the conventional MIL network in Fig . [ reference ] ( a ) , this strategy forces network to \u201c see \u201d larger parts of objects by assigning object labels to proposals that cover larger parts of objects directly , which fills the gap between classification and detection to some extent . While effective , this strategy still has potential ambiguities , because assigning the same object label to proposals that cover different parts of objects simultaneously may confuse the network and will hurt the discriminative power of the detector . To address this problem , we propose to treat each proposal cluster as a small new bag to train refined instance classifiers , as in Fig . [ reference ] ( c ) . Most of the proposals in these new bags should have relatively high classification scores because the cluster centers covers at least parts of objects and proposals in the same cluster are spatially adjacent ( except for the background cluster ) . In the same time , not all proposals in the bags should have high classification scores . Thus compared with the directly assigning label strategy , this strategy is more flexible and can reduce the ambiguities to some extent . We name our method Proposal Cluster Learning ( PCL ) because it learns refined instance classifiers based on proposal clusters . To implement our idea effectively and efficiently , we further propose an online training approach . Our network has multiple output streams as in Fig . [ reference ] . The first stream is a basic MIL network which aggregates proposal scores into final image scores to train basic instance classifiers , and the other streams refine the instance classifiers iteratively . During the forward process of training , proposal classification scores are obtained and proposal clusters are generated consequently for each stream . Then based on these proposal clusters , supervisions are generated to compute losses for the next stream . According to the losses , these refined classifiers are trained during back - propagation . Except for the first stream that is supervised by image labels , the other streams are supervised by the image labels as well as outputs from their preceding streams . As our method forces the network to \u201c see \u201d larger parts of objects , the detector can discover the whole object instead of parts gradually by performing refinement multiple times ( i.e . , multiple output streams ) . But at the start of training , all classifiers are almost untrained , which will result in very noisy proposal clusters , and so the training will deviate from the correct solutions a lot . Thus we design a weighted loss further by associating different proposals with different weights in different training iterations . After that , all training procedures can thus be integrated into a single end - to - end network . This can improve the performance benefiting from our PCL - based classifier refinement procedure . It is also very computational efficient in both training and testing . In addition , performance can be improved by sharing proposal features among different output streams . We elaborately conduct many experiments on the challenging PASCAL VOC , ImageNet detection , and MS - COCO datasets to confirm the effectiveness of our method . Our method achieves mAP and CorLoc on VOC 2007 which is more than absolute improvement compared with previous best performed methods . This paper is an extended version of our previous work . In particular , we give more analyses of our method and enrich literatures of most recent related works , making the manuscript more complete . In addition , we make two methodological improvements : the first one is to generate proposal clusters using graphs of top ranking proposals instead of using the highest scoring proposal , and the second one is to treat each proposal cluster as a small new bag . In addition , we provide more discussions of experimental results , and show the effectiveness of our method on the challenging ImageNet detection and MS - COCO datasets . The rest of our paper is organized as follows . In Section [ reference ] , some related works are introduced . In Section [ reference ] , the details of our method are described . Elaborate experiments and analyses are conducted in Section [ reference ] . Finally , conclusions and future directions are presented in Section [ reference ] . section : Related work subsection : Multiple instance learning MIL , first proposed for drug activity prediction , is a classical weakly supervised learning problem . Many variants have been proposed for MIL . In MIL , a set of bags are given , and each bag is associated with a collection of instances . It is natural to treat WSOD as an MIL problem . Then the problem turns into finding instance classifiers only given bag labels . Our method also follows the MIL strategy and makes several improvements to WSOD . In particular , we learn refined instance classifiers based on proposal clusters according to both instance scores and spatial relations in an online manner . MIL has many applications to computer vision , such as image classification , weakly supervised semantic segmentation , object detection , object tracking , etc . The strategy of treating proposal clusters as bags was partly inspired by , where proposes to train MIL for patches around groundtruth locations and proposes to train MIL for patches around predicted object locations . However , they require groundtruth locations for either all training samples or the beginning time frames , whereas WSOD does not have such annotations . Therefore , it is much harder to generate proposal clusters only guided by image - level supervisions for WSOD . In addition , we incorporate the strategy of treating proposal clusters as bags into the network training whereas do not . Oquab et al . also train a CNN network using the max pooing MIL strategy to localize objects . But their methods can only coarsely localize objects regardless of their sizes and aspect ratios , whereas our method can detect objects more accurately . subsection : Weakly supervised object detection WSOD has attracted great interests nowadays because the amount of data with image - level annotations is much bigger and is growing much faster than that with boundingbox - level annotations . Many methods are emerging for the WSOD problem . For example , Chum and Zisserman first initialize object locations by discriminative visual words and then introduce an exemplar model to measure similarity between image pairs for updating locations . Deselaers et al . propose to initialize boxes by objectness and use a CRF - based model to iteratively localize objects . Pandey and Lazebnik train a DPM model under weak supervisions for WSOD . Shi et al . use Bayesian latent topic models to jointly model different object classes and background . Song et al . develop a technology to discover frequent discriminative configurations of visual patterns for robust WSOD . Cinbis et al . iteratively train a multi - fold MIL to avoid the detector being locked onto inaccurate local optima . Wang et al . relax the MIL constraints into a derivable loss function to train detectors more efficient . Recently , with the revolution of CNNs in computer vision , many works also try to combine the WSOD with CNNs . Early works treat CNN models pre - trained on ImageNet as off - the - shelf feature extractors . They extract CNN features for each candidate regions , and then train their own detectors on top of these features . These methods have shown that CNN descriptors can boost performance against traditional hand - designed features . More recent efforts tend to train end - to - end networks for WSOD . They integrate the MIL constraints into the network training by aggregating proposal classification scores into final image classification scores , and then image - level supervision can be directly added to image classification scores . For example , Tang et al . propose to use max pooling for aggregation . Bilen and Vedaldi develop a weighted sum pooing strategy . Building on , Kantorov et al . argue that context information can improve the performance . Diba et al . show that weakly supervised segmentation map can be used as guidance to filter proposals , and jointly train the weakly supervised segmentation network and WSOD end - to - end . Our method is built on these networks and any of them can be chosen as our basic network . Our strategy proposes to learn refined instance classifiers based on proposal clusters , and propose a novel online approach to train our network effectively and efficiently . Experimental results show our strategies can boost the results significantly . In addition to the weighted sum pooing , also proposes a \u201c spatial regulariser \u201d that forces features of the highest scoring proposal and its spatially adjacent proposals to be the same . Unlike this , we show that finding proposal cluster centers using graph and treating proposal clusters as bags are more effective . The contemporary work uses a graph model to generate seed proposals . Their network training has many steps : first , an MIL network is trained ; second , seed proposals are generated using the graph ; third , based on these seed proposals , a Fast R - CNN like detector is trained . Our method differs from in many aspects : first , we propose to generate proposal clusters for each training iteration and thus our network is trained end - to - end instead of step - by - step , which is more efficient and can benefit from sharing proposal features among different streams ; second , we propose to treat proposal clusters as bags for training better classifiers . As evidenced by experiments , our method obtains much better and more robust results . subsection : End - to - end and its variants In standard end - to - end training , the update requires optimizing losses w.r.t . all functions of network parameters . For example , the Fast R - CNN optimizes their classification loss and boundingbox regression loss w.r.t . proposal classification and feature extraction for fully supervised object detection . The MIL networks in optimize their MIL loss w.r.t . proposal classification and feature extraction for WSOD . Unlike the standard end - to - end training , there exists a variant of end - to - end training . The variant contains functions which depend on network parameters , but losses are not optimized w.r.t . all these functions . As we described in Section [ reference ] , the \u201c spatial regulariser \u201d in forces features of the highest scoring proposal and its spatially adjacent proposals to be the same . They use a function of network parameters to compute the highest scoring proposal , and do not optimize their losses w.r.t . this function . Diba et al . filter out background proposals using a function of network parameters and use these filtered proposals in their latter network computations . They also do not optimize their losses w.r.t . this function . Inspired by , we use this variant of end - to - end training . More precisely , we do not optimize our losses w.r.t . the generated supervisions for instance classifier refinement . subsection : Others There are many other important related works that do not focus on weakly supervised learning but should be discussed . Similar to other end - to - end MIL networks , our method is built on top of the Region of Interest ( RoI ) pooling layer or Spatial Pyramid Pooling ( SPP ) layer to share convolutional computations among different proposals for model acceleration . But both and require boundingbox - level annotations to train their detectors . The sharing proposal feature strategy in our network is similar to multi - task learning . Unlike the multi - task learning that each output stream has their own relatively independent external supervisions for different tasks , in our method , all streams have the same task and supervisions of later streams depend on the outputs from their preceding streams . section : Method The overall architecture of our method is shown in Fig . [ reference ] . Given an image , about object proposals from Selective Search or EdgeBox are generated . During the forward process of training , the image and these proposals are fed into some convolutional ( conv ) layers with an SPP layer to produce a fixed - size conv feature map per - proposal . After that , proposal feature maps are fed into two fully connected ( fc ) layers to produce proposal features . These features are branched into different streams : the first one is an MIL network to train basic instance classifiers and the others refine the classifiers iteratively . For each stream , proposal classification scores are obtained and proposal clusters are generated consequently . Then based on these proposal clusters , supervisions are generated to compute losses for the next stream . During the back - propagation process of training , the network losses are optimized to train proposal features and classifiers . As shown in the figure , supervisions of the - st refined classifier depend on the output from the basic classifier , and supervisions of - th refined classifier depend on outputs from - th refined classifier . In this section , we will introduce our method of learning refined instance classifiers based on proposal clusters in detail . subsection : Notations Before presenting our method , we first introduce some of the mostly used notations as follows . We have proposals with boxes for an given image and proposal features , where is the - th proposal box . The number of refined instance classifiers is ( i.e . , we refine instance classifier times ) , and thus there are streams . The number of object classes is . and are the parameters of the basic instance classifier and the - th refined instance classifier , respectively . and are the predicted score matrices of the basic instance classifier and the - th refined instance classifier , respectively , where indicates the object classes and background class . We use later for simplification , dropping the dependence on . is the predicted score of the - th proposal for class from the - th instance classifier . is the image label vector , where or indicates the image with or without object class . is the supervision of the - th instance classifier , where is the image label vector . is the loss function to train the - th instance classifier . We compute proposal cluster centers for the - th refinement . The - th cluster center consists of a proposal box , an object label ( indicates the - th object class ) , and a confidence score indicating the confidence that covers at least part of an object of class . We have proposal clusters according to ( for background and others for objects ) . For object clusters , the - th cluster consists of proposal boxes , an object label that is the same as the cluster center label , and a confidence score that is the same as the cluster center score , where indicates the confidence that corresponds to an object of class . Unlike object clusters , the background cluster consists of proposals and a label indicating the background . The - th proposal consists of a proposal box and a confidence score indicating the confidence that is the background . subsection : Basic MIL network It is necessary to generate proposal scores and clusters to supervise refined instance classifiers . More specifically , the first refined classifier requires basic instance classifiers to generate proposal scores and clusters . Therefore , we first introduce our basic MIL network as the basic instance classifier . Our overall network is independent of the specific MIL methods , and thus any method that can be trained end - to - end could be used . There are many possible choices . Here we choose the method by Bilen and Vedaldi which proposes a weighted sum pooling strategy to obtain the instance classifier , because of its effectiveness and implementation convenience . To make our paper self - contained , we briefly introduce as follows . Given an input image and its proposal boxes , a set of proposal features are first generated by the network . Then as shown in the \u201c Basic MIL network \u201d block of Fig . [ reference ] , there are two branches which process the proposal features to produce two matrices ( we use later for simplification , dropping the dependence on ) of an input image by two fc layers , where and denote the parameters of the fc layer for and the parameters of the fc layer for , respectively . Then the two matrices are passed through two softmax layer along different directions : and . Let us denote by . The proposal scores are generated by element - wise product . Finally , the image score of the - th class is obtained by the sum over all proposals : . A simple interpretation of the two branches framework is as follows . is the probability of the - th proposal belonging to class . is the normalized weight that indicates the contribution of the - th proposal to image being classified to class . So is obtained by weighted sum pooling and falls in the range of . Given the image label vector . We train the basic instance classifier by optimizing the multi - class cross entropy loss Eq . ( [ reference ] ) w.r.t . . subsection : The overall training strategy To refine instance classifiers iteratively , we add multiple output streams in our network where each stream corresponds to a refined classifier , as shown in Fig . [ reference ] . We integrate the basic MIL network and the classifier refinement into an end - to - end network to learn the refined classifier online . Unlike the basic instance classifier , for an input image the output score matrix of the - th refined classifier is a matrix and is obtained by passing the proposal features through a single fc layer ( with parameters ) as well as a softmax over - classes layer , i.e . , , as in the \u201c Instance classifier refinement \u201d blocks of Fig . [ reference ] . Notice that we use the same proposal features for all classifiers . We use later for simplification , dropping the dependence on . As we stated before , supervisions to train the - th instance classifier are generated based on proposal scores and image label . Thus we denote the supervisions by . Then we train our overall network by optimizing the loss Eq . ( [ reference ] ) w.r.t . . We do not optimize the loss w.r.t . , which means that the supervisions are only computed in the forward process and we do not compute their gradients to train our network . The loss for the - th refined instance classifier is defined in later Eq . ( [ reference ] ) / ( [ reference ] ) / ( [ reference ] ) which are loss functions with supervisions provided by . We will give details about how to get supervisions and loss functions in Section [ reference ] . [ t ] The overall training procedure ( one iteration ) [ 1 ] An image , its proposal boxes , and its image label vector ; refinement times . An updated network . Feed the image and into the network to produce proposal score matrices ( simplified as later ) . Compute loss by Eq . ( [ reference ] ) , see Section [ reference ] . Generate supervisions , see Section [ reference ] . Compute loss by Eq . ( [ reference ] ) / ( [ reference ] ) / ( [ reference ] ) , see Section [ reference ] . Optimize , i.e . , Eq . ( [ reference ] ) , w.r.t . ( not w.r.t . ) . During the forward process of each Stochastic Gradient Descent ( SGD ) training iteration , we obtain a set of proposal scores of an input image . Accordingly , we generate the supervisions for the iteration to compute the loss Eq . ( [ reference ] ) . During the back - propagation process of each SGD training iteration , we optimize the loss Eq . ( [ reference ] ) w.r.t . proposal features and classifiers . We summarize this procedure in Algorithm [ reference ] . Note that we do not use an alternating training strategy , i.e . , fixing supervisions and training a complete model , fixing the model and updating supervisions . The reasons are that : 1 ) it is very time - consuming because it requires training models multiple times ; 2 ) training different models in different refinement steps separately may harm the performance because it hinders the process to benefit from the shared proposal features ( i.e . , ) . subsection : Proposal cluster learning Here we will introduce our methods to learn refined instance classifiers based on proposal clusters ( i.e . , proposal cluster learning ) . Recall from Section [ reference ] that we have a set of proposals with boxes . For the - th refinement , our goal is to generate supervisions for the loss functions using the proposal scores and image label in each training iteration . We use later for simplification , dropping the dependence on . We do this in three steps . 1 ) We find proposal cluster centers which are proposals corresponding to different objects . 2 ) We group the remaining proposals into different clusters , where each cluster is associated with a cluster center or corresponds to the background . 3 ) We generate the supervisions for the loss functions , enabling us to train the refined instance classifiers . For the first step , we compute proposal cluster centers based on and . The - th cluster center is defined in Section [ reference ] . We propose two algorithms to find in Section [ reference ] ( 1 ) and ( 2 ) ( also Algorithm [ reference ] and Algorithm [ reference ] ) , where the first one was proposed in the conference version paper and the second one is proposed in this paper . For the second step , according to the proposal cluster centers , proposal clusters are generated ( for background and others for objects ) . The - th object cluster and the background cluster are defined in Section [ reference ] . We use the different notation for the background cluster because background proposals are scattered in each image , and thus it is hard to determine a cluster center and accordingly a cluster score . The method to generate was proposed in the conference version paper and is described in Section [ reference ] ( also Algorithm [ reference ] ) . For the third step , supervisions to train the - th refined instance classifier are generated based on the proposal clusters . We use two strategies where are either proposal - level labels indicating whether a proposal belongs to an object class , or cluster - level labels that treats each proposal cluster as a bag . Subsequently these are used to compute the loss functions . We propose two approaches to do this as described in Section [ reference ] ( 1 ) and ( 2 ) , where the first one was proposed in the conference version paper and the second one is proposed in this paper . subsubsection : Finding proposal cluster centers In the following we introduce two algorithms to find proposal cluster centers . [ t ] Finding proposal cluster centers using the highest scoring proposal [ 1 ] Proposal boxes ; image label vector ; proposal score matrix . Proposal cluster centers . Initialize . Choose the - th proposal by Eq . ( [ reference ] ) . . ( 1 ) Finding proposal cluster centers using the highest scoring proposal . A solution for finding proposal cluster centers is to choose the highest scoring proposal , as in our conference version paper . As in Algorithm [ reference ] , suppose an image has object class label ( i.e . , ) . For the - th refinement , we first select the - th proposal which has the highest score by Eq . ( [ reference ] ) , where is the predicted score of the - th proposal , as defined in Section [ reference ] . Then this proposal is chosen as the cluster center , i.e . , , where is the box of the - th proposal . is chosen as the confidence score that the - th proposal covers at least part of an object of class , because is the predicted score of the - th proposal been categorized to class . Therefore , the highest scoring proposal can probably cover at least part of the object and thus be chosen as the cluster center . There is a potential problem that one proposal may be chosen as the cluster centers for multiple object classes . To avoid this problem , if one proposal corresponds to the cluster centers for multiple object classes , this proposal would be chosen as the cluster center only by the class with the highest predicted score and we re - choose cluster centers for other classes . ( 2 ) Finding proposal cluster centers using graphs of top ranking proposals . As stated in Section [ reference ] , although we can find good proposal cluster centers using the highest scoring proposal , this ignores that in natural images there are often more than one object for each category . Therefore , we propose a new method to find cluster centers using graphs of top ranking proposals . [ t ] Finding proposal cluster centers using graphs of top ranking proposals [ 1 ] Proposal boxes ; image label vector ; proposal score matrix . Proposal cluster centers . Initialize . Select top ranking proposals with indexes . Build a graph using the top ranking proposals . Set . Set . . Remove the - th proposal box from , or . is empty . More specifically , suppose an image has object class label . We first select the top ranking proposals with indexes for the - th refinement . Then we build an undirected unweighted graph of these proposals based on spatial similarity , where vertexes correspond to these top ranking proposals , and edges correspond to the connections between the vertexes . is determined according to the spatial similarity between two vertexes ( i.e . , proposals ) as in Eq . ( [ reference ] ) , where is the IoU between the - th and - th proposals and is a threshold ( e.g . , ) . Therefore , two vertexes are connected if they are spatially adjacent . After that , we greedily generate some cluster centers for class using this graph . That is , we iteratively select vertexes which have most connections to be the cluster centers , as in Algorithm [ reference ] . The number of cluster centers ( i.e . , ) changes for each image in each training iteration because the top ranking proposals change . See Section [ reference ] for some typical values of . We use the same method as in Section [ reference ] ( 1 ) to avoid one proposal been chosen as the cluster centers for multiple object classes . The reasons for this strategy are as follows . First , according to our observation , the top ranking proposals can always cover at least parts of objects , thus generating centers from these proposals encourages the selected centers to meet our requirements . Second , because these proposals cover objects well , better proposals ( covering more parts of objects ) should have more spatially overlapped proposals ( i.e . , have more connections ) . Third , these centers are spatially far apart , and thus different centers can correspond to different objects . This method also has the attractive characteristic that it can generate adaptive number of proposals for each object class , which is desirable because in natural images there are arbitrary number of objects per - class . We set the score of the - th proposal cluster center by ( see the - th line in Algorithm [ reference ] ) because if the adjacent proposals of a center proposal have high confidence to cover at least part of an object ( i.e . , have high classification scores ) the center proposal should also have such high confidence . There is an important issue for the graph - based method : how to select the top ranking proposals ? A simple method is to select proposals whose scores exceed a threshold . But in our case , proposal scores change in each training iteration , and thus it is hard to determine a threshold . Instead , for each positive object class , we use the - means algorithm to divide proposal scores of an image into some clusters , and choose proposals in the cluster which has the highest score center to form the top ranking proposals . This method ensures that we can select the top ranking proposals although proposal scores change during training . Other choices are possible , but this method works well in experiments . [ t ] Generating proposal clusters [ 1 ] Proposal boxes ; proposal cluster centers . Proposal clusters . Initialize . Set of to of , . Initialize and set . Compute IoUs . Choose the most spatially adjacent center . . . subsubsection : Generating proposal clusters After the cluster centers are found , we generate the proposal clusters as in our conference version paper . Except for the cluster for background , good proposal clusters require that proposals in the same cluster are associated with the same object , and thus proposals in the same cluster should be spatially adjacent . Specially , given the - th proposal , we compute a set of IoUs , where is the IoU between the - th proposal and the box of the - th cluster center . Then we assign the - th proposal to the - th object cluster if is larger than a threshold ( e.g . , ) and to the background cluster otherwise , where is the index of the most spatially adjacent cluster center as Eq . ( [ reference ] ) . The overall procedures to generate proposal clusters are summarized in Algorithm [ reference ] . We set the proposal scores for the background cluster to the scores of their most spatially adjacent centers as the 10 - the line in Algorithm [ reference ] , because if the cluster center has confidence that it covers an object , the proposal far away from should have confidence to be background . subsubsection : Learning refined instance classifiers To get supervisions and loss functions to learn the - th refined instance classifier , we design two approaches as follows . ( 1 ) Assigning proposals object labels . The most straightforward way to refine classifiers is to directly assign object labels to all proposals in object clusters because these proposals potentially correspond to whole objects , as in our conference version paper . As the cluster centers covers at least parts of objects , their adjacent proposals ( i.e . , proposals in the cluster ) can contain larger parts of objects . Accordingly , we can assign the cluster label to all proposals in the - th cluster . More specifically , the supervisions are proposal - level labels , i.e . , . is the label vector of the - th proposal for the - th refinement , where and if the - th proposal belongs to the - th clusters . Consequently , we use the standard softmax loss function to train the refined classifiers as in Eq . ( [ reference ] ) , where is the predicted score of the - th proposal as defined in Section [ reference ] . Through iterative instance classifier refinement ( i.e . , multiple times of refinement as increase ) , the detector detects larger parts of objects gradually by forcing the network to \u201c see \u201d larger parts of objects . Actually , the so learnt supervisions are very noisy , especially in the beginning of training . This results in unstable solutions . To solve this problem , we change the loss in Eq . ( [ reference ] ) to a weighted version , as in Eq . ( [ reference ] ) . is the loss weight that is the same as the cluster confidence score for object clusters or proposal confidence score for the background cluster if the - th proposal belongs to the - th cluster . From Algorithm [ reference ] , we can observe that is the same as the cluster center confidence score . The reasons for this strategy are as follows . In the beginning of training , although we can not obtain good proposal clusters , each is small , hence each is small and the loss is also small . As a consequence , the performance of the network will not decrease a lot . During the training , the top ranking proposals will cover objects well , and thus we can generate good proposal clusters . Then we can train satisfactory instance classifiers . ( 2 ) Treating clusters as bags . As we stressed before , although directly assigning proposals object labels can boost the results , it may confuse the network because we simultaneously assign the same label to different parts of objects . Focusing on this , we further propose to treat each proposal cluster as a small new bag and use the cluster label as the bag label . Thus the supervisions for the - th refinement are bag - level ( cluster - level ) labels , i.e . , . is the label of the - th bag , i.e . , the label of the - th proposal cluster , as defined in Section [ reference ] . Specially , for object clusters , we choose average MIL pooling , because these proposals should cover at least parts of objects and thus should have relatively high prediction scores . For the background cluster , we assign the background label to all proposals in the cluster according to the MIL constraints ( all instances in negative bags are negative ) . Then the loss function for refinement will be Eq . ( [ reference ] ) . , , and are the cluster confidence score of the - th object cluster , the number of proposals in the - th cluster , and the predicted score of the - th proposal , respectively , as defined in Section [ reference ] . and indicate that the - th proposal belongs to the - th object cluster and the background cluster respectively . Compared with the directly assigning label approach , this method tolerates some proposals to have low scores , which can reduce the ambiguities to some extent . subsection : Testing During testing , the proposal scores of refined instance classifiers are used as the final detection scores , as the blue arrows in Fig . [ reference ] . Here the mean output of all refined classifiers is chosen . The Non - Maxima Suppression ( NMS ) is used to filter out redundant detections . section : Experiments In this section , we first introduce our experimental setup including datasets , evaluation metrics , and implementation details . Then we conduct elaborate experiments to discuss the influence of different settings . Next , we compare our results with others to show the effectiveness of our method . After that , we show some qualitative results for further analyses . Finally , we give some runtime analyses of our method . Codes for reproducing our results are available at . subsection : Experimental setup subsubsection : Datasets and evaluation metrics We evaluate our method on four challenging datasets : the PASCAL VOC 2007 and 2012 datasets , the ImageNet detection dataset , and the MS - COCO dataset . Only image - level annotations are used to train our models . The PASCAL VOC 2007 and 2012 datasets have and images respectively for object classes . These two datasets are divided into train , val , and test sets . Here we choose the trainval set ( images for 2007 and images for 2012 ) to train our network . For testing , there are two metrics for evaluation : mAP and CorLoc . Following the standard PASCAL VOC protocol , Average Precision ( AP ) and the mean of AP ( mAP ) is the evaluation metric to test our model on the testing set . Correct Localization ( CorLoc ) is to test our model on the training set measuring the localization accuracy . All these two metrics are based on the PASCAL criterion , i.e . , IoU 0.5 between groundtruth boundingboxes and predicted boxes . The ImageNet detection dataset has hundreds of thousands of images with object classes . It is also divided into train , val , and test sets . Following , we split the val set into val1 and val2 , and randomly choose at most K images in the train set for each object class ( we call it train ) . We train our model on the mixture of train and val1 sets , and test it on the val2 set , which will lead to images for training and images for testing . We also use the mAP for evaluation on the ImageNet . The MS - COCO dataset has object classes and is divided into train , val , and test sets . Since the groundtruths on the test set are not released , we train our model on the MS - COCO 2014 train set ( about K images ) and test it on the val set ( about K images ) . For evaluation , we use two metrics mAP@0.5 and mAP@ [ .5 , .95 ] which are the standard PASCAL criterion ( i.e . , IoU 0.5 ) and the standard MS - COCO criterion ( i.e . , computing the average of mAP for IoU [ 0.5 : 0.05 : 0.95 ] ) respectively . subsubsection : Implementation details Our method is built on two pre - trained ImageNet networks VGG M and VGG16 , each of which has some conv layers with max - pooling layers and three fc layers . We replace the last max - pooling layer by the SPP layer , and the last fc layer as well as the softmax loss layer by the layers described in Section [ reference ] . To increase the feature map size from the last conv layer , we replace the penultimate max - pooling layer and its subsequent conv layers by the dilated conv layers . The newly added layers are initialized using Gaussian distributions with - mean and standard deviations . Biases are initialized to . During training , the mini - batch size for SGD is set to be , , and for PASCAL VOC , ImageNet , and MS - COCO , respectively . The learning rate is set to for the first K , K , K , and K iterations for the PASCAL VOC 2007 , PASCAL VOC 2012 , ImageNet , and MS - COCO datasets , respectively . Then we decrease the learning rate to in the following K , K , K , and K iterations for the PASCAL VOC 2007 , PASCAL VOC 2012 , ImageNet , and MS - COCO datasets , respectively . The momentum and weight decay are set to be and respectively . Selective Search , EdgeBox , and MCG are adopted to generate about proposals per - image for the PASCAL VOC , ImageNet , and MS - COCO datasets , respectively . For data augmentation , we use five image scales ( resize the shortest side to one of these scales ) with horizontal flips for both training and testing . If not specified , the instance classifiers are refined three times , i.e . , in Section [ reference ] , so there are four output streams ; the IoU threshold in Section [ reference ] ( 2 ) ( also Eq . ( [ reference ] ) ) is set to ; the number of - means clusters in the last paragraph of Section [ reference ] ( 2 ) is set to ; in Section [ reference ] ( also the - th line of Algorithm [ reference ] ) is set to . Similar to other works , we train a supervised object detector through choosing the top - scoring proposals given by our method as pseudo groundtruths to further improve our results . Here we train a Fast R - CNN ( FRCNN ) using the VGG16 model and the same five image scales ( horizontal flips only in training ) . The same proposals are chosen to train and test the FRCNN . NMS ( with IoU threshold ) is applied to compute AP . Our experiments are implemented based on the Caffe deep learning framework , using Python and C ++ . The - means algorithm to produce top ranking proposals is implemented by scikit - learn . All of our experiments are running on an NVIDIA GTX TitanX Pascal GPU and Intel ( R ) i7 - 6850 K CPU ( 3.60GHz ) . subsection : Discussions We first conduct some experiments to discuss the influence of different components of our method ( including instance classifier refinement , different proposal generation methods , different refinement strategies , and weighted loss ) and different parameter settings ( including the IoU threshold defined in Section [ reference ] ( 2 ) , the number of - means clusters described in Section [ reference ] ( 2 ) , the IoU threshold defined in Section [ reference ] , and multi - scale training and testing . ) We also discuss the number of proposal cluster centers . Without loss of generality , we only perform experiments on the VOC 2007 dataset and use the VGG M model . subsubsection : The influence of instance classifier refinement As the five curves in Fig . [ reference ] show , we observe that compared with the basic MIL network , for both refinement methods , even refining instance classifier a single time boosts the performance a lot . This confirms the necessity of refinement . If we refine the classifier multiple times , the results are improved further . But when refinement is implemented too many times , the performance gets saturated ( there are no obvious improvements from times to times ) . This is because the network tends to converge so that the supervision of the - th time is similar to the - rd time . In the rest of this paper we only refine classifiers times . Notice that in Fig . [ reference ] , the \u201c 0 time \u201d is similar to the WSDDN using Selective Search as proposals . subsubsection : The influence of different proposal cluster generation methods We discuss the influence of different proposal cluster generation methods . As shown in the Fig . [ reference ] ( green and purple solid curves for the highest scoring proposal based method , blue and red solid curves for the graph - based method ) , for all refinement times , the graph - based method obtains better performance , because it can generate better cluster centers . Thus we choose the graph - based method in the rest of our paper . subsubsection : The influence of different refinement strategies We then show the influence of different refinement strategies . The directly assigning label method is replaced by treating clusters as bags ( blue and green solid curves ) . From Fig . [ reference ] , it is obvious that the results by treating clusters as bags are better . In addition , compared with the alternating training strategy ( blue dashed curve ) , our online training boosts the performance consistently and significantly , which confirms the necessity of sharing proposal features . Online training also reduces the training time a lot , because it only requires training a single model instead of training models for times refinement in the alternating strategy . In the rest of our paper , we only report results by the \u201c PCL - OB - G \u201d method in Fig . [ reference ] because it achieves the best performance . subsubsection : The influence of weighted loss We also study the influence of our weighted loss in Eq . ( [ reference ] ) . Note that Eq . ( [ reference ] ) can be easily changed to the unweighted version by simply setting and to be . Here we train a network using the unweighted loss . The results of the unweighted loss are mAP and CorLoc . We see that if we use the unweighted loss , the improvement from refinement is very scant and the performance is even worse than the alternating strategy . Using the weighted loss achieves much better performance ( mAP and CorLoc ) , which confirms our theory in Section [ reference ] . subsubsection : The influence of the IoU threshold Here we discuss the influence of the IoU threshold defined in Section [ reference ] ( 2 ) and Eq . ( [ reference ] ) . From Fig . [ reference ] , we see that setting to obtains the best performance . Therefore , we set to for the other experiments . subsubsection : The influence of the number of - means clusters In previous experiments we set the number of - means clusters described in the last paragraph of Section [ reference ] ( 2 ) to be . Here we set it to other numbers to explore its influence . The results from other numbers of - means clusters are mAP and CorLoc for clusters , and mAP and CorLoc for clusters , which are a little worse than the results from cluster . Therefore , we set the number of - means clusters to for the other experiments . subsubsection : The influence of the IoU threshold We also analyse the influence of defined in Section [ reference ] and the - th line of Algorithm [ reference ] . As shown in Fig . [ reference ] , outperforms other choices . Therefore , we set to for the other experiments . subsubsection : The influence of multi - scale training and testing Previously our experiments are conducted based on five image scales for training and testing . Here we show the influence of this multi - scale setting . We train and test our method using a single image scale as the default scale setting of FRCNN . The single - scale results are mAP and CorLoc which are much worse than our multi - scale results ( mAP and CorLoc ) . Therefore , we use five image scales as many WSOD networks . subsubsection : The number of proposal cluster centers As we stated in Section [ reference ] ( 2 ) , the number of proposal cluster centers ( i.e . , ) changes for each image in each training iteration . Here we give some typical values of . In the beginning of training , the proposal scores are very noisy and thus the selected top ranking proposals to form graphs are scattered in images , which results in dozens of proposal cluster centers for each image . After some ( about 3 K ) training iterations , the proposal scores are more reliable and our method finds 1 3 proposal cluster centers for each positive object class . To make the training more stable in the beginning , for each positive object class we empirically select at most five proposal cluster centers which have higher scores , and the number of selected proposal cluster centers does not influence the performance much . subsection : Comparison with other methods Here we compare our best performed strategy PCL - OB - G , i.e . , using graph - based method and treating clusters as bags to train the network online , with other methods . We first report our results for each class on VOC 2007 and 2012 in Table [ reference ] , Table [ reference ] , Table [ reference ] , and Table [ reference ] . It is obvious that our method outperforms other methods using single model VGG M or VGG16 ( PCL - OB - G + VGG M and PCL - OB - G + VGG16 in tables . ) Our single model results even better than others by combining multiple different models ( e.g . , ensemble of models ) . Specially , our method obtains much better results compared with other two methods also using the same basic MIL network . Importantly , also equips the weighted sum pooling with objectness measure of EdgeBox and the spatial regulariser , and adds context information into the network , both of which are more complicated than our basic MIL network . We believe that our performance can be improved by choosing better basic MIL networks , like the complete network in and using context information . As reimplementing their method completely is non - trivial , here we only choose the simplest architecture in . Even in this simplified case , our method achieves very promising results . Our results can also be improved by combing multiple models . As shown in the tables , there are little improvements from the ensemble of the VGG M and VGG16 models ( PCL - OB - G - Ens . in tables ) . Here we do the ensemble by summing up the scores produced by the two models . Also , as mentioned in Section [ reference ] , similar to , we train a FRCNN detector using top - scoring proposals produced by PCL - OB - G - Ens . as groundtruths ( PCL - OB - G - Ens. + FRCNN in tables ) . As we can see , the performance is improved further . We then show results of our method on the large scale ImageNet detection dataset in Table [ reference ] . We observe similar phenomenon that our method outperforms other methods by a large margin . We finally report results of our method on MS - COCO in Table [ reference ] . Our method obtains better performance than the recent work . In particular , Ge et al . use the method proposed in our conference version paper as a basic component . We can expect to obtain better detection performance through replacing our conference version method in by our newly proposed method here , which we would like to explore in the future . subsection : Qualitative results We first show some proposal clusters generated by our method in Fig . [ reference ] . As we can see , the cluster centers contain at least parts of objects and are able to cover adaptive number of objects for each class . We then show qualitative comparisons among the WSDDN , the WSDDN + context , and our PCL method , both of which use the same basic MIL network . As shown in Fig . [ reference ] , we can observe that for classes such as bike , car , cat , etc . , our method tends to provide more accurate detections , whereas other two methods sometimes fails by producing boxes that are overlarge or only contain parts of objects ( the first four rows in Fig . [ reference ] ) . But for some classes such as person , our method sometimes fails by only detecting parts of objects such as the head of person ( the fifth row in Fig . [ reference ] ) . Exploiting context information sometimes help the detection ( as in WSDDN + context ) , we believe our method can be further improved by incorporating context information into our framework . All these three methods ( actually almost all weakly supervised object detection methods ) suffers from two problems : producing boxes that not only contain the target object but also include their adjacent similar objects , or only detecting parts of object for objects with deformation ( the last row in Fig . [ reference ] ) . We finally visualize some success and failure detection results on VOC 2007 trainval by PCL - Ens. + FRCNN , as in Fig . [ reference ] . We observe similar phenomena as in Fig . [ reference ] . Our method is robust to the size and aspect of objects , especially for rigid objects . The main failures for these rigid objects are always due to overlarge boxes that not only contain objects , but also include adjacent similar objects . For non - rigid objects like \u201c cat \u201d , \u201c dog \u201d , and \u201c person \u201d , they often have great deformations , but their parts ( e.g . , head of person ) have much less deformation , so our detector is still inclined to find these parts . An ideal solution is yet wanted because there is still room for improvement . subsection : Runtime The runtime comparisons between our method and our basic MIL network are shown in Table [ reference ] , where the runtime of proposal generation is not considered . As we can see , although our method has more components than our basic MIL network , our method takes almost the same testing time as it . This is because all our output streams share the same proposal feature computations . The small extra training computations of our method mainly come from the procedures to find proposal cluster centers and generate proposal clusters . Although with small extra training computations , our method obtains much better detection results than the basic MIL network . section : Conclusion In this paper , we propose to generate proposal clusters to learn refined instance classifiers for weakly supervised object detection . We propose two strategies for proposal cluster generation and classifier refinement , both of which can boost the performance significantly . The classifier refinement is implemented by multiple output streams corresponding to some instance classifiers in multiple instance learning networks . An online training algorithm is introduced to train the proposed network end - to - end for effectiveness and efficiency . Experiments show substantial and consistent improvements by our method . We observe that the most common failure cases of our algorithm are connected with the deformation of non - rigid objects . In the future , we will concentrate on this problem . In addition , we believe our learning algorithm has the potential to be applied in other weakly supervised visual learning tasks such as weakly supervised semantic segmentation . We will also explore how to apply our method to these related applications . section : Acknowledgements This work was supported by NSFC ( No . 61733007 , No . 61572207 , No . 61876212 , No . 61672336 , No . 61573160 ) , ONR with grant N00014 - 15 - 1 - 2356 , Hubei Scientific and Technical Innovation Key Project , and the Program for HUST Academic Frontier Youth Team . The corresponding author of this paper is Xinggang Wang . bibliography : References [ ] Peng Tang received the B.S. degree in Electronics and Information Engineering from Huazhong University of Science and Technology ( HUST ) in 2014 . He is currently pursuing the Ph.D. degree in the School of Electronic Information and Communications at HUST , and visiting the Department of Computer Science at Johns Hopkins University . He was an intern at Microsoft Research Asia in 2017 . His research interests include image classification and object detection in images / videos . [ ] Xinggang Wang is an assistant professor of School of Electronics Information and Communications of Huazhong University of Science and Technology ( HUST ) . He received his Bachelor degree in communication and information system and Ph.D. degree in computer vision both from HUST . From May 2010 to July 2011 , he was with the Department of Computer and Information Science , Temple University , Philadelphia , PA . , as a visiting scholar . From February 2013 to September 2013 , he was with the University of California , Los Angeles ( UCLA ) , as a visiting graduate researcher . He is a reviewer of IEEE Trans on PAMI , IEEE Trans on Image Processing , IEEE Trans . on Cybernetics , Pattern Recognition , Computer Vision and Image Understanding , Neurocomputing , NIPS , ICML , CVPR , ICCV and ECCV etc . His research interests include computer vision and machine learning , especially object recognition . [ ] Song Bai received the B.S. and Ph.D. degree in Electronics and Information Engineering from Huazhong University of Science and Technology ( HUST ) , Wuhan , China in 2013 and 2018 , respectively . He was with University of Texas at San Antonio ( UTSA ) and Johns Hopkins University ( JHU ) as a research scholar . His research interests include image retrieval and classification , 3D shape recognition , person re - identification , semantic segmentation and deep learning . More information can be found in his homepage : . [ ] Wei Shen received his B.S. and Ph.D. degree both in Electronics and Information Engineering from the Huazhong University of Science and Technology ( HUST ) , Wuhan , China , in 2007 and in 2012 . From April 2011 to November 2011 , he worked in Microsoft Research Asia as an intern . In 2012 , he joined School of Communication and Information Engineering , Shanghai University as an Assistant Professor . From 2017 , he became an Associate Professor . He is currently visiting Department of Computer Science , Johns Hopkins University . His current research interests include random forests , deep learning , object detection and segmentation . [ ] Xiang Bai received his B.S. , M.S. , and Ph.D. degrees from the Huazhong University of Science and Technology ( HUST ) , Wuhan , China , in 2003 , 2005 , and 2009 , respectively , all in electronics and information engineering . He is currently a Professor with the School of Electronic Information and Communications , HUST . He is also the Vice - director of the National Center of Anti - Counterfeiting Technology , HUST . His research interests include object recognition , shape analysis , scene text recognition and intelligent systems . He serves as an associate editor for Pattern Recognition , Pattern Recognition Letters , Neurocomputing and Frontiers of Computer Science . [ ] Wenyu Liu received the B.S. degree in Computer Science from Tsinghua University , Beijing , China , in 1986 , and the M.S. and Ph.D. degrees , both in Electronics and Information Engineering , from Huazhong University of Science and Technology ( HUST ) , Wuhan , China , in 1991 and 2001 , respectively . He is now a professor and associate dean of the School of Electronic Information and Communications , HUST . His current research areas include computer vision , multimedia , and machine learning . He is a senior member of IEEE . [ ] Alan Yuille received the B.A. degree in mathematics from the University of Cambridge in 1976 , and the Ph.D. degree in theoretical physics from Cambridge in 1980 . He then held a post - doctoral position with the Physics Department , University of Texas , Austin , and the Institute for Theoretical Physics , Santa Barbara . He then became a Research Scientists with the Artificial Intelligence Laboratory , MIT , from 1982 to 1986 , and followed this with a faculty position in the division of applied sciences , Harvard , from 1986 to 1995 , rising to the position of an associate professor . From 1995 to 2002 , he was a Senior Scientist with the Smith - Kettlewell Eye Research Institute in San Francisco . From 2002 to 2016 , he was a Full Professor with the Department of Statistics , UCLA , with joint appointments in Psychology , Computer Science , and Psychiatry . In 2016 , he became a Bloomberg Distinguished Professor of cognitive science and computer science with Johns Hopkins University . He received the Marr Prize and the Helmholtz Prize .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["ImageNet"]]], "Method": [[["PCL-OB-G-Ens___FRCNN"]]], "Metric": [[["MAP"]]], "Task": [[["Weakly_Supervised_Object_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["PCL-OB-G-Ens___FRCNN"]]], "Metric": [[["MAP"]]], "Task": [[["Weakly_Supervised_Object_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2012"]]], "Method": [[["PCL-OB-G-Ens___FRCNN"]]], "Metric": [[["MAP"]]], "Task": [[["Weakly_Supervised_Object_Detection"]]]}]}
{"docid": "TST3-SREX-0043", "doctext": "document : Aggregate Channel Features for Multi - view Face Detection Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones . While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n\u2019t meet the demand for effectively and efficiently handling faces with large appearance variance in the wild . To solve this bottleneck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form . We adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multi - scale version of features with better performance . To deal with poses of faces in the wild , we propose a multi - view detection approach featuring score re - ranking and detection adjustment . Following the learning pipelines in Viola - Jones framework , the multi - view face detector using aggregate channel features shows competitive performance against state - of - the - art algorithms on AFW and FDDB testsets , while runs at 42 FPS on VGA images . section : Introduction Human face detection have long been one of the most fundamental problems in computer vision and human - computer interaction . In the past decade , the most influential work should be the face detection framework proposed by Viola and Jones . The Viola - Jones ( abbreviated as VJ below ) framework uses rectangular Haar - like features and learns the hypothesis using Adaboost algorithm . Combined with the attentional cascade structure , the VJ detector achieved real - time face detection at that time . Despite the great success of the VJ detector , the performance is still far from satisfactory due to the large appearance variance of faces in unconstrained settings . To handle faces in the wild , many subsequences of VJ framework merged . These methods mainly get the performance gains in two aspects , more complicated features and ( or ) more powerful learning algorithms . As the combination of boosting and cascade has been proven to be quite effective in face detection , the bottleneck lies in the feature representation since complicated features adopted in the above literatures bring about limited performance gains at the cost of large computation cost . Lately in another domain of pedestrian detection , a family of channel features has achieved record performances . Channel features compute registered maps of the original images like gradients and histograms of oriented gradients and then extract features on these extended channels . The classifier learning process follows the VJ framework pipeline . In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels . Channel extension offers rich representation capacity , while simple feature form guarantees fast computation . With these two superiorities , the aggregate channel features break through the bottleneck in VJ framework and have the potential to make great advance in face detection . As we mainly concentrate our efforts to the feature representation rather than learning algorithms in this paper , we not only just adopt the aggregate channel features in face detection , but also try to explore the full potential of this novel representation . To do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , subsampling method , feature scale and so on , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners . Through the deep exploration , we find that : 1 ) multi - scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi - view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( Figure [ reference ] ) . Although multi - view detection could effectively deal with diverse poses , additional issues come up as how to merge detections output by separately trained subview detectors , and how to deal with the offsets of location and scale between output detections and ground - truth . We solve these problems by carefully designed post - processing including score re - ranking , detection merging and bounding box adjustment . The detailed experimental exploration of aggregate channel features , along with our improvements on multi - view detection , leads to large performance gain in face detection in the wild . On two challenging face databases , AFW and FDDB , the proposed multi - view face detector shows competitive performance against state - of - the - art detectors in both detection accuracy and speed . The remaining parts of this paper are organized as follows . Section 2 revisits related work in face detection . Section 3 describes how we build the face detector using aggregate channel features . Section 4 addresses problems concerning multi - view face detection . Experimental results on AFW and FDDB are shown in section 5 and we conclude the paper in section 6 . section : Related work Face detection has drawn much attention since the early time of computer vision . Although many solutions had been put forward , it was not until Viola and Jones proposed their milestone work that face detection saw surprising progress in the past decades . The VJ face detector features in three aspects : fast feature computation via integral image representation , classifier learning using Adaboost , and the attentional cascade structure . One main drawback of the VJ framework is that the features have limited representation capacity , while the feature pool size is quite large to compensate for that . Typically , in a detection window , the number of Haar - like features is 160 , 000 . To address the problem , efforts are made in two directions . Some focus on more complicated features like HoG , SURF . Some aim to speed up the feature selection in a heuristic way . However , the problem has n\u2019t been solved perfectly . In this paper , we mainly focus on the feature representation part and make a deep exploration into it , which is complementary to existing work on the learning algorithm and classifier structure in the VJ framework . Recently channel features have been proposed and shown record performance in pedestrian detection . Due to the channel extension to diverse types like gradients and local histograms , the features show richer representation capacity for classification . However , the features are extracted as rectangular sums at various locations and scales which we believe leads to a redundant feature pool . During preparation of this paper , Mathias independently discover the effectiveness of integral channel features in face detection domain . In this paper , we adopt a novel variant of channel features called aggregate channel features , which extract features directly as pixel values in extended channels without computing rectangular sums at various locations and scales . The feature has powerful representation capacity and the feature pool size is only several thousands . Through careful design in section 3 and implementation of multi - view detection in section 4 , the aggregate channel features based detector achieves state - of - the - art performance on challenging databases . section : Proposed face detector In this section , we make a full exploration of the aggregate channel features in the context of face detection . We first give a brief introduction of the feature itself , including its computation , properties and advantages over traditional Haar - like features used in VJ framework . Then the detailed experimental investigation is described in two parts , feature design and training design . Before that , some guidelines concerning how we conduct the investigation are demonstrated . Each design part is divided into several separate experiments ended with a summary explaining the specific parameters used in our proposed face detector . Note that each experiment focuses on only one parameter and the others remain constant . Through the well - designed experiments , the proposed face detector based on aggregate channel features is built step by step . Issues concerning the implementation of multi - view face detection which further improves the performance are discussed in the next section . subsection : Feature description Channel extension : The basic structure of the aggregate channel features is channel . The application of channel has a long history since digital images were invented . The most common type of channel should be the color channels of the image , with Gray - scale and RGB being typical ones . Besides color channels , many different channel types have been invented to encode different types of information for more difficult problems . Generally , channels can be defined as a registered map of the original image , whose pixels are computed from corresponding patches of original pixels . Different channels can be computed with linear or non - linear transformation of the original image . To allow for sliding window detection , the transformations are constrained to be translationally invariant . Feature computation : Based on the definition of channels , the computation of aggregate channel features is quite simple . As shown in Figure [ reference ] , given a color image , all defined channels are computed and subsampled by a pre - set factor . The aggregate pixels in all subsampled channels are then vectorized into a pixel look - up table . Note that an optional smoothing procedure can be done on each channel with a binomial filter both before computation and after subsampling . Classifier learning : The learning process is quite simple . Two changes are made compared with VJ framework . First is that weak classifier is changed from decision stump to depth - 2 decision tree . The more complex weak classifier shows stronger ability in seeking the discriminant intra and inter channel correlations for classification . Second difference is that soft - cascade structure is used . Unlike the attentional cascade structure in VJ framework which has several cascade stages , a single - stage classifier is trained on the whole training data and a threshold is then set after each weak classifier picked by Adaboost . These two changes lead to more efficient training and detection . Overall superiority : Compared with traditional Haar - like features used in VJ framework , aggregate channel features have the following differences and advantages : 1 ) The image channels are extended to more types in order to encode diverse information like color , gradients , local histograms and so on , therefore possess richer representation capacity . 2 ) Features are extracted directly as pixel values on downsampled channels rather than computing rectangular sums with various locations and scales using integral images , leading to a faster feature computation and smaller feature pool size for boosting learning . With the help of cascade structure , detection speed is accelerated more . 3 ) Due to its structure consistence with the overall image , when coupled with boosting method , the boosted classifier naturally encodes structured pattern information from large training data ( see Figure [ reference ] for an illustration ) , which gives more accurate localization of faces in the image . subsection : Investigation guidelines All investigations are trained on the AFLW face database and tested on the Annotated Faces in the Wild ( AFW ) testset . To make it clear , there are in total positive samples and negative samples selected from AFLW which are kept constant in all investigations . Testset contains natural images with faces that vary a lot in pose , appearance and illumination . To alleviate the ground - truth offset caused by different annotation styles ( Figure [ reference ] ) in training and testing set and make the evaluation more comparable , a lower Jaccard index with threshold is adopted in comparative evaluation . Practically the lower threshold wo n\u2019t cause errors being mistakenly corrected . Note that in final evaluation of the proposed face detector ( section 5 ) , the AFW testset , together with another face benchmark FDDB database , are used as testbed and the evaluation metric follows the database protocol . subsection : Feature design To fully exploit the power of aggregate channel features in face detection domain , a deep investigation into the design of the feature is done mainly on channel types , window size , subsampling method and feature scale . Results of comparative experiments are shown in Figure [ reference ] . Channel types : Three types of channels are used , which are color channel ( Gray - scale , RGB , HSV and LUV ) , gradient magnitude , and gradient histograms . The computation of the latter two channel types could be seen as a generalized version of HoG features . Specifically , gradient magnitude is the biggest response on all three color channels , and oriented gradient histograms follow the idea of HoG in that : 1 ) rectangular cell size in HoG equals the subsampling factor in aggregated channel features ; 2 ) each orientation bin results in one feature channel ( 6 orientation bins are used in this paper ) . Figure [ reference ] ( a ) ~ ( c ) show how much each of these three types alone contributes to the performance of face detection . It can be seen that the gradient histograms contribute most to the performance among all three channel types . Figure [ reference ] ( d ) shows the performances of combinations of these three types computed on different color channels . Detection window size : Detection window size is the scale to which we resize all face and non - face samples and then train our detector . Larger window size includes more pixels in feature pool and thus may improve the face detection performance . On the other hand , too large window will miss some small faces and diminish the detection efficiency . Figure [ reference ] ( e ) shows comparison of window size ranging from to with a stride of pixels . Subsampling : The factor for subsampling can be regarded as the perceptive scale for that it controls the scale at which the aggregation is done . Changing the factor from large to small leads to the feature representation shifting from coarse to fine and the feature pool size getting bigger . Experiments on different subsampling factors are shown in Figure [ reference ] ( f ) . In original aggregate channel features , the way to do subsampling is average pooling . Following the idea in Convolutional Neural Networks , another two ways of subsampling , max pooling and stochastic pooling are tested in Figure [ reference ] ( g ) . Smoothing : As described in feature description , both pre and post smoothing is done in default setting of aggregate channel features . A binomial filter with a radius of is used for smoothing . The smoothing procedure also has a great influence on the scale of the feature representation . Concretely , pre - smoothing determines how far the local neighborhood is in which local correlations are encoded before channel computation , while post - smoothing determines the neighborhood size in which the computed channel features are integrated with each other . In , the former corresponds to the \u2018 local scale \u2019 of the feature , while the latter represents the \u2018 integration scale \u2019 . We vary the filter radius used in pre and post smoothing and find that both using a radius of gets the best results . Figure [ reference ] ( h ) ~ ( i ) present the comparative results . Multi - scale : In aggregate channel features , although hidden information at different scale could be extracted at a cost of more weak classifiers , it would be better to make the integrated channel features multi - scaled and thus make themselves more discriminant . Therefore the same or better classification performance can be achieved with fewer weak classifiers . In this part , we implement three multi - scale version of aggregate channel features in the aforementioned three kinds of scale , perceptive scale ( subsampling ) , local scale ( pre - smoothing ) and integration scale ( post - smoothing ) and compare their performaces . See results in Figure [ reference ] ( j ) ~ ( l ) . Summary : The color channel , gradient magnitude and gradient histograms prove themselves a good match in aggregate channel features . However , different choices of color channel used and on which gradients are computed have a great impact on performance . According to the experiments , LUV channel and gradient magnitude and 6 - bin histograms computed on RGB color space ( in total 10 channels ) are the best choice for face detection . Larger detection window size generally gets better performance , but will miss many small faces in testing and lead to inefficient detection . In this work , we set the size to as its optimal performance . A subsampling factor of is most reasonable according to the experiments , while different pooling methods show small differences . However , max pooling and stochastic pooling are much slower than average pooling , therefore the average pooling becomes the best match for the sake of efficiency . In this way , the resulting feature pool size of our face detector is , considerably smaller than that in VJ framework . As for multi - scale version of aggregate channel features , multi - local - scale with an additional scale of radius shows the best performance . The probable reason is that pre - smoothing controls the local scale of the neighborhood feature correlations and therefore matches the intuition inside multi - scale best . Compared with other fine - tuning , the multi - scale version has a notable performance gain for that it makes up for the scale uniformity caused by subsampling to some extent . One main drawback is that it doubles the feature pool size and as a result slows down the detection speed somewhat . Based on the trade - off , we implement two face detectors with different scale settings , one is single - scaled with faster speed and the other is multi - scaled with better accuracy . We evaluate and discuss the performances of these two versions in detail in section 5 . subsection : Training design Besides careful design of the aggregate channel features , experiments on the training process which is similar to that in VJ framework are also carried out . The differences are that the weak classifier is changed into depth - 2 decision tree and soft - cascade structure is used . Details of the training design are as follows . Number of weak classifiers : Given a feature pool size of , we vary the number of weak classifiers contained in the soft - cascade . In Figure [ reference ] performances of various numbers of weak classifiers ranging from to are displayed , which shows that apparently more classifiers generate better performance , and when the number gets larger the performance begins to saturate . Since more classifiers slow down the detection speed , there \u2019s a trade - off between accuracy and speed . Searching for the saturate point as the optimal is significant during training in such framework . Training data : Empirically , more training data will get better performance given powerful representation capacity . In this case , AFLW database is used as the only positive training data . However , as images in AFLW database are very salient and the background has very less variance , negative samples cropped from the AFLW database ca n\u2019t represent the real world scenario well , which limits the face detection performance in the wild . In this part , we further use PASCAL VOC database and randomly crop windows from images without person as the new negative samples . Experiments show that the new training data containing cluttered background significantly improve the performance with . Summary : Based on observations above , we choose as the number of weak classifiers contained in the soft cascade . As each weak classifier is a depth - 2 decision tree , it takes only two comparing operations to apply a weak classifier , which is quite fast . During training , as negative data is large , we adopt a standard Bootstrap procedure to sample hard negative samples from PASCAL VOC in the implementation of the proposed face detector . section : Multi - view detection Human faces in real world usually have highly varied poses . In AFLW database , the human pose is divided into three aspects : 1 in - plane rotation \u2018 roll \u2019 and 2 out - of - plane rotations \u2018 yaw \u2019 and \u2018 pitch \u2019 . Because of this large variance in face pose , it is difficult to train a single view face detector to handle all the poses effectively . A multi - view detection is further examined in this part . Due to the adoption of soft - cascade structure , a multi - view version of face detector wo n\u2019t cause too much computation burden . Typically , we divide the out - of - plane rotation \u00a1 \u00b0 yaw\u00a1\u00b1 into different views and let the classifier itself tolerate the pose variance in the other two types of rotations . Adopting multi - view detection also brings about many troublesome issues . If handled improperly , the performance will differ greatly . First , detectors of different view will each produce a set of candidate positive windows followed with a set of confidence scores . For application purpose , we need to merge these detections from different views and also remove duplicated windows . A typical approach is Non - Maximum Suppression ( NMS ) . An issue rises on how to compare confidence scores from different classifiers and how to do window merging in the trade - off between high precision rate and high detection rate . Second , as for detection evaluation , usually the overlap of bounding boxes is used as the criterion . However , annotations in different data sets may not have a consistent style ( Figure [ reference ] ( a ) ) . This diversity suffers more in profile faces . Since our face detector is trained and tested on different data sets , this issue impacts the performance a lot . Third , detectors of different views need to be trained with different samples separately . How to divide the views therefore becomes another concerning problem . In this section , we address the above three issues successfully by careful designs and therefore fully exploit the advantage of multi - view detection . subsection : View partition In the scenario of detecting faces in the wild , pose variation caused by yaw is usually severer than pitch and roll . Therefore we divide the faces in AFLW database according to yaw angle . We have subviews which are horizontally symmetric ( see Figure . [ reference ] ( b ) ) because we flip each image in the training set . Specifically , there are , , , , , images in views from to . Benefitting from the symmetry of our model , we can only train three subview detectors of the right side for simplicity , and use these trained right - side detectors to generate the left - side detectors . Detections of all six detectors are then merged to get the final detections . Though multi - view detection significantly improves the detection performance ( especially the recall rate ) , the post - processing of detections from different detectors becomes a trouble . If handled improperly , the performance degrades a lot . subsection : Post - processing Difficulties in the post - processing of multi - view detection mainly reflect on the following aspects : 1 ) different score distributions and ; 2 ) different bounding box styles . Concretely , as each subview detector is trained separately , their output confidence scores usually have different distributions . What \u2019s more , due to the annotation rule in the AFLW database that the face \u2019s nose is approximately at the center location of the bounding box ground - truth , as the subview changes , the bounding box shifts . This bounding box offset causes difficulty both in detection merging and final evaluation using Jaccard index metric . To solve these annoying issues and make the best use of multi - view detection , we introduce the following methods for post - processing . Score re - ranking : We propose the following three kinds of score re - ranking : 1 ) normalizing scores of different views to [ 0 , 1 ] ; 2 ) defining a new score that has uniform distribution and ; 3 ) taking overlapping detections into consideration . : After training a classifier , calculate the output range of the classifier and use the range to do normalization later so that output score has a range of [ 0 , 1 ] . : Originally , each weak classifier in the soft - cascade owns a score and final score is the sum of all scores . Instead , we use the number of weak classifier that the image patch passed positively as the new score . Therefore the upper limit of the new score is in our case . : Given an image , multiple detections from multi - view detectors exist each with a score . For each detection , we first calculate the number of overlapped detection it has ( overlap threshold is ) and then multiply score of each detection with a factor of its overlapping number ranking . : Instead of using overlapping as a multiply factor , here we use the sum of overlapped detections \u2019 scores as the current detection \u2019s new score . Detection merging : Apart from the version of Non Maximum Suppression , we also use the detection combination introduced in . It averages the locations of overlapped detections rather than suppresses them . Detection adjustment : As shown in Figure [ reference ] ( a ) , different databases have different annotation styles of ground - truth . Specifically , AFLW has square annotations with nose located approximately at the center . AFW uses tight rectangular bounding boxes as annotations with the eye - brow being the approximate upper bound . FDDB uses elliptical annotations bounding the whole head . As our detector is trained on AFLW and tested on AFW and FDDB , there exist offsets in both detection position and scale . According to observations , the offsets vary as face pose changes . Therefore we adopt a view - specific detection adjustment to alleviate the offsets . Note that the adjustment is constant for all images and faces in the same database , see Figure [ reference ] ( b ) for details . Summary : According to experimental results ( Table [ reference ] ) , seems to be the best score re - ranking method . The underlying reason may be that true positives usually have many overlapped detections , while the false positives would only get a few responses . Therefore leveraging this overlapping information in score re - ranking can reduce many false positives . However , in practice , overlap related methods and detection combination both cost much time to process , which is infeasible in a large majority of applications . We finally adopt score re - ranking combined with Non Maximum Suppression for the sake of detection speed . section : Experiments In this section , we compare our method with state - of - the - art methods on AFW and FDDB databases which contain challenging faces in the wild . In AFW , we compare with three commercial systems ( Google Picasa , Face.com and Face ++ ) and five academic methods ( Shen , Zhu , DPM , multiHOG and Kalal ) . In FDDB , we compare with one commercial system ( Olaworks ) and six academic methods ( Yan , Boosted Exemplar , SURF multiview , PEP - Adapt , XZJY and Zhu ) listed on FDDB results page . subsection : Evaluation on benchmark face database As shown in Figure [ reference ] , in AFW , our multi - scale detector achieves an ap value of , outperforming other academic methods by a large margin . When it comes to commercial systems , ours is better than Face.com and almost equal to Face ++ and Google Picasa . Note that most of our false positives on AFW database are faces that have n\u2019t been annotated ( small , seriously occluded or artificial faces like mask and cartoon character ) . When evaluated on FDDB database , we follow the evaluation protocol in and report the average discrete and continuous ROC of the ten subfolders . For equality , we fix the number of false positives to ( equivalent to an average of False Positive Per Image ) and compare the true positive rate . In discrete score where evaluation metric is the same as in AFW , our detector achieves , which is a little better than Yan . Note that the ground - truth in FDDB are elliptical faces , therefore the evaluation metric of an overlap ratio bigger than can not reveal the true performance of the proposed detector well . When using continuous score which takes the overlap ratio as the score , our method gets true positive rate at FPPI for multi - scale version , surpassing other methods which output rectangular detections by a notable margin ( the Yan detector outputs the same elliptical detections as the ground - truth , therefore having advantages with this metric ) . Our detector using single - scale features performs a little worse with the benefit of faster detection speed . subsection : Discussion Training efficiency : We implement the method with Piotr \u2019s MATLAB toolbox on a PC with Intel Core i7 - 3770 CPU and 16 GB RAM . With positive images and negative images in total 6 views , the training process takes about mins for a single - scale subview detector containing weak classifiers and mins for multi - scale version . Note that we use much fewer training data than SURF multiview whilst still outperforming their performance . Comparative results : When inspecting detections of the proposed face detector and other algorithms on the testsets , some patterns can be found to explain why our detector outperforms others . One evident strength lies in detecting faces with extreme poses . Because we adopt multi - view detection and train each subview detector separately , our detector handles pose variations very well . Second is the outstanding illumination invariance of our detector , which is mainly owing to the extension of channel types to LUV color space and gradient - related channels . Detection speed : Due to the simple form of aggregate channel features and fast computation of feature pyramid , detection is quite efficient . For full yaw pose face detection in VGA image , the proposed detector using single - scale features runs at FPS on a single thread and FPS if threads are used . If only frontal faces are concerned , the detector runs at FPS and FPS after parallelization . When it comes to the proposed detector using multi - scale features , the above four indices reduce to , , and FPS . Considering the large performance gain and similar speed , the proposed method can replace Viola - Jones detector for face detection in the wild . section : Conclusion A novel feature representation called aggregate channel features possesses the merits of fast feature extraction and powerful representation capacity . In this paper , we successfully apply the feature representation to face detection domain through a deep investigation into the feature design , and propose a multi - scale version of feature which further enriches the representation capacity . Combined with our efforts into solving issues concerning multi - view detection , the proposed multi - view face detector shows state - of - the - art performance in both effectiveness and efficiency on faces in the wild . The proposed method appeals to real world application demands and has the potential to be embedded into low power devices . section : Acknowledgement This work was supported by the Chinese National Natural Science Foundation Projects # 61105023 , # 61103156 , # 61105037 , # 61203267 , # 61375037 , National Science and Technology Support Program Project # 2013BAK02B01 , Chinese Academy of Sciences Project No . KGZD - EW - 102 - 2 , and AuthenMetric R & D Funds . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["WIDER_Face__Easy_"]]], "Method": [[["ACF-WIDER"]]], "Metric": [[["AP"]]], "Task": [[["Face_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WIDER_Face__Hard_"]]], "Method": [[["ACF-WIDER"]]], "Metric": [[["AP"]]], "Task": [[["Face_Detection"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WIDER_Face__Medium_"]]], "Method": [[["ACF-WIDER"]]], "Metric": [[["AP"]]], "Task": [[["Face_Detection"]]]}]}
{"docid": "TST3-SREX-0044", "doctext": "document : Training Region - based Object Detectors with Online Hard Example Mining The field of object detection has made significant advances riding on the wave of region - based ConvNets , but their training procedure still includes many heuristics and hyperparameters that are costly to tune . We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm for training region - based ConvNet detectors . Our motivation is the same as it has always been \u2013 detection datasets contain an overwhelming number of easy examples and a small number of hard examples . Automatic selection of these hard examples can make training more effective and efficient . OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use . But more importantly , it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012 . Its effectiveness increases as datasets become larger and more difficult , as demonstrated by the results on the MS COCO dataset . Moreover , combined with complementary advances in the field , OHEM leads to state - of - the - art results of 78.9 % and 76.3 % mAP on PASCAL VOC 2007 and 2012 respectively . section : Introduction Image classification and object detection are two fundamental computer vision tasks . Object detectors are often trained through a reduction that converts object detection into an image classification problem . This reduction introduces a new challenge that is not found in natural image classification tasks : the training set is distinguished by a large imbalance between the number of annotated objects and the number of background examples ( image regions not belonging to any object class of interest ) . In the case of sliding - window object detectors , such as the deformable parts model ( DPM ) , this imbalance may be as extreme as 100 , 000 background examples to every one object . The recent trend towards object - proposal - based detectors mitigates this issue to an extent , but the imbalance ratio may still be high ( , 70:1 ) . This challenge opens space for learning techniques that cope with imbalance and yield faster training , higher accuracy , or both . Unsurprisingly , this is not a new challenge and a standard solution , originally called bootstrapping ( and now often called hard negative mining ) , has existed for at least 20 years . Bootstrapping was introduced in the work of Sung and Poggio in the mid - 1990 \u2019s ( if not earlier ) for training face detection models . Their key idea was to gradually grow , or bootstrap , the set of background examples by selecting those examples for which the detector triggers a false alarm . This strategy leads to an iterative training algorithm that alternates between updating the detection model given the current set of examples , and then using the updated model to find new false positives to add to the bootstrapped training set . The process typically commences with a training set consisting of all object examples and a small , random set of background examples . Bootstrapping has seen widespread use in the intervening decades of object detection research . Dalal and Triggs used it when training SVMs for pedestrian detection . Felzenszwalb later proved that a form of bootstrapping for SVMs converges to the global optimal solution defined on the entire dataset . Their algorithm is often referred to as hard negative mining and is frequently used when training SVMs for object detection . Bootstrapping was also successfully applied to a variety of other learning models , including shallow neural networks and boosted decision trees . Even modern detection methods based on deep convolutional neural networks ( ConvNets ) , such as R - CNN and SPPnet , still employ SVMs trained with hard negative mining . It may seem odd then that the current state - of - the - art object detectors , embodied by Fast R - CNN and its descendants , do not use bootstrapping . The underlying reason is a technical difficulty brought on by the shift towards purely online learning algorithms , particularly in the context of deep ConvNets trained with stochastic gradient descent ( SGD ) on millions of examples . Bootstrapping , and its variants in the literature , rely on the aforementioned alternation template : ( a ) for some period of time a fixed model is used to find new examples to add to the active training set ; ( b ) then , for some period of time the model is trained on the fixed active training set . Training deep ConvNet detectors with SGD typically requires hundreds of thousands of SGD steps and freezing the model for even a few iterations at a time would dramatically slow progress . What is needed , instead , is a purely online form of hard example selection . In this paper , we propose a novel bootstrapping technique called online hard example mining ( OHEM ) for training state - of - the - art detection models based on deep ConvNets . The algorithm is a simple modification to SGD in which training examples are sampled according to a non - uniform , non - stationary distribution that depends on the current loss of each example under consideration . The method takes advantage of detection - specific problem structure in which each SGD mini - batch consists of only one or two images , but thousands of candidate examples . The candidate examples are subsampled according to a distribution that favors diverse , high loss instances . Gradient computation ( backpropagation ) is still efficient because it only uses a small subset of all candidates . We apply OHEM to the standard Fast R - CNN detection method and show three benefits compared to the baseline training algorithm : It removes the need for several heuristics and hyperparameters commonly used in region - based ConvNets . It yields a consistent and significant boosts in mean average precision . Its effectiveness increases as the training set becomes larger and more difficult , as demonstrated by results on the MS COCO dataset . Moreover , the gains from OHEM are complementary to recent improvements in object detection , such as multi - scale testing and iterative bounding - box regression . Combined with these tricks , OHEM gives state - of - the - art results of 78.9 % and 76.3 % mAP on PASCAL VOC 2007 and 2012 , respectively . section : Related work Object detection is one of the oldest and most fundamental problems in computer vision . The idea of dataset bootstrapping , typically called hard negative mining in recent work , appears in the training of most successful object detectors . Many of these approaches use SVMs as the detection scoring function , even after training a deep convolutional neural network ( ConvNet ) for feature extraction . One notable exception is the Fast R - CNN detector and its descendants , such as Faster R - CNN . Since these models do not use SVMs , and are trained purely online with SGD , existing hard example mining techniques can not be immediately applied . This work addresses that problem by introducing an online hard example mining algorithm that improves optimization and detection accuracy . We briefly review hard example mining , modern ConvNet - based object detection , and relationships to concurrent works using hard example selection for training deep networks . paragraph : Hard example mining . There are two hard example mining algorithms in common use . The first is used when optimizing SVMs . In this case , the training algorithm maintains a working set of examples and alternates between training an SVM to convergence on the working set , and updating the working set by removing some examples and adding others according to a specific rule . The rule removes examples that are \u201c easy \u201d in the sense that they are correctly classified beyond the current model \u2019s margin . Conversely , the rule adds new examples that are hard in the sense that they violate the current model \u2019s margin . Applying this rule leads to the global SVM solution . Importantly , the working set is usually a small subset of the entire training set . The second method is used for non - SVMs and has been applied to a variety of models including shallow neural networks and boosted decision trees . This algorithm usually starts with a dataset of positive examples and a random set of negative examples . The machine learning model is then trained to convergence on that dataset and subsequently applied to a larger dataset to harvest false positives . The false positives are then added to the training set and then the model is trained again . This process is usually iterated only once and does not have any convergence proofs . paragraph : ConvNet - based object detection . In the last three years significant gains have been made in object detection . These improvements were made possible by the successful application of deep ConvNets to ImageNet classification . The R - CNN and OverFeat detectors lead this wave with impressive results on PASCAL VOC and ImageNet detection . OverFeat is based on the sliding - window detection method , which is perhaps the most intuitive and oldest search method for detection . R - CNN , in contrast , uses region proposals , a method that was made popular by the selective search algorithm . Since R - CNN , there has been rapid progress in region - based ConvNets , including SPPnet , MR - CNN , and Fast R - CNN , which our work builds on . paragraph : Hard example selection in deep learning . There is recent work concurrent to our own that selects hard examples for training deep networks . Similar to our approach , all these methods base their selection on the current loss for each datapoint . independently selects hard positive and negative example from a larger set of random examples based on their loss to learn image descriptors . Given a positive pair of patches , finds hard negative patches from a large set using triplet loss . Akin to our approach , investigates online selection of hard examples for mini - batch SGD methods . Their selection is also based on loss , but the focus is on ConvNets for image classification . Complementary to , we focus on online hard example selection strategy for region - based object detectors . section : Overview of Fast R - CNN We first summarize the Fast R - CNN ( FRCN ) framework . FRCN takes as input an image and a set of object proposal regions of interest ( RoIs ) . The FRCN network itself can be divided into two sequential parts : a convolutional ( conv ) network with several convolution and max - pooling layers ( Figure [ reference ] , \u201c Convolutional Network \u201d ) ; and an RoI network with an RoI - pooling layer , several fully - connected ( fc ) layers and two loss layers ( Figure [ reference ] , \u201c RoI Network \u201d ) . During inference , the conv network is applied to the given image to produce a conv feature map , size of which depends on the input image dimensions . Then , for each object proposal , the RoI - pooling layer projects the proposal onto the conv feature map and extracts a fixed - length feature vector . Each feature vector is fed into the fc layers , which finally give two outputs : ( 1 ) a softmax probability distribution over the object classes and background ; and ( 2 ) regressed coordinates for bounding - box relocalization . There are several reasons for choosing FRCN as our base object detector , apart from it being a fast end - to - end system . Firstly , the basic two network setup ( conv and RoI ) is also used by other recent detectors like SPPnet and MR - CNN ; therefore , our proposed algorithm is more broadly applicable . Secondly , though the basic setup is similar , FRCN also allows for training the entire conv network , as opposed to both SPPnet and MR - CNN which keep the conv network fixed . And finally , both SPPnet and MR - CNN require features from the RoI network to be cached for training a separate SVM classifier ( using hard negative mining ) . FRCN uses the RoI network itself to train the desired classifiers . In fact , shows that in the unified system using the SVM classifiers at later stages was unnecessary . subsection : Training Like most deep networks , FRCN is trained using stochastic gradient descent ( SGD ) . The loss per example RoI is the sum of a classification log loss that encourages predicting the correct object ( or background ) label and a localization loss that encourages predicting an accurate bounding box ( see for details ) . To share conv network computation between RoIs , SGD mini - batches are created hierarchically . For each mini - batch , images are first sampled from the dataset , and then RoIs are sampled from each image . Setting and works well in practice . The RoI sampling procedure uses several heuristics , which we describe briefly below . One contribution of this paper is to eliminate some of these heuristics and their hyperparameters . paragraph : Foreground RoIs . For an example RoI to be labeled as foreground ( fg ) , its intersection over union ( IoU ) overlap with a ground - truth bounding box should be at least . This is a fairly standard design choice , in part inspired by the evaluation protocol of the PASCAL VOC object detection benchmark . The same criterion is used in the SVM hard mining procedures of R - CNN , SPPnet , and MR - CNN . We use the same setting . paragraph : Background RoIs . A region is labeled background ( bg ) if its maximum IoU with ground truth is in the interval bg_lo , 0.5 ) . A lower threshold of bg_lo is used by both FRCN and SPPnet , and is hypothesized in to crudely approximate hard negative mining ; the assumption is that regions with some overlap with the ground truth are more likely to be the confusing or hard ones . We show in Section [ reference ] that although this heuristic helps convergence and detection accuracy , it is suboptimal because it ignores some infrequent , but important , difficult background regions . Our method removes the bg_lo threshold . paragraph : Balancing fg - bg RoIs : To handle the data imbalance described in Section [ reference ] , designed heuristics to rebalance the foreground - to - background ratio in each mini - batch to a target of by undersampling the background patches at random , thus ensuring that of a mini - batch is fg RoIs . We found that this is an important design decision for the training FRCN . Removing this ratio ( randomly sampling RoIs ) , or increasing it , decreases accuracy by points mAP . With our proposed method , we can remove this ratio hyperparameter with no ill effect . section : Our approach We propose a simple yet effective online hard example mining algorithm for training Fast R - CNN ( or any Fast R - CNN style object detector ) . We argue that the current way of creating mini - batches for SGD ( Section [ reference ] ) is inefficient and suboptimal , and we demonstrate that our approach leads to better training ( lower training loss ) and higher testing performance ( mAP ) . subsection : Online hard example mining Recall the alternating steps that define a hard example mining algorithm : ( a ) for some period of time a fixed model is used to find new examples to add to the active training set ; ( b ) then , for some period of time the model is trained on the fixed active training set . In the context of SVM - based object detectors , such as the SVMs trained in R - CNN or SPPnet , step ( a ) inspects a variable number of images ( often 10 \u2019s or 100 \u2019s ) until the active training set reaches a threshold size , and then in step ( b ) the SVM is trained to convergence on the active training set . This process repeats until the active training set contains all support vectors . Applying an analogous strategy to FRCN ConvNet training slows learning because no model updates are made while selecting examples from the 10 \u2019s or 100 \u2019s of images . Our main observation is that these alternating steps can be combined with how FRCN is trained using online SGD . The key is that although each SGD iteration samples only a small number of images , each image contains thousands of example RoIs from which we can select the hard examples rather than a heuristically sampled subset . This strategy fits the alternation template to SGD by \u201c freezing \u201d the model for only one mini - batch . Thus the model is updated exactly as frequently as with the baseline SGD approach and therefore learning is not delayed . More specifically , the online hard example mining algorithm ( OHEM ) proceeds as follows . For an input image at SGD iteration , we first compute a conv feature map using the conv network . Then the RoI network uses this feature map and the all the input RoIs , instead of a sampled mini - batch , to do a forward pass . Recall that this step only involves RoI pooling , a few fc layers , and loss computation for each RoI. The loss represents how well the current network performs on each RoI. Hard examples are selected by sorting the input RoIs by loss and taking the examples for which the current network performs worst . Most of the forward computation is shared between RoIs via the conv feature map , so the extra computation needed to forward all RoIs is relatively small . Moreover , because only a small number of RoIs are selected for updating the model , the backward pass is no more expensive than before . However , there is a small caveat : co - located RoIs with high overlap are likely to have correlated losses . Moreover , these overlapping RoIs can project onto the same region in the conv feature map , because of resolution disparity , thus leading to loss double counting . To deal with these redundant and correlated regions , we use standard non - maximum suppression ( NMS ) to perform deduplication ( the implementation from ) . Given a list of RoIs and their losses , NMS works by iteratively selecting the RoI with the highest loss , and then removing all lower loss RoIs that have high overlap with the selected region . We use a relaxed IoU threshold of to suppress only highly overlapping RoIs . We note that the procedure described above does not need a fg - bg ratio for data balancing . If any class were neglected , its loss would increase until it has a high probability of being sampled . There can be images where the fg RoIs are easy ( canonical view of a car ) , so the network is free to use only bg regions in a mini - batch ; and vice - versa when bg is trivial ( sky , grass ) , the mini - batch can be entirely fg regions . subsection : Implementation details There are many ways to implement OHEM in the FRCN detector , each with different trade - offs . An obvious way is to modify the loss layers to do the hard example selection . The loss layer can compute loss for all RoIs , sort them based on this loss to select hard RoIs , and finally set the loss of all non - hard RoIs to . Though straightforward , this implementation is inefficient as the RoI network still allocates memory and performs backward pass for all RoIs , even though most RoIs have loss and hence no gradient updates ( a limitation of current deep learning toolboxes ) . To overcome this , we propose the architecture presented in Figure [ reference ] . Our implementation maintains two copies of the RoI network , one of which is readonly . This implies that the readonly RoI network ( Figure [ reference ] ( a ) ) allocates memory only for forward pass of all RoIs as opposed to the standard RoI network , which allocates memory for both forward and backward passes . For an SGD iteration , given the conv feature map , the readonly RoI network performs a forward pass and computes loss for all input RoIs ( Figure [ reference ] , green arrows ) . Then the hard RoI sampling module uses the procedure described in Section [ reference ] to select hard examples , which are input to the regular RoI network ( Figure [ reference ] ( b ) , red arrows ) ) . This network computes forward and backward passes only for , accumulates the gradients and passes them to the conv network . In practice , we use all RoIs from all images as , therefore the effective batch size for the readonly RoI network is and for the regular RoI network is the standard from Section [ reference ] . We implement both options described above using the Caffe framework ( see ) . Our implementation uses gradient accumulation with forward - backward passes of single image mini - batches . Following FRCN , we use ( which results in ) and . Under these settings , the proposed architecture ( Figure [ reference ] ) has similar memory footprint as the first option , but is faster . Unless specified otherwise , the architecture and settings described above will be used throughout this paper . section : Analyzing online hard example mining This section compares FRCN training with online hard example mining ( OHEM ) to the baseline heuristic sampling approach . We also compare FRCN with OHEM to a less efficient approach that uses all available example RoIs in each mini - batch , not just the hardest examples . subsection : Experimental setup We conduct experiments with two standard ConvNet architectures : VGG_CNN_M_1024 ( VGGM , for short ) from , which is a wider version of AlexNet , and VGG16 from . All experiments in this section are performed on the PASCAL VOC07 dataset . Training is done on the trainval set and testing on the test set . Unless specified otherwise , we will use the default settings from FRCN . We train all methods with SGD for 80k mini - batch iterations , with an initial learning rate of 0.001 and we decay the learning rate by 0.1 every 30k iterations . The baseline numbers reported in Table [ reference ] ( row 1 - 2 ) were reproduced using our training schedule and are slightly higher than the ones reported in . subsection : OHEM vs. heuristic sampling Standard FRCN , reported in Table [ reference ] ( rows ) , uses as a heuristic for hard mining ( Section [ reference ] ) . To test the importance of this heuristic , we ran FRCN with . Table [ reference ] ( rows ) shows that for VGGM , mAP drops by points , whereas for VGG16 it remains roughly the same . Now compare this to training FRCN with OHEM ( rows ) . OHEM improves mAP by points compared to FRCN with the heuristic for VGGM , and points without the heuristic . This result demonstrates the sub - optimality of these heuristics and the effectiveness of our hard mining approach . subsection : Robust gradient estimates One concern over using only images per batch is that it may cause unstable gradients and slow convergence because RoIs from an image may be highly correlated . FRCN reports that this was not a practical issue for their training . But this detail might raise concerns over our training procedure because we use examples with high loss from the same image and as a result they may be more highly correlated . To address this concern , we experiment with in order to increase correlation in an effort to break our method . As seen in Table [ reference ] ( rows ) , performance of the original FRCN drops by point with , but when using our training procedure , mAP remains approximately the same . This shows that OHEM is robust in case one needs fewer images per batch in order to reduce GPU memory usage . subsection : Why just hard examples , when you can use all ? Online hard example mining is based on the hypothesis that it is important to consider all RoIs in an image and then select hard examples for training . But what if we train with all the RoIs , not just the hard ones ? The easy examples will have low loss , and wo n\u2019t contribute much to the gradient ; training will automatically focus on the hard examples . To compare this option , we ran standard FRCN training with a large mini - batch size of , using , and with other hyperparameters fixed . Because this experiment uses a large mini - batch , it \u2019s important to tune the learning rate to adjust for this change . We found optimal results by increasing it to for VGG16 and for VGGM . The outcomes are reported in Table [ reference ] ( rows ) . Using these settings , mAP of both VGG16 and VGGM increased by point compared to , but the improvement from our approach is still points over using all RoIs . Moreover , because we compute gradients with a smaller mini - batch size training is faster . Removing hard mining heuristic ( Section [ reference ] ) Fewer images per batch ( Section [ reference ] ) Bigger batch , High LR ( Section [ reference ] ) Our Approach subsection : Better optimization Finally , we analyze the training loss for the various FRCN training methods discussed above . It \u2019s important to measure training loss in a way that does not depend on the sampling procedure and thus results in a valid comparison between methods . To achieve this goal , we take model snapshots from each method every 20k steps of optimization and run them over the entire VOC07 trainval set to compute the average loss over all RoIs . This measures the training set loss in a way that does not depend on the example sampling scheme . Figure [ reference ] shows the average loss per RoI for VGG16 with the various hyperparameter settings discussed above and presented in Table [ reference ] . We see that results in the highest training loss , while using the heuristic results in a much lower training loss . Increasing the mini - batch size to and increasing the learning rate lowers the training loss below the heuristic . Our proposed online hard example mining method achieves the lowest training loss of all methods , validating our claims that OHEM leads to better training for FRCN . * : uses gradient accumulation over two forward / backward passes subsection : Computational cost OHEM adds reasonable computational and memory overhead , as reported in Table [ reference ] . OHEM costs 0.09s per training iteration for VGGM network ( 0.43s for VGG16 ) and requires 1 G more memory ( 2.3 G for VGG16 ) . Given that FRCN is a fast detector to train , the increase in training time is likely acceptable to most users . , 2http: // host.robots.ox.ac.uk:8080 / anonymous / H49PTT.html , 3http: // host.robots.ox.ac.uk:8080 / anonymous / LSANTB.html , 4http: // host.robots.ox.ac.uk:8080 / anonymous / R7EAMX.html section : PASCAL VOC and MS COCO results In this section , we evaluate our method on VOC 2012 as well as the more challenging MS COCO dataset . We demonstrate consistent and significant improvement in FRCN performance when using the proposed OHEM approach . Per - class results are also presented on VOC 2007 for comparison with prior work . paragraph : Experimental setup . We use VGG16 for all experiments . When training on VOC07 trainval , we use the SGD parameters as in Section [ reference ] and when using extra data ( 07 + 12 and 07 ++ 12 , see Table [ reference ] and [ reference ] ) , we use 200k mini - batch iterations , with an initial learning rate of 0.001 and decay step size of 40k . When training on MS COCO , we use 240k mini - batch iterations , with an initial learning rate of 0.001 and decay step size of 160k , owing to a larger epoch size . subsection : VOC 2007 and 2012 results Table [ reference ] shows that on VOC07 , OHEM improves the mAP of FRCN from 67.2 % to 69.9 % ( and 70.0 % to 74.6 % with extra data ) . On VOC12 , OHEM leads to an improvement of 4.1 points in mAP ( from 65.7 % to 69.8 % ) . With extra data , we achieve an mAP of 71.9 % as compared to 68.4 % mAP of FRCN , an improvement of 3.5 points . Interestingly the improvements are not uniform across categories . Bottle , chair , and tvmonitor show larger improvements that are consistent across the different PASCAL splits . Why these classes benefit the most is an interesting and open question . subsection : MS COCO results To test the benefit of using OHEM on a larger and more challenging dataset , we conduct experiments on MS COCO and report numbers from test - dev 2015 evaluation server ( Table [ reference ] ) . On the standard COCO evaluation metric , FRCN scores 19.7 % AP , and OHEM improves it to 22.6 % AP . Using the VOC overlap metric of , OHEM gives a 6.6 points boost in AP . It is also interesting to note that OHEM helps improve the AP of medium sized objects by 4.9 points on the strict COCO AP evaluation metric , which indicates that the proposed hard example mining approach is helpful when dealing with smaller sized objects . Note that FRCN with and without OHEM were trained on MS COCO train set . section : Adding bells and whistles We \u2019 ve demonstrated consistent gains in detection accuracy by applying OHEM to FRCN training . In this section , we show that these improvements are orthogonal to recent bells and whistles that enhance object detection accuracy . OHEM with the following two additions yields state - of - the - art results on VOC and competitive results on MS COCO . paragraph : Multi - scale ( M ) . We adopt the multi - scale strategy from SPPnet ( and used by both FRCN and MR - CNN ) . Scale is defined as the size of the shortest side ( ) of an image . During training , one scale is chosen at random , whereas at test time inference is run on all scales . For VGG16 networks , we use for training , and during testing , with the max dimension capped at 1000 . The scales and caps were chosen because of GPU memory constraints . paragraph : Iterative bounding - box regression ( B ) . We adopt the iterative localization and bounding - box ( bbox ) voting scheme from . The network evaluates each proposal RoI to get scores and relocalized boxes . High - scoring boxes are the rescored and relocalized , yielding boxes . Union of and is used as the final set for post - processing , where is obtained using NMS on with an IoU threshold of 0.3 and weighted voting is performed on each box in using boxes in with an IoU of 0.5 with ( see for details ) . from the leaderboard , * trained on trainval set subsection : VOC 2007 and 2012 results We report the results on VOC benchmarks in Table [ reference ] and [ reference ] . On VOC07 , FRCN with the above mentioned additions achieves 72.4 % mAP and OHEM improves it to 75.1 % , which is currently the highest reported score under this setting ( 07 data ) . When using extra data ( 07 + 12 ) , OHEM achieves 78.9 % mAP , surpassing the current state - of - the - art MR - CNN ( 78.2 % mAP ) . We note that MR - CNN uses selective search and edge boxes during training , whereas we only use selective search boxes . Our multi - scale implementation is also different , using fewer scales than MR - CNN . On VOC12 ( Table [ reference ] ) , we consistently perform better than MR - CNN . When using extra data , we achieve state - of - the - art mAP of 76.3 % ( 73.9 % mAP of MR - CNN ) . paragraph : Ablation analysis . We now study in detail the impact of these two additions and whether OHEM is complementary to them , and report the analysis in Table [ reference ] . Baseline FRCN mAP improves from 67.2 % to 68.6 % when using multi - scale during both training and testing ( we refer to this as M ) . However , note that there is only a marginal benefit of using it at training time . Iterative bbox regression ( B ) further improves the FRCN mAP to 72.4 % . But more importantly , using OHEM improves it to 75.1 % mAP , which is state - of - the - art for methods trained on VOC07 data ( see Table [ reference ] ) . In fact , using OHEM consistently results in higher mAP for all variants of these two additions ( see Table [ reference ] ) . Iterative bbox reg . ( B ) subsection : MS COCO results MS COCO test - dev 2015 evaluation server results are reported in Table [ reference ] . Using multi - scale improves the performance of our method to 24.4 % AP on the standard COCO metric and to 44.4 % AP on the VOC metric . This again shows the complementary nature of using multi - scale and OHEM . Finally , we train our method using the entire MS COCO trainval set , which further improves performance to 25.5 % AP ( and 45.9 % AP ) . In the 2015 MS COCO Detection Challenge , a variant of this approach finished place overall . section : Conclusion We presented an online hard example mining ( OHEM ) algorithm , a simple and effective method to train region - based ConvNet detectors . OHEM eliminates several heuristics and hyperparameters in common use by automatically selecting hard examples , thus simplifying training . We conducted extensive experimental analysis to demonstrate the effectiveness of the proposed algorithm , which leads to better training convergence and consistent improvements in detection accuracy on standard benchmarks . We also reported state - of - the - art results on PASCAL VOC 2007 and 2012 when using OHEM with other orthogonal additions . Though we used Fast R - CNN throughout this paper , OHEM can be used for training any region - based ConvNet detector . Our experimental analysis was based on the overall detection accuracy , however it will be an interesting future direction to study the impact of various training methodologies on individual category performance . paragraph : Acknowledgment . This project started as an intern project at Microsoft Research and continued at CMU . We thank Larry Zitnick , Ishan Misra and Sean Bell for many helpful discussions . AS was supported by the Microsoft Research PhD Fellowship . This work was also partially supported by ONR MURI N000141612007 . We thank NVIDIA for donating GPUs . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["OHEM"]]], "Metric": [[["MAP"]]], "Task": [[["Object_Detection"]]]}]}
{"docid": "TST3-SREX-0045", "doctext": "document : The Microsoft 2016 Conversational Speech Recognition System We describe Microsoft \u2019s conversational speech recognition system , in which we combine recent developments in neural - network - based acoustic and language modeling to advance the state of the art on the Switchboard recognition task . Inspired by machine learning ensemble techniques , the system uses a range of convolutional and recurrent neural networks . I - vector modeling and lattice - free MMI training provide significant gains for all acoustic model architectures . Language model rescoring with multiple forward and backward running RNNLMs , and word posterior - based system combination provide a 20 % boost . The best single system uses a ResNet architecture acoustic model with RNNLM rescoring , and achieves a word error rate of 6.9 % on the NIST 2000 Switchboard task . The combined system has an error rate of 6.2 % , representing an improvement over previously reported results on this benchmark task . W.Xiong , J.Droppo , X.Huang , F.Seide , M.Seltzer , A.Stolcke , D.YuandG.Zweig MicrosoftResearch Conversational speech recognition , convolutional neural networks , recurrent neural networks , VGG , ResNet , LACE , BLSTM . section : Introduction Recent years have seen a rapid reduction in speech recognition error rates as a result of careful engineering and optimization of convolutional and recurrent neural networks . While the basic structures have been well known for a long period , it is only recently that they have dominated the field as the best models for speech recognition . Surprisingly , this is the case for both acoustic modeling and language modeling . In comparison to standard feed - forward MLPs or DNNs , these acoustic models have the ability to model a large amount of acoustic context with temporal invariance , and in the case of convolutional models , with frequency invariance as well . In language modeling , recurrent models appear to improve over classical N - gram models through the generalization ability of continuous word representations . In the meantime , ensemble learning has become commonly used in several neural models , to improve robustness by reducing bias and variance . In this paper , we use ensembles of models extensively , as well as improvements to individual component models , to to advance the state - of - the - art in conversational telephone speech recognition ( CTS ) , which has been a benchmark speech recognition task since the 1990s . The main features of this system are : An ensemble of two fundamental acoustic model architectures , convolutional neural nets ( CNNs ) and long - short - term memory nets ( LSTMs ) , with multiple variants of each An attention mechanism in the LACE CNN which differentially weights distant context Lattice - free MMI training The use of i - vector based adaptation in all models Language model ( LM ) rescoring with multiple , recurrent neural net LMs running in both forward and reverse direction Confusion network system combination coupled with search for best system subset , as necessitated by the large number of candidate systems . The remainder of this paper describes our system in detail . Section [ reference ] describes the CNN and LSTM models . Section [ reference ] describes our implementation of i - vector adaptation . Section [ reference ] presents out lattice - free MMI training process . Language model rescoring is a significant part of our system , and described in Section [ reference ] . Experimental results are presented in Section [ reference ] , followed by a discussion of related work and conclusions . section : Convolutional and LSTM Neural Networks We use three CNN variants . The first is the VGG architecture of . Compared to the networks used previously in image recognition , this network uses small ( 3x3 ) filters , is deeper , and applies up to five convolutional layers before pooling . The second network is modeled on the ResNet architecture , which adds highway connections , i.e. a linear transform of each layer \u2019s input to the layer \u2019s output . The only difference is that we move the Batch Normalization node to the place right before each ReLU activation . The last CNN variant is the LACE ( layer - wise context expansion with attention ) model . LACE is a TDNN variant in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames . In other words , each higher layer exploits broader context than lower layers . Lower layers focus on extracting simple local patterns while higher layers extract complex patterns that cover broader contexts . Since not all frames in a window carry the same importance , an attention mask is applied . The LACE model differs from the earlier TDNN models e.g. in the use of a learned attention mask and ResNet like linear pass - through . As illustrated in detail in Figure [ reference ] , the model is composed of 4 blocks , each with the same architecture . Each block starts with a convolution layer with stride 2 which sub - samples the input and increases the number of channels . This layer is followed by 4 RELU - convolution layers with jump links similar to those used in ResNet . Table [ reference ] compares the layer structure and parameters of the three CNN architectures . While our best performing models are convolutional , the use of long short - term memory networks is a close second . We use a bidirectional architecture without frame - skipping . The core model structure is the LSTM defined in . We found that using networks with more than six layers did not improve the word error rate on the development set , and chose 512 hidden units , per direction , per layer , as that provided a reasonable trade - off between training time and final model accuracy . Network parameters for different configurations of the LSTM architecture are summarized in Table [ reference ] . section : Speaker Adaptive Modeling Speaker adaptive modeling in our system is based on conditioning the network on an i - vector characterization of each speaker . A 100 - dimensional i - vector is generated for each conversation side . For the LSTM system , the conversation - side i - vector is appended to each frame of input . For convolutional networks , this approach is inappropriate because we do not expect to see spatially contiguous patterns in the input . Instead , for the CNNs , we add a learnable weight matrix to each layer , and add to the activation of the layer before the nonlinearity . Thus , in the CNN , the i - vector essentially serves as an additional bias to each layer . Note that the i - vectors are estimated using MFCC features ; by using them subsequently in systems based on log - filterbank features , we may benefit from a form of feature combination . section : Lattice - Free Sequence Training After standard cross - entropy training , we optimize the model parameters using the maximum mutual information ( MMI ) objective function . Denoting a word sequence by and its corresponding acoustic realization by , the training criterion is As noted in , the necessary gradient for use in backpropagation is a simple function of the posterior probability of a particular acoustic model state at a given time , as computed by summing over all possible word sequences in an unconstrained manner . As first done in , and more recently in , this can be accomplished with a straightforward alpha - beta computation over the finite state acceptor representing the decoding search space . In , the search space is taken to be an acceptor representing the composition for a unigram language model on words . In , a language model on phonemes is used instead . In our implementation , we use a mixed - history acoustic unit language model . In this model , the probability of transitioning into a new context - dependent phonetic state ( senone ) is conditioned both the senone and phone history . We found this model to perform better than either purely word - based or phone - based models . Based on a set of initial experiments , we developed the following procedure : Perform a forced alignment of the training data to select lexical variants and determine frame - aligned senone sequences . Compress consecutive framewise occurrences of a single senone into a single occurrence . Estimate an unsmoothed , variable - length N - gram language model from this data , where the history state consists of the previous phone and previous senones within the current phone . To illustrate this , consider the sample senone sequence { s _ s2.1288 , s _ s3.1061 , s _ s4.1096 } , { eh _ s2.527 , eh _ s3.128 , eh _ s4.66 } , { t _ s2.729 , t _ s3.572 , t _ s4.748}. When predicting the state following eh _ s4.66 the history consists of ( s , eh _ s2.527 , eh _ s3.128 , eh _ s4.66 ) , and following t _ s2.729 , the history is ( eh , t _ s2.729 ) . We construct the denominator graph from this language model , and HMM transition probabilities as determined by transition - counting in the senone sequences found in the training data . Our approach not only largely reduces the complexity of building up the language model but also provides very reliable training performance . We have found it convenient to do the full computation , without pruning , in a series of matrix - vector operations on the GPU . The underlying acceptor is represented with a sparse matrix , and we maintain a dense likelihood vector for each time frame . The alpha and beta recursions are implemented with CUSPARSE level - 2 routines : sparse - matrix , dense vector multiplies . Run time is about 100 times faster than real time . As in , we use cross - entropy regularization . In all the lattice - free MMI ( LFMMI ) experiments mentioned below we use a trigram language model . Most of the gain is usually obtained after processing 24 to 48 hours of data . section : LM Rescoring and System Combination An initial decoding is done with a WFST decoder , using the architecture described in . We use an N - gram language model trained and pruned with the SRILM toolkit . The first - pass LM has approximately 15.9 million bigrams , trigrams , and 4grams , and a vocabulary of 30 , 500 words . It gives a perplexity of 69 on the 1997 CTS evaluation transcripts . The initial decoding produces a lattice with the pronunciation variants marked , from which 500 - best lists are generated for rescoring purposes . Subsequent N - best rescoring uses an unpruned LM comprising 145 million N - grams . All N - gram LMs were estimated by a maximum entropy criterion as described in . subsection : RNNLM setup The N - best hypotheses are then rescored using a combination of the large N - gram LM and several RNNLMs , trained and evaluated using the CUED - RNNLM toolkit . Our RNNLM configuration has several distinctive features , as described below . 1 ) We trained both standard , forward - predicting RNNLMs and backward RNNLMs that predict words in reverse temporal order . The log probabilities from both models are added . 2 ) As is customary , the RNNLM probability estimates are interpolated at the word - level with corresponding N - gram LM probabilities ( separately for the forward and backward models ) . In addition , we trained a second RNNLM for each direction , obtained by starting with different random initial weights . The two RNNLMs and the N - gram LM for each direction are interpolated with weights of ( 0.375 , 0.375 , 0.25 ) . 3 ) In order to make use of LM training data that is not fully matched to the target conversational speech domain , we start RNNLM training with the union of in - domain ( here , CTS ) and out - of - domain ( e.g. , Web ) data . Upon convergence , the network undergoes a second training phase using the in - domain data only . Both training phases use in - domain validation data to regulate the learning rate schedule and termination . Because the size of the out - of - domain data is a multiple of the in - domain data , a standard training on a simple union of the data would not yield a well - matched model , and have poor perplexity in the target domain . 4 ) We found best results with an RNNLM configuration that had a second , non - recurrent hidden layer . This produced lower perplexity and word error than the standard , single - hidden - layer RNNLM architecture . The overall network architecture thus had two hidden layers with 1000 units each , using ReLU nonlinearities . Training used noise - contrastive estimation ( NCE ) . 5 ) The RNNLM output vocabulary consists of all words occurring more than once in the in - domain training set . While the RNNLM estimates a probability for unknown words , we take a different approach in rescoring : The number of out - of - set words is recorded for each hypothesis and a penalty for them is estimated for them when optimizing the relative weights for all model scores ( acoustic , LM , pronunciation ) , using the SRILM nbest - optimize tool . subsection : Training data The 4 - gram language model for decoding was trained on the available CTS transcripts from the DARPA EARS program : Switchboard ( 3 M words ) , BBN Switchboard - 2 transcripts ( 850k ) , Fisher ( 21 M ) , English CallHome ( 200k ) , and the University of Washington conversational Web corpus ( 191 M ) . A separate model was trained from each source and interpolated with weights optimized on RT - 03 transcripts . For the unpruned large rescoring 4 - gram , an additional LM component was added , trained on 133 M word of LDC Broadcast News texts . The N - gram LM configuration is modeled after that described in , except that maxent smoothing was used . The RNNLMs were trained on Switchboard and Fisher transcripts as in - domain data ( 20 M words for gradient computation , 3 M for validation ) . To this we added 62 M words of UW Web data as out - of - domain data , for use in the two - phase training procedure described above . subsection : RNNLM performance Table [ reference ] gives perplexity and word error performance for various RNNLM setups , from simple to more complex . The acoustic model used was the ResNet CNN . As can be seen , each of the measures described earlier adds incremental gains , which , while small individually , add up to a 9 % relative error reduction over a plain RNNLM . subsection : System Combination The LM rescoring is carried out separately for each acoustic model . The rescored N - best lists from each subsystem are then aligned into a single confusion network using the SRILM nbest - rover tool . However , the number of potential candidate systems is too large to allow an all - out combination , both for practical reasons and due to overfitting issues . Instead , we perform a greedy search , starting with the single best system , and successively adding additional systems , to find a small set of systems that are maximally complementary . The RT - 02 Switchboard set was used for this search procedure . The relative weighting ( for confusion - network mediated voting ) of the different systems is optimized using an EM algorithm , using the same data , and smoothed hierarchically by interpolating each set of system weights with the preceding one in the search . section : Experimental Setup and Results subsection : Speech corpora We train with the commonly used English CTS ( Switchboard and Fisher ) corpora . Evaluation is carried out on the NIST 2000 CTS test set , which comprises both Switchboard ( SWB ) and CallHome ( CH ) subsets . The Switchboard - 1 portion of the NIST 2002 CTS test set was used for tuning and development . The acoustic training data is comprised by LDC corpora 97S62 , 2004S13 , 2005S13 , 2004S11 and 2004S09 ; see for a full description . subsection : 1 - bit SGD Training All presented models are costly to train . To make training feasible , we parallelize training with our previously proposed 1 - bit SGD parallelization technique . This data - parallel method distributes minibatches over multiple worker nodes , and then aggregates the sub - gradients . While the necessary communication time would otherwise be prohibitive , the 1 - bit SGD method eliminates the bottleneck by two techniques : 1 - bit quantization of gradients and automatic minibatch - size scaling . In , we showed that gradient values can be quantized to just a single bit , if one carries over the quantization error from one minibatch to the next . Each time a sub - gradient is quantized , the quantization error is computed and remembered , and then added to the next minibatch \u2019s sub - gradient . This reduces the required bandwidth 32 - fold with minimal loss in accuracy . Secondly , automatic minibatch - size scaling progressively decreases the frequency of model updates . At regular intervals ( e.g. every 72h of training data ) , the trainer tries larger minibatch sizes on a small subset of data and picks the largest that maintains training loss . subsection : Acoustic Model Details Forty - dimensional log - filterbank features were extracted every 10 milliseconds , using a 25 - millisecond analysis window . The CNN models used window sizes as indicated in Table [ reference ] , and the LSTMs processed one frame of input at a time . The bulk of our models use three state left - to - right triphone models with 9000 tied states . Additionally , we have trained several models with 27k tied states . The phonetic inventory includes special models for noise , vocalized - noise , laughter and silence . We use a 30k - vocabulary derived from the most common words in the Switchboard and Fisher corpora . The decoder uses a statically compiled unigram graph , and dynamically applies the language model score . The unigram graph has about 300k states and 500k arcs . All acoustic models were trained using the open - source Computational Network Toolkit ( CNTK ) . Table [ reference ] shows the result of i - vector adaptation and LFMMI training on several of our systems . We achieve a 5\u20138 % relative improvement from i - vectors , including on CNN systems . The last row of Table [ reference ] shows the effect of LFMMI training on the different models . We see a consistent 7\u201310 % further relative reduction in error rate for all models . Considering the great increase in procedural simplicity of LFMMI over the previous practice of writing lattices and post - processing them , we consider LFMMI to be a significant advance in technology . subsection : Comparative System Performance Model performance for our individual models as well as relevant comparisons from the literature are shown in Table [ reference ] . Out of the 15 models built , only models given non - zero weight in the final system combination are shown . section : Relation to Prior Work Compared to earlier applications of CNNs to speech recognition , our networks are much deeper , and use linear bypass connections across convolutional layers . They are similar in spirit to those studied more recently by . We improve on these architectures with the LACE model , which iteratively expands the effective window size , layer - by - layer , and adds an attention mask to differentially weight distant context . Our use of lattice - free MMI is distinctive , and extends previous work by proposing the use of a mixed triphone / phoneme history in the language model . On the language modeling side , we achieve a performance boost by combining multiple RNNLMs in both forward and backward directions , and by using a two - phase training regimen to get best results from out - of - domain data . For our best CNN system , RNNLM rescoring yields a relative word error reduction of 20 % , and a 16 % relative gain for the combined recognition system . ( Elsewhere we report further improvements , using LSTM - based LMs . ) section : Conclusions We have described Microsoft \u2019s conversational speech recognition system for 2016 . The use of CNNs in the acoustic model has proved singularly effective , as has the use of RNN language models . Our best single system achieves an error rate of 6.9 % on the NIST 2000 Switchboard set . We believe this is the best performance reported to date for a recognition system not based on system combination . An ensemble of acoustic models advances the state of the art to 6.2 % on the Switchboard test data . Acknowledgments . We thank X. Chen from CUED for valuable assistance with the CUED - RNNLM toolkit , and ICSI for compute and data resources . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Switchboard___Hub500"]]], "Method": [[["Microsoft_2016"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Switchboard___Hub500"]]], "Method": [[["RNNLM"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Switchboard___Hub500"]]], "Method": [[["VGG_Resnet_LACE_BiLSTM_acoustic_model_trained_on_SWB_Fisher_CH__N-gram___RNNLM_language_model_trained_on_Switchboard_Fisher_Gigaword_Broadcast"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}, {"incident_type": "SciREX_incident", "Material": [[["swb_hub_500_WER_fullSWBCH"]]], "Method": [[["VGG_Resnet_LACE_BiLSTM_acoustic_model_trained_on_SWB_Fisher_CH__N-gram___RNNLM_language_model_trained_on_Switchboard_Fisher_Gigaword_Broadcast"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Speech_Recognition"]]]}]}
{"docid": "TST3-SREX-0046", "doctext": "document : Pyramid Scene Parsing Network Scene parsing is challenging for unrestricted open vocabulary and diverse scenes . In this paper , we exploit the capability of global context information by different - region - based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network ( PSPNet ) . Our global prior representation is effective to produce good quality results on the scene parsing task , while PSPNet provides a superior framework for pixel - level prediction . The proposed approach achieves state - of - the - art performance on various datasets . It came first in ImageNet scene parsing challenge 2016 , PASCAL VOC 2012 benchmark and Cityscapes benchmark . A single PSPNet yields the new record of mIoU accuracy 85.4 % on PASCAL VOC 2012 and accuracy 80.2 % on Cityscapes . section : Introduction Scene parsing , based on semantic segmentation , is a fundamental topic in computer vision . The goal is to assign each pixel in the image a category label . Scene parsing provides complete understanding of the scene . It predicts the label , location , as well as shape for each element . This topic is of broad interest for potential applications of automatic driving , robot sensing , to name a few . Difficulty of scene parsing is closely related to scene and label variety . The pioneer scene parsing task is to classify 33 scenes for 2 , 688 images on LMO dataset . More recent PASCAL VOC semantic segmentation and PASCAL context datasets include more labels with similar context , such as chair and sofa , horse and cow , etc . The new ADE20 K dataset is the most challenging one with a large and unrestricted open vocabulary and more scene classes . A few representative images are shown in Fig . [ reference ] . To develop an effective algorithm for these datasets needs to conquer a few difficulties . State - of - the - art scene parsing frameworks are mostly based on the fully convolutional network ( FCN ) . The deep convolutional neural network ( CNN ) based methods boost dynamic object understanding , and yet still face challenges considering diverse scenes and unrestricted vocabulary . One example is shown in the first row of Fig . [ reference ] , where a boat is mistaken as a car . These errors are due to similar appearance of objects . But when viewing the image regarding the context prior that the scene is described as boathouse near a river , correct prediction should be yielded . Towards accurate scene perception , the knowledge graph relies on prior information of scene context . We found that the major issue for current FCN based models is lack of suitable strategy to utilize global scene category clues . For typical complex scene understanding , previously to get a global image - level feature , spatial pyramid pooling was widely employed where spatial statistics provide a good descriptor for overall scene interpretation . Spatial pyramid pooling network further enhances the ability . Different from these methods , to incorporate suitable global features , we propose pyramid scene parsing network ( PSPNet ) . In addition to traditional dilated FCN for pixel prediction , we extend the pixel - level feature to the specially designed global pyramid pooling one . The local and global clues together make the final prediction more reliable . We also propose an optimization strategy with deeply supervised loss . We give all implementation details , which are key to our decent performance in this paper , and make the code and trained models publicly available . Our approach achieves state - of - the - art performance on all available datasets . It is the champion of ImageNet scene parsing challenge 2016 , and arrived the 1st place on PASCAL VOC 2012 semantic segmentation benchmark , and the 1st place on urban scene Cityscapes data . They manifest that PSPNet gives a promising direction for pixel - level prediction tasks , which may even benefit CNN - based stereo matching , optical flow , depth estimation , etc . in follow - up work . Our main contributions are threefold . We propose a pyramid scene parsing network to embed difficult scenery context features in an FCN based pixel prediction framework . We develop an effective optimization strategy for deep ResNet based on deeply supervised loss . We build a practical system for state - of - the - art scene parsing and semantic segmentation where all crucial implementation details are included . section : Related Work In the following , we review recent advances in scene parsing and semantic segmentation tasks . Driven by powerful deep neural networks , pixel - level prediction tasks like scene parsing and semantic segmentation achieve great progress inspired by replacing the fully - connected layer in classification with the convolution layer . To enlarge the receptive field of neural networks , methods of used dilated convolution . Noh proposed a coarse - to - fine structure with deconvolution network to learn the segmentation mask . Our baseline network is FCN and dilated network . Other work mainly proceeds in two directions . One line is with multi - scale feature ensembling . Since in deep networks , higher - layer feature contains more semantic meaning and less location information . Combining multi - scale features can improve the performance . The other direction is based on structure prediction . The pioneer work used conditional random field ( CRF ) as post processing to refine the segmentation result . Following methods refined networks via end - to - end modeling . Both of the two directions ameliorate the localization ability of scene parsing where predicted semantic boundary fits objects . Yet there is still much room to exploit necessary information in complex scenes . To make good use of global image - level priors for diverse scene understanding , methods of extracted global context information with traditional features not from deep neural networks . Similar improvement was made under object detection frameworks . Liu proved that global average pooling with FCN can improve semantic segmentation results . However , our experiments show that these global descriptors are not representative enough for the challenging ADE20 K data . Therefore , different from global pooling in , we exploit the capability of global context information by different - region - based context aggregation via our pyramid scene parsing network . section : Pyramid Scene Parsing Network We start with our observation and analysis of representative failure cases when applying FCN methods to scene parsing . They motivate proposal of our pyramid pooling module as the effective global context prior . Our pyramid scene parsing network ( PSPNet ) illustrated in Fig . [ reference ] is then described to improve performance for open - vocabulary object and stuff identification in complex scene parsing . subsection : Important Observations The new ADE20 K dataset contains 150 stuff / object category labels ( , wall , sky , and tree ) and 1 , 038 image - level scene descriptors ( , airport_terminal , bedroom , and street ) . So a large amount of labels and vast distributions of scenes come into existence . Inspecting the prediction results of the FCN baseline provided in , we summarize several common issues for complex - scene parsing . paragraph : Mismatched Relationship Context relationship is universal and important especially for complex scene understanding . There exist co - occurrent visual patterns . For example , an airplane is likely to be in runway or fly in sky while not over a road . For the first - row example in Fig . [ reference ] , FCN predicts the boat in the yellow box as a \u201c car \u201d based on its appearance . But the common knowledge is that a car is seldom over a river . Lack of the ability to collect contextual information increases the chance of misclassification . paragraph : Confusion Categories There are many class label pairs in the ADE20 K dataset that are confusing in classification . Examples are field and earth ; mountain and hill ; wall , house , building and skyscraper . They are with similar appearance . The expert annotator who labeled the entire dataset , still makes 17.60 % pixel error as described in . In the second row of Fig . [ reference ] , FCN predicts the object in the box as part of skyscraper and part of building . These results should be excluded so that the whole object is either skyscraper or building , but not both . This problem can be remedied by utilizing the relationship between categories . paragraph : Inconspicuous Classes Scene contains objects / stuff of arbitrary size . Several small - size things , like streetlight and signboard , are hard to find while they may be of great importance . Contrarily , big objects or stuff may exceed the receptive field of FCN and thus cause discontinuous prediction . As shown in the third row of Fig . [ reference ] , the pillow has similar appearance with the sheet . Overlooking the global scene category may fail to parse the pillow . To improve performance for remarkably small or large objects , one should pay much attention to different sub - regions that contain inconspicuous - category stuff . To summarize these observations , many errors are partially or completely related to contextual relationship and global information for different receptive fields . Thus a deep network with a suitable global - scene - level prior can much improve the performance of scene parsing . subsection : Pyramid Pooling Module With above analysis , in what follows , we introduce the pyramid pooling module , which empirically proves to be an effective global contextual prior . In a deep neural network , the size of receptive field can roughly indicates how much we use context information . Although theoretically the receptive field of ResNet is already larger than the input image , it is shown by Zhou that the empirical receptive field of CNN is much smaller than the theoretical one especially on high - level layers . This makes many networks not sufficiently incorporate the momentous global scenery prior . We address this issue by proposing an effective global prior representation . Global average pooling is a good baseline model as the global contextual prior , which is commonly used in image classification tasks . In , it was successfully applied to semantic segmentation . But regarding the complex - scene images in ADE20 K , this strategy is not enough to cover necessary information . Pixels in these scene images are annotated regarding many stuff and objects . Directly fusing them to form a single vector may lose the spatial relation and cause ambiguity . Global context information along with sub - region context is helpful in this regard to distinguish among various categories . A more powerful representation could be fused information from different sub - regions with these receptive fields . Similar conclusion was drawn in classical work of scene / image classification . In , feature maps in different levels generated by pyramid pooling were finally flattened and concatenated to be fed into a fully connected layer for classification . This global prior is designed to remove the fixed - size constraint of CNN for image classification . To further reduce context information loss between different sub - regions , we propose a hierarchical global prior , containing information with different scales and varying among different sub - regions . We call it pyramid pooling module for global scene prior construction upon the final - layer - feature - map of the deep neural network , as illustrated in part ( c ) of Fig . [ reference ] . The pyramid pooling module fuses features under four different pyramid scales . The coarsest level highlighted in red is global pooling to generate a single bin output . The following pyramid level separates the feature map into different sub - regions and forms pooled representation for different locations . The output of different levels in the pyramid pooling module contains the feature map with varied sizes . To maintain the weight of global feature , we use convolution layer after each pyramid level to reduce the dimension of context representation to of the original one if the level size of pyramid is . Then we directly upsample the low - dimension feature maps to get the same size feature as the original feature map via bilinear interpolation . Finally , different levels of features are concatenated as the final pyramid pooling global feature . Noted that the number of pyramid levels and size of each level can be modified . They are related to the size of feature map that is fed into the pyramid pooling layer . The structure abstracts different sub - regions by adopting varying - size pooling kernels in a few strides . Thus the multi - stage kernels should maintain a reasonable gap in representation . Our pyramid pooling module is a four - level one with bin sizes of , , and respectively . For the type of pooling operation between max and average , we perform extensive experiments to show the difference in Section [ reference ] . subsection : Network Architecture With the pyramid pooling module , we propose our pyramid scene parsing network ( PSPNet ) as illustrated in Fig . [ reference ] . Given an input image in Fig . [ reference ] ( a ) , we use a pretrained ResNet model with the dilated network strategy to extract the feature map . The final feature map size is of the input image , as shown in Fig . [ reference ] ( b ) . On top of the map , we use the pyramid pooling module shown in ( c ) to gather context information . Using our 4 - level pyramid , the pooling kernels cover the whole , half of , and small portions of the image . They are fused as the global prior . Then we concatenate the prior with the original feature map in the final part of ( c ) . It is followed by a convolution layer to generate the final prediction map in ( d ) . To explain our structure , PSPNet provides an effective global contextual prior for pixel - level scene parsing . The pyramid pooling module can collect levels of information , more representative than global pooling . In terms of computational cost , our PSPNet does not much increase it compared to the original dilated FCN network . In end - to - end learning , the global pyramid pooling module and the local FCN feature can be optimized simultaneously . section : Deep Supervision for ResNet - Based FCN Deep pretrained networks lead to good performance . However , increasing depth of the network may introduce additional optimization difficulty as shown in for image classification . ResNet solves this problem with skip connection in each block . Latter layers of deep ResNet mainly learn residues based on previous ones . We contrarily propose generating initial results by supervision with an additional loss , and learning the residue afterwards with the final loss . Thus , optimization of the deep network is decomposed into two , each is simpler to solve . An example of our deeply supervised ResNet101 model is illustrated in Fig . [ reference ] . Apart from the main branch using softmax loss to train the final classifier , another classifier is applied after the fourth stage , i.e. , the res4b22 residue block . Different from relay backpropagation that blocks the backward auxiliary loss to several shallow layers , we let the two loss functions pass through all previous layers . The auxiliary loss helps optimize the learning process , while the master branch loss takes the most responsibility . We add weight to balance the auxiliary loss . In the testing phase , we abandon this auxiliary branch and only use the well optimized master branch for final prediction . This kind of deeply supervised training strategy for ResNet - based FCN is broadly useful under different experimental settings and works with the pre - trained ResNet model . This manifests the generality of such a learning strategy . More details are provided in Section [ reference ] . section : Experiments Our proposed method is successful on scene parsing and semantic segmentation challenges . We evaluate it in this section on three different datasets , including ImageNet scene parsing challenge 2016 , PASCAL VOC 2012 semantic segmentation and urban scene understanding dataset Cityscapes . subsection : Implementation Details For a practical deep learning system , devil is always in the details . Our implementation is based on the public platform Caffe . Inspired by , we use the \u201c poly \u201d learning rate policy where current learning rate equals to the base one multiplying . We set base learning rate to 0.01 and power to 0.9 . The performance can be improved by increasing the iteration number , which is set to 150 K for ImageNet experiment , 30 K for PASCAL VOC and 90 K for Cityscapes . Momentum and weight decay are set to 0.9 and 0.0001 respectively . For data augmentation , we adopt random mirror and random resize between 0.5 and 2 for all datasets , and additionally add random rotation between - 10 and 10 degrees , and random Gaussian blur for ImageNet and PASCAL VOC . This comprehensive data augmentation scheme makes the network resist overfitting . Our network contains dilated convolution following . During the course of experiments , we notice that an appropriately large \u201c cropsize \u201d can yield good performance and \u201c batchsize \u201d in the batch normalization layer is of great importance . Due to limited physical memory on GPU cards , we set the \u201c batchsize \u201d to 16 during training . To achieve this , we modify Caffe from together with branch and make it support batch normalization on data gathered from multiple GPUs based on OpenMPI . For the auxiliary loss , we set the weight to 0.4 in experiments . subsection : ImageNet Scene Parsing Challenge 2016 paragraph : Dataset and Evaluation Metrics The ADE20 K dataset is used in ImageNet scene parsing challenge 2016 . Different from other datasets , ADE20 K is more challenging for the up to 150 classes and diverse scenes with a total of 1 , 038 image - level labels . The challenge data is divided into 20K / 2K / 3 K images for training , validation and testing . Also , it needs to parse both objects and stuff in the scene , which makes it more difficult than other datasets . For evaluation , both pixel - wise accuracy ( Pixel Acc . ) and mean of class - wise intersection over union ( Mean IoU ) are used . paragraph : Ablation Study for PSPNet To evaluate PSPNet , we conduct experiments with several settings , including pooling types of max and average , pooling with just one global feature or four - level features , with and without dimension reduction after the pooling operation and before concatenation . As listed in Table [ reference ] , average pooling works better than max pooling in all settings . Pooling with pyramid parsing outperforms that using global pooling . With dimension reduction , the performance is further enhanced . With our proposed PSPNet , the best setting yields results 41.68 / 80.04 in terms of Mean IoU and Pixel Acc . ( % ) , exceeding global average pooling of 40.07 / 79.52 as idea in Liu by 1.61 / 0.52 . And compared to the baseline , PSPNet outperforming it by 4.45 / 2.03 in terms of absolute improvement and 11.95 / 2.60 in terms of relative difference . paragraph : Ablation Study for Auxiliary Loss The introduced auxiliary loss helps optimize the learning process while not influencing learning in the master branch . We experiment with setting the auxiliary loss weight between 0 and 1 and show the results in Table [ reference ] . The baseline uses ResNet50 - based FCN with dilated network , with the master branch \u2019s softmax loss for optimization . Adding the auxiliary loss branch , = 0.4 yields the best performance . It outperforms the baseline with an improvement of 1.41 / 0.94 in terms of Mean IoU and Pixel Acc . ( % ) . We believe deeper networks will benefit more given the new augmented auxiliary loss . paragraph : Ablation Study for Pre - trained Model Deeper neural networks have been shown in previous work to be beneficial to large scale data classification . To further analyze PSPNet , we conduct experiments for different depths of pre - trained ResNet . We test four depths of { 50 , 101 , 152 , 269}. As shown in Fig . [ reference ] , with the same setting , increasing the depth of ResNet from 50 to 269 can improve the score of ( Mean IoU + Pixel Acc . ) / 2 ( % ) from 60.86 to 62.35 , with 1.49 absolute improvement . Detailed scores of PSPNet pre - trained from different depth ResNet models are listed in Table [ reference ] . paragraph : More Detailed Performance Analysis We show our more detailed analysis on the validation set of ADE20 K in Table [ reference ] . All our results except the last - row one use single - scale test . \u201c ResNet269 + DA + AL + PSP + MS \u201d uses multi - scale testing . Our baseline is adapted from ResNet50 with dilated network , which yields MeanIoU 34.28 and Pixel Acc . 76.35 . It already outperforms other prior systems possibly due to the powerful ResNet . Our proposed architecture makes further improvement compared to the baseline . Using data augmentation , our result exceeds the baseline by 1.54 / 0.72 and reaches 35.82 / 77.07 . Using the auxiliary loss can further improve it by 1.41 / 0.94 and reaches 37.23 / 78.01 . With PSPNet , we notice relatively more significant progress for improvement of 4.45 / 2.03 . The result reaches 41.68 / 80.04 . The difference from the baseline result is 7.40 / 3.69 in terms of absolute improvement and 21.59 / 4.83 ( % ) in terms of relativity . A deeper network of ResNet269 yields even higher performance up to 43.81 / 80.88 . Finally , the multi - scale testing scheme moves the scores to 44.94 / 81.69 . paragraph : Results in Challenge Using the proposed architecture , our team came in the 1st place in ImageNet scene parsing challenge 2016 . Table [ reference ] shows a few results in this competition . Our ensemble submission achieves score 57.21 % on the testing set . Our single - model yields score 55.38 % , which is even higher than a few other multi - model ensemble submissions . This score is lower than that on the validation set possibly due to the difference of data distributions between validation and testing sets . As shown in column ( d ) of Fig . [ reference ] , PSPNet solves the common problems in FCN . Fig . [ reference ] shows another few parsing results on validation set of ADE20K. Our results contain more accurate and detailed structures compared to the baseline . subsection : PASCAL VOC 2012 Our PSPNet also works satisfyingly on semantic segmentation . We carry out experiments on the PASCAL VOC 2012 segmentation dataset , which contains 20 object categories and one background class . Following the procedure of , we use augmented data with the annotation of resulting 10 , 582 , 1 , 449 and 1 , 456 images for training , validation and testing . Results are shown in Table [ reference ] , we compare PSPNet with previous best - performing methods on the testing set based on two settings , i.e. , with or without pre - training on MS - COCO dataset . Methods pre - trained with MS - COCO are marked by \u2018 \u2020 \u2019 . For fair comparison with current ResNet based frameworks in scene parsing / semantic segmentation task , we build our architecture based on ResNet101 while without post - processing like CRF . We evaluate PSPNet with several - scale input and use the average results following . As shown in Table [ reference ] , PSPNet outperforms prior methods on both settings . Trained with only VOC 2012 data , we achieve 82.6 % accuracy \u2013 we get the highest accuracy on all 20 classes . When PSPNet is pre - trained with MS - COCO dataset , it reaches 85.4 % accuracy where 19 out of the 20 classes receive the highest accuracy . Intriguingly , our PSPNet trained with only VOC 2012 data outperforms existing methods trained with the MS - COCO pre - trained model . One may argue that our based classification model is more powerful than several prior methods since ResNet was recently proposed . To exhibit our unique contribution , we show that our method also outperforms state - of - the - art frameworks that use the same model , including FCRNs , LRR , and DeepLab . In this process , we even do not employ time - consuming but effective post - processing , such as CRF , as that in . Several examples are shown in Fig . [ reference ] . For \u201c cows \u201d in row one , our baseline model treats it as \u201c horse \u201d and \u201c dog \u201d while PSPNet corrects these errors . For \u201c aeroplane \u201d and \u201c table \u201d in the second and third rows , PSPNet finds missing parts . For \u201c person \u201d , \u201c bottle \u201d and \u201c plant \u201d in following rows , PSPNet performs well on these small - size - object classes in the images compared to the baseline model . More visual comparisons between PSPNet and other methods are included in Fig . [ reference ] . subsection : Cityscapes Cityscapes is a recently released dataset for semantic urban scene understanding . It contains 5 , 000 high quality pixel - level finely annotated images collected from 50 cities in different seasons . The images are divided into sets with numbers 2 , 975 , 500 , and 1 , 525 for training , validation and testing . It defines 19 categories containing both stuff and objects . Also , 20 , 000 coarsely annotated images are provided for two settings in comparison , i.e. , training with only fine data or with both the fine and coarse data . Methods trained using both fine and coarse data are marked with \u2018 \u2019 . Detailed results are listed in Table [ reference ] . Our base model is ResNet101 as in DeepLab for fair comparison and the testing procedure follows Section [ reference ] . Statistics in Table [ reference ] show that PSPNet outperforms other methods with notable advantage . Using both fine and coarse data for training makes our method yield 80.2 accuracy . Several examples are shown in Fig . [ reference ] . Detailed per - class results on testing set are shown in Table [ reference ] . section : Concluding Remarks We have proposed an effective pyramid scene parsing network for complex scene understanding . The global pyramid pooling feature provides additional contextual information . We have also provided a deeply supervised optimization strategy for ResNet - based FCN network . We hope the implementation details publicly available can help the community adopt these useful strategies for scene parsing and semantic segmentation and advance related techniques . section : Acknowledgements We would like to thank Gang Sun and Tong Xiao for their help in training the basic classification models , Qun Luo for technical support . This work is supported by a grant from the Research Grants Council of the Hong Kong SAR ( project No . 2150760 ) . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["ADE20K"]]], "Method": [[["PSPNet"]]], "Metric": [[["Test_Score"]]], "Task": [[["Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ADE20K"]]], "Method": [[["PSPNet"]]], "Metric": [[["Validation_mIoU"]]], "Task": [[["Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CamVid"]]], "Method": [[["PSPNet"]]], "Metric": [[["Frame__fps_"]]], "Task": [[["Real-Time_Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CamVid"]]], "Method": [[["PSPNet"]]], "Metric": [[["Mean_IoU"]]], "Task": [[["Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CamVid"]]], "Method": [[["PSPNet"]]], "Metric": [[["Time__ms_"]]], "Task": [[["Real-Time_Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["CamVid"]]], "Method": [[["PSPNet"]]], "Metric": [[["mIoU"]]], "Task": [[["Real-Time_Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes"]]], "Method": [[["PSPNet"]]], "Metric": [[["Frame__fps_"]]], "Task": [[["Real-Time_Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes"]]], "Method": [[["PSPNet"]]], "Metric": [[["Mean_IoU"]]], "Task": [[["Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes"]]], "Method": [[["PSPNet"]]], "Metric": [[["Time__ms_"]]], "Task": [[["Real-Time_Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes"]]], "Method": [[["PSPNet"]]], "Metric": [[["mIoU"]]], "Task": [[["Real-Time_Semantic_Segmentation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2012"]]], "Method": [[["PSPNet"]]], "Metric": [[["Mean_IoU"]]], "Task": [[["Semantic_Segmentation"]]]}]}
{"docid": "TST3-SREX-0047", "doctext": "document : ArtTrack : Articulated Multi - person Tracking in the Wild In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos . Our starting point is a model that resembles existing architectures for single - frame pose estimation but is substantially faster . We achieve this in two ways : ( 1 ) by simplifying and sparsifying the body - part relationship graph and leveraging recent methods for faster inference , and ( 2 ) by offloading a substantial share of computation onto a feed - forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter . We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio - temporal grouping of such proposals . This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only . We report results on a public \u201c MPII Human Pose \u201d benchmark and on a new \u201c MPII Video Pose \u201d dataset of image sequences with multiple people . We demonstrate that our model achieves state - of - the - art results while using only a fraction of time and is able to leverage temporal information to improve state - of - the - art for crowded scenes . section : Introduction This paper addresses the task of articulated human pose tracking in monocular video . We focus on scenes of realistic complexity that often include fast motions , large variability in appearance and clothing , and person - person occlusions . A successful approach must thus identify the number of people in each video frame , determine locations of the joints of each person and associate the joints over time . One of the key challenges in such scenes is that people might overlap and only a subset of joints of the person might be visible in each frame either due to person - person occlusion or truncation by image boundaries ( . Fig . [ reference ] ) . Arguably , resolving such cases correctly requires reasoning beyond purely geometric information on the arrangement of body joints in the image , and requires incorporation of a variety of image cues and joint modeling of several persons . The design of our model is motivated by two factors . We would like to leverage bottom - up end - to - end learning to directly capture image information . At the same time we aim to address a complex multi - person articulated tracking problem that does not naturally lend itself to an end - to - end prediction task and for which training data is not available in the amounts usually required for end - to - end learning . To leverage the available image information we learn a model for associating a body joint to a specific person in an end - to - end fashion relying on a convolutional network . We then incorporate these part - to - person association responses into a framework for jointly reasoning about assignment of body joints within the image and over time . To that end we use the graph partitioning formulation that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking . To facilitate efficient inference in video we resort to fast inference methods based on local combinatorial optimization and aim for a sparse model that keeps the number of connections between variables to a minimum . As we demonstrate , in combination with feed - forward reasoning for joint - to - person association this allows us to achieve substantial speed - ups compared to state - of - the - art while maintaining the same level of accuracy . The main contribution of this work is a new articulated tracking model that operates by bottom - up assembly of part detections within each frame and over time . In contrast to this model is suitable for scenes with an unknown number of subjects and reasons jointly across multiple people incorporating inter - person exclusion constraints and propagating strong observations to neighboring frames . Our second contribution is a formulation for single - frame pose estimation that relies on a sparse graph between body parts and a mechanism for generating body - part proposals conditioned on a person \u2019s location . This is in contrast to state - of - the - art approaches that perform expensive inference in a full graph and rely on generic bottom - up proposals . We demonstrate that a sparse model with a few spatial edges performs competitively with a fully - connected model while being much more efficient . Notably , a simple model that operates in top - down / bottom - up fashion exceeds the performance of a fully - connected model while being x faster at inference time ( cf . Tab . [ reference ] ) . This is due to offloading of a large share of the reasoning about body - part association onto a feed - forward convolutional architecture . Finally , we contribute a new challenging dataset for evaluation of articulated body joint tracking in crowded realistic environments with multiple overlapping people . Related work . Convolutional networks have emerged as an effective approach to localizing body joints of people in images and have also been extended for joint estimation of body configurations over time , and 3D pose estimation in outdoor environments in multi - camera setting . Current approaches are increasingly effective for estimating body configurations of single people achieving high accuracies on this task , but are still failing on fast moving and articulated limbs . More complex recent models jointly reason about entire scenes , but are too complex and inefficient to directly generalize to image sequences . Recent feed - forward models are able to jointly infer body joints of the same person and even operate over time but consider isolated persons only and do not generalize to the case of multiple overlapping people . Similarly , consider a simplified task of tracking upper body poses of isolated upright individuals . We build on recent CNN detectors that are effective in localizing body joints in cluttered scenes and explore different mechanisms for assembling the joints into multiple person configurations . To that end we rely on a graph partitioning approach closely related to . In contrast to who focus on pedestrian tracking , and who perform single frame multi - person pose estimation , we solve a more complex problem of articulated multi - person pose tracking . Earlier approaches to articulated pose tracking in monocular videos rely on hand - crafted image representations and focus on simplified tasks , such as tracking upper body poses of frontal isolated people , or tracking walking pedestrians with little degree of articulation . In contrast , we address a harder problem of multi - person articulated pose tracking and do not make assumptions about the type of body motions or activities of people . Our approach is closely related to who propose a similar formulation based on graph partitioning . Our approach differs from primarily in the type of body - part proposals and the structure of the spatio - temporal graph . In our approach we introduce a person - conditioned model that is trained to associate body parts of a specific person already at the detection stage . This is in contrast to the approach of that relies on the generic body - part detectors . Overview . Our model consists of the two components : ( 1 ) a convolutional network for generating body part proposals and ( 2 ) an approach to group the proposals into spatio - temporal clusters . In Sec . [ reference ] we introduce a general formulation for multi - target tracking that follows and allows us to define pose estimation and articulated tracking in a unified framework . We then describe the details of our articulated tracking approach in Sec . [ reference ] , and introduce two variants of our formulation : bottom - up ( BU ) and top - down / bottom - up ( TD / BU ) . We present experimental results in Sec . [ reference ] . section : Tracking by Spatio - temporal Grouping Our body part detector generates a set of proposals for each frame of the video . Each proposal is given by , where denotes the index of the video frame , is the spatial location of the proposal in image coordinates , is the probability of correct detection , and is the type of the body joint ( ankle or shoulder ) . Let be a graph whose nodes are the joint detections in a video and whose edges connect pairs of detections that hypothetically correspond to the same target . The output of the tracking algorithm is a subgraph of , where is a subset of nodes after filtering redundant and erroneous detections and are edges linking nodes corresponding to the same target . We specify via binary variables and that define subsets of edges and nodes included in . In particular each track will correspond to a connected component in . As a general way to introduce constraints on edge configurations that correspond to a valid tracking solution we introduce a set and define a combination of edge and node indicator variables to be feasible if and only if . An example of a constraint encoded through is that endpoint nodes of an edge included by must also be included by . Note that the variables and are coupled though . Moreover , assuming that we are free to set components of and independently to maximize the tracking objective . Given image observations we compute a set of features for each node and edge in the graph . We denote such node and edge features as and respectively . Assuming independence of the feature vectors the conditional probability of indicator functions of nodes and of edges given features and and given a feasible set is given by where assigns a constant non - zero probability to every feasible solution and is equal to zero otherwise . Minimizing the negative log - likelihood of Eq . [ reference ] is equivalent to solving the following integer - linear program : where is the cost of retaining as part of the solution , and is the cost of assigning the detections linked by an edge to the same track . We define the set of constraints as in : Jointly with the objective in Eq . [ reference ] the constraints ( [ reference ] ) - ( [ reference ] ) define an instance of the minimum cost subgraph multicut problem . The constraints ( [ reference ] ) and ( [ reference ] ) ensure that assignment of node and edge variables is consistent . The constraint ( [ reference ] ) ensures that for every two nodes either all or none of the paths between these nodes in graph are contained in one of the connected components of subgraph . This constraint is necessary to unambigously assign person identity to a body part proposal based on its membership in a specific connnected component of . section : Articulated Multi - person Tracking In Sec . [ reference ] we introduced a general framework for multi - object tracking by solving an instance of the subgraph multicut problem . The subgraph multicut problem is NP - hard , but recent work has shown that efficient approximate inference is possible with local search methods . The framework allows for a variety of graphs and connectivity patterns . Simpler connectivity allows for faster and more efficient processing at the cost of ignoring some of the potentially informative dependencies between model variables . Our goal is to design a model that is efficient , with as few edges as possible , yet effective in crowded scenes , and that allows us to model temporal continuity and inter - person exclusion . Our articulated tracking approach proceeds by constructing a graph that couples body part proposals within the same frame and across neighboring frames . In general the graph will have three types of edges : ( 1 ) cross - type edges shown in Fig . [ reference ] ( a ) and Fig . [ reference ] ( b ) that connect two parts of different types , ( 2 ) same - type edges shown in Fig . [ reference ] ( b ) that connect two nodes of the same type in the same image , and ( 3 ) temporal edges shown in Fig . [ reference ] ( c ) that connect nodes in the neighboring frames . We now define two variants of our model that we denote as Bottom - Up ( BU ) and Top - Down / Bottom - Up ( TD / BU ) . In the BU model the body part proposals are generated with our publicly available convolutional part detector . In the TD / BU model we substitute these generic part detectors with a new convolutional body - part detector that is trained to output consistent body configurations conditioned on the person location . This alows to further reduce the complexity of the model graph since the task of associating body parts is addressed within the proposal mechanism . As we show in Sec . [ reference ] this leads to considerable gains in performance and allows for faster inference . Note that the BU and TD / BU models have identical same - type and temporal pairwise terms , but differ in the form of cross - type pairwise terms , and the connectivity of the nodes in . For both models we rely on the solver from for inference . subsection : Bottom - Up Model ( BU ) . For each body part proposal the detector outputs image location , probability of detection , and a label that indicates the type of the detected part ( shoulder or ankle ) . We directly use the probability of detection to derive the unary costs in Eq . [ reference ] as . Image features in this case correspond to the image representation generated by the convolutional network . We consider two connectivity patterns for nodes in the graph . We either define edges for every pair of proposals which results in a fully connected graph in each image . Alternatively we obtain a sparse version of the model by defining edges for a subset of part types only as is shown in Fig . [ reference ] ( a ) . The rationale behind the sparse version is to obtain a simpler and faster version of the model by omitting edges between parts that carry little information about each other \u2019s image location ( left ankle and right arm ) . Edge costs . In our Bottom - Up model the cost of the edges connecting two body part detections and is defined as a function of the detection types and . Following we thus train for each pair of part types a regression function that predicts relative image location of the parts in the pair . The cost is given by the output of the logistic regression given the features computed from offset and angle of the predicted and actual location of the other joint in the pair . We refer to for more details on these pairwise terms . Note that our model generalizes in that the edge cost depends on the type of nodes linked by the edge . It also generalizes by allowing to be sparse . This is achieved by reformulating the model with a more general type of cycle constraint ( [ reference ] ) , in contrast to simple triangle inequalities used in . subsection : Top - Down / Bottom - up Model ( TD / BU ) We now introduce a version of our model that operates by first generating body part proposals conditioned on the locations of people in the image and then performing joint reasoning to group these proposals into spatio - temporal clusters corresponding to different people . We follow the intuition that it is considerably easier to identify and detect individual people ( e.g. by detecting their heads ) compared to correctly associating body parts such as ankles and wrists to each person . We select person \u2019s head as a root part that is responsible for representing the person location , and delegate the task of identifying body parts of the person corresponding to a head location to a convolutional network . The structure of TD / BU model is illustrated in Fig . [ reference ] ( b ) for the simplified case of two distinct head detections . Let us denote the set of all root part detections as . For each pair of the root nodes we explicitly set the corresponding edge indicator variables . This implements a \u201c must - not - link \u201d constraint between these nodes , and in combination with the cycle inequality ( [ reference ] ) implies that each proposal can be connected to one of the \u201c person nodes \u201d only . The cost for an edge connecting detection proposal and a \u201c person node \u201d is based on the conditional distribution generated by the convolutional network . The output of such network is a set of conditional distributions , one for each node type . We augment the graph with attractive / repulsive and temporal terms as described in Sec . [ reference ] and Sec . [ reference ] and set the unary costs for all indicator variables to a constant . Any proposal not connected to any of the root nodes is excluded from the final solution . We use the solver from for consistency , but a simpler KL - based solver as in could be used as well since the TD / BU model effectively ignores the unary variables . The processing stages of TD / BU model are shown in Fig . [ reference ] . Note that the body - part heatmaps change depending on the person - identity signal provided by the person \u2019s neck , and that the bottom - up step was able to correct the predictions on the forearms of the front person . Implementation details . For head detection , we use a version of our model that contains the two head parts ( neck and head top ) . This makes our TD / BU model related to the hierarchical model defined in that also uses easier - to - detect parts to guide the rest of the inference process . However here we replace all the stages in the hierarchical inference except the first one with a convolutional network . The structure of the convolutional network used to generate person - conditioned proposals is shown on Fig . [ reference ] . The network uses the ResNet - 101 from that we modify to bring the stride of the network down to 8 pixels . The network generates predictions for all body parts after the conv4_4 block . We use the cross - entropy binary classification loss at this stage to predict the part heatmaps . At each training iteration we forward pass an image with multiple people potentially in close proximity to each other . We select a single person from the image and condition the network on the person \u2019s neck location by zeroing out the heatmap of the neck joint outside the ground - truth region . We then pass the neck heatmap through a convolutional layer to match the dimensionality of the feature channels and add them to the main stream of the ResNet . We finally add a joint prediction layer at the end of the network with a loss that considers predictions to be correct only if they correspond to the body joints of the selected person . Spatial propagation ( SP ) . In our network the person identity signal is provided by the location of the head . In principle the receptive field size of the network is large enough to propagate this signal to all body parts . However we found that it is useful to introduce an additional mechanism to propagate the person identity signal . To that end we inject intermediate supervision layers for individual body parts arranged in the order of kinematic proximity to the root joint ( Fig . [ reference ] ) . We place prediction layers for shoulders at conv4_8 , for elbows and hips at conv4_14 and for knees at conv4_18 . We empirically found that such an explicit form of spatial propagation significantly improves performance on joints such as ankles , that are typically far from the head in the image space ( see Tab . [ reference ] for details ) . Training . We use Caffe \u2019s ResNet implementation and initialize from the ImageNet - pre - trained models . Networks are trained on the MPII Human Pose dataset with SGD for 1 M iterations with stepwise learning rate ( lr=0.002 for 400k , lr=0.0002 for 300k and lr=0.0001 for 300k ) . subsection : Attractive / Repulsive Edges Attractive / repulsive edges are defined between two proposals of the same type within the same image . The costs of these edges is inversely - proportional to distance . The decision to group two nodes is made based on the evidence from the entire image , which is in contrast to typical non - maximum suppression based on the state of just two detections . Inversely , these edges prevent grouping of multiple distant hypothesis of the same type , prevent merging two heads of different people . subsection : Temporal Model Regardless of the type of within frame model ( BU or TD / BU ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames . We derive the costs for such temporal edges via logistic regression . Given the feature vector the probability that the two proposals and in adjacent frames correspond to the same body part is given by : , where , and - , is Euclidean distance between the SIFT descriptors computed at and , and and measure the agreement with the dense motion field computed with the DeepMatching approach of . For SIFT features we specify the location of the detection proposal , but rely on SIFT to identify the local orientation . In cases with multiple local maxima in orientation estimation we compute SIFT descriptor for each orientation and set to the minimal distance among all pairs of descriptors . We found that this makes the SIFT distance more robust in the presence of rotations of the body limbs . We define the features and as in . Let be an squared image region centered on the part proposal . We define as a ratio of the number of point correspondences between the regions and and the total number of point correspondences in either of them . Specifically , let be a set of point correspondences between the two images computed with DeepMatching , where and and denote the corresponding points in the first and second image respectively . Using this notation we define : The rationale behind computing by aggregating across multiple correspondences is to make the feature robust to outliers and to inaccuracies in body part detection . is defined analogously , but using the DeepMatching correspondences obtained by inverting the order of images . Discussion . As we demonstrate in Sec . [ reference ] , we found the set of features described above to be complementary to each other . Euclidean distance between proposals is informative for finding correspondences for slow motions , but fails for faster motions and in the presence of multiple people . DeepMatching is usually effective in finding corresponding regions between the two images , but occasionally fails in the case of sudden background changes due to fast motion or large changes in body limb orientation . In these cases SIFT is often still able to provide a meaningful measure of similarity due to its rotation invariance . section : Experiments subsection : Datasets and evaluation measure Single frame . We evaluate our single frame models on the MPII Multi - Person dataset . We report all intermediate results on a validation set of images sampled uniformly at random ( MPII Multi - Person Val ) , while major results and comparison to the state of the art are reported on the test set . Video . In order to evaluate video - based models we introduce a novel \u201c MPII Video Pose \u201d dataset . To this end we manually selected challenging keyframes from MPII Multi - Person dataset . Selected keyframes represent crowded scenes with highly articulated people engaging in various dynamic activities . In addition to each keyframe , we include + /- neighboring frames from the corresponding publicly available video sequences , and annotate every second frame . Each body pose was annotated following the standard annotation procedure , while maintaining person identity throughout the sequence . In contrast to MPII Multi - Person where some frames may contain non - annotated people , we annotate all people participating in the activity captured in the video , and add ignore regions for areas that contain dense crowds ( e.g. static spectators in the dancing sequences ) . In total , our dataset consists of sequences with over annotated poses . Evaluation details . The average precision ( AP ) measure is used for evaluation of pose estimation accuracy . For each algorithm we also report run time of the proposal generation and of the graph partitioning stages . All time measurements were conducted on a single core Intel Xeon GHz . Finally we also evaluate tracking perfomance using standard MOTA metric . Evaluation on our \u201c MPII Video Pose \u201d dataset is performed on the full frames using the publicly available evaluation kit of . On MPII Multi - Person we follow the official evaluation protocol and evaluate on groups using the provided rough group location and scale . subsection : Single - frame models We compare the performance of different variants of our Bottom - Up ( BU ) and Top - Down / Bottom - Up ( TD / BU ) models introduced in Sec . [ reference ] and Sec . [ reference ] . For BU we consider a model that ( 1 ) uses a fully - connected graph with up to detection proposals and jointly performs partitioning and body - part labeling similar to ( BU - full , label ) ; ( 2 ) is same as ( 1 ) , but labeling of detection proposals is done based on detection score ( BU - full ) ; ( 3 ) is same as ( 2 ) , but uses a sparsely - connected graph ( BU - sparse ) . The results are shown in Tab . [ reference ] . BU - full , label achieves % AP with a median inference run - time of s / f . BU - full achieves run - time reduction ( vs. s / f ) : pre - labeling detection candidates based on detection score significantly reduces the number of variables in the problem graph . Interestingly , pre - labeling also improves the performance ( vs. % AP ) : some of the low - scoring detections may complicate the search for an optimal labeling . BU - sparse further reduces run - time ( vs. s / f ) , as it reduces the complexity of the initial problem by sparsifying the graph , at a price of a drop in performance ( vs. % AP ) . In Tab . [ reference ] we compare the variants of the TD / BU model . Our TD approach achieves % AP , performing on par with a more complex BU - full . Explicit spatial propagation ( TD + SP ) further improves the results ( vs. % AP ) . The largest improvement is observed for ankles : progressive prediction that conditions on the close - by parts in the tree hierarchy reduces the distance between the conditioning signal and the location of the predicted body part and simplifies the prediction task . Performing inference ( TD / BU + SP ) improves the performance to % AP , due to more optimal assignment of part detection candidates to corresponding persons . Graph simplification in TD / BU allows to further reduce the inference time for graph partitioning ( vs. for BU - sparse ) . Single Frame Tracking Single Frame Tracking Comparison to the State of the Art . We compare the proposed single - frame approaches to the state of the art on MPII Multi - Person Test and WAF datasets . Comparison on MPII is shown in Tab . [ reference ] . Both BU - full and TD / BU improve over the best published result of DeeperCut , achieving and % AP respectively vs. % AP by DeeperCut . For the TD / BU the improvements on articulated parts ( elbows , wrists , ankles , knees ) are particularly pronounced . We argue that this is due to using the network that is directly trained to disambiguate body parts of different people , instead of using explicit geometric pairwise terms that only serve as a proxy to person \u2019s identity . Overall , the performance of our best TD / BU method is noticeably higher ( vs. % AP ) . Remarkably , its run - time of graph partitioning stage is orders of magnitude faster compared to DeeperCut . This speed - up is due to two factors . First , TD / BU relies on a faster solver that tackles the graph - partitioning problem via local search , in contrast to the exact solver used in . Second , in the case of TD / BU model the graph is sparse and a large portion of the computation is performed by the feed - forward CNN introduced in Sec . [ reference ] . On WAF dataset TD / BU substantially improves over the best published result ( vs. % AP by ) . We refer to supplemental material for details . subsection : Multi - frame models Comparison of video - based models . Performance of the proposed video - based models is compared in Tab . [ reference ] . Video - based models outperform single - frame models in each case . BU - full + temporal slightly outperforms BU - full , where improvements are noticeable for ankle , knee and head . BU - sparse + temporal noticeably improves over BU - sparse ( vs. % AP ) . We observe significant improvements on the most difficult parts such as ankles ( % AP ) and wrists ( % AP ) . Interestingly , BU - sparse + temporal outperforms : longer - range connections such as , , head to ankle , may introduce additional confusion when information is propagated over time . Finally , TD / BU + temporal improves over TD / BU ( % AP ) . Similarly to BU - sparse + temporal , improvement is most prominent on ankles ( % AP ) and wrists ( % AP ) . Note that even the single - frame TD / BU outperforms the best temporal BU model . We show examples of articulated tracking on \u201c MPII Video Pose \u201d in Fig . [ reference ] . Temporal reasoning helps in cases when image information is ambiguous due to close proximity of multiple people . For example the video - based approach succeeds in correctly localizing legs of the person in Fig . [ reference ] ( d ) and ( h ) . Temporal features . We perform an ablative experiment on the \u201c MPII Video Pose \u201d dataset to evaluate the individual contribution of the temporal features introduced in Sec . [ reference ] . The Euclidean distance alone achieves AP , adding DeepMatching features improves the resuls to AP , whereas the combination of all features achieves the best result of AP ( details in supplemental material ) . Tracking evaluation . In Tab . [ reference ] we present results of the evaluation of multi - person articulated body tracking . We treat each body joint of each person as a tracking target and measure tracking performance using a standard multiple object tracking accuracy ( MOTA ) metric that incorporates identity switches , false positives and false negatives . We experimentally compare to a baseline model that first tracks people across frames and then performs per - frame pose estimation . To track a person we use a reduced version of our algorithm that operates on the two head joints only . This allows to achieve near perfect person tracking results in most cases . Our tracker still fails when the person head is occluded for multiple frames as it does not incorporate long - range connectivity between target hypothesis . We leave handling of long - term occlusions for the future work . For full - body tracking we use the same inital head tracks and add them to the set of body part proposals , while also adding must - link and must - cut constraints for the temporal edges corresponding to the head parts detections . The rest of the graph remains unchanged so that at inference time the body parts can be freely assigned to different person tracks . For the BU - sparse the full body tracking improves performance by and MOTA on wrists and ankles , and by and MOTA on elbows and knees respectively . TD / BU benefits from adding temporal connections between body parts as well , but to a lesser extent than BU - sparse . The most significant improvement is for ankles ( MOTA ) . BU - sparse also achieves the best overall score of compared to by TD / BU . This is surprising since TD / BU outperformed BU - sparse on the pose estimation task ( see Tab . [ reference ] and [ reference ] ) . We hypothesize that limited improvement of TD / BU could be due to balancing issues between the temporal and spatial pairwise terms that are estimated independently of each other . section : Conclusion In this paper we introduced an efficient and effective approach to articulated body tracking in monocular video . Our approach defines a model that jointly groups body part proposals within each video frame and across time . Grouping is formulated as a graph partitioning problem that lends itself to efficient inference with recent local search techniques . Our approach improves over state - of - the - art while being substantially faster compared to other related work . Acknowledgements . This work has been supported by the Max Planck Center for Visual Computing and Communication . The authors thank Varvara Obolonchykova and Bahar Tarakameh for their help in creating the video dataset . section : Additional Results on the MPII Multi - Person Dataset We perform qualitative comparison of the proposed single - frame based TD / BU and BU - full methods on challenging scenes containing highly articulated and strongly overlapping individuals . Results are shown in Fig . [ reference ] and Figure [ reference ] . The BU - full works well when persons are sufficiently separated ( images 11 and 12 ) . However , it fails on images where people significantly overlap ( images 1 - 3 , 5 - 10 ) or exhibit high degree of articulation ( image 4 ) . This is due to the fact that geometric image - conditioned pairwise may get confused in the presence of multiple overlapping individuals and thus mislead post - CNN bottom - up assembling of body poses . In contrast , TD / BU performs explicit modeling of person identity via top - dop bottom - up reasoning while offloading the larger share of the reasoning about body - part association onto feed - forward convolutional architecture , and thus is able to resolve such challenging cases . Interestingly , TD / BU is able to correctly predict lower limbs of people in the back through partial occlusion ( image 3 , 5 , 7 , 10 ) . TD / BU model occasionally incorrectly assembles body parts in kinematically implausible manner ( image 12 ) , as it does not explicitly model geometric body part relations . Finally , both models fail in presense of high variations in scale ( image 13 ) . We envision that reasoning over multiple scales is likely to improve the results . BU TD / BU BU TD / BU BU TD / BU BU TD / BU section : Results on the We Are Family dataset We compare our proposed TD / BU model to the state - of - the - art methods on the \u201c We Are Family \u201d ( WAF ) dataset and present results in Table [ reference ] . We use evaluation protocol from and report the AP evaluation measure . TD / BU model outperforms the best published results across all body parts ( vs % AP ) as well improves on articulated parts such as wrists ( % AP ) and elbows ( % AP ) . We attribute that to the ability of top - down model to better learn part associations compared to explicit modeling geometric pairwise relations as in . section : Evaluation of temporal features . We evaluate the importance of combining temporal features introduced in Sec . 3.4 of the paper on our Multi - Person Video dataset . To that end , we consider BU - sparse + temporal model and compare results to BU - sparse in Tab . [ reference ] . Single - frame BU - sparse achieves % AP . It can be seen that using geometry based det - distance features slightly improves the results to % AP , as it enables the propagation of information from neighboring frames . Using deepmatch features slightly improves the performance further as it helps to link the same body part of the same person over time based on the body part appearance . It is especially helpful in the case of fast motion where det - distance may fail . The combination of both geometry and appearance based features further improves the performance to % , which shows their complementarity . Finally , adding the sift - distance feature improves the results to % , since it copes better with the sudden changes in background and body part orientations . Overall , using a combination of temporal features in BU - sparse + temporal results in a % AP improvement over the single - frame BU - sparse . This demonstrates the advantages of the proposed approach to improve pose estimation performance using temporal information . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["MPII_Multi-Person"]]], "Method": [[["Articulated_Tracking"]]], "Metric": [[["AP"]]], "Task": [[["Multi-Person_Pose_Estimation"]]]}]}
{"docid": "TST3-SREX-0048", "doctext": "Question Answering with Subgraph Embeddings section : Abstract . This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features . Our model learns low - dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers . Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature . section : Introduction Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been a long standing goal in Artificial Intelligence . With the rise of large scale structured knowledge bases ( KBs ) , this problem , known as open - domain question answering ( or open QA ) , boils down to being able to query efficiently such databases with natural language . These KBs , such as Freebase [ reference ] encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format . However , the scale and the difficulty for machines to interpret natural language still makes this task a challenging problem . The state - of - the - art techniques in open QA can be classified into two main classes , namely , information retrieval based and semantic parsing based . Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine - grained detection heuristics to identify the exact answer [ reference ][ reference ][ reference ] . On the other hand , semantic parsing methods focus on the correct interpretation of the meaning of a question by a semantic parsing system . A correct interpretation converts a question into the exact database query that returns the correct answer . Interestingly , recent works [ reference ][ reference ][ reference ][ reference ] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large - scale regimes , while bypassing most of the annotation costs . Yet , even if both kinds of system have shown the ability to handle largescale KBs , they still require experts to hand - craft lexicons , grammars , and KB schema to be effective . This non - negligible human intervention might not be generic enough to conveniently scale up to new databases with other schema , broader vocabularies or languages other than English . In contrast , [ reference ] proposed a framework for open QA requiring almost no human annotation . Despite being an interesting approach , this method is outperformed by other competing methods . [ reference ] introduced an embedding model , which learns low - dimensional vector representations of words and symbols ( such as KBs constituents ) and can be trained with even less supervision than the system of [ reference ] while being able to achieve better prediction performance . However , this approach is only compared with [ reference ] which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods . In this paper , we improve the model of [ reference ] by providing the ability to answer more complicated questions . sThe main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( [ reference ] considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which encodes the question - answer path and surrounding subgraph of the KB . Our approach is competitive with the current state - of - the - art on the recent benchmark WebQuestions [ reference ] without using any lexicon , rules or additional system for partof - speech tagging , syntactic or dependency parsing during training as most other systems do . section : Task Definition Our main motivation is to provide a system for open QA able to be trained as long as it has access to : ( 1 ) a training set of questions paired with answers and ( 2 ) a KB providing a structure among answers . We suppose that all potential answers are entities in the KB and that questions are sequences of words that include one identified KB entity . When this entity is not given , plain string matching is used to perform entity resolution . Smarter methods could be used but this is not our focus . We use WebQuestions [ 1 ] as our evaluation bemchmark . Since it contains few training samples , it is impossible to learn on it alone , and this section describes the various data sources that were used for training . These are similar to those used in [ reference ] . WebQuestions This dataset is built using Freebase as the KB and contains 5 , 810 question - answer pairs . It was created by crawling questions through the Google Suggest API , and then obtaining answers using Amazon Mechanical Turk . We used the original split ( 3 , 778 examples for training and 2 , 032 for testing ) , and isolated 1k questions from the training set for validation . WebQuestions is built on Freebase since all answers are defined as Freebase entities . In each question , we identified one Freebase entity using string matching between words of the question and entity names in Freebase . When the same string matches multiple entities , only the entity appearing in most triples , i.e. the most popular in Freebase , was kept . Example questions ( answers ) in the dataset include \" Where did Edgar Allan Poe died ? \" ( baltimore ) or \" What degrees did Barack Obama get ? \" ( bachelor of arts , juris doctor ) . Freebase Freebase [ reference ] is a huge and freely available database of general facts ; data is organized as triplets ( subject , type1.type2.predicate , object ) , where two entities subject and object ( identified by mids ) are connected by the relation type type1.type2.predicate . We used a subset , created by only keeping triples where one of the entities was appearing in either the WebQuestions training / validation set or in ClueWeb extractions . We also removed all entities appearing less than 5 times and finally obtained a Freebase set containing 14 M triples made of 2.2 M entities and 7k relation types . section : WebQuestions 1 Since the format of triples does not correspond to any structure one could find in language , we decided to transform them into automatically generated questions . Hence , all triples were converted into questions \" What is the predicate of the type2 subject ? \" ( using the mid of the subject ) with the answer being object . An example is \" What is the nationality of the person barack obama ? \" ( united states ) . More examples and details are given in a longer version of this paper [ reference ] . section : ClueWeb Extractions Freebase data allows to train our model on 14 M questions but these have a fixed lexicon and vocabulary , which is not realistic . Following [ reference ] , we also created questions using ClueWeb extractions provided by [ reference ] . Using string matching , we ended up with 2 M extractions structured as ( subject , \" text string \" , object ) with both subject and object linked to Freebase . We also converted these triples into questions by using simple patterns and Freebase types . An example of generated question is \" Where barack obama was allegedly bear in ? \" ( hawaii ) . Paraphrases The automatically generated questions that are useful to connect Freebase triples and natural language , do not provide a satisfactory modeling of natural language because of their semi - automatic wording and rigid syntax . To overcome this issue , we follow [ reference ] Table 2 . Examples of questions , answer paths and paraphrases used in this paper . as rephrasings of each other : [ reference ] harvested a set of 2 M distinct questions from WikiAnswers , which were grouped into 350k paraphrase clusters . section : Embedding Questions and Answers Inspired by [ reference ] , our model works by learning low - dimensional vector embeddings of words appearing in questions and of entities and relation types of Freebase , so that representations of questions and of their corresponding answers are close to each other in the joint embedding space . Let q denote a question and a a candidate answer . Learning embeddings is achieved by learning a scoring function S ( q , a ) , so that S generates a high score if a is the correct answer to the question q , and a low score otherwise . Note that both q and a are represented as a combination of the embeddings of their individual words and / or symbols ; hence , learning S essentially involves learning these embeddings . In our model , the form of the scoring function is : Let W be a matrix of R k\u00d7N , where k is the dimension of the embedding space which is fixed a - priori , and N is the dictionary of embeddings to be learned . Let N W denote the total number of words and N S the total number of entities and relation types . With N = N W + N S , the i - th column of W is the embedding of the i - th element ( word , entity or relation type ) in the dictionary . The function f ( . ) , which maps the questions into the embedding space R k is defined as f ( q ) = W\u03c6 ( q ) , where \u03c6 ( q ) \u2208 N N , is a sparse vector indicating the number of times each word appears in the question q ( usually 0 or 1 ) . Likewise the function g ( . ) which maps the answer into the same embedding space R k as the questions , is given by g ( a ) = W\u03c8 ( a ) . Here \u03c8 ( a ) \u2208 N N is a sparse vector representation of the answer a , which we now detail . section : Embedding model Freebase subgraph section : Binary encoding of the subgraph \u03c8 ( a ) section : Embedding of the subgraph g ( a ) Binary encoding of the ques0on \u03a6 ( q ) section : Embedding of the ques0on f ( q ) Ques0on q Subgraph of a candidate answer a ( here K. Preston ) section : Score S ( q , a ) How the candidate answer fits the ques0on section : Dot product Embedding matrix W Fig . 1 . Illustration of the subgraph embedding model scoring a candidate answer : ( i ) locate entity in the question ; ( ii ) compute path from entity to answer ; ( iii ) represent answer as path plus all connected entities to the answer ( the subgraph ) ; ( iv ) embed both the question and the answer subgraph separately using the learnt embedding vectors , and score the match via their dot product . section : Representing Candidate Answers We now describe possible feature representations for a single candidate answer . ( When there are multiple correct answers , we average these representations , see Section 3.4 . ) We consider three different types of representation , corresponding to different subgraphs of Freebase around it . ( i ) Single Entity . The answer is represented as a single entity from Freebase : \u03c8 ( a ) is a 1 - of - N S coded vector with 1 corresponding to the entity of the answer , and 0 elsewhere . ( ii ) Path Representation . The answer is represented as a path from the entity mentioned in the question to the answer entity . In our experiments , we considered 1 - or 2 - hops paths ( i.e. with either 1 or 2 edges to traverse ) : ( barack obama , people.person.place of birth , honolulu ) is a 1 - hop path and ( barack obama , people.person.place of birth , location . location.containedby , hawaii ) a 2 - hops path . This results in a \u03c8 ( a ) which is a 3 - of - N S or 4 - of - N S coded vector , expressing the start and end entities of the path and the relation types ( but not entities ) in - between . ( iii ) Subgraph Representation . We encode both the path representation from ( ii ) , and the entire subgraph of entities connected to the candidate answer entity . That is , for each entity connected to the answer we include both the relation type and the entity itself in the representation \u03c8 ( a ) . In order to represent the answer path differently to the surrounding subgraph ( so the model can differentiate them ) , we double the dictionary size for entities , and use one embedding representation if they are in the path and another if they are in the subgraph . Thus we now learn a parameter matrix R k\u00d7N where N = N W + 2N S ( N S is the total number of entities and relation types ) . If there are C connected entities with D relation types to the candidate answer , its representation is a 3 + C + D or 4 + C + D - of - N S coded vector , depending on the path length . Our hypothesis is that including more information about the answer in its representation will lead to improved results . While it is possible that all required information could be encoded in the k dimensional embedding of the single entity ( i ) , it is unclear what dimension k should be to make this possible . For example the embedding of a country entity encoding all of its citizens seems unrealistic . Similarly , only having access to the path ignores all the other information we have about the answer entity , unless it is encoded in the embeddings of either the entity of the question , the answer or the relations linking them , which might be quite complicated as well . We thus adopt the subgraph approach . Figure 1 illustrates our model . section : Training and Loss Function As in [ reference ] , we train our model using a margin - based ranking loss function . Let D = { ( q i , a i ) : i = 1 , . . . , |D| } be the training set of questions q i paired with their correct answer a i . The loss function we minimize is where m is the margin ( fixed to 0.1 ) . Minimizing Eq . ( 2 ) learns the embedding matrix W so that the score of a question paired with a correct answer is greater than with any incorrect answer\u0101 by at least m.\u0101 is sampled from a set of incorrect candidates\u0100. This is achieved by sampling 50 % of the time from the set of entities connected to the entity of the question ( i.e. other candidate paths ) , and by replacing the answer entity by a random one otherwise . Optimization is accomplished using stochastic gradient descent , multi - threaded with Hogwild ! [ reference ] , with the constraint that the columns w i of W remain within the unit - ball , i.e. , \u2200 i , ||w i || 2 \u2264 1 . section : Multitask Training of Embeddings Since a large number of questions in our training datasets are synthetically generated , they do not adequately cover the range of syntax used in natural language . Hence , we also multi - task the training of our model with the task of paraphrase prediction . We do so by alternating the training of S with that of a scoring function S prp ( q 1 , q 2 ) = f ( q 1 ) f ( q 2 ) , which uses the same embedding matrix W and makes the embeddings of a pair of questions ( q 1 , q 2 ) similar to each other if they are paraphrases ( i.e. if they belong to the same paraphrase cluster ) , and make them different otherwise . Training S prp is similar to that of S except that negative samples are obtained by sampling a question from another paraphrase cluster . We also multitask the training of the embeddings with the mapping of the mids of Freebase entities to the actual words of their names , so that the model learns that the embedding of the mid of an entity should be similar to the embedding of the word ( s ) that compose its name ( s ) . section : Inference Once W is trained , at test time , for a given question q the model predicts the answer with : \u00e2 = argmax a \u2208A ( q ) S ( q , a ) where A ( q ) is the candidate answer set . This candidate set could be the whole KB but this has both speed and potentially precision issues . Instead , we create a candidate set A ( q ) for each question . We recall that each question contains one identified Freebase entity . A ( q ) is first populated with all triples from Freebase involving this entity . This allows to answer simple factual questions whose answers are directly connected to them ( i.e. 1 - hop paths ) . This strategy is denoted C 1 . Since a system able to answer only such questions would be limited , we supplement A ( q ) with examples situated in the KB graph at 2 - hops from the entity of the question . We do not add all such quadruplets since this would lead to very large candidate sets . Instead , we consider the following general approach : given that we are predicting a path , we can predict its elements in turn using a beam search , and hence avoid scoring all candidates . Specifically , our model first ranks relation types using Eq . ( 1 ) , i.e. selects which relation types are the most likely to be expressed in q. We keep the top 10 types ( 10 was selected on the validation set ) and only add 2 - hops candidates to A ( q ) when these relations appear in their path . Scores of 1 - hop triples are weighted by 1.5 since they have one less element than 2 - hops quadruplets . This strategy , denoted C 2 , is used by default . A prediction a can commonly actually be a set of candidate answers , not just one answer , for example for questions like \" Who are David Beckham 's children ? \" . This is achieved by considering a prediction to be all the entities that lie on the same 1 - hop or 2 - hops path from the entity found in the question . Hence , all answers to the above question are connected to david beckham via the same path ( david beckham , people.person.children , * ) . The feature representation of the prediction is then the average over each candidate entity 's features ( see Section 3.1 ) , i.e. \u03c8 all ( a ) = 1 |a | a j : a \u03c8 ( a j ) where a j are the individual entities in the overall prediction a . In the results , we compare to a baseline method that can only predict single candidates , which understandly performs poorly . section : Experiments We compare our system in terms of F1 score as computed by the official evaluation script 2 ( F1 ( Berant ) ) but also with a slightly different F1 definition , termed F1 ( Yao ) which was used in [ reference ] ( the difference being the way that questions with no answers are dealt with ) , and precision @ 1 ( p@1 ) of the first candidate entity ( even when there are a set of correct answers ) , comparing to recently published systems . 3 The upper part of Table 3 indicates that our approach outperforms [ reference ] , [ reference ] and [ reference ] , and performs similarly as [ reference ] . The lower part of Table 3 compares various versions of our model . Our default approach uses the Subgraph representation for answers and C 2 as the candidate answers set . Replacing C 2 by C 1 induces a large drop in performance because many questions do not have answers thatare directly connected to their inluded entity ( not in C 1 ) . However , using all 2 - hops connections as a candidate set is also detrimental , because the larger number of candidates confuses ( and slows a lot ) our ranking based inference . Our results also verify our hypothesis of Section 3.1 , that a richer representation for answers ( using the local subgraph ) can store more pertinent information . Finally , we demonstrate that we greatly improve upon the model of [ reference ] , which actually corresponds to a setting with the Path representation and C 1 as candidate set . We also considered an ensemble of our approach and that of [ reference ] . As we only had access to their test predictions we used the following combination method . Our approach gives a score S ( q , a ) for the answer it predicts . We chose a threshold such that our approach predicts 50 % of the time ( when S ( q , a ) is above its value ) , and the other 50 % of the time we use the prediction of [ reference ] instead . We aimed for a 50 / 50 ratio because both methods perform similarly . The ensemble improves the state - of - the - art , and indicates that our models are significantly different in their design . section : Conclusion This paper presented an embedding model that learns to perform open QA using training data made of questions paired with their answers and of a KB to provide a structure among answers , and can achieve promising performance on the competitive benchmark WebQuestions . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["WebQuestions"]]], "Method": [[["Subgraph_embeddings"]]], "Metric": [[["F1"]]], "Task": [[["Question_Answering"]]]}]}
{"docid": "TST3-SREX-0049", "doctext": "document : [ Combinatorial features are essential for the success of many commercial models . Manually crafting these features usually comes with high cost due to the variety , volume and velocity of raw data in web - scale systems . Factorization based models , which measure interactions in terms of vector product , can learn patterns of combinatorial features automatically and generalize to unseen features as well . With the great success of deep neural networks ( DNNs ) in various fields , recently researchers have proposed several DNN - based factorization model to learn both low - and high - order feature interactions . Despite the powerful ability of learning an arbitrary function from data , plain DNNs generate feature interactions implicitly and at the bit - wise level . In this paper , we propose a novel Compressed Interaction Network ( CIN ) , which aims to generate feature interactions in an explicit fashion and at the vector - wise level . We show that the CIN share some functionalities with convolutional neural networks ( CNNs ) and recurrent neural networks ( RNNs ) . We further combine a CIN and a classical DNN into one unified model , and named this new model eXtreme Deep Factorization Machine ( xDeepFM ) . On one hand , the xDeepFM is able to learn certain bounded - degree feature interactions explicitly ; on the other hand , it can learn arbitrary low - and high - order feature interactions implicitly . We conduct comprehensive experiments on three real - world datasets . Our results demonstrate that xDeepFM outperforms state - of - the - art models . We have released the source code of xDeepFM at . Combining Explicit and Implicit Feature Interactions for Recommender Systems ] xDeepFM : Combining Explicit and Implicit Feature Interactions for Recommender Systems J. Lian ] Jianxun Lian X. Zhou ] Xiaohuan Zhou F. Zhang ] Fuzheng Zhang Z. Chen ] Zhongxia Chen X. Xie ] Xing Xie G. Sun ] Guangzhong Sun \u00a1 ccs2012 \u00bf \u00a1 concept \u00bf \u00a1 concept_id\u00bf10002951.10003260.10003261.10003271\u00a1 / concept_id \u00bf \u00a1 concept_desc\u00bfInformation systems Personalization\u00a1 / concept_desc \u00bf \u00a1 concept_significance\u00bf500\u00a1 / concept_significance \u00bf \u00a1 / concept \u00bf \u00a1 concept \u00bf \u00a1 concept_id\u00bf10010147.10010257.10010293.10010294\u00a1 / concept_id \u00bf \u00a1 concept_desc\u00bfComputing methodologies Neural networks\u00a1 / concept_desc \u00bf \u00a1 concept_significance\u00bf500\u00a1 / concept_significance \u00bf \u00a1 / concept \u00bf \u00a1 concept \u00bf \u00a1 concept_id\u00bf10010147.10010257.10010293.10010309\u00a1 / concept_id \u00bf \u00a1 concept_desc\u00bfComputing methodologies Factorization methods\u00a1 / concept_desc \u00bf \u00a1 concept_significance\u00bf500\u00a1 / concept_significance \u00bf \u00a1 / concept \u00bf \u00a1 / ccs2012 \u00bf [ 500 ] Information systems Personalization [ 500 ] Computing methodologies Neural networks [ 500 ] Computing methodologies Factorization methods 2018 2018 acmcopyright [ KDD \u2019 18 ] The 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningAugust 19\u201323 , 2018London , United Kingdom KDD \u2019 18 : The 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , August 19\u201323 , 2018 , London , United Kingdom 15.00 10.1145 / 3219819.3220023 978 - 1 - 4503 - 5552 - 0 / 18 / 08 section : introduction Features play a central role in the success of many predictive systems . Because using raw features can rarely lead to optimal results , data scientists usually spend a lot of work on the transformation of raw features in order to generate best predictive systems or to win data mining games . One major type of feature transformation is the cross - product transformation over categorical features . These features are called cross features or multi - way features , they measure the interactions of multiple raw features . For instance , a 3 - way feature AND ( user_organization = msra , item_category = deeplearning , time = monday ) has value 1 if the user works at Microsoft Research Asia and is shown a technical article about deep learning on a Monday . There are three major downsides for traditional cross feature engineering . First , obtaining high - quality features comes with a high cost . Because right features are usually task - specific , data scientists need spend a lot of time exploring the potential patterns from the product data before they become domain experts and extract meaningful cross features . Second , in large - scale predictive systems such as web - scale recommender systems , the huge number of raw features makes it infeasible to extract all cross features manually . Third , hand - crafted cross features do not generalize to unseen interactions in the training data . Therefore , learning to interact features without manual engineering is a meaningful task . Factorization Machines ( FM ) embed each feature to a latent factor vector , and pairwise feature interactions are modeled as the inner product of latent vectors : . In this paper we use the term bit to denote a element ( such as ) in latent vectors . The classical FM can be extended to arbitrary higher - order feature interactions , but one major downside is that , proposes to model all feature interactions , including both useful and useless combinations . As revealed in , the interactions with useless features may introduce noises and degrade the performance . In recent years , deep neural networks ( DNNs ) have become successful in computer vision , speech recognition , and natural language processing with their great power of feature representation learning . It is promising to exploit DNNs to learn sophisticated and selective feature interactions . proposes a Factorisation - machine supported Neural Network ( FNN ) to learn high - order feature interactions . It uses the pre - trained factorization machines for field embedding before applying DNN . further proposes a Product - based Neural Network ( PNN ) , which introduces a product layer between embedding layer and DNN layer , and does not rely on pre - trained FM . The major downside of FNN and PNN is that they focus more on high - order feature interactions while capture little low - order interactions . The Wide & Deep and DeepFM models overcome this problem by introducing hybrid architectures , which contain a shallow component and a deep component with the purpose of learning both memorization and generalization . Therefore they can jointly learn low - order and high - order feature interactions . All the abovementioned models leverage DNNs for learning high - order feature interactions . However , DNNs model high - order feature interactions in an implicit fashion . The final function learned by DNNs can be arbitrary , and there is no theoretical conclusion on what the maximum degree of feature interactions is . In addition , DNNs model feature interactions at the bit - wise level , which is different from the traditional FM framework which models feature interactions at the vector - wise level . Thus , in the field of recommender systems , whether DNNs are indeed the most effective model in representing high - order feature interactions remains an open question . In this paper , we propose a neural network - based model to learn feature interactions in an explicit , vector - wise fashion . Our approach is based on the Deep & Cross Network ( DCN ) , which aims to efficiently capture feature interactions of bounded degrees . However , we will argue in Section [ reference ] that DCN will lead to a special format of interactions . We thus design a novel compressed interaction network ( CIN ) to replace the cross network in the DCN . CIN learns feature interactions explicitly , and the degree of interactions grows with the depth of the network . Following the spirit of the Wide & Deep and DeepFM models , we combine the explicit high - order interaction module with implicit interaction module and traditional FM module , and name the joint model eXtreme Deep Factorization Machine ( xDeepFM ) . The new model requires no manual feature engineering and release data scientists from tedious feature searching work . To summarize , we make the following contributions : We propose a novel model , named eXtreme Deep Factorization Machine ( xDeepFM ) , that jointly learns explicit and implicit high - order feature interactions effectively and requires no manual feature engineering . We design a compressed interaction network ( CIN ) in xDeepFM that learns high - order feature interactions explicitly . We show that the degree of feature interactions increases at each layer , and features interact at the vector - wise level rather than the bit - wise level . We conduct extensive experiments on three real - world dataset , and the results demonstrate that our xDeepFM outperforms several state - of - the - art models significantly . The rest of this paper is organized as follows . Section [ reference ] provides some preliminary knowledge which is necessary for understanding deep learning - based recommender systems . Section [ reference ] introduces our proposed CIN and xDeepFM model in detail . We will present experimental explorations on multiple datasets in Section [ reference ] . Related works are discussed in Section [ reference ] . Section [ reference ] concludes this paper . section : Preliminaries subsection : Embedding Layer In computer vision or natural language understanding , the input data are usually images or textual signals , which are known to be spatially and / or temporally correlated , so DNNs can be applied directly on the raw feature with dense structures . However , in web - scale recommender systems , the input features are sparse , of huge dimension , and present no clear spatial or temporal correlation . Therefore , multi - field categorical form is widely used by related works . For example , one input instance [ user_id = s02 , gender = male , organization = msra , interests = comedy & rock ] is normally transformed into a high - dimensional sparse features via field - aware one - hot encoding : An embedding layer is applied upon the raw feature input to compress it to a low dimensional , dense real - value vector . If the field is univalent , the feature embedding is used as the field embedding . Take the above instance as an example , the embedding of feature male is taken as the embedding of field gender . If the field is multivalent , the sum of feature embedding is used as the field embedding . The embedding layer is illustrated in Figure [ reference ] . The result of embedding layer is a wide concatenated vector : where denotes the number of fields , and denotes the embedding of one field . Although the feature lengths of instances can be various , their embeddings are of the same length , where is the dimension of field embedding . subsection : Implicit High - order Interactions FNN , Deep Crossing , and the deep part in Wide & Deep exploit a feed - forward neural network on the field embedding vector to learn high - order feature interactions . The forward process is : where is the layer depth , is an activation function , and is the output of the - th layer . The visual structure is very similar to what is shown in Figure [ reference ] , except that they do not include the FM or Product layer . This architecture models the interaction in a bit - wise fashion . That is to say , even the elements within the same field embedding vector will influence each other . PNN and DeepFM modify the above architecture slightly . Besides applying DNNs on the embedding vector , they add a two - way interaction layer in the architecture . Therefore , both bit - wise and vector - wise interaction is included in their model . The major difference between PNN and DeepFM , is that PNN connects the outputs of product layer to the DNNs , whereas DeepFM connects the FM layer directly to the output unit ( refer to Figure [ reference ] ) . subsection : Explicit High - order Interactions proposes the Cross Network ( CrossNet ) whose architecture is shown in Figure [ reference ] . It aims to explicitly model the high - order feature interactions . Unlike the classical fully - connected feed - forward network , the hidden layers are calculated by the following cross operation : where are weights , bias and output of the - th layer , respectively . We argue that the CrossNet learns a special type of high - order feature interactions , where each hidden layer in the CrossNet is a scalar multiple of . Consider a - layer cross network with the ( i + 1 )- th layer defined as . Then , the output of the cross network is a scalar multiple of . When = 1 , according to the associative law and distributive law for matrix multiplication , we have : where the scalar is actually a linear regression of . Thus , is a scalar multiple of . Suppose the scalar multiple statement holds for = . For = , we have : where , is a scalar . Thus is still a scalar multiple of . By induction hypothesis , the output of cross network is a scalar multiple of . Note that the scalar multiple does not mean is linear with . The coefficient is sensitive with . The CrossNet can learn feature interactions very efficiently ( the complexity is negligible compared with a DNN model ) , however the downsides are : ( 1 ) the output of CrossNet is limited in a special form , with each hidden layer is a scalar multiple of ; ( 2 ) interactions come in a bit - wise fashion . section : Our proposed model .33 .32 .32 subsection : Compressed Interaction Network We design a new cross network , named Compressed Interaction Network ( CIN ) , with the following considerations : ( 1 ) interactions are applied at vector - wise level , not at bit - wise level ; ( 2 ) high - order feature interactions is measured explicitly ; ( 3 ) the complexity of network will not grow exponentially with the degree of interactions . Since an embedding vector is regarded as a unit for vector - wise interactions , hereafter we formulate the output of field embedding as a matrix , where the - th row in is the embedding vector of the - th field : , and is the dimension of the field embedding . The output of the - th layer in CIN is also a matrix , where denotes the number of ( embedding ) feature vectors in the - th layer and we let . For each layer , are calculated via : where , is the parameter matrix for the - th feature vector , and denotes the Hadamard product , for example , . Note that is derived via the interactions between and , thus feature interactions are measured explicitly and the degree of interactions increases with the layer depth . The structure of CIN is very similar to the Recurrent Neural Network ( RNN ) , where the outputs of the next hidden layer are dependent on the last hidden layer and an additional input . We hold the structure of embedding vectors at all layers , thus the interactions are applied at the vector - wise level . It is interesting to point out that Equation [ reference ] has strong connections with the well - known Convolutional Neural Networks ( CNNs ) in computer vision . As shown in Figure [ reference ] , we introduce an intermediate tensor , which is the outer products ( along each embedding dimension ) of hidden layer and original feature matrix . Then can be regarded as a special type of image and is a filter . We slide the filter across along the embedding dimension ( D ) as shown in Figure [ reference ] , and get an hidden vector , which is usually called a feature map in computer vision . Therefore , is a collection of different feature maps . The term \u201c compressed \u201d in the name of CIN indicates that the - th hidden layer compress the potential space of vectors down to vectors . Figure [ reference ] provides an overview of the architecture of CIN . Let T denotes the depth of the network . Every hidden layer has a connection with output units . We first apply sum pooling on each feature map of the hidden layer : for . Thus , we have a pooling vector with length for the - th hidden layer . All pooling vectors from hidden layers are concatenated before connected to output units : . If we use CIN directly for binary classification , the output unit is a sigmoid node on : where are the regression parameters . subsection : CIN Analysis We analyze the proposed CIN to study the model complexity and the potential effectiveness . subsubsection : Space Complexity The - th feature map at the - th layer contains parameters , which is exactly the size of . Thus , there are parameters at the - th layer . Considering the last regression layer for the output unit , which has parameters , the total number of parameters for CIN is . Note that CIN is independent of the embedding dimension . In contrast , a plain - layers DNN contains parameters , and the number of parameters will increase with the embedding dimension . Usually and will not be very large , so the scale of is acceptable . When necessary , we can exploit a - order decomposition and replace with two smaller matrices and : where and . Hereafter we assume that each hidden layer has the same number ( which is ) of feature maps for simplicity . Through the - order decomposition , the space complexity of CIN is reduced from to . In contrast , the space complexity of the plain DNN is , which is sensitive to the dimension ( D ) of field embedding . subsubsection : Time Complexity The cost of computing tensor ( as shown in Figure [ reference ] ) is time . Because we have feature maps in one hidden layer , computing a - layers CIN takes time . A - layers plain DNN , by contrast , takes time . Therefore , the major downside of CIN lies in the time complexity . subsubsection : Polynomial Approximation Next we examine the high - order interaction properties of CIN . For simplicity , we assume that numbers of feature maps at hidden layers are all equal to the number of fields . Let denote the set of positive integers that are less than or equal to . The - th feature map at the first layer , denoted as , is calculated via : Therefore , each feature map at the first layer models pair - wise interactions with coefficients . Similarly , the - th feature map at the second layer is : Note that all calculations related to the subscript and is already finished at the previous hidden layer . We expand the factors in Equation [ reference ] just for clarity . We can observe that each feature map at the second layer models 3 - way interactions with new parameters . A classical - order polynomial has coefficients . We show that CIN approximate this class of polynomial with only parameters in terms of a chain of feature maps . By induction hypothesis , we can prove that the - th feature map at the - th layer is : For better illustration , here we borrow the notations from . Let denote a multi - index , and . We omit the original superscript from , and use to denote it since we only we the feature maps from the - th layer ( which is exactly the field embeddings ) for the final expanded expression ( refer to Eq . [ reference ] ) . Now a superscript is used to denote the vector operation , such as . Let denote a multi - vector polynomial of degree : Each vector polylnomial in this class has coefficients . Then , our CIN approaches the coefficient with : where , is a multi - index , and is the set of all the permutations of the indices . subsection : Combination with Implicit Networks As discussed in Section [ reference ] , plain DNNs learn implicit high - order feature interactions . Since CIN and plain DNNs can complement each other , an intuitive way to make the model stronger is to combine these two structures . The resulting model is very similar to the Wide & Deep or DeepFM model . The architecture is shown in Figure [ reference ] . We name the new model eXtreme Deep Factorization Machine ( xDeepFM ) , considering that on one hand , it includes both low - order and high - order feature interactions ; on the other hand , it includes both implicit feature interactions and explicit feature interactions . Its resulting output unit becomes : where is the sigmoid function , is the raw features . are the outputs of the plain DNN and CIN , respectively . and are learnable parameters . For binary classifications , the loss function is the log loss : where is the total number of training instances . The optimization process is to minimize the following objective function : where denotes the regularization term and denotes the set of parameters , including these in linear part , CIN part , and DNN part . subsubsection : Relationship with FM and DeepFM Suppose all fields are univalent . It \u2019s not hard to observe from Figure [ reference ] that , when the depth and feature maps of the CIN part are both set to 1 , xDeepFM is a generalization of DeepFM by learning the linear regression weights for the FM layer ( note that in DeepFM , units of FM layer are directly linked to the output unit without any coefficients ) . When we further remove the DNN part , and at the same time use a constant sum filter ( which simply takes the sum of inputs without any parameter learning ) for the feature map , then xDeepFM is downgraded to the traditional FM model . section : Experiments In this section , we conduct extensive experiments to answer the following questions : ( Q1 ) How does our proposed CIN perform in high - order feature interactions learning ? ( Q2 ) Is it necessary to combine explicit and implicit high - order feature interactions for recommender systems ? ( Q3 ) How does the settings of networks influence the performance of xDeepFM ? We will answer these questions after presenting some fundamental experimental settings . subsection : Experiment Setup subsubsection : Datasets . We evaluate our proposed models on the following three datasets : 1 . Criteo Dataset . It is a famous industry benchmarking dataset for developing models predicting ad click - through rate , and is publicly accessible . Given a user and the page he is visiting , the goal is to predict the probability that he will clik on a given ad . 2 . Dianping Dataset . Dianping.com is the largest consumer review site in China . It provides diverse functions such as reviews , check - ins , and shops \u2019 meta information ( including geographical messages and shop attributes ) . We collect 6 months \u2019 users check - in activities for restaurant recommendation experiments . Given a user \u2019s profile , a restaurant \u2019s attributes and the user \u2019s last three visited POIs ( point of interest ) , we want to predict the probability that he will visit the restaurant . For each restaurant in a user \u2019s check - in instance , we sample four restaurants which are within 3 kilometers as negative instances by POI popularity . 3 . Bing News Dataset . Bing News is part of Microsoft \u2019s Bing search engine . In order to evaluate the performance of our model in a real commercial dataset , we collect five consecutive days \u2019 impression logs on news reading service . We use the first three days \u2019 data for training and validation , and the next two days for testing . For the Criteo dataset and the Dianping dataset , we randomly split instances by 8:1:1 for training , validation and test . The characteristics of the three datasets are summarized in Table [ reference ] . subsubsection : Evaluation Metrics . We use two metrics for model evaluation : AUC ( Area Under the ROC curve ) and Logloss ( cross entropy ) . These two metrics evaluate the performance from two different angels : AUC measures the probability that a positive instance will be ranked higher than a randomly chosen negative one . It only takes into account the order of predicted instances and is insensitive to class imbalance problem . Logloss , in contrast , measures the distance between the predicted score and the true label for each instance . Sometimes we rely more on Logloss because we need to use the predicted probability to estimate the benefit of a ranking strategy ( which is usually adjusted as CTR bid ) . subsubsection : Baselines . We compare our xDeepFM with LR ( logistic regression ) , FM , DNN ( plain deep neural network ) , PNN ( choose the better one from iPNN and oPNN ) , Wide & Deep , DCN ( Deep & Cross Network ) and DeepFM . As introduced and discussed in Section [ reference ] , these models are highly related to our xDeepFM and some of them are state - of - the - art models for recommender systems . Note that the focus of this paper is to learn feature interactions automatically , so we do not include any hand - crafted cross features . subsubsection : Reproducibility We implement our method using Tensorflow . Hyper - parameters of each model are tuned by grid - searching on the validation set , and the best settings for each model will be shown in corresponding sections . Learning rate is set to 0.001 . For optimization method , we use the Adam with a mini - batch size of 4096 . We use a L2 regularization with for DNN , DCN , Wide & Deep , DeepFM and xDeepFM , and use dropout 0.5 for PNN . The default setting for number of neurons per layer is : ( 1 ) 400 for DNN layers ; ( 2 ) 200 for CIN layers on Criteo dataset , and 100 for CIN layers on Dianping and Bing News datasets . Since we focus on neural networks structures in this paper , we make the dimension of field embedding for all models be a fixed value of 10 . We conduct experiments of different settings in parallel with 5 Tesla K80 GPUs . The source code is available at . subsection : Performance Comparison among Individual Neural Components ( Q1 ) We want to know how CIN performs individually . Note that FM measures 2 - order feature interactions explicitly , DNN model high - order feature interactions implicitly , CrossNet tries to model high - order feature interactions with a small number of parameters ( which is proven not effective in Section [ reference ] ) , and CIN models high - order feature interactions explicitly . There is no theoretic guarantee of the superiority of one individual model over the others , due to that it really depends on the dataset . For example , if the practical dataset does not require high - order feature interactions , FM may be the best individual model . Thus we do not have any expectation for which model will perform the best in this experiment . Table [ reference ] shows the results of individual models on the three practical datasets . Surprisingly , our CIN outperform the other models consistently . On one hand , the results indicate that for practical datasets , higher - order interactions over sparse features are necessary , and this can be verified through the fact that DNN , CrossNet and CIN outperform FM significantly on all the three datasets . On the other hand , CIN is the best individual model , which demonstrates the effectiveness of CIN on modeling explicit high - order feature interactions . Note that a - layer CIN can model - degree feature interactions . It is also interesting to see that it take 5 layers for CIN to yield the best result ON the Bing News dataset . subsection : Performance of Integrated Models ( Q2 ) xDeepFM integrates CIN and DNN into an end - to - end model . While CIN and DNN covers two distinct properties in learning feature interactions , we are interested to know whether it is indeed necessary and effective to combine them together for jointly explicit and implicit learning . Here we compare several strong baselines which are not limited to individual models , and the results are shown in Table [ reference ] . We observe that LR is far worse than all the rest models , which demonstrates that factorization - based models are essential for measuring sparse features . Wide & Deep , DCN , DeepFM and xDeepFM are significantly better than DNN , which directly reflects that , despite their simplicity , incorporating hybrid components are important for boosting the accuracy of predictive systems . Our proposed xDeepFM achieves the best performance on all datasets , which demonstrates that combining explicit and implicit high - order feature interaction is necessary , and xDeepFM is effective in learning this class of combination . Another interesting observation is that , all the neural - based models do not require a very deep network structure for the best performance . Typical settings for the depth hyper - parameter are 2 and 3 , and the best depth setting for xDeepFM is 3 , which indicates that the interactions we learned are at most 4 - order . subsection : Hyper - Parameter Study ( Q3 ) We study the impact of hyper - parameters on xDeepFM in this section , including ( 1 ) the number of hidden layers ; ( 2 ) the number of neurons per layer ; and ( 3 ) activation functions . We conduct experiments via holding the best settings for the DNN part while varying the settings for the CIN part . .32 .32 .32 .32 .32 .32 Depth of Network . Figure [ reference ] and [ reference ] demonstrate the impact of number of hidden layers . We can observe that the performance of xDeepFM increases with the depth of network at the beginning . However , model performance degrades when the depth of network is set greater than 3 . It is caused by overfitting evidenced by that we notice that the loss of training data still keeps decreasing when we add more hidden layers . Number of Neurons per Layer . Adding the number of neurons per layer indicates increasing the number of feature maps in CIN . As shown in Figure [ reference ] and [ reference ] , model performance on Bing News dataset increases steadily when we increase the number of neurons from to , while on Dianping dataset , is a more suitable setting for the number of neurons per layer . In this experiment we fix the depth of network at 3 . Activation Function . Note that we exploit the identity as activation function on neurons of CIN , as shown in Eq . [ reference ] . A common practice in deep learning literature is to employ non - linear activation functions on hidden neurons . We thus compare the results of different activation functions on CIN ( for neurons in DNN , we keep the activation function with relu ) . As shown in Figure [ reference ] and [ reference ] , identify function is indeed the most suitable one for neurons in CIN . section : related work subsection : Classical Recommender Systems subsubsection : Non - factorization Models For web - scale recommender systems ( RSs ) , the input features are usually sparse , categorical - continuous - mixed , and high - dimensional . Linear models , such as logistic regression with FTRL , are widely adopted as they are easy to manage , maintain , and deploy . Because linear models lack the ability of learning feature interactions , data scientists have to spend a lot of work on engineering cross features in order to achieve better performance . Considering that some hidden features are hard to design manually , some researchers exploit boosting decision trees to help build feature transformations . subsubsection : Factorization Models A major downside of the aforementioned models is that they can not generalize to unseen feature interactions in the training set . Factorization Machines overcome this problem via embedding each feature into a low dimension latent vector . Matrix factorization ( MF ) , which only considers IDs as features , can be regarded as a special kind of FM . Recommendations are made via the product of two latent vectors , thus it does not require the co - occurrence of user and item in the training set . MF is the most popular model - based collaborative filtering method in the RS literature . extend MF to leveraging side information , in which both a linear model and a MF model are included . On the other hand , for many recommender systems , only implicit feedback datasets such as users \u2019 watching history and browsing activities are available . Thus researchers extend the factorization models to a Bayesian Personalized Ranking ( BPR ) framework for implicit feedback . subsection : Recommender Systems with Deep Learning Deep learning techniques have achieved great success in computer vision , speech recognition and natural language understanding . As a result , an increasing number of researchers are interested in employing DNNs for recommender systems . subsubsection : Deep Learning for High - Order Interactions To avoid manually building up high - order cross features , researchers apply DNNs on field embedding , thus patterns from categorical feature interactions can be learned automatically . Representative models include FNN , PNN , DeepCross , NFM , DCN , Wide & Deep , and DeepFM . These models are highly related to our proposed xDeepFM . Since we have reviewed them in Section [ reference ] and Section [ reference ] , we do not further discuss them in detail in this section . We have demonstrated that our proposed xDeepFM has two special properties in comparison with these models : ( 1 ) xDeepFM learns high - order feature interactions in both explicit and implicit fashions ; ( 2 ) xDeepFM learns feature interactions at the vector - wise level rather than at the bit - wise level . subsubsection : Deep Learning for Elaborate Representation Learning We include some other deep learning - based RSs in this section due to that they are less focused on learning feature interactions . Some early work employs deep learning mainly to model auxiliary information , such as visual data and audio data . Recently , deep neural networks are used to model the collaborative filtering ( CF ) in RSs . proposes a Neural Collaborative Filtering ( NCF ) so that the inner product in MF can be replaced with an arbitrary function via a neural architecture . model CF base on the autoencoder paradigm , and they have empirically demonstrated that autoencoder - based CF outperforms several classical MF models . Autoencoders can be further employed for jointly modeling CF and side information with the purpose of generating better latent factors . employ neural networks to jointly train multiple domains \u2019 latent factors . proposes the Attentive Collaborative Filtering ( ACF ) to learn more elaborate preference at both item - level and component - level . shows tha traditional RSs can not capture interest diversity and local activation effectively , so they introduce a Deep Interest Network ( DIN ) to represent users \u2019 diverse interests with an attentive activation mechanism . section : Conclusions In this paper , we propose a novel network named Compressed Interaction Network ( CIN ) , which aims to learn high - order feature interactions explicitly . CIN has two special virtues : ( 1 ) it can learn certain bounded - degree feature interactions effectively ; ( 2 ) it learns feature interactions at a vector - wise level . Following the spirit of several popular models , we incorporate a CIN and a DNN in an end - to - end framework , and named the resulting model eXtreme Deep Factorization Machine ( xDeepFM ) . Thus xDeepFM can automatically learn high - order feature interactions in both explicit and implicit fashions , which is of great significance to reducing manual feature engineering work . We conduct comprehensive experiments and the results demonstrate that our xDeepFM outperforms state - of - the - art models consistently on three real - world datasets . There are two directions for future work . First , currently we simply employ a sum pooling for embedding multivalent fields . We can explore the usage of the DIN mechanism to capture the related activation according to the candidate item . Second , as discussed in section [ reference ] , the time complexity of the CIN module is high . We are interested in developing a distributed version of xDeepFM which can be trained efficiently on a GPU cluster . section : Acknowledgements The authors would like to thank the anonymous reviewers for their insightful reviews , which are very helpful on the revision of this paper . This work is supported in part by Youth Innovation Promotion Association of CAS . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Bing_News"]]], "Method": [[["xDeepFM"]]], "Metric": [[["AUC"]]], "Task": [[["Click-Through_Rate_Prediction"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Bing_News"]]], "Method": [[["xDeepFM"]]], "Metric": [[["Log_Loss"]]], "Task": [[["Click-Through_Rate_Prediction"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Criteo"]]], "Method": [[["xDeepFM"]]], "Metric": [[["AUC"]]], "Task": [[["Click-Through_Rate_Prediction"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Criteo"]]], "Method": [[["xDeepFM"]]], "Metric": [[["Log_Loss"]]], "Task": [[["Click-Through_Rate_Prediction"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Dianping"]]], "Method": [[["xDeepFM"]]], "Metric": [[["AUC"]]], "Task": [[["Click-Through_Rate_Prediction"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Dianping"]]], "Method": [[["xDeepFM"]]], "Metric": [[["Log_Loss"]]], "Task": [[["Click-Through_Rate_Prediction"]]]}]}
{"docid": "TST3-SREX-0050", "doctext": "document : Explaining and Harnessing Adversarial Examples Several machine learning models , including neural networks , consistently misclassify adversarial examples \u2014inputs formed by applying small but intentionally worst - case perturbations to examples from the dataset , such that the perturbed input results in the model outputting an incorrect answer with high confidence . Early attempts at explaining this phenomenon focused on nonlinearity and overfitting . We argue instead that the primary cause of neural networks \u2019 vulnerability to adversarial perturbation is their linear nature . This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them : their generalization across architectures and training sets . Moreover , this view yields a simple and fast method of generating adversarial examples . Using this approach to provide examples for adversarial training , we reduce the test set error of a maxout network on the MNIST dataset . * section : 0pt1pt1pt section : Introduction Szegedy - ICLR2014 made an intriguing discovery : several machine learning models , including state - of - the - art neural networks , are vulnerable to adversarial examples . That is , these machine learning models misclassify examples that are only slightly different from correctly classified examples drawn from the data distribution . In many cases , a wide variety of models with different architectures trained on different subsets of the training data misclassify the same adversarial example . This suggests that adversarial examples expose fundamental blind spots in our training algorithms . The cause of these adversarial examples was a mystery , and speculative explanations have suggested it is due to extreme nonlinearity of deep neural networks , perhaps combined with insufficient model averaging and insufficient regularization of the purely supervised learning problem . We show that these speculative hypotheses are unnecessary . Linear behavior in high - dimensional spaces is sufficient to cause adversarial examples . This view enables us to design a fast method of generating adversarial examples that makes adversarial training practical . We show that adversarial training can provide an additional regularization benefit beyond that provided by using dropout dropout alone . Generic regularization strategies such as dropout , pretraining , and model averaging do not confer a significant reduction in a model \u2019s vulnerability to adversarial examples , but changing to nonlinear model families such as RBF networks can do so . Our explanation suggests a fundamental tension between designing models that are easy to train due to their linearity and designing models that use nonlinear effects to resist adversarial perturbation . In the long run , it may be possible to escape this tradeoff by designing more powerful optimization methods that can succesfully train more nonlinear models . section : Related work Szegedy - ICLR2014 demonstrated a variety of intriguing properties of neural networks and related models . Those most relevant to this paper include : Box - constrained L - BFGS can reliably find adversarial examples . On some datasets , such as ImageNet ImageNet , the adversarial examples were so close to the original examples that the differences were indistinguishable to the human eye . The same adversarial example is often misclassified by a variety of classifiers with different architectures or trained on different subsets of the training data . Shallow softmax regression models are also vulnerable to adversarial examples . Training on adversarial examples can regularize the model \u2014 however , this was not practical at the time due to the need for expensive constrained optimization in the inner loop . These results suggest that classifiers based on modern machine learning techniques , even those that obtain excellent performance on the test set , are not learning the true underlying concepts that determine the correct output label . Instead , these algorithms have built a Potemkin village that works well on naturally occuring data , but is exposed as a fake when one visits points in space that do not have high probability in the data distribution . This is particularly disappointing because a popular approach in computer vision is to use convolutional network features as a space where Euclidean distance approximates perceptual distance . This resemblance is clearly flawed if images that have an immeasurably small perceptual distance correspond to completely different classes in the network \u2019s representation . These results have often been interpreted as being a flaw in deep networks in particular , even though linear classifiers have the same problem . We regard the knowledge of this flaw as an opportunity to fix it . Indeed , Luca and causal have already begun the first steps toward designing models that resist adversarial perturbation , though no model has yet succesfully done so while maintaining state of the art accuracy on clean inputs . section : The linear explanation of adversarial examples We start with explaining the existence of adversarial examples for linear models . In many problems , the precision of an individual input feature is limited . For example , digital images often use only 8 bits per pixel so they discard all information below of the dynamic range . Because the precision of the features is limited , it is not rational for the classifier to respond differently to an input than to an adversarial input if every element of the perturbation is smaller than the precision of the features . Formally , for problems with well - separated classes , we expect the classifier to assign the same class to and so long as , where is small enough to be discarded by the sensor or data storage apparatus associated with our problem . Consider the dot product between a weight vector and an adversarial example : The adversarial perturbation causes the activation to grow by . We can maximize this increase subject to the max norm constraint on by assigning . If has dimensions and the average magnitude of an element of the weight vector is , then the activation will grow by . Since does not grow with the dimensionality of the problem but the change in activation caused by perturbation by can grow linearly with , then for high dimensional problems , we can make many infinitesimal changes to the input that add up to one large change to the output . We can think of this as a sort of \u201c accidental steganography , \u201d where a linear model is forced to attend exclusively to the signal that aligns most closely with its weights , even if multiple signals are present and other signals have much greater amplitude . This explanation shows that a simple linear model can have adversarial examples if its input has sufficient dimensionality . Previous explanations for adversarial examples invoked hypothesized properties of neural networks , such as their supposed highly non - linear nature . Our hypothesis based on linearity is simpler , and can also explain why softmax regression is vulnerable to adversarial examples . section : Linear perturbation of non - linear models The linear view of adversarial examples suggests a fast way of generating them . We hypothesize that neural networks are too linear to resist linear adversarial perturbation . LSTMs lstm , ReLUs Jarrett - ICCV2009 , Glorot + al - AI - 2011 , and maxout networks Goodfellow - et - al - ICML2013 are all intentionally designed to behave in very linear ways , so that they are easier to optimize . More nonlinear models such as sigmoid networks are carefully tuned to spend most of their time in the non - saturating , more linear regime for the same reason . This linear behavior suggests that cheap , analytical perturbations of a linear model should also damage neural networks . Let be the parameters of a model , the input to the model , the targets associated with ( for machine learning tasks that have targets ) and be the cost used to train the neural network . We can linearize the cost function around the current value of , obtaining an optimal max - norm constrained pertubation of We refer to this as the \u201c fast gradient sign method \u201d of generating adversarial examples . Note that the required gradient can be computed efficiently using backpropagation . We find that this method reliably causes a wide variety of models to misclassify their input . See Fig . [ reference ] for a demonstration on ImageNet . We find that using , we cause a shallow softmax classifier to have an error rate of 99.9 % with an average confidence of 79.3 % on the MNIST LeCun + 98 test set . In the same setting , a maxout network misclassifies 89.4 % of our adversarial examples with an average confidence of 97.6 % . Similarly , using , we obtain an error rate of 87.15 % and an average probability of 96.6 % assigned to the incorrect labels when using a convolutional maxout network on a preprocessed version of the CIFAR - 10 KrizhevskyHinton2009 test set . Other simple methods of generating adversarial examples are possible . For example , we also found that rotating by a small angle in the direction of the gradient reliably produces adversarial examples . The fact that these simple , cheap algorithms are able to generate misclassified examples serves as evidence in favor of our interpretation of adversarial examples as a result of linearity . The algorithms are also useful as a way of speeding up adversarial training or even just analysis of trained networks . section : Adversarial training of linear models versus weight decay Perhaps the simplest possible model we can consider is logistic regression . In this case , the fast gradient sign method is exact . We can use this case to gain some intuition for how adversarial examples are generated in a simple setting . See Fig . [ reference ] for instructive images . If we train a single model to recognize labels with where is the logistic sigmoid function , then training consists of gradient descent on where is the softplus function . We can derive a simple analytical form for training on the worst - case adversarial perturbation of rather than itself , based on gradient sign perturbation . Note that the sign of the gradient is just , and that . The adversarial version of logistic regression is therefore to minimize This is somewhat similar to regularization . However , there are some important differences . Most significantly , the penalty is subtracted off the model \u2019s activation during training , rather than added to the training cost . This means that the penalty can eventually start to disappear if the model learns to make confident enough predictions that saturates . This is not guaranteed to happen \u2014 in the underfitting regime , adversarial training will simply worsen underfitting . We can thus view weight decay as being more \u201c worst case \u201d than adversarial training , because it fails to deactivate in the case of good margin . If we move beyond logistic regression to multiclass softmax regression , weight decay becomes even more pessimistic , because it treats each of the softmax \u2019s outputs as independently perturbable , when in fact it is usually not possible to find a single that aligns with all of the class \u2019s weight vectors . Weight decay overestimates the damage achievable with perturbation even more in the case of a deep network with multiple hidden units . Because weight decay overestimates the amount of damage an adversary can do , it is necessary to use a smaller weight decay coefficient than the associated with the precision of our features . When training maxout networks on MNIST , we obtained good results using adversarial training with . When applying weight decay to the first layer , we found that even a coefficient of .0025 was too large , and caused the model to get stuck with over 5 % error on the training set . Smaller weight decay coefficients permitted succesful training but conferred no regularization benefit . section : Adversarial training of deep networks The criticism of deep networks as vulnerable to adversarial examples is somewhat misguided , because unlike shallow linear models , deep networks are at least able to represent functions that resist adversarial perturbation . The universal approximator theorem Hornik89 guarantees that a neural network with at least one hidden layer can represent any function to an arbitary degree of accuracy so long as its hidden layer is permitted to have enough units . Shallow linear models are not able to become constant near training points while also assigning different outputs to different training points . Of course , the universal approximator theorem does not say anything about whether a training algorithm will be able to discover a function with all of the desired properties . Obviously , standard supervised training does not specify that the chosen function be resistant to adversarial examples . This must be encoded in the training procedure somehow . Szegedy - ICLR2014 showed that by training on a mixture of adversarial and clean examples , a neural network could be regularized somewhat . Training on adversarial examples is somewhat different from other data augmentation schemes ; usually , one augments the data with transformations such as translations that are expected to actually occur in the test set . This form of data augmentation instead uses inputs that are unlikely to occur naturally but that expose flaws in the ways that the model conceptualizes its decision function . At the time , this procedure was never demonstrated to improve beyond dropout on a state of the art benchmark . However , this was partially because it was difficult to experiment extensively with expensive adversarial examples based on L - BFGS . We found that training with an adversarial objective function based on the fast gradient sign method was an effective regularizer : In all of our experiments , we used . Other values may work better ; our initial guess of this hyperparameter worked well enough that we did not feel the need to explore more . This approach means that we continually update our supply of adversarial examples , to make them resist the current version of the model . Using this approach to train a maxout network that was also regularized with dropout , we were able to reduce the error rate from 0.94 % without adversarial training to 0.84 % with adversarial training . We observed that we were not reaching zero error rate on adversarial examples on the training set . We fixed this problem by making two changes . First , we made the model larger , using 1600 units per layer rather than the 240 used by the original maxout network for this problem . Without adversarial training , this causes the model to overfit slightly , and get an error rate of 1.14 % on the test set . With adversarial training , we found that the validation set error leveled off over time , and made very slow progress . The original maxout result uses early stopping , and terminates learning after the validation set error rate has not decreased for 100 epochs . We found that while the validation set error was very flat , the adversarial validation set error was not . We therefore used early stopping on the adversarial validation set error . Using this criterion to choose the number of epochs to train for , we then retrained on all 60 , 000 examples . Five different training runs using different seeds for the random number generators used to select minibatches of training examples , initialize model weights , and generate dropout masks result in four trials that each had an error rate of 0.77 % on the test set and one trial that had an error rate of 0.83 % . The average of 0.782 % is the best result reported on the permutation invariant version of MNIST , though statistically indistinguishable from the result obtained by fine - tuning DBMs with dropout dropout at 0.79 % . The model also became somewhat resistant to adversarial examples . Recall that without adversarial training , this same kind of model had an error rate of 89.4 % on adversarial examples based on the fast gradient sign method . With adversarial training , the error rate fell to 17.9 % . Adversarial examples are transferable between the two models but with the adversarially trained model showing greater robustness . Adversarial examples generated via the original model yield an error rate of 19.6 % on the adversarially trained model , while adversarial examples generated via the new model yield an error rate of 40.9 % on the original model . When the adversarially trained model does misclassify an adversarial example , its predictions are unfortunately still highly confident . The average confidence on a misclassified example was 81.4 % . We also found that the weights of the learned model changed significantly , with the weights of the adversarially trained model being significantly more localized and interpretable ( see Fig . [ reference ] ) . The adversarial training procedure can be seen as minimizing the worst case error when the data is perturbed by an adversary . That can be interpreted as learning to play an adversarial game , or as minimizing an upper bound on the expected cost over noisy samples with noise from added to the inputs . Adversarial training can also be seen as a form of active learning , where the model is able to request labels on new points . In this case the human labeler is replaced with a heuristic labeler that copies labels from nearby points . We could also regularize the model to be insensitive to changes in its features that are smaller than the precision simply by training on all points within the max norm box , or sampling many points within this box . This corresponds to adding noise with max norm during training . However , noise with zero mean and zero covariance is very inefficient at preventing adversarial examples . The expected dot product between any reference vector and such a noise vector is zero . This means that in many cases the noise will have essentially no effect rather than yielding a more difficult input . In fact , in many cases the noise will actualy result in a lower objective function value . We can think of adversarial training as doing hard example mining among the set of noisy inputs , in order to train more efficiently by considering only those noisy points that strongly resist classification . As control experiments , we trained training a maxout network with noise based on randomly adding to each pixel , or adding noise in to each pixel . These obtained an error rate of 86.2 % with confidence 97.3 % and an error rate of 90.4 % with a confidence of 97.8 % respectively on fast gradient sign adversarial examples . Because the derivative of the sign function is zero or undefined everywhere , gradient descent on the adversarial objective function based on the fast gradient sign method does not allow the model to anticipate how the adversary will react to changes in the parameters . If we instead adversarial examples based on small rotations or addition of the scaled gradient , then the perturbation process is itself differentiable and the learning can take the reaction of the adversary into account . However , we did not find nearly as powerful of a regularizing result from this process , perhaps because these kinds of adversarial examples are not as difficult to solve . One natural question is whether it is better to perturb the input or the hidden layers or both . Here the results are inconsistent . Szegedy - ICLR2014 reported that adversarial perturbations yield the best regularization when applied to the hidden layers . That result was obtained on a sigmoidal network . In our experiments with the fast gradient sign method , we find that networks with hidden units whose activations are unbounded simply respond by making their hidden unit activations very large , so it is usually better to just perturb the original input . On saturating models such as the Rust model we found that perturbation of the input performed comparably to perturbation of the hidden layers . Perturbations based on rotating the hidden layers solve the problem of unbounded activations growing to make additive perturbations smaller by comparison . We were able to succesfully train maxout networks with rotational perturbations of the hidden layers . However , this did not yield nearly as strong of a regularizing effect as additive perturbation of the input layer . Our view of adversarial training is that it is only clearly useful when the model has the capacity to learn to resist adversarial examples . This is only clearly the case when a universal approximator theorem applies . Because the last layer of a neural network , the linear - sigmoid or linear - softmax layer , is not a universal approximator of functions of the final hidden layer , this suggests that one is likely to encounter problems with underfitting when applying adversarial perturbations to the final hidden layer . We indeed found this effect . Our best results with training using perturbations of hidden layers never involved perturbations of the final hidden layer . section : Different kinds of model capacity One reason that the existence of adversarial examples can seem counter - intuitive is that most of us have poor intuitions for high dimensional spaces . We live in three dimensions , so we are not used to small effects in hundreds of dimensions adding up to create a large effect . There is another way that our intuitions serve us poorly . Many people think of models with low capacity as being unable to make many different confident predictions . This is not correct . Some models with low capacity do exhibit this behavior . For example shallow RBF networks with are only able to confidently predict that the positive class is present in the vicinity of . Elsewhere , they default to predicting the class is absent , or have low - confidence predictions . RBF networks are naturally immune to adversarial examples , in the sense that they have low confidence when they are fooled . A shallow RBF network with no hidden layers gets an error rate of 55.4 % on MNIST using adversarial examples generated with the fast gradient sign method and . However , its confidence on mistaken examples is only . Its average confidence on clean test examples is % . We ca n\u2019t expect a model with such low capacity to get the right answer at all points of space , but it does correctly respond by reducing its confidence considerably on points it does not \u201c understand . \u201d RBF units are unfortunately not invariant to any significant transformations so they can not generalize very well . We can view linear units and RBF units as different points on a precision - recall tradeoff curve . Linear units achieve high recall by responding to every input in a certain direction , but may have low precision due to responding too strongly in unfamiliar situations . RBF units achieve high precision by responding only to a specific point in space , but in doing so sacrifice recall . Motivated by this idea , we decided to explore a variety of models involving quadratic units , including deep RBF networks . We found this to be a difficult task \u2014 very model with sufficient quadratic inhibition to resist adversarial perturbation obtained high training set error when trained with SGD . section : Why do adversarial examples generalize ? An intriguing aspect of adversarial examples is that an example generated for one model is often misclassified by other models , even when they have different architecures or were trained on disjoint training sets . Moreover , when these different models misclassify an adversarial example , they often agree with each other on its class . Explanations based on extreme non - linearity and overfitting can not readily account for this behavior \u2014 why should multiple extremely non - linear model with excess capacity consistently label out - of - distribution points in the same way ? This behavior is especially surprising from the view of the hypothesis that adversarial examples finely tile space like the rational numbers among the reals , because in this view adversarial examples are common but occur only at very precise locations . Under the linear view , adversarial examples occur in broad subspaces . The direction need only have positive dot product with the gradient of the cost function , and need only be large enough . Fig . [ reference ] demonstrates this phenomenon . By tracing out different values of we see that adversarial examples occur in contiguous regions of the 1 - D subspace defined by the fast gradient sign method , not in fine pockets . This explains why adversarial examples are abundant and why an example misclassified by one classifier has a fairly high prior probability of being misclassified by another classifier . To explain why mutiple classifiers assign the same class to adversarial examples , we hypothesize that neural networks trained with current methodologies all resemble the linear classifier learned on the same training set . This reference classifier is able to learn approximately the same classification weights when trained on different subsets of the training set , simply because machine learning algorithms are able to generalize . The stability of the underlying classification weights in turn results in the stability of adversarial examples . To test this hypothesis , we generated adversarial examples on a deep maxout network and classified these examples using a shallow softmax network and a shallow RBF network . On examples that were misclassified by the maxout network , the RBF network predicted the maxout network \u2019s class assignment only 16.0 % of the time , while the softmax classifier predict the maxout network \u2019s class correctly 54.6 % of the time . These numbers are largely driven by the differing error rate of the different models though . If we exclude our attention to cases where both models being compared make a mistake , then softmax regression predict \u2019s maxout \u2019s class 84.6 % of the time , while the RBF network is able to predict maxout \u2019s class only 54.3 % of the time . For comparison , the RBF network can predict softmax regression \u2019s class 53.6 % of the time , so it does have a strong linear component to its own behavior . Our hypothesis does not explain all of the maxout network \u2019s mistakes or all of the mistakes that generalize across models , but clearly a significant proportion of them are consistent with linear behavior being a major cause of cross - model generalization . section : Alternative hypotheses We now consider and refute some alternative hypotheses for the existence of adversarial examples . First , one hypothesis is that generative training could provide more constraint on the training process , or cause the model to learn what to distinguish \u201c real \u201d from \u201c fake \u201d data and be confident only on \u201c real \u201d data . The MP - DBM mpdbm provides a good model to test this hypothesis . Its inference procedure gets good classification accuracy ( an 0.88 % error rate ) on MNIST . This inference procedure is differentiable . Other generative models either have non - differentiable inference procedures , making it harder to compute adversarial examples , or require an additional non - generative discriminator model to get good classification accuracy on MNIST . In the case of the MP - DBM , we can be sure that the generative model itself is responding to adversarial examples , rather than the non - generative classifier model on top . We find that the model is vulnerable to adversarial examples . With an of 0.25 , we find an error rate of 97.5 % on adversarial examples generated from the MNIST test set . It remains possible that some other form of generative training could confer resistance , but clearly the mere fact of being generative is not alone sufficient . Another hypothesis about why adversarial examples exist is that individual models have strange quirks but averaging over many models can cause adversarial examples to wash out . To test this hypothesis , we trained an ensemble of twelve maxout networks on MNIST . Each network was trained using a different seed for the random number generator used to initialize the weights , generate dropout masks , and select minibatches of data for stochastic gradient descent . The ensemble gets an error rate of 91.1 % on adversarial examples designed to perturb the entire ensemble with . If we instead use adversarial examples designed to perturb only one member of the ensemble , the error rate falls to 87.9 % . Ensembling provides only limited resistance to adversarial perturbation . section : Summary and discussion As a summary , this paper has made the following observations : Adversarial examples can be explained as a property of high - dimensional dot products . They are a result of models being too linear , rather than too nonlinear . The generalization of adversarial examples across different models can be explained as a result of adversarial perturbations being highly aligned with the weight vectors of a model , and different models learning similar functions when trained to perform the same task . The direction of perturbation , rather than the specific point in space , matters most . Space is not full of pockets of adversarial examples that finely tile the reals like the rational numbers . Because it is the direction that matters most , adversarial perturbations generalize across different clean examples . We have introduced a family of fast methods for generating adversarial examples . We have demonstrated that adversarial training can result in regularization ; even further regularization than dropout . We have run control experiments that failed to reproduce this effect with simpler but less efficient regularizers including weight decay and adding noise . Models that are easy to optimize are easy to perturb . Linear models lack the capacity to resist adversarial perturbation ; only structures with a hidden layer ( where the universal approximator theorem applies ) should be trained to resist adversarial perturbation . RBF networks are resistant to adversarial examples . Models trained to model the input distribution are not resistant to adversarial examples . Ensembles are not resistant to adversarial examples . Some further observations concerning rubbish class examples are presented in the appendix : Rubbish class examples are ubiquitous and easily generated . Shallow linear models are not resistant to rubbish class examples . RBF networks are resistant to rubbish class examples . Gradient - based optimization is the workhorse of modern AI . Using a network that has been designed to be sufficiently linear \u2013 whether it is a ReLU or maxout network , an LSTM , or a sigmoid network that has been carefully configured not to saturate too much\u2013 we are able to fit most problems we care about , at least on the training set . The existence of adversarial examples suggests that being able to explain the training data or even being able to correctly label the test data does not imply that our models truly understand the tasks we have asked them to perform . Instead , their linear responses are overly confident at points that do not occur in the data distribution , and these confident predictions are often highly incorrect . This work has shown we can partially correct for this problem by explicitly identifying problematic points and correcting the model at each of these points . However , one may also conclude that the model families we use are intrinsically flawed . Ease of optimization has come at the cost of models that are easily misled . This motivates the development of optimization procedures that are able to train models whose behavior is more locally stable . subsubsection : Acknowledgments We would like to thank Geoffrey Hinton and Ilya Sutskever for helpful discussions . We would also like to thank Jeff Dean , Greg Corrado , and Oriol Vinyals for their feedback on drafts of this article . We would like to thank the developers of Theano bergstra + al:2010 - scipy , Bastien - Theano - 2012 , Pylearn2 pylearn2_arxiv_2013 , and DistBelief distbelief . plus 0.3ex bibliography : References appendix : Rubbish class examples A concept related to adversarial examples is the concept of examples drawn from a \u201c rubbish class . \u201d These examples are degenerate inputs that a human would classify as not belonging to any of the categories in the training set . If we call these classes in the training set \u201c the positive classes , \u201d then we want to be careful to avoid false positives on rubbish inputs \u2013 i.e . , we do not want to classify a degenerate input as being something real . In the case of separate binary classifiers for each class , we want all classes output near zero probability of the class being present , and in the case of a multinoulli distribution over only the positive classes , we would prefer that the classifier output a high - entropy ( nearly uniform ) distribution over the classes . The traditional approach to reducing vulnerability to rubbish inputs is to introduce an extra , constant output to the model representing the rubbish class LeCun + 98 . fool recently re - popularized the concept of the rubbish class in the context of computer vision under the name fooling images . As with adversarial examples , there has been a misconception that rubbish class false positives are hard to find , and that they are primarily a problem faced by deep networks . Our explanation of adversarial examples as the result of linearity and high dimensional spaces also applies to analyzing the behavior of the model on rubbish class examples . Linear models produce more extreme predictions at points that are far from the training data than at points that are near the training data . In order to find high confidence rubbish false positives for such a model , we need only generate a point that is far from the data , with larger norms yielding more confidence . RBF networks , which are not able to confidently predict the presence of any class far from the training data , are not fooled by this phenomenon . We generated 10 , 000 samples from and fed them into various classifiers on the MNIST dataset . In this context , we consider assigning a probability greater than 0.5 to any class to be an error . A naively trained maxout network with a softmax layer on top had an error rate of 98.35 % on Gaussian rubbish examples with an average confidence of 92.8 % on mistakes . Changing the top layer to independent sigmoids dropped the error rate to 68 % with an average confidence on mistakes of 87.9 % . On CIFAR - 10 , using 1 , 000 samples from , a convolutional maxout net obtains an error rate of 93.4 % , with an average confidence of 84.4 % . These experiments suggest that the optimization algorithms employed by fool are overkill ( or perhaps only needed on ImageNet ) , and that the rich geometric structure in their fooling images are due to the priors encoded in their search procedures , rather than those structures being uniquely able to cause false positives . Though fool focused their attention on deep networks , shallow linear models have the same problem . A softmax regression model has an error rate of 59.8 % on the rubbish examples , with an average confidence on mistakes of 70.8 % . If we use instead an RBF network , which does not behave like a linear function , we find an error rate of 0 % . Note that when the error rate is zero the average confidence on a mistake is undefined . fool focused on the problem of generating fooling images for a specific class , which is a harder problem than simply finding points that the network confidently classifies as belonging to any one class despite being defective . The above methods on MNIST and CIFAR - 10 tend to have a very skewed distribution over classes . On MNIST , 45.3 % of a naively trained maxout network \u2019s false positives were classified as 5s , and none were classified as 8s . Likewise , on CIFAR - 10 , 49.7 % of the convolutional network \u2019s false positives were classified as frogs , and none were classified as airplanes , automobiles , horses , ships , or trucks . To solve the problem introduced by fool of generating a fooling image for a particular class , we propose adding to a Gaussian sample as a fast method of generating a fooling image classified as class . If we repeat this sampling process until it succeeds , we a randomized algorithm with variable runtime . On CIFAR - 10 , we found that one sampling step had a 100 % success rate for frogs and trucks , and the hardest class was airplanes , with a success rate of 24.7 % per sampling step . Averaged over all ten classes , the method has an average per - step success rate of 75.3 % . We can thus generate any desired class with a handful of samples and no special priors , rather than tens of thousands of generations of evolution . To confirm that the resulting examples are indeed fooling images , and not images of real classes rendered by the gradient sign method , see Fig . [ reference ] . The success rate of this method in terms of generating members of class may degrade for datasets with more classes , since the risk of inadvertently increasing the activation of a different class increases in that case . We found that we were able to train a maxout network to have a zero percent error rate on Gaussian rubbish examples ( it was still vulnerable to rubbish examples generated by applying a fast gradient sign step to a Gaussian sample ) with no negative impact on its ability to classify clean examples . Unfortunately , unlike training on adversarial examples , this did not result in any significant reduction of the model \u2019s test set error rate . In conclusion , it appears that a randomly selected input to deep or shallow models built from linear parts is overwhelmingly likely to be processed incorrectly , and that these models only behave reasonably on a very thin manifold encompassing the training data .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["MNIST"]]], "Method": [[["Explaining_and_Harnessing_Adversarial_Examples"]]], "Metric": [[["Percentage_error"]]], "Task": [[["Image_Classification"]]]}]}
{"docid": "TST3-SREX-0051", "doctext": "document : aNMM : Ranking Short Answer Texts with Attention - Based Neural Matching Model As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers . To achieve good results , however , these models have been combined with additional features such as word overlap or BM25 scores . Without this combination , these models perform significantly worse than methods based on linguistic feature engineering . In this paper , we propose an attention based neural matching model for ranking short answer text . We adopt value - shared weighting scheme instead of position - shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network . Using the popular benchmark TREC QA data , we show that the relatively simple aNMM model can significantly outperform other neural network models that have been used for the question answering task , and is competitive with models that are combined with additional features . When aNMM is combined with additional features , it outperforms all baselines . 2016 , October 24 - 28 , 2016 , Indianapolis , IN , USA \u00a1 ccs2012 \u00bf \u00a1 concept \u00bf \u00a1 concept_id\u00bf10002951.10003317.10003338\u00a1 / concept_id \u00bf \u00a1 concept_desc\u00bfInformation systems Retrieval models and ranking\u00a1 / concept_desc \u00bf \u00a1 concept_significance\u00bf500\u00a1 / concept_significance \u00bf \u00a1 / concept \u00bf \u00a1 concept \u00bf \u00a1 concept_id\u00bf10002951.10003317.10003347.10003348\u00a1 / concept_id \u00bf \u00a1 concept_desc\u00bfInformation systems Question answering\u00a1 / concept_desc \u00bf \u00a1 concept_significance\u00bf500\u00a1 / concept_significance \u00bf \u00a1 / concept \u00bf \u00a1 / ccs2012 \u00bf [ 500 ] Information systems Retrieval models and ranking systems Question answering section : Introduction Question answering ( QA ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search . Many of current QA systems use a learning to rank approach that encodes question / answer pairs with complex linguistic features including lexical , syntactic and semantic features . For instance , Surdeanu et al . investigated a wide range of feature types including similarity features , translation features , density / frequency features and web correlation features for learning to rank answers and show improvements in accuracy . However , such methods rely on manual feature engineering , which is often time - consuming and requires domain dependent expertise and experience . Moreover , they may need additional NLP parsers or external knowledge sources that may not be available for some languages . Recently , researchers have been studying deep learning approaches to automatically learn semantic match between questions and answers . Such methods are built on the top of neural network models such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) . The proposed models have the benefit of not requiring hand - crafted linguistic features and external resources . Some of them achieve state - of - the art performance for the answer sentence selection task benchmarked by the TREC QA track . However , the weakness of the existing studies is that the proposed deep models , either based on CNNs or LSTMs , need to be combined with additional features such as word overlap features and BM25 to perform well . Without combining these additional features , their performance is significantly worse than the results obtained by the state - of - the - art methods based on linguistic feature engineering . This led us to propose the following research questions : RQ1 Without combining additional features , could we build deep learning models that can achieve comparable or even better performance than methods using feature engineering ? RQ2 By combining additional features , could our model outperform state - of - the - art models for question answering ? To address these research questions , we analyze the existing current deep learning architectures for answer ranking and make the following two key observations : Architectures not specifically designed for question / answer matching : Some methods employ CNNs for question / answer matching . However , CNNs are originally designed for computer vision ( CV ) , which uses position - shared weights with local perceptive filters , to learn spatial regularities in many CV tasks . However , such spatial regularities may not exist in semantic matching between questions and answers , since important similarity signals between question and answer terms could appear in any position due to the complex linguistic property of natural languages . Meanwhile , models based on LSTMs view the question / answer matching problem in a sequential way . Without direct interactions between question and answer terms , the model may not be able to capture sufficiently detailed matching signals between them . Lack of modeling question focus : Understanding the focus of questions , e.g. , important terms in a question , is helpful for ranking the answers correctly . For example , given a question like \u201c Where was the first burger king restaurant opened ? \u201d , it is critical for the answer to talk about \u201c burger \u201d , \u201c king \u201d , \u201c open \u201d , etc . Most existing text matching models do not explicitly model question focus . For example , models based on CNNs treat all the question terms as equally important when matching to answer terms . Models based on LSTMs usually model question terms closer to the end to be more important . To handle these issues in the existing deep learning architectures for ranking answers , we propose an attention based neural matching model ( aNMM ) . The novel properties of the proposed model and our contributions can be summarized as follows : Deep neural network with value - shared weights : We introduce a novel value - shared weighting scheme in deep neural networks as a counterpart of the position - shared weighting scheme in CNNs , based on the idea that semantic matching between a question and answer is mainly about the ( semantic similarity ) value regularities rather than spatial regularities . Incorporate attention scheme over question terms : We incorporate the attention scheme over the question terms using a gating function , so that we can explicitly discriminate the question term importance . Extensive experimental evaluation and promising results . We perform a thorough experimental study based on the TREC QA dataset from TREC QA tracks 8 - 13 , which appears to be one of the most widely used benchmarks for answer re - ranking . Unlike previous methods using CNNs and LSTMs , which showed inferior results without combining additional features , our model can achieve better performance than a state - of - art method using linguistic feature engineering and comparable performance with previous deep learning models with combined additional features . If we combine our model with a simple additional feature like QL , our method can achieve the state - of - the - art performance among current existing methods for ranking answers under multiple metrics . Roadmap . The rest of our paper is organized as follows . We will review related work in Section [ reference ] . In Section [ reference ] , we will present the proposed aNMM model with two components : value - shared weights and question attention network with gating functions . Two different architectures will be presented and analyzed . Section [ reference ] is a systematic experimental analysis using the TREC QA benchmark dataset . Finally , we conclude our paper and discuss future work in Section [ reference ] . section : Related Work Our work is related to several research areas , including deep learning models for text matching , factoid question answering , answer ranking in CQA and answer passage / sentence retrieval . Deep Learning Models for Text Matching . Recently there have been many deep learning models proposed for text matching and ranking . Such deep learning models include DSSM , CDSSM , ARC - I / ARC - II , DCNN , DeepMatch , MultiGranCNN and MatchPyramid . DSSM performs a non - linear projection to map the query and the documents to a common semantic space . The neural network models are trained using clickthrough data such that the conditional likelihood of the clicked document given the query is maximized . DeepMatch uses a topic model to construct the interactions between two texts and then makes different levels of abstractions with a deep architecture to model the relationships between topics . ARC - I and ARC - II are two different architectures proposed by Hu et . al . for matching natural language sentences . ARC - I firstly finds the representation of each sentence and then compares the representations of the two sentences with a multi - layer perceptron ( MLP ) . The drawback of ARC - I is that it defers the interaction between two sentences until their individual representation matures in the convolution model , and therefore has the risk of losing details , which could be important for the matching task . On the other hand , ARC - II is built directly on the interaction space between two sentences . Thus ARC - II makes two sentences meet before their own high - level representations mature , while still retaining the space for individual development of abstraction of each sentence . Our aNMM architecture adopts a similar design with ARC - II in the QA matching matrix where we build neural networks directly on the interaction of sentence term pairs . However , we adopt value - shared weights instead of position - shared weights as in the CNN used by ARC - II . We also add attention scheme to learn question term importance . Factoid Question Answering . There have been many previous studies on factoid question answering , most of which use the benchmark data from TREC QA track . Yih et . al . formulated answer sentence selection as a semantic matching problem with a latent word - alignment structure and conducted a series of experimental studies on leveraging proposed lexical semantic models . Iyyer et . al . introduced a recursive neural network ( RNN ) model that can reason over text that contains very few individual words by modeling textual compositionality . Yu et al . proposed an approach for answer sentence selection via distributed representations , and learned to match questions with answers by considering their semantic encoding . They combined the learning results of their model with word overlap features by training a logistic regression classifier . Wang and Nyberg proposed a method which uses a stacked bidirectional Long - Short Term Memory ( BLSTM ) network to sequentially read words from question and answer sentences , and then output their relevance scores . Their system needs to combine the stacked BLSTM relevance model with a BM25 score to achieve good performance . Severyn and Moschitti presented a convolutional neural network architecture for re - ranking pairs of short texts , where they learned the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data . They also need to combine additional features into their model to outperform previous methods . Unlike the previous research , our method can outperform previous methods using feature engineering without combining any additional features . With an additional simple feature like QL , our model is significantly better than the previous state - of - the - art methods including deep learning methods . Answer Ranking in CQA . There is also previous research on ranking answers from community question answering ( CQA ) sites . Surdeanu et al . investigated a wide range of feature types such as similarity features , translation features , density / frequency features for ranking answers to non - factoid questions in Yahoo ! Answers . Jansen et al . presented an answer re - ranking model for non - factoid questions that integrate lexical semantics with discourse information driven by two representations of discourse . Xue et al . proposed a retrieval model that combines a translation - based language model for the question part with a query likelihood approach for the answer part . Questions from CQA sites are mostly non - factoid questions . Our research is closer to factoid questions such as questions in TREC QA data . Answer Passage / Sentence Retrieval . Our work is also related to previous research on answer passage / sentence retrieval . Tymoshenko and Moschitti studied the use of syntactic and semantic structures obtained with shallow and deeper syntactic parsers in the answer passage re - ranking task . Keikha et al . developed an annotated data set for non - factoid answer finding using TREC GOV2 collections and topics . They annotated passage - level answers , revisited several passage retrieval models with this data , and came to the conclusion that the current methods are not effective for this task . Yang et al . developed effective methods for answer sentence retrieval using this annotated data by combining semantic features , context features and basic text matching features with a learning to rank approach . Our model is built on attention - based neural matching model with value - shared weighting schema . Unlike learning to rank approaches with feature engineering , our model can achieve good performance for ranking answers without any additional manual feature engineering , preprocessing of NLP parsers and external resources like knowledge bases . section : Attention - based Neural Matching Model In this section we present the proposed model referred as aNMM ( attention - based Neural Matching Model ) , which is shown in Figure [ reference ] . Before we introduce our model , we firstly define some terminologies . subsection : Terminology Short Answer Text : We use Short Answer Text to refer to a short fact , answer sentences or answer passages that can address users \u2019 information needs in the issued questions . This is the ranking object in this paper and includes answers in various lengths . In the experiments of this paper , we mainly focus on ranking answer sentences that contain correct answer facts as in TREC QA data . QA Matching Matrix : We use QA Matching Matrix to refer to a matrix which represents the semantic matching information of term pairs from a question and answer pair . Given a question with length and an answer with length , a QA matching matrix is an by matrix , where denote the semantic similarity between term and term measured by the cosine similarity of the corresponding word embeddings of terms . If and are the same term , we assign as . QA Matching Vector : We use QA Matching Vector to refer to a row in the QA matching matrix . As presented before , the - th row of the QA matching matrix contains the semantic similarity between and all terms in answer . subsection : Model Overview Our method contains three steps as follows : We construct QA matching matrix for each question and answer pair with pre - trained word embeddings . We then employ a deep neural network with value - shared weighting scheme in the first layer , and fully connected layers in the rest to learn hierarchical abstraction of the semantic matching between question and answer terms . Finally , we employ a question attention network to learn question term importance and produce the final ranking score . We propose two neural matching model architectures and compare the effectivenesses of them . We firstly describe a basic version of the architecture , which is referred to as aNMM - 1 . In the following sections , we will explain in detail the two major designs of aNMM - 1 , i.e. , value - shared weights and question attention network . subsection : Value - shared Weighting We first train word embeddings with the Word2Vec tool by Mikolov et al . with the English Wikipedia dump to construct QA matching matrices . Given a question sentence and an answer sentence , we compute the dot product of the normalized word embeddings of all term pairs to construct the QA matching matrix as defined in Section [ reference ] . A major problem with the QA matching matrix is the variable size due to the different lengths of answers for a given question . To solve this problem , one can use CNN with pooling strategy to handle the variable size . However , as we have mentioned before , CNNs basically use position - shared weighting scheme which may not fit semantic matching between questions and answers . Important question terms and semantically similar answer words could appear anywhere in questions / answers due to the complex linguistic property of natural languages . Thus we adopt the following method to handle the various length problem : Value - shared Weights : For this method , the assumption is that matching signals in different ranges play different roles in deciding the final ranking score . Thus we introduce the value - shared weighting scheme to learn the importance of different levels of matching signals . The comparison between the position - shared weight and value - shared weight is shown in Figure [ reference ] . We can see that for position - shared weights , the weight associated with a node only depends on its position or relative location as specified by the filters in CNN . However in our model , the weight associated with a node depends on its value . The value of a node denotes the strength of the matching signal between term pairs of questions and answers from the QA matching matrix , as explained in Section [ reference ] . Such a setting enables us to use the learned weights to encode how to combine different levels of matching signals . After this step , the size of the hidden representation becomes fixed and we can use normal fully connected layers to learn higher level representations . We use the term bin to denote a specific range of matching signals . since , if we set the size of bins as , then we have bins where there is a separate bin for to denote exact match of terms . Specifically , value - shared weights are adopted in the forward propagation prediction process from the input layer to the hidden layer over each question term in aNMM - 1 as follows : Input Layer to Hidden Layer . Let denote a dimensional model parameter from input layer to hidden layer . denotes the sum of all matching signals within the - th value range or bin . For each QA matching vector of a given query , the combined score after the activation function of the - th node in hidden layer is defined as where is the index of question words in . We use the sigmoid function as the activation function , which is commonly adopted in many neural network architectures . subsection : Question Attention Network In addition to value - shared weighting , another model component of aNMM - 1 is the question attention network . In a committee of neural networks which consists of multiple networks , we need to combine the output of these networks to output a final decision vector . The question attention network uses the gating function to control the output of each network in this process . Specifically , in aNMM - 1 we use the softmax gate function to combine the output of multiple networks where each network corresponds to a question term as shown in Figure [ reference ] . We feed the dot product of query word embedding and model parameter to the softmax function to represent the query term importance . In this setting , we can directly compare the relative term importance of query words within the same query with softmax function . We also tried sigmoid gate function , but this did not perform as well as softmax gate function . Softmax gate function is used in the forward propagation process from the hidden layer to the output layer as follows : Hidden Layer to Output Layer . From the hidden layer to the output layer , we add a softmax gate function to learn question attention . Let denote a dimensional vector which is a model parameter . We feed the dot product of query word embedding and to the softmax function to represent the query term importance as shown in Equation [ reference ] . Note that we normalize the query word embedding before computing the dot product . Unlike previous models like CNNs and BLSTM , which learn the semantic match score between questions and answers through representation learning from matching matrix or question / answer pair sequences , aNMM achieves this by combining semantic matching signals of term pairs in questions and answers weighted by the output of question attention network , where softmax gate functions help discriminate the term importance or attention on different question terms . subsection : Model Training For aNMM - 1 , the model parameters contain two sets : 1 ) The value - shared weights for combining matching signals from the input layer to the hidden layer . 2 ) The parameters in the gating function from the hidden layer to the output layer . To learn the model parameters from the training data , we adopt a pair - wise learning strategy with a large margin objective . Firstly we construct triples from the training data , with matched with better than with . We have the ranking - based loss as the objective function as following : where denote the predicted matching score for QA pair . During training stage , we will scan all the triples in training data . Given a triple , we will compute . If , we will skip this triple . Otherwise , we need to update model parameters with back propagation algorithm to minimize the objective function . Under softmax gate function setting , the gradients of w.r.t . from hidden layer to the output layer is shown in Equation [ reference ] where can be derived as The gradient of w.r.t . from input layer to hidden layer is shown in Equation [ reference ] . With the formulas of gradients , we can perform stochastic gradient descent to learn model parameters . We use mini - batch gradient descent to achieve more robust performance on the ranking task . For the learning rate , we adopt adaptive learning rate : , where will approach with more iterations . Such a setting has better guarantee for convergence . subsection : Extension to Deep Neural Networks with Multiple Sets of Value - shared Weights In aNMM - 1 , we can only use one set of value - shared weights for each QA matching vector . We further propose a more flexible neural network architecture which could enable us to use multiple sets of value - shared weights for each QA matching vector , leading to multiple intermediate nodes in the first hidden layer , as shown in Figure [ reference ] by the yellow color . We refer to this extended model as aNMM - 2 . The model architecture shown in Figure [ reference ] is corresponding to aNMM - 2 . subsubsection : Forward Propagation Prediction For aNMM - 2 , we add a hidden layer in the neural network where we learn multiple combined scores from the input layer . With this hidden layer , we define multiple weight vectors as . Thus becomes a two dimensional matrix . The formula for the forward propagation prediction is as follows : where and denote the softmax gate function . is the number of nodes in hidden layer 1 . is the model parameter from hidden layer 1 to hidden layer 2 , where we feed the linear combination of outputs of nodes in hidden layer 1 to an extra activation function comparing with Equation [ reference ] . Then from hidden layer 2 to output layer , we sum over all outputs of nodes in hidden layer 2 weighted by the outputs of softmax gate functions , which also form the question attention network . subsubsection : Back Propagation for Model Training For aNMM - 2 , we have three sets of model parameters : 1 ) from input layer to hidden layer 1 ; 2 ) from hidden layer 1 to hidden layer 2 ; 3 ) from hidden layer 2 to output layer . All three sets of parameters are updated through back propagation . The definition of the objective function is the same as Equation [ reference ] . The back propagation process for model parameter learning is described as follows : From hidden layer 2 to output layer . The gradients of the objective function w.r.t . is as following : Where From hidden layer 1 to hidden layer 2 . The gradients of the objective function w.r.t . is as following : Where . From input layer to hidden layer 1 . The gradients of the objective function w.r.t . is as following : Where Initially we will randomly give the values of model parameters . Then we will use back propagation to update the model parameters . When the learning process converge , we use the learned model parameters for prediction to rank short answer texts . section : Experiments subsection : Data Set and Experiment Settings We use the TREC QA data set created by Wang et . al . from TREC QA track 8 - 13 data , with candidate answers automatically selected from each question \u2019s document pool using a combination of overlapping non - stop word counts and pattern matching . This data set is one of the most widely used benchmarks for answer re - ranking . Table [ reference ] shows the statistics of this data set . The dataset contains a set of factoid questions with candidate answers which are limited to a single sentence . There are two training data sets : TRAIN and TRAIN - ALL . Answers in TRAIN have manual judgments for the answer correctness . The manual judgment of candidate answer sentences is provided for the entire TREC 13 set and for a part of questions from TREC 8 - 12 . TRAIN - ALL is another training set with much larger number of questions . The correctness of candidate answer sentences in TRAIN - ALL is identified by matching answer sentences with answer pattern regular expressions provided by TREC . This data set is more noisy , however it provides many more QA pairs for model training . There is a DEV set for hyper - parameter optimization and TEST set for model testing . We use the same train / dev / test partition in our experiments to directly compare our results with previous works . For data preprocess , we perform tokenization without stemming . We maintain stop words during the model training stage . Word Embeddings . We obtain pre - trained word embeddings with the Word2Vec tool by Mikolov et al . with the English Wikipedia dump . We use the skip - gram model with window size and filter words with frequency less than following the common practice in many neural embedding models . For the word vector dimension , we tune it as a hyper - parameter on the validation data starting from to . Embeddings for words not present are randomly initialized with sampled numbers from uniform distribution U [- 0.25 , 0.25 ] , which follows the same setting as . Model Hyper - parameters . For the setting of hyper - parameters , we set the number of bins as , word embedding dimension as for aNNM - 1 , the number of bins as , word embedding dimension as for aNNM - 2 after we tune hyper - parameters on the provided DEV set of TREC QA data . subsection : Evaluation and Metrics For evaluation , we rank answer sentences with the predicted score of each method and compare the rank list with the ground truth to compute metrics . We choose Mean Average Precision ( MAP ) and Mean Reciprocal Rank ( MRR ) , which are commonly used in information retrieval and question answering , as the metric to evaluate our model . The definition of MRR is as follows : where is the position of the first correct answer in the rank list . Thus MRR is only based on the rank of the first correct answer . It is more suitable for the cases where the rank of the first correct answer is emphasized or each question only have one correct answer . On the other hand , MAP looks at the ranks of all correct answers . It is computed as following : where is the average precision for each query . Thus MAP is the average performance on all correct answers . We use the official scripts for computing these metrics . subsection : Model Learning Results In this section , we give some qualitative analysis and visualization of our model learning results . Specifically , we analyze the learned value - shard weights and question term importance by aNMM . subsubsection : Value - shared Weight We take the learned value - shared weights of aNMM - 1 as the example . Figure [ reference ] shows the learned value - shared weights by aNMM - 1 . In aNMM - 1 , for each QA matching vector , there is only one bin node . Thus the learned value - shared weights for aNMM - 1 is a one dimension vector . For aNMM - 1 , we set the number of bins as as presented in Section [ reference ] . Note that the x - axis is the index of bin range and the y - axis is the value - shared weights corresponding to each bin range . The range of match signals is [ - 1 , 1 ] from the left to the right . We make the following observations : ( 1 ) The exact match signal which is corresponding to in the last bin is tied with a very large weight , which shows that exact match information is very important . ( 2 ) For positive matching score from , which is corresponding to bin index , the learned value - shared weights are different for matching score range ( bin index ) and matching score range ( bin index ) . We can observe many positive value - shared weights for matching score range and negative value - shared weights for matching score range . This makes sense since high semantic matching scores are positive indicators on answer correctness , whereas low semantic matching scores indicate that the candidate answer sentences contain irrelevant terms . ( 3 ) For negative matching scores from , we can see there is not a lot of differences between value - shared weights for different ranges . A major reason is that most similarity scores based on word embeddings are positive . Therefore , we can remove bins corresponding to negative matching scores to reduce the dimension of value - shared weight vectors , which can help improve the efficiency of the model training process . We will show more quantitative results on the comparison between value - shared weights and position - shared weights in CNN in Section [ reference ] . subsubsection : Question Term Importance Next we analyze the learned question term importance of our model . Due to the space limit , we also use the learned question term importance of aNMM - 1 as an example . Table [ reference ] shows the examples of learned question term importance by aNMM - 1 . We also visualize the question term importance in Figure [ reference ] . Based on the results in the table and the figure , we can clearly see that aNMM - 1 learns reasonable term importance . For instance , with the question attention network , aNMM - 1 discovers important terms like \u201c khmer \u201d , \u201c rouge \u201d , \u201c power \u201d as for the question \u201c When did the khmer rouge come into power ? \u201d . Terms like \u201c age \u201d , \u201c rossinin \u201d , \u201c stop \u201d , \u201c writing\u201d , \u201copera \u201d are highlighted for the question \u201c At what age did rossini stop writing opera ? \u201d . For the question \u201c Where was the first burger king restaurant opened ? \u201d mentioned in Section [ reference ] , \u201c burger \u201d , \u201c king \u201d , \u201c opened \u201d are treated as important question terms . An interesting question is how the learned term importance compare with traditional IR term weighting methods such as IDF . We design an experiment to compare aNMM - 1 / aNMM - 2 with aNMM - IDF , which is a degenerate version of our model where we use IDF to directly replace the output of question attention network . In this case , in Equation [ reference ] is replaced by the IDF of the j - th question term . Table [ reference ] shows the results . We find that if we replace the output of question attention network of aNMM with IDF , it will decrease the answer ranking performance , especially on TRAIN data . Thus , we can see that with the optimization process in the back propagation training process , aNMM can learn better question term weighting score than heuristic term weighting functions like IDF . subsection : Experimental Results for Ranking Answers subsubsection : Learning without Combining Additional Features Our first experimental setting is ranking answer sentences directly by the predicted score from aNMM without combining any additional features . This will enable us to answer RQ1 proposed in Section [ reference ] . Table [ reference ] shows the results of TREC QA on TRAIN and TRAIN - ALL without combining additional features . In this table , we compare the results of aNMM with other previous deep learning methods including CNN and LSTM . We summarize our observations as follows : ( 1 ) Both aNMM - 1 and aNMM - 2 show significant improvements for MAP and MRR on TRAIN and TRAIN - ALL data sets comparing with previous deep learning methods . Specifically , if we compare the results of aNMM - 1 with the strongest deep learning baseline method by Severyn et al . based on CNN , we can see aNMM - 1 outperform CNN for % in MAP on TRAIN , % in MAP on TRAIN - ALL . For MRR , we can also observe similar significant improvements of aNMM - 1 . These results show that with the value - shared weight scheme instead of the position - shared weight scheme in CNN and term importance learning with question attention network , aNMM can predict ranking scores with much higher accuracy comparing with previous deep learning models for ranking answers . ( 2 ) If we compare the results of aNMM - 1 and aNMM - 2 , we can see their results are very close . aNMM - 1 has slightly better performance than aNMM - 2 . This result indicates that adding one more hidden layer to incorporate multiple bin nodes does not necessarily increase the performance for answer ranking in TREC QA data . From the perspective of model efficiency , aNMM - 1 could be a better choice since it can be trained much faster with good prediction accuracy . However , for larger training data sets than TREC QA data , aNMM - 2 could have better performance since it has more model parameters and is suitable for fitting larger training data set . We leave the study of impact of the number of hidden layers in aNMM to future work . Table [ reference ] shows the comparison between aNMM with previous methods using feature engineering on TRAIN - ALL without combining additional features . We find that both aNMM - 1 and aNMM - 2 achieve better performance comparing with other methods using feature engineering . Specifically , comparing the results of aNMM - 1 with the strongest baseline by Yih et al . based on enhanced lexical semantic models , aNMM - 1 achieves % gain for MAP and % gain for MRR . These results show that it is possible to build a uniform deep learning model such that it can achieve better performance than methods using feature engineering . To the best of our knowledge , aNMM is the first deep learning model that can achieve good performance comparing with previous methods either based on deep learning models or feature engineering for ranking answers without any additional features , syntactic parsers and external resources except for pre - trained word embeddings . subsubsection : Learning with Combining Additional Features Our second experimental setting is to address RQ2 proposed in Section [ reference ] , where we ask whether our model can outperform the state - of - the - art performance achieved by CNN and LSTM for answer ranking when combining additional features . We combine the predicted score from aNMM - 1 and aNMM - 2 with the Query Likelihood ( QL ) score using LambdaMART following a similar approach to . We use the implementation of LambdaMART in jforests We compare the results with previous deep learning models with additional features . Table [ reference ] shows the results on TRAIN and TRAIN - ALL when combining additional features . We can see that with combined features , both aNMM - 1 and aNMM - 2 have better performance . aNMM - 1 also outperforms CNN by Severyn et al . which is the current state - of - the - art method for ranking answers in terms of both MAP and MRR on TRAIN and TRAIN - ALL . We also tried to combine aNMM score with other additional features such as word overlap features , IDF weighted word overlap features and BM25 as in previous research . The results were either similar or worse than combining aNMM score with QL . For aNMM , the gains after combining additional features are not as large as neural network models like CNN in and LSTM in . We think the reasons for this are two - fold : ( 1 ) The QA matching matrix in aNMM model can capture exact match information by assigning to matrix elements if the corresponding answer term and question term are the same . This exact match information include match between numbers and proper nouns , which are highlighted in previous research work as especially important for factoid questions answering , where most of the questions are of type what , when , who that are looking for answers containing numbers or proper nouns . Within aNMM architecture , this problem has already been handled with QA matching matrix . Thus incorporating word overlap features will not help much for improving the performance of aNMM . ( 2 ) In addition to exact match information , aNMM could also learn question term importance like IDF information through question attention network . Instead of empirically designing heuristic functions like IDF , aNMM can get learning based question term importance score . As analyzed in Section [ reference ] , with the optimization process in the back propagation training process , aNMM can learn similar or even better term weighting score than IDF . Thus combining aNMM score with features like IDF weighted word overlap features and BM25 may not increase the performance of aNMM by a large margin as the case in related research works . subsubsection : Results Summary [ b ] 0.48 [ b ] 0.48 Finally we summarize the results of previously published systems on the QA answer ranking task in Table [ reference ] . We can see aNMM trained with TRAIN - ALL set beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models . These results are very promising since aNMM requires no manual feature engineering , no expensive processing by various NLP parsers and no external results like large scale knowledge base except for pre - trained word embeddings . Furthermore , even without combining additional features , aNMM still performs well for answer ranking , showing significant improvements over previous deep learning model with no additional features and linguistic feature engineering methods . subsection : Parameter Sensitivity Analysis We perform parameter sensitivity analysis of our proposed model aNMM . We focus on aNMM - 1 as the example due to the space limitation . For aNMM - 1 , we fix the number of bins as and change the dimension of word vectors . Similarly , we fix the dimension of word vectors as and vary the number of bins . Figure [ reference ] shows the change of MAP and MRR on the validation data as we vary the hyper - parameters . We summarize our observations as follows : ( 1 ) For word vector dimension , the range is a good choice as much lower or higher word vector dimensions will hurt the performance . The choice of word vector dimension also depends on the size of training corpus . Larger corpus requires higher dimension of word vectors to embed terms in vocabulary . ( 2 ) For the number of bins , we can see that MAP and MRR will decrease as the bin number increase . Too many bins will increase the model complexity , which leads aNMM to be more likely to overfit the training data . Thus choosing suitable number of bins by optimizing hyper - parameter on validation data can help improve the performance of aNMM . section : Conclusions and Future Work In this paper , we propose an attention based neural matching model for ranking short answer text . We adopt value - shared weighting scheme instead of position - shared weighting scheme for combing different matching signals and incorporate question term importance learning using a question attention network . We perform a thorough experimental study with the TREC QA dataset from TREC QA tracks 8 - 13 and show promising results . Unlike previous methods including CNN as in and LSTM as in , which only show inferior results without combining additional features , our model can achieve better performance than the state - of - art method using linguistic feature engineering without additional features . With a simple additional feature , our method can achieve the new state - of - the - art performance among current existing methods . For further work , we will study other deep learning architectures for answer ranking and extend our work to include non - factoid question answering data sets . section : Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval , in part by NSF IIS - 1160894 , and in part by NSF grant # IIS - 1419693 . Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["TrecQA"]]], "Method": [[["aNMM"]]], "Metric": [[["MAP"]]], "Task": [[["Question_Answering"]]]}, {"incident_type": "SciREX_incident", "Material": [[["TrecQA"]]], "Method": [[["aNMM"]]], "Metric": [[["MRR"]]], "Task": [[["Question_Answering"]]]}]}
{"docid": "TST3-SREX-0052", "doctext": "document : Adversarial Discriminative Domain Adaptation Adversarial learning methods are a promising approach to training robust deep networks , and can generate complex samples across diverse domains . They also can improve recognition despite the presence of domain shift or dataset bias : several adversarial approaches to unsupervised domain adaptation have recently been introduced , which reduce the difference between the training and test domain distributions and thus improve generalization performance . Prior generative approaches show compelling visualizations , but are not optimal on discriminative tasks and can be limited to smaller shifts . Prior discriminative approaches could handle larger domain shifts , but imposed tied weights on the model and did not exploit a GAN - based loss . We first outline a novel generalized framework for adversarial adaptation , which subsumes recent state - of - the - art approaches as special cases , and we use this generalized view to better relate the prior approaches . We propose a previously unexplored instance of our general framework which combines discriminative modeling , untied weight sharing , and a GAN loss , which we call Adversarial Discriminative Domain Adaptation ( ADDA ) . We show that ADDA is more effective yet considerably simpler than competing domain - adversarial methods , and demonstrate the promise of our approach by exceeding state - of - the - art unsupervised adaptation results on standard cross - domain digit classification tasks and a new more difficult cross - modality object classification task . section : Introduction Deep convolutional networks , when trained on large - scale datasets , can learn representations which are generically usefull across a variety of tasks and visual domains . However , due to a phenomenon known as dataset bias or domain shift , recognition models trained along with these representations on one large dataset do not generalize well to novel datasets and tasks . The typical solution is to further fine - tune these networks on task - specific datasets \u2014 however , it is often prohibitively difficult and expensive to obtain enough labeled data to properly fine - tune the large number of parameters employed by deep multilayer networks . Domain adaptation methods attempt to mitigate the harmful effects of domain shift . Recent domain adaptation methods learn deep neural transformations that map both domains into a common feature space . This is generally achieved by optimizing the representation to minimize some measure of domain shift such as maximum mean discrepancy or correlation distances . An alternative is to reconstruct the target domain from the source representation . Adversarial adaptation methods have become an increasingly popular incarnation of this type of approach which seeks to minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator . These methods are closely related to generative adversarial learning , which pits two networks against each other \u2014 a generator and a discriminator . The generator is trained to produce images in a way that confuses the discriminator , which in turn tries to distinguish them from real image examples . In domain adaptation , this principle has been employed to ensure that the network can not distinguish between the distributions of its training and test domain examples . However , each algorithm makes different design choices such as whether to use a generator , which loss function to employ , or whether to share weights across domains . For example , share weights and learn a symmetric mapping of both source and target images to the shared feature space , while decouple some layers thus learning a partially asymmetric mapping . In this work , we propose a novel unified framework for adversarial domain adaptation , allowing us to effectively examine the different factors of variation between the existing approaches and clearly view the similarities they each share . Our framework unifies design choices such as weight - sharing , base models , and adversarial losses and subsumes previous work , while also facilitating the design of novel instantiations that improve upon existing ones . In particular , we observe that generative modeling of input image distributions is not necessary , as the ultimate task is to learn a discriminative representation . On the other hand , asymmetric mappings can better model the difference in low level features than symmetric ones . We therefore propose a previously unexplored unsupervised adversarial adaptation method , Adversarial Discriminative Domain Adaptation ( ADDA ) , illustrated in Figure [ reference ] . ADDA first learns a discriminative representation using the labels in the source domain and then a separate encoding that maps the target data to the same space using an asymmetric mapping learned through a domain - adversarial loss . Our approach is simple yet surprisingly powerful and achieves state - of - the - art visual adaptation results on the MNIST , USPS , and SVHN digits datasets . We also test its potential to bridge the gap between even more difficult cross - modality shifts , without requiring instance constraints , by transferring object classifiers from RGB color images to depth observations . section : Related work There has been extensive prior work on domain transfer learning , see e.g. , . Recent work has focused on transferring deep neural network representations from a labeled source datasets to a target domain where labeled data is sparse or non - existent . In the case of unlabeled target domains ( the focus of this paper ) the main strategy has been to guide feature learning by minimizing the difference between the source and target feature distributions . Several methods have used the Maximum Mean Discrepancy ( MMD ) loss for this purpose . MMD computes the norm of the difference between two domain means . The DDC method used MMD in addition to the regular classification loss on the source to learn a representation that is both discriminative and domain invariant . The Deep Adaptation Network ( DAN ) applied MMD to layers embedded in a reproducing kernel Hilbert space , effectively matching higher order statistics of the two distributions . In contrast , the deep Correlation Alignment ( CORAL ) method proposed to match the mean and covariance of the two distributions . Other methods have chosen an adversarial loss to minimize domain shift , learning a representation that is simultaneously discriminative of source labels while not being able to distinguish between domains . proposed adding a domain classifier ( a single fully connected layer ) that predicts the binary domain label of the inputs and designed a domain confusion loss to encourage its prediction to be as close as possible to a uniform distribution over binary labels . The gradient reversal algorithm ( ReverseGrad ) proposed in also treats domain invariance as a binary classification problem , but directly maximizes the loss of the domain classifier by reversing its gradients . DRCN takes a similar approach but also learns to reconstruct target domain images . In related work , adversarial learning has been explored for generative tasks . The Generative Adversarial Network ( GAN ) method is a generative deep model that pits two networks against one another : a generative model G that captures the data distribution and a discriminative model D that distinguishes between samples drawn from G and images drawn from the training data by predicting a binary label . The networks are trained jointly using backprop on the label prediction loss in a mini - max fashion : simultaneously update G to minimize the loss while also updating D to maximize the loss ( fooling the discriminator ) . The advantage of GAN over other generative methods is that there is no need for complex sampling or inference during training ; the downside is that it may be difficult to train . GANs have been applied to generate natural images of objects , such as digits and faces , and have been extended in several ways . The BiGAN approach extends GANs to also learn the inverse mapping from the image data back into the latent space , and shows that this can learn features useful for image classification tasks . The conditional generative adversarial net ( CGAN ) is an extension of the GAN where both networks G and D receive an additional vector of information as input . This might contain , say , information about the class of the training example . The authors apply CGAN to generate a ( possibly multi - modal ) distribution of tag - vectors conditional on image features . Recently the CoGAN approach applied GANs to the domain transfer problem by training two GANs to generate the source and target images respectively . The approach achieves a domain invariant feature space by tying the high - level layer parameters of the two GANs , and shows that the same noise input can generate a corresponding pair of images from the two distributions . Domain adaptation was performed by training a classifier on the discriminator output and applied to shifts between the MNIST and USPS digit datasets . However , this approach relies on the generators finding a mapping from the shared high - level layer feature space to full images in both domains . This can work well for say digits which can be difficult in the case of more distinct domains . In this paper , we observe that modeling the image distributions is not strictly necessary to achieve domain adaptation , as long as the latent feature space is domain invariant , and propose a discriminative approach . section : Generalized adversarial adaptation We present a general framework for adversarial unsupervised adaptation methods . In unsupervised adaptation , we assume access to source images and labels drawn from a source domain distribution , as well as target images drawn from a target distribution , where there are no label observations . Our goal is to learn a target representation , and classifier that can correctly classify target images into one of categories at test time , despite the lack of in domain annotations . Since direct supervised learning on the target is not possible , domain adaptation instead learns a source representation mapping , , along with a source classifier , , and then learns to adapt that model for use in the target domain . In adversarial adaptive methods , the main goal is to regularize the learning of the source and target mappings , and , so as to minimize the distance between the empirical source and target mapping distributions : and . If this is the case then the source classification model , , can be directly applied to the target representations , elimating the need to learn a separate target classifier and instead setting , . The source classification model is then trained using the standard supervised loss below : We are now able to describe our full general framework view of adversarial adaptation approaches . We note that all approaches minimize source and target representation distances through alternating minimization between two functions . First a domain discriminator , , which classifies whether a data point is drawn from the source or the target domain . Thus , is optimized according to a standard supervised loss , where the labels indicate the origin domain , defined below : Second , the source and target mappings are optimized according to a constrained adversarial objective , whose particular instantiation may vary across methods . Thus , we can derive a generic formulation for domain adversarial techniques below : In the next sections , we demonstrate the value of our framework by positioning recent domain adversarial approaches within our framework . We describe the potential mapping structure , mapping optimization constraints ( ) choices and finally choices of adversarial mapping loss , . subsection : Source and target mappings In the case of learning a source mapping alone it is clear that supervised training through a latent space discriminative loss using the known labels results in the best representation for final source recognition . However , given that our target domain is unlabeled , it remains an open question how best to minimize the distance between the source and target mappings . Thus the first choice to be made is in the particular parameterization of these mappings . Because unsupervised domain adaptation generally considers target discriminative tasks such as classification , previous adaptation methods have generally relied on adapting discriminative models between domains . With a discriminative base model , input images are mapped into a feature space that is useful for a discriminative task such as image classification . For example , in the case of digit classification this may be the standard LeNet model . However , Liu and Tuzel achieve state of the art results on unsupervised MNIST - USPS using two generative adversarial networks . These generative models use random noise as input to generate samples in image space \u2014 generally , an intermediate feature of an adversarial discriminator is then used as a feature for training a task - specific classifier . Once the mapping parameterization is determined for the source , we must decide how to parametrize the target mapping . In general , the target mapping almost always matches the source in terms of the specific functional layer ( architecture ) , but different methods have proposed various regularization techniques . All methods initialize the target mapping parameters with the source , but different methods choose different constraints between the source and target mappings , . The goal is to make sure that the target mapping is set so as to minimize the distance between the source and target domains under their respective mappings , while crucially also maintaining a target mapping that is category discriminative . Consider a layered representations where each layer parameters are denoted as , or , for a given set of equivalent layers , . Then the space of constraints explored in the literature can be described through layerwise equality constraints as follows : where each individual layer can be constrained independently . A very common form of constraint is source and target layerwise equality : It is also common to leave layers unconstrained . These equality constraints can easily be imposed within a convolutional network framework through weight sharing . For many prior adversarial adaptation methods , all layers are constrained , thus enforcing exact source and target mapping consistency . Learning a symmetric transformation reduces the number of parameters in the model and ensures that the mapping used for the target is discriminative at least when applied to the source domain . However , this may make the optimization poorly conditioned , since the same network must handle images from two separate domains . An alternative approach is instead to learn an asymmetric transformation with only a subset of the layers constrained , thus enforcing partial alignment . Rozantsev et al . showed that partially shared weights can lead to effective adaptation in both supervised and unsupervised settings . As a result , some recent methods have favored untying weights ( fully or partially ) between the two domains , allowing models to learn parameters for each domain individually . subsection : Adversarial losses Once we have decided on a parametrization of , we employ an adversarial loss to learn the actual mapping . There are various different possible choices of adversarial loss functions , each of which have their own unique use cases . All adversarial losses train the adversarial discriminator using a standard classification loss , , previously stated in Equation [ reference ] . However , they differ in the loss used to train the mapping , . The gradient reversal layer of optimizes the mapping to maximize the discriminator loss directly : This optimization corresponds to the true minimax objective for generative adversarial networks . However , this objective can be problematic , since early on during training the discriminator converges quickly , causing the gradient to vanish . When training GANs , rather than directly using the minimax loss , it is typical to train the generator with the standard loss function with inverted labels . This splits the optimization into two independent objectives , one for the generator and one for the discriminator , where remains unchanged , but becomes : This objective has the same fixed - point properties as the minimax loss but provides stronger gradients to the target mapping . We refer to this modified loss function as the \u201c GAN loss function \u201d for the remainder of this paper . Note that , in this setting , we use independent mappings for source and target and learn only adversarially . This mimics the GAN setting , where the real image distribution remains fixed , and the generating distribution is learned to match it . The GAN loss function is the standard choice in the setting where the generator is attempting to mimic another unchanging distribution . However , in the setting where both distributions are changing , this objective will lead to oscillation \u2014 when the mapping converges to its optimum , the discriminator can simply flip the sign of its prediction in response . Tzeng et al . instead proposed the domain confusion objective , under which the mapping is trained using a cross - entropy loss function against a uniform distribution : This loss ensures that the adversarial discriminator views the two domains identically . section : Adversarial discriminative domain adaptation The benefit of our generalized framework for domain adversarial methods is that it directly enables the development of novel adaptive methods . In fact , designing a new method has now been simplified to the space of making three design choices : whether to use a generative or discriminative base model , whether to tie or untie the weights , and which adversarial learning objective to use . In light of this view we can summarize our method , adversarial discriminative domain adaptation ( ADDA ) , as well as its connection to prior work , according to our choices ( see Table [ reference ] \u201c ADDA \u201d ) . Specifically , we use a discriminative base model , unshared weights , and the standard GAN loss . We illustrate our overall sequential training procedure in Figure [ reference ] . First , we choose a discriminative base model , as we hypothesize that much of the parameters required to generate convincing in - domain samples are irrelevant for discriminative adaptation tasks . Most prior adversarial adaptive methods optimize directly in a discriminative space for this reason . One counter - example is CoGANs . However , this method has only shown dominance in settings where the source and target domain are very similar such as MNIST and USPS , and in our experiments we have had difficulty getting it to converge for larger distribution shifts . Next , we choose to allow independent source and target mappings by untying the weights . This is a more flexible learing paradigm as it allows more domain specific feature extraction to be learned . However , note that the target domain has no label access , and thus without weight sharing a target model may quickly learn a degenerate solution if we do not take care with proper initialization and training procedures . Therefore , we use the pre - trained source model as an intitialization for the target representation space and fix the source model during adversarial training . In doing so , we are effectively learning an asymmetric mapping , in which we modify the target model so as to match the source distribution . This is most similar to the original generative adversarial learning setting , where a generated space is updated until it is indistinguishable with a fixed real space . Therefore , we choose the inverted label GAN loss described in the previous section . Our proposed method , ADDA , thus corresponds to the following unconstrained optimization : We choose to optimize this objective in stages . We begin by optimizing over and by training using the labeled source data , and . Because we have opted to leave fixed while learning , we can thus optimize and without revisiting the first objective term . A summary of this entire training process is provided in Figure [ reference ] . We note that the unified framework presented in the previous section has enabled us to compare prior domain adversarial methods and make informed decisions about the different factors of variation . Through this framework we are able to motivate a novel domain adaptation method , ADDA , and offer insight into our design decisions . In the next section we demonstrate promising results on unsupervised adaptation benchmark tasks , studying adaptation across digits and across modalities . section : Experiments We now evaluate ADDA for unsupervised classification adaptation across four different domain shifts . We explore three digits datasets of varying difficulty : MNIST , USPS , and SVHN . We additionally evaluate on the NYUD dataset to study adaptation across modalities . Example images from all experimental datasets are provided in Figure [ reference ] . For the case of digit adaptation , we compare against multiple state - of - the - art unsupervised adaptation methods , all based upon domain adversarial learning objectives . In 3 of 4 of our experimental setups , our method outperforms all competing approaches , and in the last domain shift studied , our approach outperforms all but one competing approach . We also validate our model on a real - world modality adaptation task using the NYU depth dataset . Despite a large domain shift between the RGB and depth modalities , ADDA learns a useful depth representation without any labeled depth data and improves over the nonadaptive baseline by over 50 % ( relative ) . subsection : MNIST , USPS , and SVHN digits datasets We experimentally validate our proposed method in an unsupervised adaptation task between the MNIST , USPS , and SVHN digits datasets , which consist 10 classes of digits . Example images from each dataset are visualized in Figure [ reference ] and Table [ reference ] . For adaptation between MNIST and USPS , we follow the training protocol established in , sampling 2000 images from MNIST and 1800 from USPS . For adaptation between SVHN and MNIST , we use the full training sets for comparison against . All experiments are performed in the unsupervised settings , where labels in the target domain are withheld , and we consider adaptation in three directions : MNIST USPS , USPS MNIST , and SVHN MNIST . For these experiments , we use the simple modified LeNet architecture provided in the Caffe source code . When training with ADDA , our adversarial discriminator consists of 3 fully connected layers : two layers with 500 hidden units followed by the final discriminator output . Each of the 500 - unit layers uses a ReLU activation function . Results of our experiment are provided in Table [ reference ] . On the easier MNIST and USPS shifts ADDA achieves comparable performance to the current state - of - the - art , CoGANs , despite being a considerably simpler model . This provides compelling evidence that the machinery required to generate images is largely irrelevant to enabling effective adaptation . Additionally , we show convincing results on the challenging SVHN and MNIST task in comparison to other methods , indicating that our method has the potential to generalize to a variety of settings . In contrast , we were unable to get CoGANs to converge on SVHN and MNIST \u2014 because the domains are so disparate , we were unable to train coupled generators for them . subsection : Modality adaptation bathtub bed bookshelf box chair counter desk door dresser garbage bin lamp monitor night stand pillow sink sofa table television toilet overall We use the NYU depth dataset , which contains bounding box annotations for 19 object classes in 1449 images from indoor scenes . The dataset is split into a train ( 381 images ) , val ( 414 images ) and test ( 654 ) sets . To perform our cross - modality adaptation , we first crop out tight bounding boxes around instances of these 19 classes present in the dataset and evaluate on a 19 - way classification task over object crops . In order to ensure that the same instance is not seen in both domains , we use the RGB images from the train split as the source domain and the depth images from the val split as the target domain . This corresponds to 2 , 186 labeled source images and 2 , 401 unlabeled target images . Figure [ reference ] visualizes samples from each of the two domains . We consider the task of adaptation between these RGB and HHA encoded depth images , using them as source and target domains respectively . Because the bounding boxes are tight and relatively low resolution , accurate classification is quite difficult , even when evaluating in - domain . In addition , the dataset has very few examples for certain classes , such as toilet and bathtub , which directly translates to reduced classification performance . For this experiment , our base architecture is the VGG - 16 architecture , initializing from weights pretrained on ImageNet . This network is then fully fine - tuned on the source domain for 20 , 000 iterations using a batch size of 128 . When training with ADDA , the adversarial discriminator consists of three additional fully connected layers : 1024 hidden units , 2048 hidden units , then the adversarial discriminator output . With the exception of the output , these additionally fully connected layers use a ReLU activation function . ADDA training then proceeds for another 20 , 000 iterations , again with a batch size of 128 . We find that our method , ADDA , greatly improves classification accuracy for this task . For certain categories , like counter , classification accuracy goes from 2.9 % under the source only baseline up to 44.7 % after adaptation . In general , average accuracy across all classes improves significantly from 13.9 % to 21.1 % . However , not all classes improve . Three classes have no correctly labeled target images before adaptation , and adaptation is unable to recover performance on these classes . Additionally , the classes of pillow and nightstand suffer performance loss after adaptation . For additional insight on what effect ADDA has on classification , Figure [ reference ] plots confusion matrices before adaptation , after adaptation , and in the hypothetical best - case scenario where the target labels are present . Examining the confusion matrix for the source only baseline reveals that the domain shift is quite large \u2014 as a result , the network is poorly conditioned and incorrectly predicts pillow for the majority of the dataset . This tendency to output pillow also explains why the source only model achieves such abnormally high accuracy on the pillow class , despite poor performance on the rest of the classes . In contrast , the classifier trained using ADDA predicts a much wider variety of classes . This leads to decreased accuracy for the pillow class , but significantly higher accuracies for many of the other classes . Additionally , comparison with the \u201c train on target \u201d model reveals that many of the mistakes the ADDA model makes are reasonable , such as confusion between the chair and table classes , indicating that the ADDA model is learning a useful representation on depth images . section : Conclusion We have proposed a unified framework for unsupervised domain adaptation techniques based on adversarial learning objectives . Our framework provides a simplified and cohesive view by which we may understand and connect the similarities and differences between recently proposed adaptation methods . Through this comparison , we are able to understand the benefits and key ideas from each approach and to combine these strategies into a new adaptation method , ADDA . We present evaluation across four domain shifts for our unsupervised adaptation approach . Our method generalizes well across a variety of tasks , achieving strong results on benchmark adaptation datasets as well as a challenging cross - modality adaptation task . Additional analysis indicates that the representations learned via ADDA resemble features learned with supervisory data in the target domain much more closely than unadapted features , providing further evidence that ADDA is effective at partially undoing the effects of domain shift . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["SVNH-to-MNIST"]]], "Method": [[["ADDA"]]], "Metric": [[["Classification_Accuracy"]]], "Task": [[["Unsupervised_Image-To-Image_Translation"]]]}]}
{"docid": "TST3-SREX-0053", "doctext": "document : Exploiting temporal information for 3D human pose estimation In this work , we address the problem of 3D human pose estimation from a sequence of 2D human poses . Although the recent success of deep networks has led many state - of - the - art methods for 3D pose estimation to train deep networks end - to - end to predict from images directly , the top - performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps : using a state - of - the - art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space . They also showed that a low - dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy . However , estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter . Therefore , in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses . We designed a sequence - to - sequence network composed of layer - normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training . We found that the knowledge of temporal consistency improves the best reported result on Human3.6 M dataset by approximately and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails . section : Introduction The task of estimating 3D human pose from 2D representations like monocular images or videos is an open research problem among the computer vision and graphics community for a long time . An understanding of human posture and limb articulation is important for high level computer vision tasks such as human action or activity recognition , sports analysis , augmented and virtual reality . A 2D representation of human pose , which is considered to be much easier to estimate , can be used for these tasks . However , 2D poses can be ambiguous because of occlusion and foreshortening . Additionally poses that are totally different can appear to be similar in 2D because of the way they are projected as shown in Figure [ reference ] . The depth information in 3D representation of human pose makes it free from such ambiguities and hence can improve performance for higher level tasks . Moreover , 3D pose can be very useful in computer animation , where the articulated pose of a person in 3D can be used to accurately model human posture and movement . However , 3D pose estimation is an ill - posed problem because of the inherent ambiguity in back - projecting a 2D view of an object to the 3D space maintaining its structure . Since the 3D pose of a person can be projected in an infinite number of ways on a 2D plane , the mapping from a 2D pose to 3D is not unique . Moreover , obtaining a dataset for 3D pose is difficult and expensive . Unlike the 2D pose datasets where the users can manually label the keypoints by mouse clicks , 3D pose datasets require a complicated laboratory setup with motion capture sensors and cameras . Hence , there is a lack of motion capture datasets for images in - the - wild . Over the years , different techniques have been used to address the problem of 3D pose estimation . Earlier methods used to focus on extracting features , invariant to factors such as background scenes , lighting , and skin color from images and mapping them into 3D human pose . With the success of deep networks , recent methods tend to focus on training a deep convolutional neural network ( CNN ) end - to - end to estimate 3D poses from images directly . Some approaches divided the 3D pose estimation task into first predicting the joint locations in 2D using 2D pose estimators and then back - projecting them to estimate the 3D joint locations . These results suggest the effectiveness of decoupling the task of 3D pose estimation where 2D pose estimator abstracts the complexities in the image . In this paper , we also adopt the decoupled approach to 3D pose estimation . However , predicting 3D pose for each frame individually can lead to jitter in videos because the errors in each frame are independent of each other . Therefore , we designed a sequence - to - sequence network with shortcut connections on the decoder side that predicts a sequence of temporally consistent 3D poses given a sequence of 2D poses . Each unit of our network is a Long Short - Term Memory ( LSTM ) unit with layer normalization and recurrent dropout . We also imposed a temporal smoothness constraint on the predicted 3D poses during training to ensure that our predictions are smooth over a sequence . Our network achieves the state - of - the - art result on the Human3.6 M dataset improving the previous best result by approximately . We also obtained the lowest error for every action class in Human3.6 M dataset . Moreover , we observed that our network predicted meaningful 3D poses on Youtube videos , even when the detections from the 2D pose detector were extremely noisy or meaningless . This shows the effectiveness of using temporal information . In short our contributions in this work are : Designing an efficient sequence - to - sequence network that achieves the state - of - the - art results for every action class of Human3.6 M dataset and can be trained very fast . Exploiting the ability of sequence - to - sequence networks to take into account the events in the past , to predict temporally consistent 3D poses . Effectively imposing temporal consistency constraint on the predicted 3D poses during training so that the errors in the predictions are distributed smoothly over the sequence . Using only the previous frames to understand temporal context so that it can be deployed online and real - time . section : Related Work paragraph : Representation of 3D pose Both model - based and model - free representations of 3D human pose have been used in the past . The most common model - based representation is a skeleton defined by a kinematic tree of a set of joints , parameterized by the offset and rotational parameters of each joint relative to its parent . Several 3D pose methods have used this representation . Others model 3D pose as a sparse linear combination of an over - complete dictionary of basis poses . However , we have chosen a model - free representation of 3D pose , where a 3D pose is simply a set of 3D joint locations relative to the root node like several recent approaches . This representation is much simpler and low - dimensional . paragraph : Estimating 3D pose from 2D joints Lee and Chen were the first to infer 3D joint locations from their 2D projections given the bone lengths using a binary decision tree where each branch corresponds to two possible states of a joint relative to its parent . Jiang used the 2D joint locations to estimate a set of hypothesis 3D poses using Taylor \u2019s algorithm and used them to query a large database of motion capture data to find the nearest neighbor . Gupta et al . and Chen and Ramanan also used this idea of using the detected 2D pose to query a large database of exemplar poses to find the nearest nearest neighbor 3D pose . Another common approach to estimating 3D joint locations given the 2D pose is to separate the camera pose variability from the intrinsic deformation of the human body , the latter of which is modeled by learning an over - complete dictionary of basis 3D poses from a large database of motion capture data . A valid 3D pose is defined by a sparse linear combination of the bases and by transforming the points using transformation matrix representing camera extrinsic parameters . Moreno - Nouguer used the pair - wise distance matrix of 2D joints to learn a distance matrix for 3D joints , which they found invariant up to a rigid similarity transform with the ground truth 3D and used multi - dimensional scaling ( MDS ) with pose - priors to rule out the ambiguities . Martinez et al . designed a fully connected network with shortcut connections every two linear layers to estimate 3D joint locations relative to the root node in the camera coordinate space . paragraph : Deep network based methods With the success of deep networks , many have designed networks that can be trained end - to - end to predict 3D poses from images directly . Li et al . and Park et al . designed CNNs to jointly predict 2D and 3D poses . Mehta et al . and Sun et al . used transfer learning to transfer the knowledge learned for 2D human pose estimation to the task of 3D pose estimation . Pavlakos et al . extended the stacked - hourglass network originally designed to predict 2D heatmaps of each joint to make it predict 3D volumetric heatmaps . Tome et al . also extended a 2D pose estimator called Convolutional Pose Machine ( CPM ) to make it predict 3D pose . Rogesz and Schmid and Varol et al . augmented the training data with synthetic images and trained CNNs to predict 3D poses from real images . Sun et al . designed a unified network that can regress both 2D and 3D poses at the same time given an image . Hence during training time , in - the - wild images which do not have any ground truth 3D poses can be combined with the data with ground truth 3D poses . A similar idea of exploiting in - the - wild images to learn pose structure was used by Fang et al . . They learned a pose grammar that encodes the possible human pose configurations . paragraph : Using temporal information Since estimating poses for each frame individually leads to incoherent and jittery predictions over a sequence , many approaches tried to exploit temporal information . Andriluka et al . used tracking - by - detection to associate 2D poses detected in each frame individually and used them to retrieve 3D pose . Tekin et al . used a CNN to first align bounding boxes of successive frames so that the person in the image is always at the center of the box and then extracted 3D HOG features densely over the spatio - temporal volume from which they regress the 3D pose of the central frame . Mehta et al . implemented a real - time system for 3D pose estimation that applies temporal filtering across 2D and 3D poses from previous frames to predict a temporally consistent 3D pose . Lin et al . performed a multi - stage sequential refinement using LSTMs to predict 3D pose sequences using previously predicted 2D pose representations and 3D pose . We focus on predicting temporally consistent 3D poses by learning the temporal context of a sequence using a form of sequence - to - sequence network . Unlike Lin et al . our method does not need multiple stages of refinement . It is simpler and requires fewer parameters to train , leading to much improved performance . section : Our Approach paragraph : Network Design We designed a sequence - to - sequence network with LSTM units and residual connections on the decoder side to predict a temporally coherent sequence of 3D poses given a sequence of 2D joint locations . Figure [ reference ] shows the architecture of our network . The motivation behind using a sequence - to - sequence network comes from its application on the task of Neural Machine Translation ( NMT ) by Sutskever et al . , where their model translates a sentence in one language to a sentence in another language e.g. English to French . In a language translation model , the input and output sentences can have different lengths . Although our case is analogous to the NMT , the input and output sequences always have the same length while the input vectors to the encoder and decoder have different dimensions . The encoder side of our network takes a sequence of 2D poses and encodes them in a fixed size high dimensional vector in the hidden state of its final LSTM unit . Since the LSTMs are excellent in memorizing events and information from the past , the encoded vector stores the 2D pose information of all the frames . The initial state of the decoder is initialized by the final state of the encoder . A token is passed as initial input to the decoder , which in our case is a vector of ones , telling it to start decoding . Given a 3D pose estimate at a time step each decoder unit predicts the 3D pose for next time step . Note that the order of the input sequence is reversed as recommended by Sutskever et al . . The shortcut connections on the decoder side cause each decoder unit to estimate the amount of perturbation in the 3D pose from the previous frame instead of having to estimate the actual 3D pose for each frame . As suggested by He et al . , such a mapping is easier to learn for the network . We use layer normalization and recurrent dropout to regularize our network . Ba et al . came up with the idea of layer normalization which estimates the normalization statistics ( mean and standard deviation ) from the summed inputs to the recurrent neurons of hidden layer on a single training example to regularize the RNN units . Similarly , Zaremba et al . proposed the idea of applying dropout only on the non - recurrent connections of the network with a certain probability while always keeping the recurrent connections intact because they are necessary for the recurrent units to remember the information from the past . paragraph : Loss function Given a sequence of 2D joint locations as input , our network predicts a sequence of 3D joint locations relative to the root node ( central hip ) . We predict each 3D pose in the camera coordinate space instead of predicting them in an arbitrary global frame as suggested by Martinez et al . . We impose a temporal smoothness constraint on the predicted 3D joint locations to ensure that the prediction of each joint in one frame does not differ too much from its previous frame . Because the 2D pose detectors work on individual frames , even with the minimal movement of the subject in the image , the detections from successive frames may vary , particularly for the joints which move fast or are prone to occlusion . Hence , we made an assumption that the subject does not move too much in successive frames given the frame rate is high enough . Therefore , we added the L2 norm of the first order derivative on the 3D joint locations with respect to time to our loss function during training . This constraint helps us to estimate 3D poses reliably even when the 2D pose detector fails for a few frames within the temporal window without any post - processing . Empirically we found that certain joints are more difficult to estimate accurately e.g. wrist , ankle , elbow compared to others . To address this issue , we partitioned the joints into three disjoint sets , and based on their contribution to overall error . We observed that the joints connected to the torso and the head e.g. hips , shoulders , neck are always predicted with high accuracy compared to those joints belonging to the limbs and therefore put them in the set . The joints of the limbs , especially the joints on the arms , are always more difficult to predict due to their high range of motion and occlusion . We put the knees and the ankles in the set and the elbow and wrist in . We multiply the derivatives of each set of joints with different scalar values based on their contribution to the overall error . Therefore our loss function consists of the sum of two separate terms : Mean Squared Error ( MSE ) of different sequences of 3D joint locations ; and the mean of the L2 norm of the first order derivative of sequences of 3D joint locations with respect to time , where the joints are divided into three disjoint sets . The MSE over sequences , each of time - steps , of 3D joint locations is given by Here , denotes the estimated 3D joint locations while denotes 3D ground truth . The mean of L2 norm of the first order derivative of sequences of 3D joint locations , each of length , with respect to time is given by In the above equation , , and denotes the predicted 3D locations of joints belonging to the sets , and respectively . The and are scalar hyper - parameters to control the significance of the derivatives of 3D locations of each of the three set of joints . A higher weight is assigned to the set of joints which are generally predicted with higher error . The overall loss function for our network is given as Here and are scalar hyper - parameters regulating the importance of each of the two terms in the loss function . section : Experimental Evaluation paragraph : Datasets and protocols We perform quantitative evaluation on the Human 3.6 M dataset and on the HumanEva dataset . Human 3.6 M , to the best of our knowledge , is the largest publicly available dataset for human 3D pose estimation . The dataset contains 3.6 million images of 7 different professional actors performing 15 everyday activities like walking , eating , sitting , making a phone call . The dataset consists of 2D and 3D joint locations for each corresponding image . Each video is captured using 4 different calibrated high resolution cameras . In addition to 2D and 3D pose ground truth , the dataset also provides ground truth for bounding boxes , the camera parameters , the body proportion of all the actors and high resolution body scans or meshes of each actor . HumanEva , on the other hand , is a much smaller dataset . It has been largely used to benchmark previous work over the last decade . Most of the methods report results on two different actions and on three actors . For qualitative evaluation , we used the some videos from Youtube and the Human3.6 M dataset . We follow the standard protocols of the Human3.6 M dataset used in the literature . We used subjects 1 , 5 , 6 , 7 , and 8 for training , and subjects 9 and 11 for testing and the error is evaluated on the predicted 3D pose without any transformation . We refer this as protocol # 1 . Another common approach used by many to evaluate their methods is to align the predicted 3D pose with the ground truth using a similarity transformation ( Procrustes analysis ) . We refer this as protocol # 2 . We use the average error per joint in millimeters between the estimated and the ground truth 3D pose relative to the root node as the error metric . For the HumanEva dataset , we report results on each subject and action separately after performing rigid alignment with the ground truth data , following the protocol used by the previous methods . paragraph : 2D detections We fine - tuned a model of stacked - hourglass network , initially trained on the MPII dataset ( a benchmark dataset for 2D pose estimation ) , on the images of the Human3.6 M dataset to obtain 2D pose estimations for each image . We used the bounding box information provided with the dataset to first compute the center of the person in the image and then cropped a region across the person and resized it to . We fine - tuned the network for 250 iterations and used a batch size of 3 and a learning rate of . paragraph : Baselines Since many of the previous methods are based on single frame predictions , we used two baselines for comparison . To show that our method is much better than naive post processing , we applied a mean filter and a median filter on the 3D pose predictions of Martinez et al . . We used a window size of 5 frames and a stride length of 1 to apply the filters . Although non - rigid structure from motion ( NRSFM ) is one of the most general approaches for any 3D reconstruction problem from a sequence of 2D correspondences , we did not use it as a baseline because Zhou et al . did not find NRSFM techniques to be effective for 3D human pose estimation . They found that the NRSFM techniques do not work well with slow camera motion . Since the videos in the Human3.6 M dataset are captured by stationary cameras , the subjects in the dataset do not rotate that much to provide alternative views for NRSFM algorithm to perform well . Another reason is that human pose reconstruction is a specialized problem in which constraints from human body structure apply . paragraph : Data pre - processing We normalized the 3D ground truth poses , the noisy 2D pose estimates from stacked - hourglass network and the 2D ground truth by subtracting the mean and dividing by standard deviation . We do not predict the 3D location of the root joint i.e. central hip joint and hence zero center the 3D joint locations relative to the global position of the root node . To obtain the ground truth 3D poses in camera coordinate space , an inverse rigid body transformation is applied on the the ground truth 3D poses in global coordinate space using the given camera parameters . To generate both training and test sequences , we translated a sliding window of length by one frame . Hence there is an overlap between the sequences . This gives us more data to train on , which is always an advantage for deep learning systems . During test time , we initially predict the first frames of the sequence and slide the window by a stride length of 1 to predict the next frame using the previous frames . paragraph : Training details We trained our network for 100 epochs , where each epoch makes a complete pass over the entire Human 3.6 M dataset . We used the Adam optimizer for training the network with a learning rate of which is decayed exponentially per iteration . The weights of the LSTM units are initialized by Xavier uniform initializer . We used a mini - batch batch size of 32 i.e. 32 sequences . For most of our experiments we used a sequence length of 5 , because it allows faster training with high accuracy . We experimented with different sequence lengths and found sequence length 4 , 5 and 6 to generally give better results , which we will discuss in detail in the results section . We trained a single model for all the action classes . Our code is implemented in Tensorflow . We perform cross - validation on the training set to select the hyper - parameter values and of our loss function to and respectively . Similarly , using cross - validation , the three hyper - parameters of the temporal consistency constraint and , are set to and respectively . A single training step for sequences of length 5 takes only 34 ms approximately , while a forward pass takes only about 16ms on NVIDIA Titan X GPU . Therefore given the 2D joint locations from a pose detector , our network takes about 3.2ms to predict 3D pose per frame . subsection : Quantitative results paragraph : Evaluation on estimated 2D pose As mentioned before , we used a sequence length of 5 to perform both qualitative and quantitative evaluation of our network . The results on Human3.6 M dataset under protocol # 1 are shown in Table [ reference ] . From the table we observe that our model achieves the lowest error for every action class under protocol # 1 , unlike many of the previous state - of - the - art methods . Note that we train a single model for all the action classes unlike many other methods which trained a model for each action class . Our network significantly improves the state - of - the - art result of Sun et al . by approximately ( by mm ) . The results under protocol # 2 , which aligns the predictions to the ground truth using a rigid body similarity transform before computing the error , is reported in Table [ reference ] . Our network improves the reported state - of - the - art results by ( by mm ) and achieves the lowest error for each action in protocol # 2 as well . From the results , we observe the effectiveness of exploiting temporal information across multiple sequences . By using the information of temporal context , our network reduced the overall error in estimating 3D joint locations , especially on actions like phone , photo , sit and sitting down on which most previous methods did not perform well due to heavy occlusion . We also observe that our method outperforms both the baselines by a large margin on both the protocols . This shows that our method learned the temporal context of the sequences and predicted temporally consistent 3D poses , which naive post - processing techniques like temporal mean and median filters over frame - wise prediction failed to do . Like most previous methods , we report the results on action classes Walking and Jogging of the HumanEva dataset in Table [ reference ] . We obtained the lowest error in four of the six cases and the lowest average error for the two actions . We also obtained the second best result on subject 2 of action Walking . However , HumanEva is a smaller dataset than Human3.6 M and the same subjects appear in both training and testing . paragraph : Evaluation on 2D ground truth As suggested by Martinez et al . , we also found that the more accurate the 2D joint locations are , the better are the estimates for 3D pose . We trained our model on ground truth 2D poses for a sequence length of 5 . The results under protocol # 1 are reported in Table [ reference ] . As seen from the table , our model improves the lower bound error of Martinez et al . by almost . The results on ground truth 2D joint input for protocol # 2 are reported in Table [ reference ] . When there is no noise in 2D joint locations , our network performs better than the models by Martinez et al . and Moreno - Nouguer . These results suggest that the information of temporal consistency from previous frames is a valuable cue for the task of estimating 3D pose even when the detections are noise free . paragraph : Robustness to noise We carried out some experiments to test the tolerance of our model to different levels of noise in the input data by training our network on 2D ground truth poses and testing on inputs corrupted by different levels of Gaussian noise . Table [ reference ] shows how our final model compares against the models by Moreno - Nouguer and Martinez et al . . Our network is significantly more robust than Moreno - Nouguer \u2019s model . When compared against Martinez et al . our network performs better when the level of input noise is low i.e. standard deviation less than or equal to 10 . However , for higher levels of noise our network performs slightly worse than Martinez et al . . We would like to attribute the cause of this to the temporal smoothness constraint imposed during training which distributes the error of individual frames over the entire sequence . However , its usefulness can be observed in the qualitative results ( See Figure [ reference ] and Figure [ reference ] ) . paragraph : Ablative analysis To show the usefulness of each component and design decision of our network , we perform an ablative analysis . We follow protocol # 1 for performing ablative analysis and trained a single model for all the actions . The results are reported in Table [ reference ] . We observe that the biggest improvement in result is due the the residual connections on the decoder side , which agrees with the hypothesis of He et al . . Removing the residual connections massively increases the error by mm . When we do not apply layer normalization on LSTM units , the error increases by mm . On the other hand when dropout is not performed , the error raises by mm . When both layer normalization and recurrent dropout are not used the results get worse by mm . Although the temporal consistency constraint may seem to have less impact ( only mm ) quantitatively on the performance of our network , it ensures that the predictions over a sequence are smooth and temporally consistent which is apparent from our qualitative results as seen in Figure [ reference ] and Figure [ reference ] . To show the effectiveness of our model on detections from different 2D pose detectors , we also experimented with the detections from CPM and from stacked - hourglass ( SH ) module which is not fine - tuned on Human3.6 M dataset . We observe that even for the non - fine tuned stacked hourglass detections , our model achieves the state - of - the - art results . For detections from CPM , our model achieves competitive accuracy for the predictions . paragraph : Performance on different sequence lengths The results reported so far have been for input and output sequences of length 5 . We carried out experiments to see how our network performs for different sequence lengths ranging from 2 to 10 . The results are shown in Figure [ reference ] . As can be seen , the performance of our network remains stable for sequences of varying lengths . Even for a sequence length of 2 , which only considers the previous and the current frame , our model generates very good results . Particularly the best results were obtained for length 4 , 5 and 6 . However , we chose sequence length 5 for carrying out our experiments as a compromise between training time and accuracy . subsection : Qualitative Analysis We provide qualitative results on some videos of Human3.6 M and Youtube . We apply the model trained on the Human3.6 M dataset on some videos gathered from Youtube , The bounding box for each person in the Youtube video is labeled manually and for Human3.6 M the ground truth bounding box is used . The 2D poses are detected using the stacked - hourglass model fine - tuned on Human3.6 M data . The qualitative result for Youtube videos is shown in Figure [ reference ] and for Human3.6 M in Figure [ reference ] . The real advantage of using the temporal smoothness constraint during training is apparent in these figures . For Figure [ reference ] , we can see that even when the 2D pose estimator breaks or generates extremely noisy detections , our system can recover temporally coherent 3D poses by exploiting the temporal consistency information . A similar trend can also be found for Human3.6 M videos in Figure [ reference ] , particularly for the action sitting down of subject 11 . We have provided more qualitative results in the supplementary material . section : Conclusion Both the quantitative and qualitative results for our network show the effectiveness of exploiting temporal information over multiple sequences to estimate 3D poses which are temporally smooth . Our network achieved the best accuracy till date on all of the 15 action classes in the Human3.6 M dataset . Particularly , most of the previous methods struggled with actions which have a high degree of occlusion like taking photo , talking on the phone , sitting and sitting down . Our network has significantly better results on these actions . Additionally we found that our network is reasonably robust to noisy 2D poses . Although the contribution of temporal smoothness constraint is not apparent in the ablative analysis in Table [ reference ] , its effectiveness is clearly visible in the qualitative results , particularly on challenging Youtube videos ( see Figure [ reference ] ) . Our network effectively demonstrates the power of using temporal context information which we achieved using a sequence - to - sequence network that can be trained efficiently in a reasonably quick time . Also our network makes predictions from 2D poses at 3ms per frame on average which suggests that , given the 2D pose detector is real time , our network can be applied in real - time scenarios . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Human3_6M"]]], "Method": [[["Sequence-to-sequence_network_"]]], "Metric": [[["Average_3D_Error"]]], "Task": [[["3D_Human_Pose_Estimation"]]]}]}
{"docid": "TST3-SREX-0054", "doctext": "Probabilistic Model - Agnostic Meta - Learning section : Abstract Meta - learning for few - shot learning entails acquiring a prior over previous tasks and experiences , such that new tasks be learned from small amounts of data . However , a critical challenge in few - shot learning is task ambiguity : even when a powerful prior can be meta - learned from a large number of prior tasks , a small dataset for a new task can simply be too ambiguous to acquire a single model ( e.g. , a classifier ) for that task that is accurate . In this paper , we propose a probabilistic meta - learning algorithm that can sample models for a new task from a model distribution . Our approach extends model - agnostic meta - learning , which adapts to new tasks via gradient descent , to incorporate a parameter distribution that is trained via a variational lower bound . At meta - test time , our algorithm adapts via a simple procedure that injects noise into gradient descent , and at meta - training time , the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior . Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few - shot learning problems . section : Introduction Learning from a few examples is a key aspect of human intelligence . One way to make it possible to acquire solutions to complex tasks from only a few examples is to leverage past experience to learn a prior over tasks . The process of learning this prior entails discovering the shared structure across different tasks from the same family , such as commonly occurring visual features or semantic cues . Structure is useful insofar as it yields efficient learning of new tasks - a mechanism known as learning - to - learn , or meta - learning [ reference ] . However , when the end goal of few - shot meta - learning is to learn solutions to new tasks from small amounts of data , a critical issue that must be dealt with is task ambiguity : even with the best possible prior , there might simply not be enough information in the examples for a new task to resolve that task with high certainty . It is therefore quite desireable to develop few - shot meta - learning methods that can propose multiple potential solutions to an ambiguous few - shot learning problem . Such a method could be used to evaluate uncertainty ( by measuring agreement between the samples ) , perform active learning , or elicit direct human supervision about which sample is preferable . For example , in safety - critical applications , such as few - shot medical image classification , uncertainty is crucial for determining if the learned classifier should be trusted . When learning from such small amounts of data , uncertainty estimation can also help predict if additional data would be beneficial for learning and improving the estimate of the rewards . Finally , while we do not experiment with this in this paper , we expect that modeling this ambiguity will be helpful for reinforcement learning problems , where it can be used to aid in exploration . While recognizing and accounting for ambiguity is an important aspect of the few - shot learning problem , it is particularly challenging to model when scaling to high - dimensional data , large function approximators , and multimodal task structure . Representing distributions over functions is relatively straightforward when using simple function approximators , such as linear functions , and has been done extensively in early few - shot learning approaches using Bayesian models [ reference ][ reference ] . But this problem becomes substantially more challenging when reasoning over high - dimensional function approximators such as deep neural networks , since explicitly representing expressive distributions over thousands or millions of parameters if often intractable . As a result , recent more scalable approaches to few - shot learning have focused on acquiring deterministic learning algorithms that disregard ambiguity over the underlying function . Can we develop an approach that has the benefits of both classes of few - shot learning methods - scalability and uncertainty awareness ? To do so , we build upon tools in amortized variational inference for developing a probabilistic meta - learning approach . In particular , our method builds on model - agnostic meta - learning ( MAML ) [ reference ] , a few shot metalearning algorithm that uses standard gradient descent to adapt the model at meta - test time to a new few - shot task , and trains the model parameters at meta - training time to enable rapid adaptation , essentially optimizing for a neural network initialization that is well - suited for few shot learning . MAML can be shown to retain the generality of black - box meta - learners such as RNNs [ reference ] , while being applicable to standard neural network architectures . Our approach extends MAML to model a distribution over prior model parameters , which leads to an appealing simple stochastic adaptation procedure that simply injects noise into gradient descent at meta - test time . The meta - training procedure then optimizes for this simple inference process to produce samples from an approximate model posterior . The primary contribution of this paper is a reframing of MAML as a graphical model inference problem , where variational inference can provide us with a principled and natural mechanism for modeling uncertainty and ambiguity . Our approach enables sampling multiple potential solutions to a few - shot learning problem at meta - test time , and our experiments show that this ability can be utilized to sample multiple possible regressors for an ambiguous regression problem , as well as multiple possible classifiers for ambiguous few - shot attribute classification tasks . section : Related Work Hierarchical Bayesian models are a long - standing approach for few - shot learning that naturally allow for the ability to reason about uncertainty over functions [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] . While these approaches have been demonstrated on simple few - shot image classification datasets [ reference ] , they have yet to scale to the more complex problems , such as the experiments in this paper . A number of works have approached the problem of few - shot learning from a meta - learning perspective [ reference ][ reference ] , including black - box [ reference ][ reference ][ reference ] and optimization - based approaches [ reference ][ reference ] . While these approaches scale to large - scale image datasets [ reference ] and visual reinforcement learning problems [ reference ] , they typically lack the ability to reason about uncertainty . Our work is most related to methods that combine deep networks and probabilistic methods for few - shot learning [ reference ][ reference ][ reference ] . One approach that considers hierarchical Bayesian models for few - shot learning is the neural statistician [ reference ] , which uses an explicit task variable to model task distributions . Our method is fully model agnostic , and directly samples model weights for each task for any network architecture . Our experiments show that our approach improves on MAML [ reference ] , which outperforms the model by Edwards and Storkey [ reference ] . Other work that considers model uncertainty in the few - shot learning setting is the LLAMA method [ reference ] , which also builds on the MAML algorithm . LLAMA makes use of a local Laplace approximation for modeling the task parameters ( post - update parameters ) , which introduces the need to approximate a high dimensional covariance matrix . We instead propose a method that approximately infers the pre - update parameters , which we make tractable through a choice of approximate posterior parameterized by gradient operations . Bayesian neural networks [ reference ][ reference ][ reference ][ reference ] have been studied extensively as a way to incorporate uncertainty into deep networks . Although exact inference in Bayesian neural networks is impractical , approximations based on backpropagation and sampling [ reference ][ reference ][ reference ][ reference ] have been effective in incorporating uncertainty into the weights of generic networks . Our approach differs from these methods in that we explicitly train a hierarchical Bayesian model over weights , where a posterior task - specific parameter distribution is inferred at meta - test time conditioned on a learned weight prior and a ( few - shot ) training set , while conventional Bayesian neural networks directly learn only the posterior weight distribution for a single task . Our method draws on amortized variational inference methods [ reference ][ reference ][ reference ] to make this possible , but the key modification is that the model and inference networks share the same parameters . The resulting method corresponds structurally to a Bayesian version of model - agnostic meta - learning [ reference ] . into the center model after performing inference over \u03c6i . We find it beneficial to introduce additional dependencies of the prior on the training data to compensate for using the MAP estimate to approximate p ( \u03c6i ) , as shown on the right . section : Preliminaries In the meta - learning problem setting that we consider , the goal is to learn models that can learn new tasks from small amounts of data . To do so , meta - learning algorithms require a set of meta - training and meta - testing tasks drawn from some distribution p ( T ) . The key assumption of learning - to - learn is that the tasks in this distribution share common structure that can be exploited for faster learning of new tasks . Thus , the goal of the meta - learning process is to discover that structure . In this section , we will introduce notation and overview the model - agnostic meta - learning ( MAML ) algorithm [ reference ] . Meta - learning algorithms proceed by sampling data from a given task , and splitting the sampled data into a set of a few datapoints , D where \u03c6 i is used to denote the parameters updated by gradient descent and where the loss corresponds to negative log likelihood of the data . In particular , in the case of supervised classification with inputs { x j } , their corresponding labels { y j } , and a classifier f \u03b8 , we will denote the negative log likelihood of the data under the classifier as L ( \u03b8 , D ) = \u2212 ( xj , yj ) \u2208D log p ( y j |x j , \u03b8 ) . This corresponds to the cross entropy loss function . section : Method Our goal is to build a meta - learning method that can handle the uncertainty and ambiguity that occurs when learning from small amounts of data , while scaling to highly - expressive function approximators such as neural networks . To do so , we set up a graphical model for the few - shot learning problem . In particular , we want a hierarchical Bayesian model that includes random variables for the prior distribution over function parameters , \u03b8 , the distribution over parameters for a particular task , \u03c6 i , and the task training and test datapoints . This graphical model is illustrated in Figure 1 ( left ) , where tasks are indexed over i and datapoints are indexed over j. We will use the shorthand x . Therefore , posterior inference over \u03c6 i must take into account both the evidence ( training set ) and the prior imposed by p ( \u03b8 ) and p ( \u03c6 i |\u03b8 ) . Conventional MAML can be interpreted as approximating maximum a posteriori inference under a simplified model where p ( \u03b8 ) is a delta function , and inference is performed by running gradient descent on log p ( y tr |x tr , \u03c6 i ) for a fixed number of iterations starting from \u03c6 [ reference ] . The corresponding distribution p ( \u03c6 i |\u03b8 ) is approximately Gaussian , with a mean that depends on the step size and number of gradient steps . When p ( \u03b8 ) is not deterministic , we must make a further approximation to account for the random variable \u03b8 . One way we can do this is by using structured variational inference . In structured variational inference , we approximate the distribution over the hidden variables \u03b8 and \u03c6 i for each task with some approximate distribution q i ( \u03b8 , \u03c6 i ) . There are two reasonable choices we can make for q i ( \u03b8 , \u03c6 i ) . First , we can approximate it as a product of independent marginals , according to However , this approximation does not permit uncertainty to propagate effectively from \u03b8 to \u03c6 i . A more expressive approximation is the structured variational approximation We can further avoid storing a separate variational distribution q i ( \u03c6 i |\u03b8 ) and q i ( \u03b8 ) for each task T i by employing an amortized variational inference technique [ reference ][ reference ][ reference ] , where we instead set , where q \u03c8 is defined by some function approximator with parameters \u03c8 that takes x tr i , y tr i as input , and the same q \u03c8 is used for all tasks . Similarly , we can define We can now write down the variational lower bound on the log - likelihood as The likelihood terms on the first line can be evaluated efficiently : given a sample \u03b8 , \u03c6 i \u223c q ( \u03b8 , \u03c6 i |x , the training and test likelihoods simply correspond to the loss of the network with parameters \u03c6 i . The prior p ( \u03b8 ) can be chosen to be Gaussian , with a learned mean and ( diagonal ) covariance to provide for flexibility to choose the prior parameters . This corresponds to a Bayesian version of the MAML algorithm . We will define these parameters as \u00b5 \u03b8 and \u03c3 2 \u03b8 . Lastly , p ( \u03c6 i |\u03b8 ) must be chosen . This choice is more delicate . One way to ensure a tractable likelihood is to use a Gaussian with mean \u03b8 . This choice is reasonable , because it encourages \u03c6 i to stay close to the prior parameters \u03c6 i , but we will see in the next section how a more expressive implicit conditional can be obtained using gradient descent , resulting in a procedure that more closely resembles the original MAML algorithm while still modeling the uncertainty . Lastly , we must choose a form for the inference networks q \u03c8 ( \u03c6 i |\u03b8 , They must be chosen so that their entropies on the second line of the above equation are tractable . Furthermore , note that both of these distributions model very high - dimensional random variables : a deep neural network can have hundreds of thousands or millions of parameters . So while we can use an arbitrary function approximator , we would like to find a scalable solution . One convenient solution is to allow q \u03c8 to reuse the learned mean of the prior \u00b5 \u03b8 . We observe that adapting the parameters with gradient descent is a good way to update them to a given training set x where v q is a learned ( diagonal ) covariance , and the mean has an additional parameter beyond \u00b5 \u03b8 , which is a \" learning rate \" vector \u03b3 q that is pointwise multiplied with the gradient . While this choice may at first seem arbitrary , there is a simple intuition : the inference network should produce a sample of \u03b8 that is close to the posterior p ( \u03b8|x A reasonable way to arrive at a value of \u03b8 close to this posterior is to adapt it to both the training set and test set . [ reference ] Note that this is only done during meta - training . It remains to choose q \u03c8 ( \u03c6 i |\u03b8 , , which can also be formulated as a conditional Gaussian with mean given by applying gradient descent . Although this variational distribution is substantially more compact in terms of parameters than a separate neural network , it only provides estimates of the posterior during meta - training . At meta - test time , we must obtain the posterior p ( \u03c6 i |x . We can train a separate set of inference networks to perform this operation , potentially also using gradient descent within the inference network . However , these networks do not receive any gradient information during meta - training , and may not work well in practice . In the next section we propose an even simpler and more practical approach that uses only a single inference network during meta - training , and none during meta - testing . section : Algorithm 1 Meta - training , differences from MAML in red Require : p ( T ) : distribution over tasks 1 : initialize \u0398 : = { \u00b5 \u03b8 , \u03c3 2 \u03b8 , vq , \u03b3p , \u03b3q } 2 : while not done do 3 : Sample batch of tasks Ti \u223c p ( T ) 4 : for all Ti do 5 : Evaluate Compute adapted parameters with gradient descent : Compute \u2207\u0398 Update \u0398 using Adam section : Algorithm 2 Meta - testing section : Probabilistic Model - Agnostic Meta - Learning Approach with Hybrid Inference To formulate a simpler variational meta - learning procedure , we recall the probabilistic interpretation of MAML : as discussed by Grant et al . [ reference ] , MAML can be interpreted as approximate inference for the posterior p ( y where we use the maximum a posteriori ( MAP ) value \u03c6 i . It can be shown that , for likelihoods that are Gaussian in \u03c6 i , gradient descent for a fixed number of iterations using x tr i , y tr i corresponds exactly to maximum a posteriori inference under a Gaussian prior p ( \u03c6 i |\u03b8 ) [ reference ] . In the case of non - Gaussian likelihoods , the equivalence is only locally approximate , and the exact form of the prior p ( \u03c6 i |\u03b8 ) is intractable . However , in practice this implicit prior can actually be preferable to an explicit ( and simple ) Gaussian prior , since it incorporates the rich nonlinear structure of the neural network parameter manifold , and produces good performance in practice [ reference ][ reference ] . We can interpret this MAP approximation as inferring an approximate posterior on \u03c6 i of the form p ( \u03c6 i |x where \u03c6 i is obtained via gradient descent on the training set x tr i , y tr i starting from \u03b8 . Incorporating this approximate inference procedure transforms the graphical model in Figure 1 ( a ) into the one in Figure 1 ( b ) , where there is now a factor over p ( \u03c6 i |x tr i , y tr i , \u03b8 ) . While this is a crude approximation to the likelihood , it provides us with an empirically effective and simple tool that greatly simplifies the variational inference procedure described in the previous section , in the case where we aim to model a distribution over the global parameters p ( \u03b8 ) . After using gradient descent to estimate p ( \u03c6 i | x is not observed . Thus , we can now write down a variational lower bound for the logarithm of the approximate likelihood on the second line , which is given by In this bound , we essentially perform approximate inference via MAP on \u03c6 i to obtain p ( \u03c6 i |x To evaluate the variational lower bound during training , we can use the following procedure : first , we evaluate the mean by starting from \u00b5 \u03b8 and taking one ( or more ) gradient steps on log p ( y test i |x test i , \u03b8 current ) , where \u03b8 current starts at \u00b5 \u03b8 . We then add noise with variance v q , which is made differentiable via the reparameterization trick [ reference ] . We then take additional gradient steps on the training likelihood log p ( y tr i |x tr i , \u03b8 current ) . This accounts for the MAP inference procedure on \u03c6 i . Training of \u00b5 \u03b8 , \u03c3 2 \u03b8 , and v q is performed by backpropagating gradients through this entire procedure with respect to the variational lower bound , which includes a term for the likelihood tr , y tr , \u03c6 i ) and the KL - divergence between the sample \u03b8 \u223c q \u03c8 and the prior p ( \u03b8 ) . This meta - training procedure is detailed in Algorithm 1 . At meta - test time , the inference procedure is much simpler . The test labels are not available , so we simply sample \u03b8 \u223c p ( \u03b8 ) and perform MAP inference on \u03c6 i using the training set , which corresponds to gradient steps on log p ( y tr i |x tr i , \u03b8 current ) , where \u03b8 current starts at the sampled \u03b8 . This meta - testing procedure is detailed in Algorithm 2 . section : Adding Additional Dependencies In the transformed graphical model , the training data x tr i , y tr i and the prior \u03b8 are conditionally independent . However , since we have only a crude approximation to p ( \u03c6 i | x tr i , y tr i , \u03b8 ) , this independence often does n't actually hold . We can allow the model to compensate for this approximation by additionally conditioning the learned prior p ( \u03b8 ) on the training data . In this case , the learned \" prior \" has the form p ( \u03b8 i |x tr i , y tr i ) , where \u03b8 i is now task - specific , but with global parameters \u00b5 \u03b8 and \u03c3 2 \u03b8 . We thus obtain the modified graphical model in Figure 1 ( c ) . Similarly to the inference network q \u03c8 , we parameterize the learned prior as follows : With this new form for distribution over \u03b8 , the variational training objective uses the likelihood term log p ( \u03b8 i |x In our experiments , we find that this more expressive distribution often leads to better performance . section : Experiments The goal of our experimental evaluation is to answer the following questions : ( 1 ) can our approach enable sampling from the distribution over potential functions underlying the training data ? , ( 2 ) does our approach improve upon the MAML algorithm when there is ambiguity over the class of functions ? , and ( 3 ) can our approach scale to deep convolutional networks ? We study two illustrative toy examples and a realistic ambiguous few - shot image classification problem . For the both experimental domains , we compare MAML to our probabilistic approach . We will refer to our version of MAML as a PLATIPUS ( Probabilistic LATent model for Incorporating Priors and Uncertainty in few - Shot learning ) , due to its unusual combination of two approximate inference methods : amortized inference and MAP . Both PLATIPUS and MAML use the same neural network architecture and the same number of inner gradient steps . We additionally provide a comparison on the MiniImagenet benchmark and specify the hyperparameters in the supplementary appendix . , and Gaussian noise with a standard deviation of 0.3 is added to the labels . We trained both MAML and PLATIPUS for 5 - shot regression . In Figure 2 , we show the qualitative performance of both methods , where the ground truth underlying function is shown in gray and the datapoints in D tr are shown as purple triangles . We show the function f \u03c6i learned by MAML in black . For PLATIPUS , we sample 10 sets of parameters from p ( \u03c6 i |\u03b8 ) and plot the resulting functions in different colors . In the top row , we can see that PLATIPUS allows the model to effectively reason over the set of functions underlying the provided datapoints , with increased variance in parts of the function where there is more uncertainty . Further , we see that PLATIPUS is able to capture the multimodal structure , as the curves are all linear or sinusoidal . A particularly useful application of uncertainty estimates in few - shot learning is estimating when more data would be helpful . In particular , seeing a large variance in a particular part of the input space suggests that more data would be helpful for learning the function in that part of the input space . On the bottom of Figure 2 , we show the results for a single task at meta - test time with increasing numbers of training datapoints . Even though the model was only trained on training set sizes of 5 datapoints , we observe that PLATIPUS is able to effectively reduce its uncertainty as more and more datapoints are available . This suggests that the uncertainty provided by PLATIPUS can be used for approximately gauging when more data would be helpful for learning a new task . consisting of both positive and negative examples . We plot the results using the same scheme as before , except that we plot the decision boundary ( rather than the regression function ) and visualize the single positive datapoint with a green plus . As seen in Figure 3 , we see that PLATIPUS captures a broad distribution over possible decision boundaries , all of which are roughly circular . MAML provides a single decision boundary of average size . Ambiguous image classification . The ambiguity illustrated in the previous settings is common in real world tasks where images can share multiple attributes . We study an ambiguous extension to the celebA attribute classification task . Our meta - training dataset is formed by sampling two attributes at random to form a positive class and taking the same number of random examples without either attribute to from the negative classes . To evaluate the ability to capture multiple decision boundaries while simultaneously obtaining good performance , we evaluate our method as follows : We sample from a test set of three attributes and a corresponding set of images with those attributes . Since the tasks involve classifying images that have two attributes , this task is ambiguous , and there are three possible combinations of two attributes that explain the training set . We sample models from our prior as described in Section 4 and assign each of the sampled models to one of the three possible tasks based on its log - likelihood . If each of the three possible tasks is assigned a nonzero number of samples , this means that the model effectively covers all three possible modes that explain the ambiguous training set . We can measure coverage and accuracy from this protocol . The coverage score indicates the average number of tasks ( between 1 and 3 ) that receive at least one sample for each ambiguous training set , and the accuracy score is the average number of correct classifications on these tasks ( according to the sampled models assigned to them ) . A highly random method will achieve good coverage but poor accuracy , while a deterministic method will have a coverage of 1 . Our results are summarized in Table 5 and Fig . 4 . The accuracy of our method is comparable to standard , deterministic MAML . However , the deterministic algorithm only ever captures one mode observes five positives that share three attributes , and five negatives . A classifier that uses any two attributes can correctly classify the training set . On the right , we show each of the possible two - attribute tasks that this training set can correspond to , and illustrate the labels ( positive indicated by red border ) assigned by the best sample for that task . We see that the different samples are able to make reasonable predictions with no hats ( 2nd column ) or pay attention to them ( 1st and 3rd column ) , and can effectively capture the three possible explanations . for each ambiguous task , where the maximum is three . Our method on average captures between two and three modes . The qualitative analysis in Figure 4 illustrates 3 an example ambiguous training set , example images for the three possible two - attribute pairs that can correspond to this training set , and the classifications made by different sampled classifiers trained on the ambiguous training set . Note that the different samples each pay attention to different attributes , indicating that PLATIPUS is effective at capturing the different modes of the task . section : Discussion and Future Work We introduced an algorithm for few - shot meta - learning that enables simple and effective sampling of models for new tasks at meta - test time . Our algorithm , PLATIPUS , adapts to new tasks by running gradient descent with injected noise . During meta - training , the model parameters are optimized with respect to a variational lower bound on the likelihood for the meta - training tasks , so as to enable this simple adaptation procedure to produce approximate samples from the model posterior when conditioned on a few - shot training set . This approach has a number of benefits . The adaptation procedure is exceedingly simple , and the method can be applied to any standard model architecture . The algorithm introduces a modest number of additional parameters : besides the initial model weights , we must learn a variance on each parameter for the inference network and prior , and the number of parameters scales only linearly with the number of model weights . Our experimental results show that our method can be used to effectively sample diverse solutions to both regression and classification tasks at meta - test time , including for task families that have multi - modal task distributions . Although our approach is simple and broadly applicable , it has a number of potential limitations that could be addressed in future work . First , the current form of the method provides a relatively impoverished estimator of posterior variance , which might be less effective at gauging uncertainty in settings where different tasks have very different degrees of ambiguity . In these cases , finding a way to make the variance dependent on the few - shot training set might produce better results , and investigating how to do this without adding a large number of additional parameters would be an interesting direction for future work . Another exciting direction for future research would be to study how our approach could be applied in settings where ambiguity and uncertainty can directly guide data acquisition , so as to devise better few - shot active learning and reinforcement learning algorithms . section : Appendix A Ambiguous CelebA Details To construct our ambiguous few - shot variant of CelebA , we take the entire base set of attributes holding out 10 attributes for testing . We consider every combination of 2 attributes , discarding those with insufficient numbers of examples . This leave us with a total of 387 training tasks and 43 testing attributes . We partition our meta - training set and meta - validation set to 337 / 50 respectively . During meta - training , we sample 2 random attributes to construct a positive class and randomly sample examples with neither attribute as negative examples . During testing of our approach , we sample 3 attributes from the test set , and sample the 3 corresponding 2 - uples to form the test task . The training attributes are : section : B Experimental Details In the illustrative experiments , we use a fully connected network with 3 ReLU layers of size 100 . For CelebA , we adapt the base convolutional architecture described in Finn et al . [ reference ] which we refer the readers to for more detail . Our approximate posterior and prior have dimensionality matching the underlying model . We tune our approach over the inner learning rate \u03b1 , a weight on the D KL , the scale of the initialization of \u00b5 \u03b8 , \u03c3 2 \u03b8 , v q , \u03b3 p , \u03b3 q , with early stopping on the validation set . At meta - test time , we evaluate our approach by taking 10 samples from the prior before determining the assignments . The assignments are made based on the complete likelihood of the testing examples ( including the negatives ) . section : C MiniImagenet Comparison We provide an additional comparison on the MiniImagenet dataset . Since this benchmark does not contain a large amount of ambiguity , we do not aim to show state - of - the - art performance . Instead , our goal with this experiment is to compare our approach on to MAML and prior methods that build upon MAML on this standard benchmark . Since our goal is to compare algorithms , rather than achieving maximal performance , we decouple the effect of the meta - learning algorithm and the architecture used by using the standard 4 - block convolutional architecture used by Vinyals et al . [ reference ] , Ravi and Larochelle [ reference ] , Finn et al . [ reference ] and others . We note that better performance can likely be achieved by tuning the architecture . The results , in Table 2 indicate that our method slightly outperforms MAML and achieves comparable performance to a number of other prior methods . MiniImagenet 5 - way , 1 - shot Accuracy MAML [ reference ] 48.70 \u00b1 1.84 % LLAMA [ reference ] 49.40 \u00b1 1.83 % Reptile [ reference ] 49.97 \u00b1 0.32 % PLATIPUS ( ours ) 50.13 \u00b1 1.86 % Meta - SGD [ reference ] 50.71 \u00b1 1.87 % matching nets [ reference ] 43.56 \u00b1 0.84 % meta - learner LSTM [ reference ] 43.44 \u00b1 0.77 % SNAIL [ reference ] * 45.10 \u00b1 0.00 % prototypical networks [ reference ] 46.61 \u00b1 0.78 % mAP - DLM [ reference ] 49.82 \u00b1 0.78 % GNN [ reference ] 50.33 \u00b1 0.36 % Relation Net [ reference ] 50.44 \u00b1 0.82 % Table 2 : Comparison between our approach and prior MAML - based methods ( top ) , and other prior few - shot learning techniques on the 5 - way , 1 - shot MiniImagenet benchmark . Our approach gives a small boost over MAML , and is comparable to other approaches . We bold the approaches that are above the highest confidence interval lower - bound . * Accuracy using comparable network architecture . section : section : Acknowledgments We thank Marvin Zhang and Dibya Ghosh for comments on an earlier draft of this paper . This research was supported by an NSF Graduate Research Fellowship , NSF IIS - 1651843 , the Office of Naval Research , and NVIDIA . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Mini-ImageNet_-_1-Shot_Learning"]]], "Method": [[["PLATIPUS"]]], "Metric": [[["Accuracy"]]], "Task": [[["Few-Shot_Image_Classification"]]]}]}
{"docid": "TST3-SREX-0055", "doctext": "document : Pixel2Mesh : Generating 3D Mesh Models from Single RGB Images We propose an end - to - end deep learning architecture that produces a 3D shape in triangular mesh from a single color image . Limited by the nature of deep neural network , previous methods usually represent a 3D shape in volume or point cloud , and it is non - trivial to convert them to the more ready - to - use mesh model . Unlike the existing methods , our network represents 3D mesh in a graph - based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid , leveraging perceptual features extracted from the input image . We adopt a coarse - to - fine strategy to make the whole deformation procedure stable , and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry . Extensive experiments show that our method not only qualitatively produces mesh model with better details , but also achieves higher 3D shape estimation accuracy compared to the state - of - the - art . indicates equal contributions . indicates corresponding author . section : Introduction Inferring 3D shape from a single perspective is a fundamental human vision functionality but is extremely challenging for computer vision . Recently , great success has been achieved for 3d shape generation from a single color image using deep learning techniques . Taking advantage of convolutional layers on regular grids or multi - layer perception , the estimated 3D shape , as the output of the neural network , is represented as either a volume or point cloud . However , both representations lose important surface details , and is non - trivial to reconstruct a surface model ( Fig . [ reference ] ) , i.e. a mesh , which is more desirable for many real applications since it is lightweight , capable of modelling shape details , easy to deform for animation , to name a few . In this paper , we push along the direction of single image reconstruction , and propose an algorithm to extract a 3D triangular mesh from a single color image . Rather than directly synthesizing , our model learns to deform a mesh from a mean shape to the target geometry . This benefits us from several aspects . First , deep network is better at predicting residual , e.g. a spatial deformation , rather than structured output , e.g. a graph . Second , a series of deformations can be added up together , which allows shape to be gradually refined in detail . It also enables the control of the trade - off between the complexity of the deep learning model and the quality of the result . Lastly , it provides the chance to encode any prior knowledge to the initial mesh , e.g. topology . As a pioneer study , in this work , we specifically work on objects that can be approximated using 3D mesh with genus 0 by deforming an ellipsoid with a fixed size . In practice , we found most of the commonly seen categories can be handled well under this setting , e.g. car , plane , table , etc . To achieve this goal , there are several inherent challenges . The first challenge is how to represent a mesh model , which is essentially an irregular graph , in a neural network and still be capable of extracting shape details effectively from a given color image represented in a 2D regular grid . It requires the integration of the knowledge learned from two data modalities . On the 3D geometry side , we directly build a graph based fully convolutional network ( GCN ) on the mesh model , where the vertices and edges in the mesh are directly represented as nodes and connections in a graph . Network feature encoding information for 3D shape is saved on each vertex . Through forward propagation , the convolutional layers enable feature exchanging across neighboring nodes , and eventually regress the 3D location for each vertex . On the 2D image side , we use a VGG - 16 like architecture to extract features as it has been demonstrated to be successful for many tasks . To bridge these two , we design a perceptual feature pooling layer which allows each node in the GCN to pool image features from its 2D projection on the image , which can be readily obtained by assuming known camera intrinsic matrix . The perceptual feature pooling is enabled once after several convolutions ( i.e. a deformation block described in Sec . [ reference ] ) using updated 3D locations , and hence the image features from correct locations can be effectively integrated with 3D shapes . Given the graph representation , the next challenge is how to update the vertex location effectively towards ground truth . In practice , we observe that network trained to directly predict mesh with a large number of vertices is likely to make mistake in the beginning and hard to fix later . One reason is that a vertex can not effectively retrieve features from other vertices with a number of edges away , i.e. the limited receptive field . To solve this problem , we design a graph unpooling layer , which allows the network to initiate with a smaller number of vertices and increase during the forward propagation . With fewer vertices at the beginning stages , the network learns to distribute the vertices around to the most representative location , and then add local details as the number of vertices increases later . Besides the graph unpooling layer , we use a deep GCN enhanced by shortcut connections as the backbone of our architecture , which enables large receptive fields for global context and more steps of movements . Representing the shape in graph also benefits the learning procedure . The known connectivity allows us to define higher order loss functions across neighboring nodes , which are important to regularize 3D shapes . Specifically , we define a surface normal loss to favor smooth surface ; an edge loss to encourage uniform distribution of mesh vertices for high recall ; and a laplacian loss to prevent mesh faces from intersecting each other . All of these losses are essential to generate quality appealing mesh model , and none of them can be trivially defined without the graph representation . The contributions of this paper are mainly in three aspects . First , we propose a novel end - to - end neural network architecture that generates a 3D mesh model from a single RGB image . Second , we design a projection layer which incorporates perceptual image features into the 3D geometry represented by GCN . Third , our network predict 3D geometry in a coarse to fine fashion , which is more reliable and easy to learn . section : Related Work 3D reconstruction has been well studied based on the multi - view geometry ( MVG ) in the literature . The major research directions include structure from motion ( SfM ) for large - scale high - quality reconstruction and simultaneous localization and mapping ( SLAM ) for navigation . Though they are very successful in these scenarios , they are restricted by 1 ) the coverage that the multiple views can give and 2 ) the appearance of the object that wants to reconstruct . The former restriction means MVG can not reconstruct unseen parts of the object , and thus it usually takes a long time to get enough views for a good reconstruction ; the latter restriction means MVG can not reconstruct non - lambertian ( e.g. reflective or transparent ) or textureless objects . These restrictions lead to the trend of resorting to learning based approaches . Learning based approaches usually consider single or few images , as it largely relies on the shape priors that it can learn from data . Early works can be traced back to Hoiem et al . and Saxena et al . . Most recently , with the success of deep learning architectures and the release of large - scale 3D shape datasets such as ShapeNet , learning based approaches have achieved great progress . Huang et al . and Su et al . retrieve shape components from a large dataset , assemble them and deform the assembled shape to fit the observed image . However , shape retrieval from images itself is an ill - posed problem . To avoid this problem , Kar et al . learns a 3D deformable model for each object category and capture the shape variations in different images . However , the reconstruction is limited to the popular categories and its reconstruction result is usually lack of details . Another line of research is to directly learn 3D shapes from single images . Restricted by the prevalent grid - based deep learning architectures , most works outputs 3D voxels , which are usually with low resolutions due to the memory constraint on a modern GPU . Most recently , Tatarchenko et al . have proposed an octree representation , which allows to reconstructing higher resolution outputs with a limited memory budget . However , a 3D voxel is still not a popular shape representation in game and movie industries . To avoid drawbacks of the voxel representation , Fan et al . propose to generate point clouds from single images . The point cloud representation has no local connections between points , and thus the point positions have a very large degree of freedom . Consequently , the generated point cloud is usually not close to a surface and can not be used to recover a 3D mesh directly . Besides these typical 3D representations , there is an interesting work which uses a so - called \u201c geometry image \u201d to represent a 3D shape . Thus , their network is a 2D convolutional neural network which conducts an image to image mapping . Our works are mostly related to the two recent works and . However , the former adopts simple silhouette supervision , and hence does not perform well for complicated objects such as car , lamp , etc ; the latter needs a large model repository to generate a combined model . Our base network is a graph neural network ; this architecture has been adopted for shape analysis . In the meanwhile , there are charting - based methods which directly apply convolutions on surface manifolds for shape analysis . As far as we know , these architectures have never been adopted for 3D reconstruction from single images , though graph and surface manifold are natural representations for meshed objects . For a comprehensive understanding of the graph neural network , the charting - based methods and their applications , please refer to this survey . section : Method subsection : Preliminary : Graph - based Convolution We first provide some background about graph based convolution ; more detailed introduction can be found in . A 3D mesh is a collection of vertices , edges and faces that defines the shape of a 3D object ; it can be represented by a graph , where is the set of vertices in the mesh , is the set of edges with each connecting two vertices , and are the feature vectors attached on vertices . A graph based convolutional layer is defined on irregular graph as : where are the feature vectors on vertex before and after the convolution , and is the neighboring vertices of ; and are the learnable parameter matrices of that are applied to all vertices . Note that is shared for all edges , and thus ( [ reference ] ) works on nodes with different vertex degrees . In our case , the attached feature vector is the concatenation of the 3D vertex coordinate , feature encoding 3D shape , and feature learned from the input color image ( if they exist ) . Running convolutions updates the features , which is equivalent as applying a deformation . subsection : System Overview Our model is an end - to - end deep learning framework that takes a single color image as input and produces a 3D mesh model in camera coordinate . The overview of our framework is illustrated in Fig . [ reference ] . The whole network consists an image feature network and a cascaded mesh deformation network . The image feature network is a 2D CNN that extract perceptual feature from the input image , which is leveraged by the mesh deformation network to progressively deform an ellipsoid mesh into the desired 3D model . The cascaded mesh deformation network is a graph - based convolution network ( GCN ) , which contains three deformation blocks intersected by two graph unpooling layers . Each deformation block takes an input graph representing the current mesh model with the 3D shape feature attached on vertices , and produces new vertices locations and features . Whereas the graph unpooling layers increase the number of vertices to increase the capacity of handling details , while still maintain the triangular mesh topology . Starting from a smaller number of vertices , our model learns to gradually deform and add details to the mesh model in a coarse - to - fine fashion . In order to train the network to produce stable deformation and generate an accurate mesh , we extend the Chamfer Distance loss used by Fan et al . with three other mesh specific loss \u2013 Surface normal loss , Laplacian regularization loss , and Edge length loss . The remaining part of this section describes details of these components . subsection : Initial ellipsoid Our model does not require any prior knowledge of the 3D shape , and always deform from an initial ellipsoid with average size placed at the common location in the camera coordinate . The ellipsoid is centered at 0.8 m in front of the camera with 0.2 m , 0.2 m , 0.4 m as the radius of three axis . The mesh model is generated by implicit surface algorithm in Meshlab and contains 156 vertices . We use this ellipsoid to initialize our input graph , where the initial feature contains only the 3D coordinate of each vertex . subsection : Mesh deformation block The architecture of mesh deformation block is shown in Fig . [ reference ] ( a ) . In order to generate 3D mesh model that is consistent with the object shown in the input image , the deformation block need to pool feature ( ) from the input image . This is done in conjunction with the image feature network and a perceptual feature pooling layer given the location of vertex ( ) in the current mesh model . The pooled perceptual feature is then concatenated with the 3D shape feature attached on the vertex from the input graph ( ) and fed into a series of graph based ResNet ( G - ResNet ) . The G - ResNet produces , also as the output of the mesh deformation block , the new coordinates ( ) and 3d shape feature ( ) for each vertex . subsubsection : Perceptual feature pooling layer We use a VGG - 16 architecture up to layer conv5_3 as the image feature network as it has been widely used . Given the 3D coordinate of a vertex , we calculate its 2D projection on input image plane using camera intrinsics , and then pool the feature from four nearby pixels using bilinear interpolation . In particular , we concatenate feature extracted from layer \u2018 conv3_3 \u2019 , \u2018 conv4_3 \u2019 , and \u2018 conv5_3 \u2019 , which results in a total dimension of 1280 . This perceptual feature is then concatenated with the 128 - dim 3D feature from the input mesh , which results in a total dimension of 1408 . This is illustrated in Fig . [ reference ] ( b ) . Note that in the first block , the perceptual feature is concatenated with the 3 - dim feature ( coordinate ) since there is no learnt shape feature at the beginning . subsubsection : G - ResNet After obtaining 1408 - dim feature for each vertex representing both 3D shape and 2D image information , we design a graph based convolutional neural network to predict new location and 3D shape feature for each vertex . This requires efficient exchange of the information between vertices . However , as defined in ( [ reference ] ) , each convolution only enables the feature exchanging between neighboring pixels , which severely impairs the efficiency of information exchanging . This is equivalent as the small receptive field issue on 2D CNN . To solve this issue , we make a very deep network with shortcut connections and denote it as G - ResNet ( Fig . [ reference ] ( a ) ) . In this work , the G - ResNet in all blocks has the same structure , which consists of 14 graph residual convolutional layers with 128 channels . The serial of G - ResNet block produces a new 128 - dim 3D feature . In addition to the feature output , there is a branch which applies an extra graph convolutional layer to the last layer features and outputs the 3D coordinates of the vertex . subsection : Graph unpooling layer The goal of unpooling layer is to increase the number of vertex in the GCNN . It allows us to start from a mesh with fewer vertices and add more only when necessary , which reduces memory costs and produces better results . A straightforward approach is to add one vertex in the center of each triangle and connect it with the three vertices of the triangle ( Fig . [ reference ] ( b ) Face - based ) . However , this causes imbalanced vertex degrees , i.e. number of edges on vertex . Inspired by the vertex adding strategy of the mesh subdivision algorithm prevalent in computer graphics , we add a vertex at the center of each edge and connect it with the two end - point of this edge ( Fig . [ reference ] ( a ) ) . The 3D feature for newly added vertex is set as the average of its two neighbors . We also connect three vertices if they are added on the same triangle ( dashed line . ) Consequently , we create 4 new triangles for each triangle in the original mesh , and the number of vertex is increased by the number of edges in the original mesh . This edge - based unpooling uniformly upsamples the vertices as shown in Fig . [ reference ] ( b ) Edge - based . subsection : Losses We define four kinds of losses to constrain the property of the output shape and the deformation procedure to guarantee appealing results . We adopt the Chamfer loss to constrain the location of mesh vertices , a normal loss to enforce the consistency of surface normal , a laplacian regularization to maintain relative location between neighboring vertices during deformation , and an edge length regularization to prevent outliers . These losses are applied with equal weight on both the intermediate and final mesh . Unless otherwise stated , we use for a vertex in the predicted mesh , for a vertex in the ground truth mesh , for the neighboring pixel of , till the end of this section . subsubsection : Chamfer loss The Chamfer distance measures the distance of each point to the other set : It is reasonably good to regress the vertices close to its correct position , however is not sufficient to produce nice 3D mesh ( see the result of Fan et al . in Fig . [ reference ] ) . subsubsection : Normal loss We further define loss on surface normal to characterize high order properties : where is the closest vertex for that is found when calculating the chamfer loss , is the neighboring pixel of , is the inner product of two vectors , and is the observed surface normal from ground truth . Essentially , this loss requires the edge between a vertex with its neighbors to perpendicular to the observation from the ground truth . One may find that this loss does not equal to zero unless on a planar surface . However , optimizing this loss is equivalent as forcing the normal of a locally fitted tangent plane to be consistent with the observation , which works practically well in our experiment . Moreover , this normal loss is fully differentiable and easy to optimize . subsubsection : Regularization Even with the Chamfer loss and Normal loss , the optimization is easily stucked in some local minimum . More specifically , the network may generate some super large deformation to favor some local consistency , which is especially harmful at the beginning when the estimation is far from ground truth , and causes flying vertices ( Fig . [ reference ] ) . paragraph : Laplacian regularization To handle these problem , we first propose a Laplacian term to prevent the vertices from moving too freely , which potentially avoids mesh self - intersection . The laplaician term serves as a local detail preserving operator , that encourages neighboring vertices to have the same movement . In the first deformation block , it acts like a surface smoothness term since the input to this block is a smooth - everywhere ellipsoid ; starting from the second block , it prevents the 3D mesh model from deforming too much , so that only fine - grained details are added to the mesh model . To calculate this loss , we first define a laplacian coordinate for each vertex as and the laplacian regularization is defined as : where and are the laplacian coordinate of a vertex after and before a deformation block . paragraph : Edge length regularization . To penalize flying vertices , which ususally cause long edge , we add an edge length regularization loss : The overall loss is a weighted sum of all four losses , , where , and are the hyperparameters which balance the losses and fixed for all the experiments . section : Experiment In this section , we perform an extensive evaluation on our model . In addition to comparing with previous 3D shape generation works for evaluating the reconstruction accuracy , we also analyse the importance of each component in our model . Qualitative results on both synthetic and real - world images further show that our model produces triangular meshes with smooth surfaces and still maintains details depicted in the input images . subsection : Experimental setup subsubsection : Data . We use the dataset provided by Choy et al . . The dataset contains rendering images of 50k models belonging to 13 object categories from ShapeNet , which is a collection of 3D CAD models that are organized according to the WordNet hierarchy . A model is rendered from various camera viewpoints , and camera intrinsic and extrinsic matrices are recorded . For fair comparison , we use the same training / testing split as in Choy et . al . . subsubsection : Evaluation Metric . We adopt the standard 3D reconstruction metric . We first uniformly sample points from our result and ground truth . We calculate precision and recall by checking the percentage of points in prediction or ground truth that can find a nearest neighbor from the other within certain threshold . A F - score as the harmonic mean of precision and recall is then calculated . Following Fan et . al . , we also report the Chamfer Distance ( CD ) and Earth Mover \u2019s Distance ( EMD ) . For F - Score , larger is better . For CD and EMD , smaller is better . On the other hand , we realize that the commonly used evaluation metrics for shape generation may not thoroughly reflect the shape quality . They often capture occupancy or point - wise distance rather than surface properties , such as continuity , smoothness , high - order details , for which a standard evaluation metric is barely missing in literature . Thus , we recommend to pay attention on qualitative results for better understanding of these aspects . subsubsection : Baselines . We compare the presented approach to the most recent single image reconstruction approaches . Specifically , we compare with two state - of - the - art methods - Choy et . al . ( 3D - R2N2 ) producing 3D volume , and Fan et . al . ( PSG ) producing point cloud . Since the metrics are defined on point cloud , we can evaluate PSG directly on its output , our method by uniformly sampling point on surface , and 3D - R2N2 by uniformly sampling point from mesh created using the Marching Cube method . We also compare to Neural 3D Mesh Renderer ( N3MR ) which is so far the only deep learning based mesh generation model with code public available . For fair comparison , the models are trained with the same data using the same amount of time . subsubsection : Training and Runtime . Our network receives input images of size , and initial ellipsoid with 156 vertices and 462 edges . The network is implemented in Tensorflow and optimized using Adam with weight decay 1e - 5 . The batch size is 1 ; the total number of training epoch is 50 ; the learning rate is initialized as 3e - 5 and drops to 1e - 5 after 40 epochs . The total training time is 72 hours on a Nvidia Titan X. During testing , our model takes 15.58ms to generate a mesh with 2466 vertices . subsection : Comparison to state of the art Tab . [ reference ] shows the F - score with different thresholds of different methods . Our approach outperforms the other methods in all categories except watercraft . Notably , our results are significantly better than the others in all categories under a smaller threshold , showing at least 10 % F - score improvement . N3MR does not perform well , and its result is about 50 % worse than ours , probably because their model only learns from limited silhouette signal in images and lacks of explicit handling of the 3D mesh . We also show the CD and EMD for all categories in Tab . [ reference ] . Our approach outperforms the other methods in most categories and achieves the best mean score . The major competitor is PSG , which produces a point cloud and has the most freedom ; this freedom leads to smaller CD and EMD , however does not necessarily leads to a better mesh model without proper regularization . To demonstrate this , we show the qualitative results to analyze why our approach outperforms the others . Fig . [ reference ] shows the visual results . To compare the quality of mesh model , we convert volumetric and point cloud to mesh using standard approaches . As we can see , the 3D volume results produced by 3D - R2N2 lack of details due to the low resolution , e.g. , the legs are missing in the chair example as shown in the 4 - th row of Fig . [ reference ] . We tried octree based solution to increase the volume resolution , but found it still hard to recover surface level details as much as our model . PSG produces sparse 3D point clouds , and it is non - trivial to recover meshes from them . This is due to the applied Chamfer loss acting like a regression loss which gives too much degree of freedom to the point cloud . N3MR produces very rough shape , which might be sufficient for some rendering tasks , however can not recover complicated objects such as chairs and tables . In contrast , our model does not suffer from these issues by leveraging a mesh representation , integration of perceptual feature , and carefully defined losses during the training . Our result is not restricted by the resolution due to the limited memory budget and contains both smooth continuous surface and local details . subsection : Ablation Study Now we conduct controlled experiments to analyse the importance of each component in our model . Tab . [ reference ] reports the performance of each model by removing one component from the full model . Again , we argue that these commonly used evaluation metrics does not necessarily reflect the quality of the recovered 3D geometry . For example , the model with no edge length regularization achieves the best performance across all , however , in fact produces the worst mesh ( Fig . [ reference ] , the last 2nd column ) . As such , we use qualitative result Fig . [ reference ] to show the contribution of each component in our system . subsubsection : Graph Unpooling We first remove the graph unpooling layers , and thus each block has the same number of vertices as in the last block of our full model . It is observed that the deformation makes mistake easier at beginning , which can not be fixed later on . Consequently , there are some obvious artifacts in some parts of the objects . subsubsection : G - ResNet We then remove the shortcut connections in G - ResNet , and make it regular GCN . As can be seen from Tab . [ reference ] , there is a huge performance gap in all four measurement metrics , which means the failure of optimizing Chamfer distance . The main reason is the degradation problem observed in the very deep 2D convolutional neural network . Such problem leads to a higher training error ( and thus higher testing error ) when adding more layers to a suitably deep model . Essetially , our network has 42 graph convolutional layers . Thus , this phenomenon has also been observed in our very deep graph neural network experiment . subsubsection : Loss terms We evaluate the function of each additional terms besides the Chamfer loss . As can be seen in Fig . [ reference ] , removing normal loss severely impairs the surface smoothness and local details , e.g. seat back ; removing Laplacian term causes intersecting geometry because the local topology changes , e.g. the hand held of the chair ; removing edge length term causes flying vertices and surfaces , which completely ruins the surface characteristics . These results demonstrate that all the components presented in this work contribute to the final performance . subsubsection : Number of Deformation Blocks We now analyze the effects of the number of blocks . Figure Fig . [ reference ] ( left ) shows the mean F - score ( ) and CD with regard to the number of blocks . The results indicate that increasing the number of blocks helps , but the benefit is getting saturated with more blocks , e.g. from 3 to 4 . In our experiment , we found that 4 blocks results in too many vertices and edges , which slow down our approach dramatically even though it provides better accuracy on evaluation metrics . Therefore , we use 3 blocks in all our experiment for the best balance of performance and efficiency . Fig . [ reference ] ( right ) shows the output of our model after each deformation block . Notice how mesh is densified with more vertices and new details are added . subsection : Reconstructing Real - World images Following Choy et . al . , we test our network on the Online Products dataset and Internet images for qualitative evaluation on real images . We use the model trained from ShapeNet dataset and directly run on real images without finetuning , and show results in Fig . [ reference ] . As can be seen , our model trained on synthetic data generalizes well to the real - world images across various categories . section : Conclusion We have presented an approach to extract 3D triangular meshes from singe images . We exploit the key advantages the mesh presentation can bring to us , and the key issues required to solve for success . The former includes surface normal constraints and information propagation along edges ; the latter includes perceptual features extracted from images as a guidance . We carefully design our network structure and propose a very deep cascaded graph convolutional neural network with \u201c shortcut \u201d connections . Meshes are progressively refined by our network trained end - to - end with the chamfer loss and normal loss . Our results are significantly better than the previous state - of - the - art using other shape representations such as 3D volume or 3D point cloud . Thus , we believe mesh representation is the next big thing in this direction , and we hope that the key components discovered in our work can support follow - up works that will further advance direct 3D mesh reconstruction from single images . subsubsection : Future work Our method only produces meshes with the same topology as the initial mesh . In the future , we will extend our approach to more general cases , such as scene level reconstruction , and learn from multiple images for multi - view reconstruction . subsubsection : Acknowledgements This work was supported by two projects from NSFC ( # 61622204 and # 61572134 ) , two projects from STCSM ( # 16JC1420401 and # 16QA1400500 ) , Eastern Scholar ( TP2017006 ) , and The Thousand Talents Plan of China ( for young professionals , D1410009 ) . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Data3D_R2N2"]]], "Method": [[["Pixel2Mesh"]]], "Metric": [[["Avg_F1"]]], "Task": [[["3D_Object_Reconstruction"]]]}]}
{"docid": "TST3-SREX-0056", "doctext": "document : Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes During the last half decade , convolutional neural networks ( CNNs ) have triumphed over semantic segmentation , which is a core task of various emerging industrial applications such as autonomous driving and medical imaging . However , to train CNNs requires a huge amount of data , which is difficult to collect and laborious to annotate . Recent advances in computer graphics make it possible to train CNN models on photo - realistic synthetic data with computer - generated annotations . Despite this , the domain mismatch between the real images and the synthetic data significantly decreases the models \u2019 performance . Hence we propose a curriculum - style learning approach to minimize the domain gap in semantic segmentation . The curriculum domain adaptation solves easy tasks first in order to infer some necessary properties about the target domain ; in particular , the first task is to learn global label distributions over images and local distributions over landmark superpixels . These are easy to estimate because images of urban traffic scenes have strong idiosyncrasies ( e.g. , the size and spatial relations of buildings , streets , cars , etc . ) . We then train the segmentation network in such a way that the network predictions in the target domain follow those inferred properties . In experiments , our method significantly outperforms the baselines as well as the only known existing approach to the same problem . 1 ] Yang Zhang 2 ] Philip David 1 ] Boqing Gong [ 1 ] Center for Research in Computer Vision , University of Central Florida [ 2 ] Computational and Information Sciences Directorate , U.S. Army Research Laboratory yangzhang@knights.ucf.edu , philip.j.david4.civ@mail.mil , bgong@crcv.ucf.edu section : Introduction This paper is concerned with domain adaptation for semantic image segmentation of urban scenes , i.e. , assigning a category label to every pixel of an image or video frame . Our interest in this problem is partially due to the exciting vision of autonomous driving , where understanding complex inner - city traffic scenes is an essential module and semantic segmentation is one of its key constituents . Machine learning methods for automatic semantic segmentation require massive amounts of high - quality annotated imagery in order to produce effective classifiers that generalize well to novel scenes . However , annotating training imagery for semantic segmentation is a very cumbersome task for humans . Cordts et al . report that the annotation and quality control take more than 1.5 hours on a single image of the Cityscapes dataset . Besides , it is very difficult and time - consuming to collect imagery that depicts the large number of variabilities possible of urban scenes in different countries , seasons , and lighting conditions , etc . To overcome both shortcomings , simulated urban environments may be used to automatically generate large amounts of annotated training imagery . This , however , introduces a new problem , that of domain mismatch between the source ( simulated ) domain and the target ( real ) domain . Figure [ reference ] illustrates some examples drawn from the synthetic SYNTHIA dataset and the real Cityscapes dataset . It is readily apparent that there are significant visual differences between the two datasets . Domain adaptation techniques may be used by machine learning methods to bridge this gap between the two domains . In computer vision , learning domain - invariant features has been a prevalent and successful strategy to tackle the discrepancy between two domains , mainly for classification and regression problems . The core idea is to infer a new feature space such that the marginal distributions of the source domain ( S ) and the target domain ( T ) are about the same , i.e. , . Furthermore , the prediction function from that space is assumed to be the same across the domains so that one can leverage the rich labeled data in the source domain to train classifiers that generalize well to the target . It is hard to verify the assumption , but the work along this line is rich and has led to impressive practical results regardless , such as the algorithms using linear transformation , kernel methods , and the recent deep learning methods that directly extract domain - invariant features from raw input images . In contrast to prior arts , the semantic segmentation we study in this paper is a highly structured prediction problem , for which domain adaptation is only sparsely explored in the literature . Under structured prediction , can we still achieve good domain adaptation results by following the above principles ? Our intuition and experimental studies ( cf . Section [ reference ] ) tell us no . Learning a decision function for structured prediction is more involved than classification because it has to resolve the predictions in an exponentially large label space . As a result , the assumption that the source and target domains share the same prediction function becomes less likely to hold . Besides , some discriminative cues in the data would be suppressed if one matches the feature representations of the two domains without taking careful account of the structured labels . Finally , data instances are the proxy to measure the domain difference . However , it is not immediately clear what comprises the instances in semantic segmentation , especially given that the top - performing segmentation methods are built upon deep neural networks . Hoffman et al . take each spatial unit in the fully convolutional network ( FCN ) as an instance . We contend that such instances are actually non - i.i.d . in either individual domain , as their receptive fields overlap with each other . How can we avoid the assumption that the source and target domains share the same prediction function in a transformed domain - invariant feature space ? Our proposed solution draws on two key observations . One is that the urban traffic scene images have strong idiosyncrasies ( e.g. , the size and spatial relations of buildings , streets , cars , etc . ) . Therefore , some tasks are \u201c easy \u201d and , more importantly , suffer less because of the domain discrepancy . Second , the structured output in semantic segmentation enables convenient posterior regularization , as opposed to the popular ( e.g. , ) regularization over model parameters . Accordingly , we propose a curriculum - style domain adaptation approach . Recall that , in domain adaptation , only the source domain supplies many labeled data while there are no or only scarce labels from the target . The curriculum domain adaptation begins with the easy tasks , in order to gain some high - level properties about the unknown pixel - level labels for each target image . It then learns a semantic segmentation network \u2014 the hard task , whose predictions over the target images are forced to follow those necessary properties as much as possible . To develop the easy tasks in the curriculum , we consider label distributions over both holistic images and some landmark superpixels of the target domain . Take the former for instance . The label distribution of an image indicates the percentage of pixels that belong to each category , respectively . We argue that such tasks are easier , despite the domain mismatch , than assigning pixel - wise labels . Indeed , we may directly estimate the label distributions without inferring the pixel - wise labels . Moreover , the relative sizes of road , vehicle , pedestrian , etc . constrain the shape of the distributions , effectively reducing the search space . Finally , models to estimate the label distributions over superpixels may benefit from the urban scenes \u2019 canonical layout that transcends domains , e.g. , buildings stand beside streets . Why and when are the seemingly simple label distributions useful for the domain adaptation of semantic segmentation ? In our experiments , we find that the segmentation networks trained on the source domain perform poorly on many target images , giving rise to disproportionate label assignments ( e.g. , many more pixels are classified to sidewalks than to streets ) . To rectify this , the image - level label distribution informs the segmentation network how to update the predictions while the label distributions of the landmark superpixels tell the network where to update . Jointly , they guide the adaptation of the networks to the target domain to , at least , generate proportional label predictions . Note that additional \u201c easy tasks \u201d can be conveniently incorporated into our framework in the future . Our main contribution is on the proposed curriculum - style domain adaptation for the semantic segmentation of urban scenes . We select into the curriculum the easy and useful tasks of inferring label distributions for the target images and landmark superpixels , in order to gain some necessary properties about the target domain . Built upon these , we learn a pixel - wise discriminative segmentation network from the labeled source data and , meanwhile , conduct a \u201c sanity check \u201d to ensure the network behavior is consistent with the previously learned knowledge about the target domain . Our approach effectively eludes the assumption about the existence of a common prediction function for both domains in a transformed feature space . It readily applies to different segmentation networks , as it does not change the network architecture or tax any intermediate layers . section : Related work We discuss some related work on domain adaptation and semantic segmentation , with special focus on that transferring knowledge from virtual images to real photos . paragraph : Domain adaptation . Conventional machine learning algorithms rely on the assumption that the training and test data are drawn i.i.d . from the same underlying distribution . However , it is often the case that there exists some discrepancy from the training to the test stage . Domain adaptation aims to rectify this mismatch and tune the models toward better generalization at testing . The existing work on domain adaptation mostly focuses on classification and regression problems , e.g. , learning from online commercial images to classify real - world objects , and , more recently , aims to improve the adaptability of deep neural networks . Among them , the most relevant work to ours is that exploring simulated data . Sun and Saenko train generic object detectors from the synthetic images , while Vazquez et al . use the virtual images to improve pedestrian detections in real environment . The other way around , i.e. , how to improve the quality of the simulated images using the real ones , is studied in . paragraph : Semantic segmentation . Semantic segmentation is the task of assigning an object label to each pixel of an image . Traditional methods rely on local image features manually designed by domain experts . After the pioneering work that introduced the convolutional neural network ( CNN ) to semantic segmentation , most recent top - performing methods are built on CNNs . An enormous amount of labor - intensive work is required to annotate the many images that are needed to obtain accurate segmentation models . The PASCAL VOC2012 Challenge contains nearly 10 , 000 annotated images for the segmentation competition , and the MS COCO Challenge includes over 200 , 000 annotated images . According to , it took about 60 minutes to manually segment each image in and about 90 minutes for each in . A plausible approach to reducing the human workload is to utilize weakly supervised information such as image labels and bounding boxes . We instead explore the use of almost effortlessly labeled virtual images for training high - quality segmentation networks . In , annotating a synthetic image took only 7 seconds on average through a computer game . For the urban scenes , we use the SYNTHIA dataset which contains images of a virtual city . paragraph : Domain adaptation for semantic segmentation . Upon observing the obvious mismatch between virtual and real data , we expect domain adaptation to enhance the segmentation performance on real images by networks trained on virtual ones . To the best of our knowledge , the only attempt to algorithmically address this problem is . While it regularizes the intermediate layers and constrains the output of the network , we propose a different curriculum domain adaptation strategy . We solve the easy task first and then use the learned knowledge about the target domain to regularize the network predictions . section : Approach In this section , we present the details of the proposed curriculum domain adaptation for semantic segmentation of urban scene images . Unlike previous work that aligns the domains via an intermediate feature space and thereby implicitly assumes the existence of the same decision function for the two domains , it is our intuition that , for structured prediction ( i.e. , semantic segmentation here ) , the cross - domain generalization of machine learning models can be more efficiently improved if we avoid this assumption and instead train them subject to necessary properties they should retain in the target domain . paragraph : Preliminaries . In particular , the properties are about the pixel - wise category labels of an arbitrary image from the target domain , where and are the width and height of the image , respectively , and is the number of categories . We use one - hot vector encoding for the groundtruth labels , i.e. , takes the value of 0 or 1 and the latter means that the - th label is assigned by a human annotator to the pixel at . Correspondingly , the prediction by a segmentation network is realized by a softmax function per pixel . We express each target property in the form of a distribution over the categories , where represents the occupancy proportion of the category over the - th target image or a superpixel of the image . Therefore , one can immediately calculate the distribution given the human annotations to the image . For instance , the image level label distribution is expressed by Similarly , we can compute the target property / distribution from the network predictions and denote it by . subsection : Domain adaptation using the target properties Ideally , we would like to have a segmentation network to imitate human annotators on the target domain . Therefore , necessarily , the properties of their annotation results should be the same too . We capture this notion by minimizing the cross entropy at training , where the first term of the right - hand side is the entropy and the second is the KL - divergence . Given a mini - batch consisting of both source images ( ) and target images ( ) , the overall objective function for training the cross - domain generalizing segmentation network is , where is the pixel - wise cross - entropy loss defined over the sufficiently labeled source domain images , enforcing the network to have the pixel level discriminative capabilities , and the second term is over the unlabeled target domain images , hinting the network what necessary properties its predictions should have in the target domain . We use to balance the two strengths in training and superscript to index different types of label distributions . Note that in the domain adaptation context , we actually can not directly compute the label distribution from the groundtruth annotations of the target domain . Nonetheless , estimating them using the labeled source data is easier than assigning labels to every single pixel of the target images . We present the details in the next section . paragraph : Remarks . Mathematically , the objective function has a similar form as in model compression . We thus borrow some concepts to gain more intuitive understanding about our domain adaptation procedure . The \u201c student \u201d network follows a curriculum to learn simple knowledge about the target domain before it addresses the hard one of semantically segmenting images . The models inferring the target properties act like \u201c teachers \u201d , as they hint what label distributions the final solution ( image annotation ) may have in the target domain at the image and superpixel levels . Another perspective is to understand the target properties as a posterior regularization for the network . The posterior regularization can conveniently encode a priori knowledge into the objective function . Some applications include weakly supervised segmentation and detection , and rule - regularized training of neural networks . In addition to the domain adaptation setting and novel target properties , another key distinction of our work is that we decouple the label distributions from the network predictions and thus avoid the EM type of optimizations . Our approach learns the segmentation network with almost effortless changes to the popular deep learning tools . subsection : Inferring the target properties Thus far we have presented the \u201c hard \u201d task in the curriculum domain adaptation . In this section , we describe the \u201c easy \u201d ones , i.e. , how to infer the target properties without accessing the image annotations of the target domain . Our contributions also include selecting the particular property of label distributions to constitute the simple tasks . subsubsection : Global label distributions of images Due to the domain disparity , a baseline segmentation network trained on the source domain ( i.e. , using the first term of eq . ( [ reference ] ) ) could be easily crippled given the target images . In our experiments , we find that our baseline network constantly mistakes streets for sidewalks and / or cars ( cf . Figure [ reference ] ) . Consequently , the predicted labels for the pixels are highly disproportionate . To rectify this , we employ the label distribution over the global image as our first property ( cf . eq . ( [ reference ] ) ) . Without access to the target labels , we have to train machine learning models from the labeled source images to estimate the label distribution for the target image . Nonetheless , we argue that this is less challenging than generating the per - pixel predictions despite that both tasks are influenced by the domain mismatch . In our experiments , we examine different approaches to this task . We extract image features using the Inception - Resnet - v2 as the input to the following models . Although multinomial logistic regression ( LR ) is mainly used for classification , its output is actually a valid distribution over the categories . For our purpose , we thus train it by replacing the one - hot vectors in the cross - entropy loss with the groundtruth label distribution , which is calculated using eq . ( [ reference ] ) and the available human labels of the source domain . Given a target image , we directly take the LR \u2019s output as the predicted label distribution . We also test a nonparametric method by simply retrieving the nearest neighbors ( NNs ) for a target image and then transferring the mean of the NNs \u2019 label distributions to the target image . We use the distance for the NN retrieval . Finally , we include two dumb predictions as the control experiment . One is , for any target image , to output the mean of all the label distributions in the source domain ( source mean ) , and the other is to output a uniform distribution . subsubsection : Local label distributions of landmark superpixels The image level label distribution globally penalizes potentially disproportional segmentation output on the target domain , and yet is inadequate in providing spatial constraints . In this section , we consider the use of label distributions over some superpixels as the anchors to drive the network towards spatially desired target properties . Note that it is not necessary , and is even harmful , to use all of the superpixels in a target image to regularize the segmentation network , because that would be too strong a force and may overrule the pixel - wise discriminativeness revealed by the labeled source images , especially when the label distributions are not inferred accurately enough . In order to have the dual effect of both estimating the label distributions of superpixels and filtering the superpixels , we simplify the problem and employ a linear SVM in this work . In particular , we segment each image into 100 superpixels using linear spectral clustering . For the superpixels of the source domain , we are able to assign a single dominant label to each of them , and then use the labels and the corresponding features extracted from the superpixels to train a multi - class SVM . Given a test superpixel of a target image , the multi - class SVM returns a class label as well as a decision value , which is interpreted as the confidence score about classifying this superpixel . We keep the top 60 % superpixels , called landmark superpixels , in the target domain and calculate their label distributions as the second type of \u201c easy \u201d tasks . In particular , the class label of a landmark superpixel is encoded into a one - hot vector , which serves as a valid distribution about the categories in the landmark superpixel . Albeit simple , we find this method works very well in our experiments . We encode both visual and contextual information to represent a superpixel . First , we use the FCN - 8s pre - trained on the PASCAL CONTEXT dataset , which has 59 distinct classes , to obtain 59 detection scores for each pixel . We then average them within each superpixel . Finally , we represent a superpixel by the concatenation of the 59D vectors of itself , its left and right superpixels , as well as the two respectively above and below it . subsection : Curriculum domain adaptation : recapitulation We recap the proposed curriculum domain adaptation using Figure [ reference ] before presenting the experiments in the next section . Our main idea is to execute the domain adaptation step by step , starting from the easy tasks that are less sensitive to the domain discrepancy than the semantic segmentation . We choose the labels distributions over global images and local landmark superpixels in this work ; more tasks will be explored in the future . The solutions to them provide useful gradients originating from the target domain ( cf . the arrows with brown color in Figure [ reference ] ) , while the source domain feeds the network with well - labeled images and segmentation masks ( cf . the dark blue arrows in Figure [ reference ] ) . section : Experiments In this section , we describe the experimental setup and compare the results of our approach , its variations , and some existing baseline methods . subsection : Segmentation network and optimization In our experiments , we use FCN - 8s as our semantic segmentation network . We initialize its convolutional layers with VGG - 19 , and then train it using the AdaDelta optimizer with default parameters . Each mini - batch is comprised of five source images and five randomly chosen target images . When we train the baseline network with no adaptation , however , we try to use the largest possible mini - batch that includes 15 source images . The network is implemented in Keras and Theano . We train different versions of the network on a single Tesla K40 GPU . Unlike the existing deep domain adaptation methods which introduce regularization to the intermediate layers , we only revise the loss function over the output . Hence , our curriculum domain adaptation can be readily applied to other segmentation networks ( e.g. , ) . subsection : Datasets and evaluation We use the publicly available Cityscpaes and SYNTHIA datasets in our experiments . Cityscapes is a real - world , vehicle - egocentric image dataset collected in 50 cities in Germany and nearby countries . It provides four disjoint subsets : 2 , 993 training images , 503 validation image , 1 , 531 test images , and 20 , 021 auxiliary images . All the training , validation , and test images are accurately annotated with per pixel category labels , while the auxiliary set is coarsely labeled . There are 34 distinct categories in the dataset . SYNTHIA is a large dataset of synthetic images and provides a particular subset , called SYNTHIA - RAND - CITYSCAPES , to pair with Cityscapes . This subset contains 9 , 400 images that are automatically annotated with 12 object categories , one void class , and some unnamed classes . Note that the virtual city used to generate the synthetic images does not correspond to any of the real cities covered by Cityscapes . We abbreviate SYNTHIA - RAND - CITYSCAPES to SYNTHIA hereon . paragraph : Domain idiosyncrasies . Although both datasets depict urban scenes , and SYNTHIA is created to be as photo - realistic as possible , they are mismatched domains in several ways . The most noticeable difference is probably the coarse - grained textures in SYNTHIA ; very similar texture patterns repeat in a regular manner across different images . In contrast , the Cityscapes images are captured by high - quality dash - cameras . Another major distinction is the variability in view angles . Since Cityscapes images are recorded by the dash cameras mounted on a moving car , they are viewed from almost a constant angle that is about parallel to the ground . More diverse view angles are employed by SYNTHIA \u2014 it seems like some cameras are placed on the buildings that are significantly higher than a bus . Finally , some of the SYNTHIA images are severely shadowed by extreme lighting conditions , while we find no such conditions in the Cityscapes images . These combined factors , among others , make domain adaptation from SYNTHIA to Cityscapes a very challenging problem . Figure [ reference ] shows some example images from both datasets . We pair each Cityscpaes image with its nearest neighbor in SYNTHIA , retrieved by the Inception - Resnet - v2 features . However , the cross - dataset nearest neighbors are visually very different from the query images , verifying the dramatic disparity between the two domains . paragraph : Experiment setup . Since our ultimate goal is to solve the semantic segmentation problem for real images of urban scenes , we take Cityscapes as the target domain and SYNTHIA as the source domain . The Cityscapes validation set is used as our test set . We split 500 images out of the Cityscpaes training set for the validation purpose ( e.g. , to monitor the convergence of the networks ) . In training , we randomly sample mini - batches from both the images ( and their labels ) of SYNTHIA and the remaining images of Cityscapes yet with no labels . As in , we manually find 16 common classes between the two datasets : sky , building , road , sidewalk , fence , vegetation , pole , car , traffic sign , person , bicycle , motorcycle , traffic light , bus , wall , and rider . The last four are unnamed and yet labeled in SYNTHIA . paragraph : Evaluation . We use the evaluation code released along with the Cityscapes dataset to evaluate our results . It calculates the PASCAL VOC intersection - over - union , i.e. , , where TP , FP , and FN are the numbers of true positive , false positive , and false negative pixels , respectively , determined over the whole test set . Since we have to resize the images before feeding them to the segmentation network , we resize the output segmentation mask back to the original image size before running the evaluation against the groundtruth annotations . subsection : Results of inferring global label distributions Before presenting the final semantic segmentation results , we first compare the different approaches to inferring the global label distributions of the target images ( cf . Section [ reference ] ) . We report the results on the held - out validation images of Cityscapes in this experiment , and then select the best method for the remaining experiments . In Table [ reference ] , we compare the estimated label distributions with the groundtruth ones using the distance , the smaller the better . We see that the baseline network ( NoAdapt ) , which is directly learned from the source domain without any adaptation methods , outperforms the dumb uniform distribution ( Uniform ) and yet no other methods . This confirms that the baseline network gives rise to severely disproportionate predictions over the target domain . Another dumb prediction ( Src mean ) , i.e. , using the mean of all label distributions over the source domain as the prediction for the target images , however , performs reasonably well . To some extent , this indicates the value of the simulated source domain for the semantic segmentation task of urban scenes . Finally , the nearest neighbors ( NN ) based method and the multinomial logistic regression ( LR ) ( cf . Section [ reference ] ) perform the best . We use the output of LR on the target domain in our remaining experiments . subsection : Comparison results bike fence wall t - sign pole mbike t - light sky bus rider veg bldg car person sidewalk road We report the final semantic segmentation results on the test data of the target domain in this section . We compare our approach to the following competing methods . We directly train the FCN - 8s model on SYNTHIA without applying any domain adaptation methods . This is the most basic baseline for our experiments . Recall that we have trained a multi - class SVM using the dominant labels of the superpixels in the source domain . We then use them to classify the target superpixels . Since we keep the top 60 % most confidently classified superpixels as the landmarks to regularize our segmentation network during training ( cf . Section [ reference ] ) , it is also interesting to examine the classification results of these superpixels . We run the evaluation after assigning the void class label to the other pixels of the images . In addition to the IoU , we have also evaluated the classification results of the superpixels by accuracy . We find that the classification accuracy is 71 % for all the superpixels of the target domain , while for the selected 60 % landmark superpixels , the classification accuracy is more than 88 % . Hoffman et al . \u2019s work is the only existing one addressing the same problem as ours , to the best of our knowledge . They introduce a pixel - level adversarial loss to the intermediate layers of the network and impose constraints to the network output . Their experimental setup is about identical to ours except that they do not specify which part of Cityscapes is considered as the test set . Nonetheless , we include their results for comparison to put our work in a better perspective . The comparison results are shown in Table [ reference ] . Immediately , we note that all our domain adaptation results are significantly better than those without adaptation ( NoAdapt ) . We denote by ( Ours ( I ) ) the network trained using the global label distributions over the target images ( and the labeled source images ) . Although one may wonder that the image - wise label distributions are too high - level to supervise the pixel - wise discriminative network , the gain is actually significant . They are able to correct some obvious errors of the baseline network , such as the disproportional predictions about road and sidewalk ( cf . the results of Ours ( I ) vs. NoAdapt in the last two columns ) . It is interesting to see that both superpixel classification - based segmentation results ( SP and SP Lndmk ) are also better than the baseline network ( NoAdapt ) . The label distributions obtained over the landmark superpixels boost the segmentation network ( Ours ( SP ) ) to the mean IoU of 28.1 % , which is better than those by either superpixel classification or the baseline network individually . We have also tried to use the label distributions over all the superpixels to train the network , and observe little improvement over NoAdapt . This is probably because it is too forceful to regularize the network output at every single superpixel especially when the estimated label distributions are not accurate enough . The superpixel - based methods , including Ours ( SP ) , miss small objects such as fences , traffic lights ( t - light ) , and traffic signs ( t - sign ) , and instead are very accurate for categories like the sky , road , and building , that typically occupy larger image regions . On the contrary , the label distributions on the images give rise to a network ( Ours ( I ) ) that performs better on the small objects than Ours ( SP ) . In other words , they mutually complement to some extent . Re - training the network by using the label distributions over both global images and local landmark superpixels ( Ours ( I + SP ) ) , we achieve the best semantic segmentation results on the target domain . In the future work , it is worth exploring other target properties , perhaps still in the form of label distributions , that handle the small objects well , in order to further complement the superpixel - level label distributions . paragraph : Comparison with FCNs in the wild . Although we use the same segmentation network ( FCN - 8s ) as , our baseline results ( NoAdapt ) are better than those reported in . This may be due to subtle differences in terms of implementation or experimental setup . Although our own baseline results are superior , we gain larger improvements ( 7 % ) over them than the performance gain of ( 3 % ) over the seemingly underperforming baseline network there . paragraph : Comparison with learning domain - invariant features . At our first attempt to solve the domain adaptation problem for the semantic segmentation of urban scenes , we tried to learn domain invariant features following the deep domain adaptation methods for classification . In particular , we impose the maximum mean discrepancy over the layer before the output . We name such network layer the feature layer . Since there are virtually three output layers in FCN - 8s , we experiment with all the three feature layers correspondingly . We have also tested the domain adaptation by reversing the gradients of a domain classifier . However , none of these efforts lead to any noticeable gain over the baseline network so the results are omitted . section : Conclusion In this paper , we address domain adaptation for the semantic segmentation of urban scenes . We propose a curriculum style approach to this problem . We learn to estimate the global label distributions of the images and local label distributions of the landmark superpixels of the target domain . Such tasks are easier to solve than the pixel - wise label assignment . Therefore , we use their results to effectively regularize our training of the semantic segmentation network such that its predictions meet the inferred label distributions over the target domain . Our method outperforms several competing methods that do domain adaptation from simulated images to real photos of urban traffic scenes . In future work , we will explore more target properties that can be conveniently inferred to enrich our curriculum domain adaptation framework . paragraph : Acknowledgements . This work is supported by the NSF award IIS # 1566511 , a gift from Adobe Systems Inc. , and a GPU from NVIDIA . We thank the anonymous reviewers and area chairs for their insightful comments . bibliography : References bike fence wall t - sign pole mbike t - light sky bus rider veg terrain train bldg car person truck sidewalk road appendix : GTA Cityscapes The main text above has been accepted to IEEE International Conference on Computer Vision ( ICCV ) 2017 . After the paper submission , we have been continuously working on the project and have got more results . We include them below to complement the experiments in the main text . The new experiment is basically the same as the one in the main text except that we replace SYNTHIA with the GTA dataset . GTA is a synthetic , vehicle - egocentric image dataset collected from the open world in the realistically rendered computer game Grand Theft Auto V ( GTA , or GTA5 ) . It contains 24 , 996 images , whose semantic segmentation annotations are fully compatible with the classes used in Cityscapes . Hence we use all the 19 official training classes in our experiment . The results are shown in Table [ reference ] . As in the main text , the same observations about our approach apply here . Additionally , we note that the results are overall better than those adapting from SYNTHIA to Cityscapes . This is not surprising , because the GTA images are more photo - realistic than SYNTHIA \u2019s .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["GTAV-to-Cityscapes_Labels"]]], "Method": [[["CDA"]]], "Metric": [[["mIoU"]]], "Task": [[["Synthetic-to-Real_Translation"]]]}]}
{"docid": "TST3-SREX-0057", "doctext": "BB8 : A Scalable , Accurate , Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth section : Abstract We introduce a novel method for 3D object detection and pose estimation from color images only . We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background . By contrast with recent patch - based methods , we rely on a \" holistic \" approach : We apply to the detected objects a Convolutional Neural Network ( CNN ) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes . This , however , is not sufficient for handling objects from the recent T - LESS dataset : These objects exhibit an axis of rotational symmetry , and the similarity of two images of such an object under two different poses makes training the CNN challenging . We solve this problem by restricting the range of poses used for training , and by introducing a classifier to identify the range of a pose at run - time before estimating it . We also use an optional additional step that refines the predicted poses . We improve the state - of - the - art on the LINEMOD dataset from 73.7 % [ 2 ] to 89.3 % of correctly registered RGB frames . We are also the first to report results on the Occlusion dataset [ 1 ] using color images only . We obtain 54 % of frames passing the Pose 6D criterion on average on several sequences of the T - LESS dataset , compared to the 67 % of the state - of - the - art [ 10 ] on the same sequences which uses both color and depth . The full approach is also scalable , as a single network can be trained for multiple objects simultaneously . section : Introduction 3D pose estimation of object instances has recently become a popular problem again , because of its application in robotics , virtual and augmented reality . Many recent approaches rely on depth maps , sometimes in conjunction with color images [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] . However , it is not always possible to use depth cameras , as they fail ( c ) ( d ) Figure 1 . Zooms on estimated poses for ( a ) the Ape of the LINEMOD dataset [ reference ] , ( b ) the Driller of the Occlusion dataset [ reference ] , ( c ) and ( d ) three objects of the T - LESS [ reference ] dataset . The green bounding boxes correspond to the ground truth poses , and the blue bounding boxes to the poses estimated with our method . The two boxes often overlap almost perfectly , showing the accuracy of our estimated poses . The parts of the bounding boxes occluded by the object were removed using the object mask rendered from our estimated pose . In ( b ) , we can still obtain a good pose despite the large occlusion by the bench vise . In ( c ) and ( d ) , we also obtain very good estimates despite large occlusions , the similarities between the objects , and the fact that the symmetries challenge the learning algorithms . outdoor or on specular objects . In addition , they drain the batteries of mobile devices , being an active sensor . It is therefore desirable to rely only on color images for 3D pose estimation , even if it is more challenging . Recent methods [ reference ][ reference ][ reference ] work by identifying the ' object coordinates ' of the pixels , which are the pixels ' 3D coordinates in a coordinate system related to the object [ reference ] . The object 3D pose can then be estimated using a PnP algorithm from these 2D - 3D correspondences . [ reference ] obtain similar correspondences by associating some pixels in selected parts of the object with virtual 3D points . However , obtaining these 2D - 3D correspondences from local patches is difficult and the output is typically very noisy for these methods . A robust optimization is then needed to estimate the pose . In this paper , we argue for a \" holistic \" approach , in the sense that we predict the pose of an object directly from its appearance , instead of identifying its individual surface points . As we will show , this approach provides significantly better results . We first detect the target objects in 2D. We show that using object segmentation performs better for this task compared to a standard sliding window detector , in particular in presence of partial occlusion . We then apply a CNN to predict the 3D pose of the detected objects . While the predicted 3D pose can be represented directly by a translation and a rotation , we achieve better accuracy by using a representation similar to the one used in [ reference ] for object parts : We predict the 2D projections of the corners of the object 's bounding box , and compute the 3D pose from these 2D - 3D correspondences with a PnP algorithm . Compared to the object coordinate approaches the predictions are typically outlier - free , and no robust estimation is thus needed . Compared to the direct prediction of the pose , this also avoids the need for a meta - parameter to balance the translation and rotation terms . Unfortunately , this simple approach performs badly on the recent and challenging T - LESS dataset . This dataset is made of manufactured objects that are not only similar to each other , but also have one axis of rotational symmetry . For example , the squared box of Fig . 1 ( c ) has an angle of symmetry of 90 \u2022 and the other object has an angle of symmetry of 0 \u2022 since it is an object of revolution ; Object # 5 in Fig . 1 ( d ) is not perfectly symmetrical but only because of the small screw on the top face . The approach described above fails on these objects because it tries to learn a mapping from the image space to the pose space . Since two images of a symmetrical object under two different poses look identical , the image - pose correspondence is in fact a one - to - many relationship . This issue is actually not restricted to our approach . For example , [ reference ] , which relies on object coordinates , does not provide results on the Bowl object of the LINEMOD dataset , an object with an axis of symmetry : It is not clear which coordinates should be assigned to the 3D points of this object , as all the points on a circle orthogonal to the axis of symmetry have the same appearance . To solve this problem , we train the method described above using images of the object under rotation in a restricted range , such that the training set does not contain ambiguous images . In order to recover the object pose under a larger range of rotation , we train a classifer to tell under which range the object rotation is . Again , this is easy to do with a \" holistic \" approach , and this classifier takes an image of the entire object as input . As we will explain in more details , we can then always use the CNN trained on the restricted range to estimate any pose . In addition , we will show how to adapt this idea to handle \" approximatively symmetrical \" objects like Object # 5 . This approach allows us to obtain good performance on the T - LESS dataset . Finally , we show that we can add an optional last step to refine the pose estimates by using the \" feedback loop \" proposed in [ reference ] for hand detection in depth images : We train a network to improve the prediction of the 2D projections by comparing the input image and a rendering of the object for the initial pose estimate . This allows us to improve even more our results on the LINEMOD and Occlusion datasets . Our full approach , which we call BB8 , for the 8 corners of the bounding box , is also very fast , as it only requires to apply Deep Networks to the input image a few times . In the remainder of the paper , we first discuss related work , describe our approach , and compare it against the state - ofthe - art on the three available datasets . section : Related Work The literature on 3D object detection is very large , thus we will focus only on recent works . Keypoint - based methods [ reference ][ reference ] were popular for a long time and perform well but only on very textured objects . The apparition of inexpensive 3D cameras favored the development of methods suitable for untextured objects : [ reference ][ reference ] rely on depth data only and use votes from pairs of 3D points and their normals to detect 3D objects . [ reference ] uses a decision tree applied to RGB - D images to simultaneously recognize the objects and predict their poses . [ reference ][ reference ] consider a template - based representation computed from RGB - D or RGB data , which allows for large scale detection [ reference ] . However , this template approach is sensitive to partial occlusions . To tackle clutter and partial occlusions , [ reference ] and [ reference ] rely on local patches recognition performed with Random Forests . In particular , [ reference ] considers ' 3D object coordinates ' : A Random Forest is trained to predict the 3D location in the object coordinate system of each image location . The prediction of this forest is integrated in an energy function together with a term that compares the depth map with a rendering of the object and a term that penalizes pixels that lie on the object rendering but predicted by the forest to not be an object point . This energy function is optimized by a RANSAC procedure . [ reference ] replaces this energy function by an energy computed from the output of a CNN trained to compare observed image features and features computed from a 3D rendering of the potentially detected object . This makes the approach very robust to partial occlusions . These works , however , are designed for RGB - D data . [ reference ] extends this work and relies on RGB data only , as we do . They use Auto - Context [ reference ] to obtain better predictions from the Random Forests , estimate a distribute over the object coordinates to handle the prediction uncertainties better , and propose a more sophisticated RANSAC - like method that scales with the number of objects . This results in an efficient and accurate method , however , robustness to partial occlusions are not demonstrated . [ 3 ] is related to [ reference ][ reference ][ reference ] but focuses on providing sparse 2D - 3D correspondences from reliable object parts . Unfortunately , it provides results on its own dataset only , not on more broadly available datasets . Like us , [ reference ] relies on a CNN to directly predict a 3D pose , but in the form of a translation and a rotation . It considers camera relocalisation in urban environment rather than 3D object detection , and uses the full image as input to the CNN . By predicting the 2D projections of the corners of the bounding box , we avoid the need for a meta - parameter to balance the position and orientation errors . As shown in our experiments , the pose appears to be more accurate when predicted in this form . Intuitively , this should not be surprising , as predicting 2D locations from a color images seems easier than predicting a 3D translation and a quaternion , for example . [ 6 ] also uses a CNN to predict the 3D pose of generic objects but from RGB - D data . It first segments the objects of interest to avoid the influence of clutter . We tried segmenting the objects before predicting the pose as well , however , this performed poorly on the LINEMOD dataset , because the segmented silhouttes were not very accurate , even with state - of - the - art segmentation methods . In summary , our method appears to be one of the first to deal with RGB data only to detect 3D objects and estimate their poses on recent datasets . As we will show in the experiments , it outperforms the accuracy of the state - of - theart [ reference ] by a large margin . section : Proposed Approach In our approach , we first find the objects in 2D , we obtain a first estimate of the 3D poses , including objects with a rotational symmetry , and we finally refine the initial pose estimates . We describe each step in this section . section : Localizing the Objects in 2D We first identify the 2D centers of the objects of interest in the input images . We could use a standard 2D object detector , but we developed an approach based on segmentation that resulted in better performance as it can provide accurate locations even under partial occlusions . Compared to our initial tests using a sliding window , this ap - proach improved our 2D detection results from about 75 % to 98.8 % correct detection rate based on a IoU of 0.5 . We only need a low resolution segmentation and thus do not need a hourglass - shaped architecture [ reference ] , which makes our segmentation more efficient . As shown in Fig . 2 , our approach performs a two - level coarse - to - fine object segmentation . For each level , we train a single network for all the objects . The first network is obtained by replacing the last layer of VGG [ reference ] by a fully connected layer with the required number of output required by each step , and fine - tune it . The second network has a simple , ad hoc architecture . More exactly , the first network is trained to provide a very low resolution binary segmentation of the objects given an image region J of size 128 \u00d7 128 by minimizing the following objective function : where T s is a training set made of image regions J , and the corresponding segmentations S for object o , ( f is the output of network f 1 \u03c6 for region J and object o. \u03c6 denotes the network 's parameters , optimized during training . For the LINEMOD and Occlusion datasets , there is at most one object for a given region J , but more objects can be present for the T - LESS dataset . At run - time , to get the segmentations , we compute : where s 1 , o is a 8 \u00d7 8 binary segmentation of J for object o , and \u03c4 1 is a threshold used to binarize the network 's output . To obtain a binary segmentation for the full input image , we split this image into regions and compute the s 1 , o for each region . This gives us one binary segmentation S 1 , o for the full input image , and each possible object . This usually results in a single connected component per visible object ; if several components are present , we keep only the largest one for each object . If the largest component in a segmentation S 1 , o is small , object o is likely not visible . For the remaining object ( s ) , we refine the shape of the largest component by applying a second network to each 16 \u00d7 16 image patch P that corresponds to an active location in S 1 : using notations similar to the ones in Eq . [ reference ] . Since the input to f 2 \u03c8 ( P ) has a low resolution , we do not need a complex network such as VGG [ reference ] , and we use a much simpler architecture with 2 convolutional layers and 2 pooling layers . We finally obtain a segmentation S 2 , o with resolution 64 \u00d7 48 for the full input image and each visible object o. We therefore get the identities o of the visible object ( s ) , and for these objects , we use the segmentation centroids as their 2D centers , to compute the 3D poses of the objects as described below . section : Predicting the 3D Pose We predict the 3D pose of an object by applying a Deep Network to an image window W centered on the 2D object center estimated as described in the previous section . As for the segmentation , we use VGG [ reference ] as a basis for this network . This allows us to handle all the objects of the target dataset with a single network . It is possible to directly predict the pose in the form of a 3 - vector and an exponential map for example , as in [ reference ] . However , a more accurate approach was proposed in [ reference ] for predicting the poses of object parts . To apply it here , we minimize the following cost function over the parameters \u0398 of network g \u0398 : where T is a training set made of image windows W containing object o under a pose defined by an exponential map e and a 3 - vector t. The M o i are the 3D coordinates of the corners of the bounding box of object o in the object coordinate system . Proj e , t ( M ) projects the 3D point M on the image from the pose defined by e and t. returns the two components of the output of g \u0398 corresponding to the predicted 2D coordinates of the i - th corner for object o. \u2022 modulo 180 \u2022 ( c ) . Our solution is to restrict the range during training to be between 0 \u2022 and 90 \u2022 . We use a classifier to detect if the pose in an input image is between 90 \u2022 and 180 \u2022 . If this is the case ( d ) , we mirror the input image ( e ) , and mirror back the predicted projections for the corners ( f ) . At run - time , the segmentation gives the identity and the 2D locations of the visible object ( s ) o. The 3D pose can then be estimated for the correspondences between the 3D points M o i and the predicted m i ( ( g \u0398 ( W ) ) [ o ] ) using a PnP algorithm . Other 3D points could be used here , however , the corners of the bounding box are a natural choice as they frame the object and are well spread in space 1 . section : Handling Objects with an Axis of Symmetry If we apply the method described so far to the T - LESS dataset , the performances are significantly lower than the performances on the LINEMOD dataset . As mentioned in the introduction , this is because training images W in Eq . ( 4 ) for the objects of this dataset can be identical while having very different expected predictions Proj e , t ( M o i ) , because of the rotational symmetry of the objects . We first remark that for an object with an angle of symmetry \u03b1 , its 3D rotation around its axis of symmetry can be defined only modulo \u03b1 , not 2\u03c0 . For an object with an angle of symmetry \u03b1 , we can therefore restrict the poses used for training to the poses where the angle of rotation around the symmetry axis is within the range [ 0 ; \u03b1 [ , to avoid the ambiguity between images . However , this solves our problem only partially : Images at one extremity of this range of poses and the images at the other extremity , while not identical , still look very similar . As a result , for input images with an angle of rotation close to 0 modulo \u03b1 , the pose prediction can still be very bad , as illustrated in Fig . 3 . To explain our solution , let us first denote by \u03b2 the rotation angle , and introduce the intervals r 1 = [ 0 ; \u03b1 / 2 [ and r 2 = [ \u03b1 / 2 ; \u03b1 [ . To avoid ambiguity , we restrict \u03b2 to be in r 1 for the training images used in the optimization problem of Eq . ( 4 ) . The drawback is of course that , without doing anything else , we would not be able to estimate the poses when \u03b2 is in r 2 . We therefore introduce a CNN classifier k ( \u00b7 ) to predict at run - time if \u03b2 is in r 1 or r 2 : If \u03b2 is in r 1 , we can estimate the pose as before ; If \u03b2 is in r 2 , one option would be to apply another g \u0398 ( \u00b7 ) network trained for this range . However , it is actually possible to use the same network g \u0398 ( \u00b7 ) for both r 1 and r 2 , as follows . If the classifier predicts that \u03b2 in in r 2 , we mirror the input image W : As illustrated in Fig . 3 ( e ) , the object appears in the mirror image with a rotation angle equal to \u03b1 \u2212 \u03b2 , which is in r 1 . Therefore we can apply g \u0398 ( \u00b7 ) to the mirrored W . To obtain the correct pose , we finally mirror back the projections of the corners predicted by g \u0398 ( \u00b7 ) . We currently consider the case where the axis of symmetry is more or less vertical in the image , and mirror the image from left to right . When the axis is closer to be horizontal , we should mirror the image from top to bottom . Objects of revolution are a special and simpler case : since their angle of symmetry is 0 \u2022 , we predict their poses under the same angle of rotation . For training the pose predictor g \u0398 ( \u00b7 ) , we use the original training images with angles of rotation in r 1 , and mirror the training images with angles of rotation in r 2 . Handling Objects that are ' Not Exactly Symmetrical ' As mentioned in the introduction , some objects of the T - LESS dataset are only approximately symmetrical , such as Object # 5 in Fig . 1 ( d ) . The small details that make the object not perfectly symmetrical , however , do not help the optimization problem of Eq . ( 4 ) , but we would still like to predict the pose of this object . In the case of Object # 5 , we consider 4 regions instead of 2 : r 1 = [ 0 ; \u03c0 / 2 [ , r 1 = [ \u03c0 / 2 ; \u03c0 [ , r 3 = [ \u03c0 ; 3\u03c0 / 2 [ , and r 4 = [ 3\u03c0 / 2 ; 2\u03c0 [ , and we train the classifier k ( \u00b7 ) to predict in which of these four regions the angle of rotation \u03b2 is . If \u03b2 \u2208 r 2 or \u03b2 \u2208 r 4 , we mirror the image before computing the pose as before . Then , if \u03b2 \u2208 r 3 or \u03b2 \u2208 r 4 , we still have to add \u03c0 to the angle of rotation of the recovered pose to get an angle between 0 and 2\u03c0 . section : Refining the Pose We also introduce an optional additional stage to improve the accuracy of the pose estimates inspired by [ reference ] . As illustrated in Fig . 4 , we train another CNN that predicts an update to improve the pose . Because this CNN takes 4 or 6 channels as input , it is not clear how we can use VGG , as we did for the previously introduced networks , and we use here one CNN per object . However , this stage is optional , and without it , we already outperform the - state - of - the - art . The first image is the image window W as for g \u0398 ( \u00b7 ) . The second image depends on the current estimate of the pose : While [ reference ] generates a depth map with a deep network , we render ( using OpenGL ) either a binary mask or a color rendering of the target object as seen from this current estimate . More formally we train this CNN by minimizing : where h \u00b5 denotes the CNN , \u00b5 its parameters ; N ( e , t ) is a set of poses sampled around pose ( e , t ) , and Render ( e , t ) a function that returns a binary mask , or a color rendering , of the target object seen from pose ( e , t ) . At run - time , given a current estimate of the object pose represented by the projections of the cornersv = [ . . .m i . . . ] , and the corresponding parameterisation ( \u00ea , t ) , we can update this estimate by invoking h \u00b5 ( \u00b7 ) : section : Generating Training Images In Section 4 , we will compare our method to the stateof - the art for 3D object detection in color images [ reference ] , and like them , for each of 15 objects of the LINEMOD dataset , we use 15 % of the images for training and use the rest for testing . The training images are selected as in [ reference ] , such that relative orientation between them should be larger than a threshold . We also tried a random selection , and there was only a slight drop in performance , for some objects only . The selection method thus does not seem critical . The T - LESS dataset provides regularly sampled training images . As shown in Fig . 5 , we also use a similar method as [ reference ] to augment the training set : We extract the objects ' silhouettes from these images , which can be done as the ground Figure 5 . Two generated training images for different objects from the LINEMOD dataset [ reference ] . The object is shifted from the center to handle the inaccuracy of the detection method , and the background is random to make sure that the network g\u0398 can not exploit the context specific to the dataset . truth poses and the objects ' 3D models are available . Note that this means the results are not influenced by the scene context , which makes the pose estimation more difficult . To be robust to clutter and scale changes , we scale the segmented objects by a factor of s \u2208 [ 0.8 , 1.2 ] , and change the background by a patch extracted from a randomly picked image from the ImageNet dataset [ reference ] . Moreover , the object is shifted by some pixels from the center of the image window in both x and y directions . This helps us to handle small object localization errors made during detection . section : Experiments In this section , we present and discuss the results of our evaluation . We first describe the three evaluation metrics used in the literature and in this paper . We evaluate our method on all the possible datasets with color images for instance 3D detection and pose estimation we are aware of : the LINEMOD [ reference ] , Occlusion [ reference ] , and T - LESS [ reference ] datasets . section : Evaluation Metrics As in [ reference ] , we use the percentage of correctly predicted poses for each sequence and each object , where a pose is considered correct if it passes the tests presented below . section : 2D Projections [ 2 ] This is a metric suited for applications such as augmented reality . A pose is considered correct if the average of the 2D distances between the projections of the object 's vertices from the estimated pose and the ground truth pose is less than 5 pixels . 6D Pose [ reference ] With this metric , a pose is considered correct if the average of the 3D distances between the transformed of the object 's vertices Table 1 . Evaluation using the 2D Projections metric of using the 2D projections of the bounding box ( ' BB ' ) , compared to the direct prediction of the pose ( ' Direct ' ) , and of the refinement methods . For this evaluation , we used the ground truth 2D object center to avoid the influence of the detection . For the objects marked with a ( * ) , we optimize the value of the weight balancing the rotation and translation terms on the test set , giving an advantage to the ' Direct ' pose method . For the other objects , we used the value that is optimal for both the Ape and the Driller . is less than 10 % of the object 's diameter . V is the set of the object 's vertices , ( \u00ea , t ) the estimated pose and ( \u0113 , t ) the ground truth pose , and Tr e , t ( \u00b7 ) a rigid transformation by rotation e , translation t. For the objects with ambigious poses due to symmetries , [ reference ] replaces this measure by : 5 cm 5 \u2022 Metric [ reference ] With this metric , a pose is considered correct if the translation and rotation errors are below 5 cm and 5 \u2022 respectively . section : Contributions of the Different Steps The columns ' BB ' , ' Mask Ref . [ reference ] , and ' RGB Ref . ' of Table 1 compare the results of our method before and after two iterations of refinement , using either a binary mask or a color rendering . For this evaluation , we used the ground truth 2D object center to avoid the influence of the detection . Using refinement improves the results on average by 4.5 % and 6.3 % for the mask and color rendering respectively . Using a color rendering systematically yields the best results , but using the binary mask yields already a significant improvement , showing that an untextured model can be used . [ reference ] and our method without and with RGB Refinement using our segmentation - based method to obtain the 2D object centers on the LINEMOD dataset . [ reference ] does not provide results for the Bowl and the Cup , hence for the sake of comparison the average is taken over the first 13 objects . section : The LINEMOD Dataset : Comparison with [ 2 ] Table 2 compares our BB8 method with and without RGB refinement against the one presented in [ reference ] on the LINEMOD dataset . Because of lack of space , we provide the results without refinement only for the 2D Projection metric , however , the results for the other metrics are comparable . For this evaluation , we used the results of our detection method presented in Section 3.1 , not the ground truth 2D object center . Our method outperforms [ reference ] by a large margin : 15.6 % for 2D Projection , 12.6 % for 6D Pose and 28.4 % for the 5 cm 5 \u2022 metric . Fig . 7 shows qualitative results for our method on this dataset . For most of the images , the two bounding boxes , for the ground truth pose and for the pose we estimate , overlap almost perfectly . section : The Occlusion Dataset : Robustness to Partial Occlusions The Occlusion dataset was created by [ reference ] from the LINEMOD dataset . The partial occlusions make it significantly more difficult , and to the best of our knowledge , the only published results use both color and depth data . [ reference ] provide results using only color images , but limited to 2D detection , not 3D pose estimation . We only use images from the LINEMOD dataset to generate our training images by using the approach explained in Section 3.5 , except that we also randomly superimpose objects extracted from the other sequences to the target ob - ject to be robust to occlusions . We do not use any image of the test sequence to avoid having occlusions similar to the ones presented in the test sequence . Although all the poses in the test sets are not visible in the training sequences , we can estimate accurate poses with a 2D Projection error lower than 15px for about 80 % of the frames for these seven objects . We do not report the performance of our method for the Eggbox , as more than 70 % of close poses are not seen in the training sequence . Some qualitative results are shown in the second row of Fig . 7 . To the best of our knowledge , we are the first to present results on this dataset using color images only . section : The T - LESS Dataset : Handling Objects with an Axis of Symmetry The test sequences of the T - LESS dataset are very challenging , with sometimes multiple instances of the same objects and a high amount of clutter and occlusion . We considered only Scenes # 1 , # 2 , # 4 , # 5 , and # 7 in our experiments . It is also difficult to compare against the only published work on T - LESS [ reference ] , as it provides the 6D pose metric averaged per object or per scene , computed using RGB - D data , while , to the best of our knowledge , we are the first to report results on the T - LESS dataset using RGB images Figure 7 . Some qualitative results . First row : LINEMOD dataset ; Second row : Occlusion dataset ; Third row : T - LESS dataset ( for objects of revolution , we represent the pose with a cylinder rather than a box ) ; Last row : Some failure cases . From left to right : An example of a pose rejected by the 2D Projections metric , a failure due to the lack of corresponding poses in the training set , two examples from T - LESS rejected by the 6D pose metric , and one failure due to the fact that some objects are made of several instances of another object . Table 3 . Our quantitative results on T - LESS [ reference ] . Most of the errors are along the z axis of the camera , as we rely on color images . only . Similarly to [ reference ] , we evaluate the poses with more than 10 % of the object surface visible in the ground truth poses . As shown in Table 3 , the 6D Pose average per scene with our method is 54 % . The object 3D orientation and translation along the x and y axes of the camera are typically very well estimated , and most of the error is along the z axis , which should not be surprising for a method using color images only . section : Computation Times Our implementation takes 140 ms for the segmentation , 130 ms for the pose prediction , and 21 ms for each refinement iteration , on an Intel Core i7 - 5820 K 3.30 GHz desktop with a GeForce TITAN X. If there is only one object of interest , we can replace VGG by a specific network with a simpler architecture , the computation times then become 20 ms for the segmentation and 12 ms for the pose prediction , with similar accuracy . section : Conclusion Our \" holistic \" approach , made possible by the remarkable abilities of Deep Networks for regression , allowed us to significantly advance the state - of - the - art on 3D pose estimation from color images , even on challenging objects from the T - LESS dataset . section : section : Acknowledgment : This work was funded by the Christian Doppler Laboratory for Semantic 3D Computer Vision . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["LineMOD"]]], "Method": [[["BB8"]]], "Metric": [[["Accuracy"]]], "Task": [[["6D_Pose_Estimation"]]]}]}
{"docid": "TST3-SREX-0058", "doctext": "document : Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) . Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors . The BiLSTM is trained jointly with the parser objective , resulting in very effective feature extractors for parsing . We demonstrate the effectiveness of the approach by applying it to a greedy transition - based parser as well as to a globally optimized graph - based parser . The resulting parsers have very simple architectures , and match or surpass the state - of - the - art accuracies on English and Chinese . noitemsep , topsep=0pt , parsep=0pt , partopsep=0pt 1.0em section : Introduction The focus of this paper is on feature representation for dependency parsing , using recent techniques from the neural - networks ( \u2018 \u2018 deep learning \u2019 \u2019 ) literature . Modern approaches to dependency parsing can be broadly categorized into graph - based and transition - based parsers . Graph - based parsers treat parsing as a search - based structured prediction problem in which the goal is learning a scoring function over dependency trees such that the correct tree is scored above all other trees . Transition - based parsers treat parsing as a sequence of actions that produce a parse tree , and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process . Perhaps the simplest graph - based parsers are arc - factored ( first order ) models , in which the scoring function for a tree decomposes over the individual arcs of the tree . More elaborate models look at larger ( overlapping ) parts , requiring more sophisticated inference and training algorithms . The basic transition - based parsers work in a greedy manner , performing a series of locally - optimal decisions , and boast very fast parsing speeds . More advanced transition - based parsers introduce some search into the process using a beam or dynamic programming . Regardless of the details of the parsing framework being used , a crucial step in parser design is choosing the right feature function for the underlying statistical model . Recent work ( see Section [ reference ] for an overview ) attempt to alleviate parts of the feature function design problem by moving from linear to non - linear models , enabling the modeler to focus on a small set of \u2018 \u2018 core \u2019 \u2019 features and leaving it up to the machine - learning machinery to come up with good feature combinations . However , the need to carefully define a set of core features remains . For example , the work of chen2014fast uses 18 different elements in its feature function , while the work of pei2015effective uses 21 different elements . Other works , notably dyer2015transitionbased and le2014insideoutside , propose more sophisticated feature representations , in which the feature engineering is replaced with architecture engineering . In this work , we suggest an approach which is much simpler in terms of both feature engineering and architecture engineering . Our proposal ( Section [ reference ] ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section [ reference ] ) . The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an \u2018 \u2018 infinite \u2019 \u2019 window around it . We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non - linear scoring function ( multi - layer perceptron ) . Crucially , the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem . If we set aside the inherent complexity of the BiLSTM itself and treat it as a black box , our proposal results in a pleasingly simple feature extractor . We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures , transition - based ( Section [ reference ] ) as well as a graph - based ( Section [ reference ] ) . In the graph - based parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder . To the best of our knowledge , we are the first to perform such end - to - end training of a structured prediction model and a recurrent feature extractor for non - sequential outputs . Aside from the novelty of the BiLSTM feature extractor and the end - to - end structured training , we rely on existing models and techniques from the parsing and structured prediction literature . We stick to the simplest parsers in each category \u2013 greedy inference for the transition - based architecture , and a first - order , arc - factored model for the graph - based architecture . Despite the simplicity of the parsing architectures and the feature functions , we achieve near state - of - the - art parsing accuracies in both English ( 93.1 UAS ) and Chinese ( 86.6 UAS ) , using a first - order parser with two features and while training solely on Treebank data , without relying on semi - supervised signals such as pre - trained word embeddings , word - clusters , or techniques such as tri - training . When also including pre - trained word embeddings , we obtain further improvements , with accuracies of 93.9 UAS ( English ) and 87.6 UAS ( Chinese ) for a greedy transition - based parser with 11 features , and 93.6 UAS ( En ) / 87.4 ( Ch ) for a greedy transition - based parser with 4 features . section : Background and Notation paragraph : Notation We use to denote a sequence of vectors . is a function parameterized with parameters . We write as shorthand for \u2013 an instantiation of with a specific set of parameters . We use to denote a vector concatenation operation , and to denote an indexing operation taking the th element of a vector . subsection : Feature Functions in Dependency Parsing Traditionally , state - of - the - art parsers rely on linear models over hand - crafted feature functions . The feature functions look at core components ( e.g. \u2018 \u2018 word on top of stack \u2019 \u2019 , \u2018 \u2018 leftmost child of the second - to - top word on the stack \u2019 \u2019 , \u2018 \u2018 distance between the head and the modifier words \u2019 \u2019 ) , and are comprised of several templates , where each template instantiates a binary indicator function over a conjunction of core elements ( resulting in features of the form \u2018 \u2018 word on top of stack is X and leftmost child is Y and \u2026 \u2019\u2019 ) . The design of the feature function \u2013 which components to consider and which combinations of components to include \u2013 is a major challenge in parser design . Once a good feature function is proposed in a paper it is usually adopted in later works , and sometimes tweaked to improve performance . Examples of good feature functions are the feature - set proposed by zhang11acl for transition - based parsing ( including roughly 20 core components and 72 feature templates ) , and the feature - set proposed by mst for graph - based parsing , with the paper listing 18 templates for a first - order parser , while the first order feature - extractor in the actual implementation \u2019s code ( MSTParser ) includes roughly a hundred feature templates . The core features in a transition - based parser usually look at information such as the word - identity and part - of - speech ( POS ) tags of a fixed number of words on top of the stack , a fixed number of words on the top of the buffer , the modifiers ( usually left - most and right - most ) of items on the stack and on the buffer , the number of modifiers of these elements , parents of words on the stack , and the length of the spans spanned by the words on the stack . The core features of a first - order graph - based parser usually take into account the word and POS of the head and modifier items , as well as POS - tags of the items around the head and modifier , POS tags of items between the head and modifier , and the distance and direction between the head and modifier . subsection : Related Research Efforts Coming up with a good feature - set for a parser is a hard and time consuming task , and many researchers attempt to reduce the required manual effort . The work of lei - EtAl:2014:P14 - 1 suggests a low - rank tensor representation to automatically find good feature combinations . taubtabib2015template suggest a kernel - based approach to implicitly consider all possible feature combinations over sets of core - features . The recent popularity of neural networks prompted a move from templates of sparse , binary indicator features to dense core feature encodings fed into non - linear classifiers . chen2014fast encode each core feature of a greedy transition - based parser as a dense low - dimensional vector , and the vectors are then concatenated and fed into a non - linear classifier ( multi - layer perceptron ) which can potentially capture arbitrary feature combinations . weiss2015structured showed further gains using the same approach coupled with a somewhat improved set of core features , a more involved network architecture with skip - layers , beam search - decoding , and careful hyper - parameter tuning . pei2015effective apply a similar methodology to graph - based parsing . While the move to neural - network classifiers alleviates the need for hand - crafting feature - combinations , the need to carefully define a set of core features remain . For example , the feature representation in chen2014fast is a concatenation of 18 word vectors , 18 POS vectors and 12 dependency - label vectors . The above works tackle the effort in hand - crafting effective feature combinations . A different line of work attacks the feature - engineering problem by suggesting novel neural - network architectures for encoding the parser state , including intermediately - built subtrees , as vectors which are then fed to non - linear classifiers . Titov and Henderson encode the parser state using incremental sigmoid - belief networks titov - henderson:2007:IWPT2007 . In the work of dyer2015transitionbased , the entire stack and buffer of a transition - based parser are encoded as a stack - LSTMs , where each stack element is itself based on a compositional representation of parse trees . le2014insideoutside encode each tree node as two compositional representations capturing the inside and outside structures around the node , and feed the representations into a reranker . A similar reranking approach , this time based on convolutional neural networks , is taken by zhu2015reranking . Finally , in kiperwasser2016ef we present an Easy - First parser based on a novel hierarchical - LSTM tree encoding . In contrast to these , the approach we present in this work results in much simpler feature functions , without resorting to elaborate network architectures or compositional tree representations . Work by vinlays2014grammar employs a sequence - to - sequence with attention architecture for constituency parsing . Each token in the input sentence is encoded in a deep - BiLSTM representation , and then the tokens are fed as input to a deep - LSTM that predicts a sequence of bracketing actions based on the already predicted bracketing as well as the encoded BiLSTM vectors . A trainable attention mechanism is used to guide the parser to relevant BiLSTM vectors at each stage . This architecture shares with ours the use of BiLSTM encoding and end - to - end training . The sequence of bracketing actions can be interpreted as a sequence of Shift and Reduce operations of a transition - based parser . However , while the parser of Vinyals et al . relies on a trainable attention mechanism for focusing on specific BiLSTM vectors , parsers in the transition - based family we use in Section [ reference ] use a human designed stack and buffer mechanism to manually direct the parser \u2019s attention . While the effectiveness of the trainable attention approach is impressive , the stack - and - buffer guidance of transition - based parsers results in more robust learning . Indeed , work by cross2016incremental , published while working on the camera - ready version of this paper , show that the same methodology as ours is highly effective also for greedy , transition - based constituency parsing , surpassing the beam - based architecture of Vinyals et al . ( 88.3F vs. 89.8F points ) when trained on the Penn Treebank dataset and without using orthogonal methods such as ensembling and up - training . subsection : Bidirectional Recurrent Neural Networks Recurrent neural networks ( RNNs ) are statistical learners for modeling sequential data . An RNN allows one to model the th element in the sequence based on the past \u2013 the elements up to and including it . The RNN model provides a framework for conditioning on the entire history without resorting to the Markov assumption which is traditionally used for modeling sequences . RNNs were shown to be capable of learning to count , as well as to model line lengths and complex phenomena such as bracketing and code indentation . Our proposed feature extractors are based on a bidirectional recurrent neural network ( BiRNN ) , an extension of RNNs that take into account both the past and the future . We use a specific flavor of RNN called a long short - term memory network ( LSTM ) . For brevity , we treat RNN as an abstraction , without getting into the mathematical details of the implementation of the RNNs and LSTMs . For further details on RNNs and LSTMs , the reader is referred to goldberg - primer and cho - primer . The recurrent neural network ( RNN ) abstraction is a parameterized function mapping a sequence of input vectors , to a sequence of output vectors . Each output vector is conditioned on all the input vectors , and can be thought of as a summary of the prefix of . In our notation , we ignore the intermediate vectors and take the output of to be the vector . A bidirectional RNN is composed of two RNNs , and , one reading the sequence in its regular order , and the other reading it in reverse . Concretely , given a sequence of vectors and a desired index , the function is defined as : The vector is then a representation of the th item in , taking into account both the entire history and the entire future by concatenating the matching Rnn s. We can view the BiRNN encoding of an item as representing the item together with a context of an infinite window around it . paragraph : Computational Complexity Computing the BiRNN vectors encoding of the th element of a sequence requires time for computing the two RNNs and concatenating their outputs . A naive approach of computing the bidirectional representation of all elements result in computation . However , it is trivial to compute the BiRNN encoding of all sequence items in linear time by pre - computing and , keeping the intermediate representations , and concatenating the required elements as needed . paragraph : BiRNN Training Initially , the BiRNN encodings do not capture any particular information . During training , the encoded vectors are fed into further network layers , until at some point a prediction is made , and a loss is incurred . The back - propagation algorithm is used to compute the gradients of all the parameters in the network ( including the BiRNN parameters ) with respect to the loss , and an optimizer is used to update the parameters according to the gradients . The training procedure causes the BiRNN function to extract from the input sequence the relevant information for the task task at hand . paragraph : Going deeper We use a variant of deep bidirectional RNN ( or - layer BiRNN ) which is composed of BiRNN functions that feed into each other : the output of becomes the input of . Stacking BiRNNs in this way has been empirically shown to be effective . In this work , we use BiRNNs and deep - BiRNNs interchangeably , specifying the number of layers when needed . paragraph : Historical Notes RNNs were introduced by elman1990finding , and extended to BiRNNs by schuster1997bidirectional . The LSTM variant of RNNs is due to hochreiter1997long . BiLSTMs were recently popularized by graves2008supervised , and deep BiRNNs were introduced to NLP by irsoy2014opinion , who used them for sequence tagging . In the context of parsing , lewis2016lstm and Vaswani:2016:NAACL use a BiLSTM sequence tagging model to assign a CCG supertag for each token in the sentence . lewis2016lstm feeds the resulting supertags sequence into an A * CCG parser . Vaswani:2016:NAACL adds an additional layer of LSTM which receives the BiLSTM representation together with the k - best supertags for each word and outputs the most likely supertag given previous tags , and then feeds the predicted supertags to a discriminitively trained parser . In both works , the BiLSTM is trained to produce accurate CCG supertags , and is not aware of the global parsing objective . section : Our Approach We propose to replace the hand - crafted feature functions in favor of minimally - defined feature functions which make use of automatically learned Bidirectional LSTM representations . Given - words input sentence with words together with the corresponding POS tags , we associate each word and POS with embedding vectors and , and create a sequence of input vectors in which each is a concatenation of the corresponding word and POS vectors : The embeddings are trained together with the model . This encodes each word in isolation , disregarding its context . We introduce context by representing each input element as its ( deep ) BiLSTM vector , : Our feature function is then a concatenation of a small number of BiLSTM vectors . The exact feature function is parser dependent and will be discussed when discussing the corresponding parsers . The resulting feature vectors are then scored using a non - linear function , namely a multi - layer perceptron with one hidden layer ( MLP ) : where are the model parameters . Beside using the BiLSTM - based feature functions , we make use of standard parsing techniques . Crucially , the BiLSTM is trained jointly with the rest of the parsing objective . This allows it to learn representations which are suitable for the parsing task . Consider a concatenation of two BiLSTM vectors ( ) scored using an MLP . The scoring function has access to the words and POS - tags of and , as well as the words and POS - tags of the words in an infinite window surrounding them . As LSTMs are known to capture length and sequence position information , it is very plausible that the scoring function can be sensitive also to the distance between and , their ordering , and the sequential material between them . paragraph : Parsing - time Complexity Once the BiLSTM is trained , parsing is performed by first computing the BiLSTM encoding for each word in the sentence ( a linear time operation ) . Then , parsing proceeds as usual , where the feature extraction involves a concatenation of a small number of the pre - computed vectors . section : Transition - based Parser We begin by integrating the feature extractor in a transition - based parser . We follow the notation in tacl2013dynamic . The transition - based parsing framework assumes a transition system , an abstract machine that processes sentences and produces parse trees . The transition system has a set of configurations and a set of transitions which are applied to configurations . When parsing a sentence , the system is initialized to an initial configuration based on the input sentence , and transitions are repeatedly applied to this configuration . After a finite number of transitions , the system arrives at a terminal configuration , and a parse tree is read off the terminal configuration . In a greedy parser , a classifier is used to choose the transition to take in each configuration , based on features extracted from the configuration itself . The parsing algorithm is presented in Algorithm [ reference ] below . [ h ] Greedy transition - based parsing [ 1 ] Input : sentence , parameterized function with parameters . not Given a sentence , the parser is initialized with the configuration ( line [ reference ] ) . Then , a feature function represents the configuration as a vector , which is fed to a scoring function Score assigning scores to ( configuration , transition ) pairs . Score scores the possible transitions , and the highest scoring transition is chosen ( line [ reference ] ) . The transition is applied to the configuration , resulting in a new parser configuration . The process ends when reaching a final configuration , from which the resulting parse tree is read and returned ( line [ reference ] ) . Transition systems differ by the way they define configurations , and by the particular set of transitions available to them . A parser is determined by the choice of a transition system , a feature function and a scoring function Score . Our choices are detailed below . paragraph : The Arc - Hybrid System Many transition systems exist in the literature . In this work , we use the arc - hybrid transition system , which is similar to the more popular arc - standard system , but for which an efficient dynamic oracle is available . In the arc - hybrid system , a configuration consists of a stack , a buffer , and a set of dependency arcs . Both the stack and the buffer hold integer indices pointing to sentence elements . Given a sentence , the system is initialized with an empty stack , an empty arc set , and , where is the special root index . Any configuration with an empty stack and a buffer containing only is terminal , and the parse tree is given by the arc set of . The arc - hybrid system allows 3 possible transitions , Shift , and , defined as : The Shift transition moves the first item of the buffer ( ) to the stack . The Left\u2113 transition removes the first item on top of the stack ( ) and attaches it as a modifier to with label , adding the arc . The Right\u2113 transition removes from the stack and attaches it as a modifier to the next item on the stack ( ) , adding the arc . paragraph : Scoring Function Traditionally , the scoring function is a discriminative linear model of the form . The linearity of Score required the feature function to encode non - linearities in the form of combination features . We follow Chen and Manning chen2014fast and replace the linear scoring model with an MLP . paragraph : Simple Feature Function The feature function is typically complex ( see Section [ reference ] ) . Our feature function is the concatenated BiLSTM vectors of the top 3 items on the stack and the first item on the buffer . I.e. , for a configuration the feature extractor is defined as : This feature function is rather minimal : it takes into account the BiLSTM representations of and , which are the items affected by the possible transitions being scored , as well as one extra stack context . Figure 1 depicts transition scoring with our architecture and this feature function . Note that , unlike previous work , this feature function does not take into account , the already built structure . The high parsing accuracies in the experimental sections suggest that the BiLSTM encoding is capable of estimating a lot of the missing information based on the provided stack and buffer elements and the sequential content between them . While not explored in this work , relying on only four word indices for scoring an action results in very compact state signatures , making our proposed feature representation very appealing for use in transition - based parsers that employ dynamic - programming search . paragraph : Extended Feature Function One of the benefits of the greedy transition - based parsing framework is precisely its ability to look at arbitrary features from the already built tree . If we allow somewhat less minimal feature function , we could add the BiLSTM vectors corresponding to the right - most and left - most modifiers of , and , as well as the left - most modifier of , reaching a total of 11 BiLSTM vectors . We refer to this as the extended feature set . As we \u2019ll see in Section [ reference ] , using the extended set does indeed improve parsing accuracies when using pre - trained word embeddings , but has a minimal effect in the fully - supervised case . subsection : Details of the Training Algorithm The training objective is to set the score of correct transitions above the scores of incorrect transitions . We use a margin - based objective , aiming to maximize the margin between the highest scoring correct action and the highest scoring incorrect action . The hinge loss at each parsing configuration is defined as : where is the set of possible transitions and is the set of correct ( gold ) transitions at the current stage . At each stage of the training process the parser scores the possible transitions , incurs a loss , selects a transition to follow , and moves to the next configuration based on it . The local losses are summed throughout the parsing process of a sentence , and the parameters are updated with respect to the sum of the losses at sentence boundaries . The gradients of the entire network ( including the MLP and the BiLSTM ) with respect to the sum of the losses are calculated using the backpropagation algorithm . As usual , we perform several training iterations over the training corpus , shuffling the order of sentences in each iteration . paragraph : Error - Exploration and Dynamic Oracle Training We follow tacl2013dynamic ; coling2012dynamic in using error exploration training with a dynamic - oracle , which we briefly describe below . At each stage in the training process , the parser assigns scores to all the possible transitions . It then selects a transition , applies it , and moves to the next step . Which transition should be followed ? A common approach follows the highest scoring transition that can lead to the gold tree . However , when training in this way the parser sees only configurations that result from following correct actions , and as a result tends to suffer from error propagation at test time . Instead , in error - exploration training the parser follows the highest scoring action in during training even if this action is incorrect , exposing it to configurations that result from erroneous decisions . This strategy requires defining the set such that the correct actions to take are well - defined also for states that can not lead to the gold tree . Such a set is called a dynamic oracle . We perform error - exploration training using the dynamic - oracle defined by tacl2013dynamic . paragraph : Aggressive Exploration We found that even when using error - exploration , after one iteration the model remembers the training set quite well , and does not make enough errors to make error - exploration effective . In order to expose the parser to more errors , we follow an aggressive - exploration scheme : we sometimes follow incorrect transitions also if they score below correct transitions . Specifically , when the score of the correct transition is greater than that of the wrong transition but the difference is smaller than a margin constant , we chose to follow the incorrect action with probability ( we use in our experiments ) . paragraph : Summary The greedy transition - based parser follows standard techniques from the literature ( margin - based objective , dynamic oracle training , error exploration , MLP - based non - linear scoring function ) . We depart from the literature by replacing the hand - crafted feature function over carefully selected components of the configuration with a concatenation of BiLSTM representations of a few prominent items on the stack and the buffer , and training the BiLSTM encoder jointly with the rest of the network . section : Graph - based Parser Graph - based parsing follows the common structured prediction paradigm : Given an input sentence ( and the corresponding sequence of vectors ) we look for the highest - scoring parse tree in the space of valid dependency trees over . In order to make the search tractable , the scoring function is decomposed to the sum of local scores for each part independently . In this work , we focus on arc - factored graph based approach presented in mst . Arc - factored parsing decomposes the score of a tree to the sum of the score of its head - modifier arcs : Given the scores of the arcs the highest scoring projective tree can be efficiently found using Eisner \u2019s decoding algorithm eisner1996dep . McDonald et al . and most subsequent work estimate the local score of an arc by a linear model parameterized by a weight vector , and a feature function assigning a sparse feature vector for an arc linking modifier to head . We follow pei2015effective and replace the linear scoring function with an MLP . The feature extractor is usually complex , involving many elements ( see Section [ reference ] ) . In contrast , our feature extractor uses merely the BiLSTM encoding of the head word and the modifier word : The final model is : The architecture is illustrated in Figure [ reference ] . paragraph : Training The training objective is to set the score function such that correct tree is scored above incorrect ones . We use a margin - based objective , aiming to maximize the margin between the score of the gold tree and the highest scoring incorrect tree . We define a hinge loss with respect to a gold tree as : Each of the tree scores is then calculated by activating the MLP on the arc representations . The entire loss can viewed as the sum of multiple neural networks , which is sub - differentiable . We calculate the gradients of the entire network ( including to the BiLSTM encoder and word embeddings ) . paragraph : Labeled Parsing Up to now , we described unlabeled parsing . A possible approach for adding labels is to score the combination of an unlabeled arc and its label by considering the label as part of the arc . This results in parts that need to be scored , leading to slow parsing speeds and arguably a harder learning problem . Instead , we chose to first predict the unlabeled structure using the model given above , and then predict the label of each resulting arc . Using this approach , the number of parts stays small , enabling fast parsing . The labeling of an arc is performed using the same feature representation fed into a different MLP predictor : As before we use a margin based hinge loss . The labeler is trained on the gold trees . The BiLSTM encoder responsible for producing and is shared with the arc - factored parser : the same BiLSTM encoder is used in the parer and the labeler . This sharing of parameters can be seen as an instance of multi - task learning . As we show in Section [ reference ] , the sharing is effective : training the BiLSTM feature encoder to be good at predicting arc - labels significantly improves the parser \u2019s unlabeled accuracy . paragraph : Loss augmented inference In initial experiments , the network learned quickly and overfit the data . In order to remedy this , we found it useful to use loss augmented inference . The intuition behind loss augmented inference is to update against trees which have high model scores and are also very wrong . This is done by augmenting the score of each part not belonging to the gold tree by adding a constant to its score . Formally , the loss transforms as follows : paragraph : Speed improvements The arc - factored model requires the scoring of arcs . Scoring is performed using an MLP with one hidden layer , resulting in matrix - vector multiplications from the input to the hidden layer , and multiplications from the hidden to the output layer . The first multiplications involve larger dimensional input and output vectors , and are the most time consuming . Fortunately , these can be reduced to multiplications and vector additions , by observing that the multiplication can be written as where and are are the first and second half of the matrix and reusing the products across different pairs . Summary The graph - based parser is straight - forward first - order parser , trained with a margin - based hinge - loss and loss - augmented inference . We depart from the literature by replacing the hand - crafted feature function with a concatenation of BiLSTM representations of the head and modifier words , and training the BiLSTM encoder jointly with the structured objective . We also introduce a novel multi - task learning approach for labeled parsing by training a second - stage arc - labeler sharing the same BiLSTM encoder with the unlabeled parser . section : Experiments and Results We evaluated our parsing model on English and Chinese data . For comparison purposes we follow the setup of dyer2015transitionbased . paragraph : Data For English , we used the Stanford Dependency ( SD ) conversion of the Penn Treebank , using the standard train / dev / test splits with the same predicted POS - tags as used in dyer2015transitionbased ; chen2014fast . This dataset contains a few non - projective trees . Punctuation symbols are excluded from the evaluation . For Chinese , we use the Penn Chinese Treebank 5.1 ( CTB5 ) , using the train / test / dev splits of with gold part - of - speech tags , also following . When using external word embeddings , we also use the same data as dyer2015transitionbased . paragraph : Implementation Details The parsers are implemented in python , using the PyCNN toolkit for neural network training . The code is available at the github repository . We use the LSTM variant implemented in PyCNN , and optimize using the Adam optimizer . Unless otherwise noted , we use the default values provided by PyCNN ( e.g. for random initialization , learning rates etc ) . The word and POS embeddings and are initialized to random values and trained together with the rest of the parsers \u2019 networks . In some experiments , we introduce also pre - trained word embeddings . In those cases , the vector representation of a word is a concatenation of its randomly - initialized vector embedding with its pre - trained word vector . Both are tuned during training . We use the same word vectors as in dyer2015transitionbased During training , we employ a variant of word dropout , and replace a word with the unknown - word symbol with probability that is inversely proportional to the frequency of the word . A word appearing times in the training corpus is replaced with the unknown symbol with probability . If a word was dropped the external embedding of the word is also dropped with probability . We train the parsers for up to 30 iterations , and choose the best model according to the UAS accuracy on the development set . paragraph : Hyperparameter Tuning We performed a very minimal hyper - parameter search with the graph - based parser , and use the same hyper - parameters for both parsers . The hyper - parameters of the final networks used for all the reported experiments are detailed in Table [ reference ] . Main Results Table [ reference ] lists the test - set accuracies of our best parsing models , compared to other state - of - the - art parsers from the literature . It is clear that our parsers are very competitive , despite using very simple parsing architectures and minimal feature extractors . When not using external embeddings , the first - order graph - based parser with 2 features outperforms all other systems that are not using external resources , including the third - order TurboParser . The greedy transition based parser with 4 features also matches or outperforms most other parsers , including the beam - based transition parser with heavily engineered features of Zhang and Nivre ( 2011 ) and the Stack - LSTM parser of dyer2015transitionbased , as well as the same parser when trained using a dynamic oracle . Moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both English and Chinese . Interestingly , when adding external word embeddings the accuracy of the graph - based parser degrades . We are not sure why this happens , and leave the exploration of effective semi - supervised parsing with the graph - based model for future work . The greedy parser does manage to benefit from the external embeddings , and using them we also see gains from moving from the simple to the extended feature set . Both feature sets result in very competitive results , with the extended feature set yielding the best reported results for Chinese , and ranked second for English , after the heavily - tuned beam - based parser of weiss2015structured . paragraph : Additional Results We perform some ablation experiments in order to quantify the effect of the different components on our best models ( Table [ reference ] ) . Loss augmented inference is crucial for the success of the graph - based parser , and the multi - task learning scheme for the arc - labeler contributes nicely to the unlabeled scores . Dynamic oracle training yields nice gains for both English and Chinese . section : Conclusion We presented a pleasingly effective approach for feature extraction for dependency parsing based on a BiLSTM encoder that is trained jointly with the parser , and demonstrated its effectiveness by integrating it into two simple parsing models : a greedy transition - based parser and a globally optimized first - order graph - based parser , yielding very competitive parsing accuracies in both cases . paragraph : Acknowledgements This research is supported by the Intel Collaborative Research Institute for Computational Intelligence ( ICRI - CI ) and the Israeli Science Foundation ( grant number 1555 / 15 ) . We thank Lillian Lee for her important feedback and efforts invested in editing this paper . We also thank the reviewers for their valuable comments . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["BIST_graph-based_parser"]]], "Metric": [[["LAS"]]], "Task": [[["Dependency_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["BIST_transition-based_parser"]]], "Metric": [[["LAS"]]], "Task": [[["Dependency_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["BIST_graph-based_parser"]]], "Metric": [[["POS"]]], "Task": [[["Dependency_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["BIST_transition-based_parser"]]], "Metric": [[["POS"]]], "Task": [[["Dependency_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["BIST_graph-based_parser"]]], "Metric": [[["UAS"]]], "Task": [[["Dependency_Parsing"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Penn_Treebank"]]], "Method": [[["BIST_transition-based_parser"]]], "Metric": [[["UAS"]]], "Task": [[["Dependency_Parsing"]]]}]}
{"docid": "TST3-SREX-0059", "doctext": "Holistic , Instance - level Human Parsing section : Abstract Object parsing - the task of decomposing an object into its semantic parts - has traditionally been formulated as a category - level segmentation problem . Consequently , when there are multiple objects in an image , current methods can not count the number of objects in the scene , nor can they determine which part belongs to which object . We address this problem by segmenting the parts of objects at an instance - level , such that each pixel in the image is assigned a part label , as well as the identity of the object it belongs to . Moreover , we show how this approach benefits us in obtaining segmentations at coarser granularities as well . Our proposed network is trained end - to - end given detections , and begins with a category - level segmentation module . Thereafter , a differentiable Conditional Random Field , defined over a variable number of instances for every input image , reasons about the identity of each part by associating it with a human detection . In contrast to other approaches , our method can handle the varying number of people in each image and our holistic network produces state - of - the - art results in instance - level part and human segmentation , together with competitive results in category - level part segmentation , all achieved by a single forward - pass through our neural network . section : Introduction Object parsing , the segmentation of an object into semantic parts , is naturally performed by humans to obtain a more detailed understanding of the scene . When performed automatically by computers , it has many practical applications , such as in human - robot interaction , human behaviour analysis and image descriptions for the visually impaired . Furthermore , detailed part information has been shown to be beneficial in other visual recognition tasks such as fine - grained recognition [ reference ] , human pose estimation [ reference ] and object detection [ reference ] . In this paper , we focus on the application of parsing humans as it is more commonly studied , although our method makes no assumptions on the type of object it is segmenting . In contrast to existing human parsing approaches [ reference ][ reference ][ reference ] , we operate at an instance level ( to our knowledge , we are the first work to do so ) . As shown in Fig . 1 , not only do we segment the various body parts of humans ( Fig . 1b ) , but we associate each of these parts to one of the humans in the scene ( Fig . 1c ) , which is particularly important for understanding scenes with multiple people . In contrast to existing instance segmentation work [ reference ] Part Segmentation Human Segmentation Figure 1 : Our proposed approach segments human parts at an instance level ( c ) ( which to our knowledge is the first work to do so ) from category - level part segmentations produced earlier in the network ( b ) . Moreover , we can easily obtain human instance segmentations ( d ) by taking the union of all pixels associated to a particular person . Therefore , our proposed end - to - end trained neural network parses humans into semantic parts at both category and instance level in a single forward - pass . Best viewed in colour . [ reference ] , we operate at a more detailed part level , enabling us to extract more comprehensive information of the scene . Furthermore , with our part - level instance segmentation of humans , we can easily recover human - level instance segmentation ( by taking the union of all parts assigned to a particular instance as shown in Fig . 1d ) , and we show significant improvement over previous state - of - the - art in human instance - segmentation when doing so . Our approach is based on a deep Convolutional Neural Network ( CNN ) , which consists of an initial category - level part segmentation module . Using the output of a human detector , we are then able to associate segmented parts with detected humans in the image using a differentiable Conditional Random Field ( CRF ) , producing a part - level instance segmentation of the image . Our formulation is robust to false - positive detections as well as imperfect bounding boxes which do not cover the entire human , in contrast to other instance segmentation methods based on object detectors [ reference ][ reference ][ reference ][ reference ][ reference ] . Given object detections , our network is trained end - to - end , given detections , with a novel loss function which allows us to handle a variable number of human instances on every image . We evaluate our approach on the Pascal Person - Parts [ reference ] dataset , which contains humans in a diverse set of poses and occlusions . We achieve state - of - the - art results on instancelevel segmentation of both body parts and humans . Moreover , our results on semantic part segmentation ( which is not - instance aware ) is also competitive with current state - of - theart . All of these results are achieved with a holistic , end - to - end trained model which parses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network . section : Related Work The problem of object parsing , which aims to decompose objects into their semantic parts , has been addressed by numerous works [ reference ][ reference ][ reference ][ reference ][ reference ] , most of which have concentrated on parsing humans . However , none of the aforementioned works have parsed objects at an instance level as shown in Fig . 1 , but rather category level . In fact , a lot of work on human parsing has focussed on datasets such as Fashionista [ reference ] , ATR [ reference ] and Deep Fashion [ reference ] where images typically contain only one , centred person . The notion of instance - level segmentation only matters when more than one person is present in an image , motivating us to evaluate our method on the Pascal Person - Parts dataset [ reference ] where multiple people can appear in unconstrained environments . Recent human parsing approaches have typically been similar to semantic segmentation works using fully convolutional networks ( FCNs ) [ reference ] , but trained to label parts [ reference ][ reference ][ reference ] instead of object classes . However , methods using only FCNs do not explicitly model the structure of a human body , and typically do not perform as well as methods which do [ reference ] . Structural priors of the human body have been encoded using pictorial structures [ reference ][ reference ] , Conditional Random Fields ( CRFs ) [ reference ][ reference ][ reference ][ reference ] and more recently , with LSTMs [ reference ][ reference ] . The HAZN approach of [ reference ] addressed the problem that some parts are often very small compared to other parts and difficult to segment with scale - variant CNNs . This scale variation was handled by a cascade of three separatelytrained FCNs , each parsing different regions of the image at different scales . An early instance segmentation work by Winn et al . [ reference ] predicted the parts of an object , and then encouraged these parts to maintain a spatial ordering , characteristic of an instance , using asymmetric pairwise potentials in a CRF . However , subsequent work has not operated at a part level . Zhang et al . [ reference ][ reference ] performed instance segmentation of vehicles using an MRF . However , this graphical model was not trained end - to - end as done by [ reference ][ reference ][ reference ] and our approach . Furthermore , they assumed a maximum of 9 cars per image . Approaches using recurrent neural networks [ reference ][ reference ] can handle a variable number of instances per image by segmenting an instance per time - step , but are currently restricted to only one object category . Our method , on the other hand , is able to handle both an arbitrary number of objects , and multiple object categories in the image with a single forward - pass through the network . Various methods of instance segmentation have also involved modifying object detection systems to output segments instead of bounding boxes [ reference ][ reference ][ reference ][ reference ] . However , these methods can not produce a segmentation map of the image , as shown in Fig . 1 , without postprocessing as they consider each detection independently . Although our method also uses an object detector , it considers all detections in the image jointly with an initial category - level segmentation , and produces segmentation maps naturally where one pixel can not belong to multiple instances in contrast to the aforementioned approaches . The idea of combining the outputs of a category - level segmentation network and an object detector to reason about different instances was also presented by [ reference ] . However , that system was not trained end - toend , could not segment instances outside the detector 's bounding box , and did not operate at a part level . section : Proposed Approach Our network ( Fig . 2 ) consists of two components : a category - level part segmentation module , and an instance segmentation module . As both of these modules are differentiable , they can be integrated into a single network and trained jointly . The instance segmentation module ( Sec . 3.2 ) uses the output of the first category - level segmentation module ( Sec . 3.1 ) as well as the outputs of an object detector as its input . It associates each pixel in the categorylevel segmentation with an object detection , resulting in an instance - level segmentation of the image . Given a H \u00d7W \u00d7 3 input image , I , the category - level part segmentation module produces a H \u00d7W \u00d7 ( P + 1 ) dimensional output Q where P is the number of part classes in the dataset and one background class . There can be a variable number , D , of human detections per image , and the output of the instance segmentation module is an H \u00d7W \u00d7 ( PD + 1 ) tensor denoting the probabilities , at each pixel in the image , of each of the P part classes belonging to one of the D detections . Two challenges of instance segmentation are the variable number of instances in every image , and the fact that permutations of instance labels lead to identical results ( in Fig . 1 , how we order the different people does not matter ) . Zhang et al . [ reference ][ reference ] resolve these issues by assuming a maximum number of instances and using the ground - truth depth ordering of instances respectively . Others have bypassed both of these issues by predicting each instance independently [ reference ][ reference ][ reference ][ reference ] , but this also allows a pixel to belong to multiple instances . Instead , we use a loss function ( Sec 3.3 ) that is based on \" matching \" the prediction to the ground - truth , allowing us to handle permutations of the ground truth . Furthermore , weight - sharing in our instance segmentation module allows us to segment a variable number of instances per image . As a result , we do not assume a maximum number of instances , consider all instances jointly , and train our network end - to - end , given object detections . section : Category - level part segmentation module The part segmentation module is a fully convolutional network [ reference ] based on ResNet - 101 [ reference ] . A common technique , presented in [ reference ][ reference ] , is to predict the image at three different scales ( with network weights shared among all the scales ) , and combine predictions together with learned , image - dependent weights . We take a different approach of fusing information at multiple scales - we pool the features after res5c [ reference ] at five different resolutions ( by varying the pooling stride ) , upsample the features to the resolution before pooling , and then concatenate these features before passing them to the final convolutional classifier , as proposed in [ reference ] . As we show in Sec 4.4 , this approach achieves better semantic segmentation results than [ reference ][ reference ] . We denote the output of this module by the tensor , Q , where Q i ( l ) is the probability of pixel i being assigned label l \u2208 { 0 , 1 , 2 , ... , P}. Further details of this module are included in the appendix . section : Instance - level segmentation module This module creates an instance - level segmentation of the image by associating each pixel in the input category - level segmentation , Q , with one of the D input human - detections or the background label . Let there be D input human - detections for the image , where the i - th detection is represented by B i , the set of pixels lying within the four corners of its bounding box , and s i \u2208 [ 0 , 1 ] , the detection score . We assume that the 0 - th detection refers to the background label . Furthermore , we define a multinomial random variable , V i , at each of the N pixels in the image , and let . This variable can take on a label from the set { 1 , 2 , ... , D } \u00d7 { 1 , 2 , ... , P } \u222a { ( 0 , 0 ) } since each of the P part labels can be associated with one of the D human detections , or that pixel could belong to the background label , ( 0 , 0 ) . We formulate a Conditional Random Field over these V variables , where the energy of the assignment v to all of the instance variables V consists of two unary terms , and one pairwise term ( whose weighting co - efficients are all learned via backpropagation ) : ( The unary and pairwise potentials are computed within our neural network , differentiable with respect to their input and parameters , and described in Sec . 3.2.1 through 3.2.3 . The Maximum - a - Posteriori ( MAP ) estimate of our CRF ( since the energy in Eq . 1 characterises a Gibbs distribution ) is computed as the final labelling produced by our network . We perform the iterative mean - field inference algorithm to approximately compute the MAP solution by minimising Eq . 1 . As shown by Zheng et al . [ reference ] , this can be formulated as a Recurrent Neural Network ( RNN ) , allowing it to be trained end - to - end as part of a larger network . However , as our network is input a variable number of detections per image , D , the label space of the CRF is dynamic . Therefore , unlike [ reference ] , the parameters of our CRF are not class - specific to allow for this variable number of \" channels \" . section : Box Consistency Term We observe that in most cases , a body part belonging to a person is located inside the bounding box of the person . Based on this observation , the box consistency term is employed to encourage pixel locations inside a human bounding box B i to be associated with the i - th human detection . The box term potential at spatial location k for body part j of a human i is assigned either 0 for k / \u2208 B i , or the product of the detection score , s i , and the category - level part segmentation confidence , Note that this potential may be robust to false - positive detections when the category - level segmentation and human detection do not agree with each other , since Q k ( l ) , the probability of a pixel k taking on body - part label l , is low . Furthermore , note that we use one humandetection to reason about the identity of all parts which constitute that human . section : Global Term A possible shortcoming for the box consistency potential is that if some pixels belonging to a human instance fall outside the bounding box and are consequently assigned 0 for the box consistency term potential , they would be lost in the final instance segmentation prediction . Visually , the generated instance masks would appear truncated along the bounding box boundaries - a problem suffered by [ reference ][ reference ][ reference ][ reference ] . To overcome this undesirable effect , we introduce the global potential : it complements the box consistency term by assuming that a pixel is equally likely to belong to any one of the detected humans . It is expressed as Prediction , P Original ground - truth , Y \" Matched \" ground - truth , Y * Figure 3 : As different permutations of the ground - truth are equivalent in the case of instance segmentation , we \" match \" the original ground - truth , Y , to our network 's prediction , P , to obtain the \" matched \" ground - truth which we use to compute our loss during training . section : Pairwise Term Our pairwise term is composed of densely - connected Gaussian kernels [ reference ] which are commonly used in segmentation literature [ reference ][ reference ] . This pairwise potential encourages both spatial and appearance consistency , and we find these priors to be suitable in the case of instancelevel segmentation as well . As in [ reference ] , the weighting parameters of these potentials are learned via backpropagation , though in our case , the weights are shared among all classes . section : Loss function and network training We first pre - train the category - level segmentation part of our network , as described in the appendix . Thereafter , we add the instance segmentation module , and train with a permutationinvariant loss function which is backpropagated through both our instance - and categorylevel segmentation networks . Since all permutations of an instance segmentation have the same qualitative result , we \" match \" the original ground - truth to our prediction before computing the loss , as shown in Fig . 3 . This matching is based on the Intersection over Union ( IoU ) [ reference ] of a predicted and ground - truth instance , similar to [ reference ] . Let Y = { y 1 , y 2 , ... , y m } , a set of m segments , denote the ground - truth labelling of an image , where each segment is an instance and has a part label assigned to it . Similarly , let P = { p 1 , p 2 , ... , p n } denote our n predicted instances , each with an associated part label . Note that m and n need not be the same as we may predict greater or fewer instances than there actually are in the image . The \" matched \" ground truth , Y * is the permutation of the original ground - truth labelling which maximises the IoU between our prediction , P and ground - truth where \u03c0 ( Y ) denotes the set of all permutations of Y. Note that we define the IoU between all segments of different labels to be 0 . Eq . 4 can be solved efficiently using the Hungarian algorithm as it can be formulated as a bipartite graph matching problem , and once we have the \" matched \" ground - truth , Y * , we can apply any loss function to it and train our network for segmentation . In our case , we use the standard cross - entropy loss function on the \" matched \" ground truth . In addition , we employ Online Hard Example Mining ( OHEM ) , and only compute our loss over the top K pixels with the highest loss in the training mini - batch . We found that during training , many pixels already had a high probability of being assigned to the correct class . By only selecting the top K pixels with the highest loss , we are able to encourage our network to improve on the pixels it is currently misclassifying , as opposed to increasing the probability of a pixel it is already classifying correctly . This approach was inspired by \" bootstrapping \" [ reference ][ reference ] or \" hard - negative mining \" [ reference ] commonly used in training object detectors . However , these methods mined hard examples from the entire dataset . Our approach is most similar to [ reference ] , who mined hard examples online from each mini - batch in the context of detection . Similar to the aforementioned works , we found OHEM to improve our overall results , as shown in Sec . 4.2 . section : Obtaining segmentations at other granularities Given the part instance prediction produced by our proposed network , we are able to easily obtain human instance segmentation and semantic part segmentation . In order to achieve human instance segmentation , we map the predicted part instance labels ( i , j ) , i.e. part j of person i , to i. Whereas to obtain semantic part segmentation , we map predicted part instance labels ( i , j ) to j instead . section : Experiments We describe our dataset and experimental set - up in Sec . 4.1 , before presenting results on instance - level part segmentation ( Fig . 1c ) , instance - level human segmentation ( Fig . 1d ) and semantic part segmentation ( Fig . 1b ) . Additional quantitative and qualitative results , failure cases and experimental details are included in the appendix . section : Experimental Set - up We evaluate our proposed method on the Pascal Person - Part dataset [ reference ] which contains 1716 training images , and 1817 test images . This dataset contains multiple people per image in unconstrained poses and environments , and contains six human body part classes ( Fig . 1b ) , as well as the background label . As described in Sec . 3.3 , we initially pre - train our categorylevel segmentation module before training for instance - level segmentation . This module is first trained on the 21 classes of the Pascal VOC dataset [ reference ] , and then finetuned on the seven classes of the Pascal Part training set using category - level annotations . Finally , we train for instance segmentation with instance - level ground truth . Full details of our training process , including all hyperparameters such as learning rate , are in the appendix . To clarify these details , we will also release our code . We use the standard AP r metric [ reference ] for evaluating instance - level segmentation : the mean Average Precision of our predictions is computed where a prediction is considered correct if its IoU with a ground - truth instance is above a certain threshold . This is similar to the AP metric used in object detection . However , in detection , the IoU between groundtruth and predicted bounding boxes is computed , whereas here , the IoU between regions is computed . Furthermore , in detection , an overlap threshold of 0.5 is used , whereas we vary this threshold . Finally , we define the AP r vol which is the mean of the AP r score for overlap thresholds varying from 0.1 to 0.9 in increments of 0.1 . We use the publicly available R - FCN detection framework [ reference ] , and train a new model with data from VOC 2012 [ reference ] that do not overlap with any of our test sets . We train with all object classes of VOC , and only use the output for the human class . Non - maximal suppression is performed on all detections before being fed into our network . Table 1 shows our results on part - level instance segmentation on the Pascal Person - Part dataset . To our knowledge , we are the first work to do this , and hence we study the effects of various design choices on overall performance . We also use the publicly available code for MNC [ reference ] , which won the MS - COCO 2016 instance segmentation challenge , and finetune their public model trained on VOC 2011 [ reference ] on Person - Part instances as a baseline . section : Results on Instance - level Part Segmentation We first train our model in a piecewise manner , by first optimising the parameters of the category - level segmentation module , and then \" freezing \" the weights of this module and only training the instance network . Initially , we only use the box consistency term ( Sec . 3.2.1 ) in the Instance CRF , resulting in an AP r at 0.5 of 38.0 % . Note that this model is equivalent to our reimplementation of [ reference ] . Adding in the global potential ( Sec . 3.2.2 ) helps us cope with bounding boxes which do not cover the whole human , and we see an improvement at all IoU thresholds . Training our entire network end - to - end gives further benefits . We then train all variants of our model with OHEM , and observe consistent improvements across all IoU thresholds with respect to the corresponding baseline . Here , we set K = 2 [ reference ] , meaning that we computed our loss over 2 [ reference ] or approximately 12 % of the hardest pixels in each training image ( since we train at full resolution ) . We also employ OHEM when pre - training the category - level segmentation module of our network , and observe minimal difference in the final result if we use OHEM when training the category - level segmentation module but not the instance segmentation module . Training end - to - end with OHEM achieves 2.6 % higher in AP r at 0.5 , and 1.8 % higher AP r vol over a piecewise - trained baseline model without OHEM and only the box term ( second row ) , which is equivalent to the model of [ reference ] . Furthermore , our AP r vol is 1.7 % greater than the strong MNC [ reference ] baseline . Note that although [ reference ] also performed instance - level segmentation on the same dataset , their evaluation was only done using human instance labels , which is similar to our following experiment on human instance segmentation . section : Results on Human Instance Segmentation We can trivially obtain instance - level segmentations of humans ( Fig 1d ) , as mentioned in Sec . 3.4 . Table 2 shows our state - of - the - art instance segmentation results for humans on the VOC 2012 validation set [ reference ] . We use the best model from the previous section as there is DeepLab * [ reference ] 53.0 Attention [ reference ] 56.4 HAZN [ reference ] 57.5 LG - LSTM [ reference ] 58.0 Graph LSTM [ reference ] 60.2 DeepLab v2 [ reference ] 64.9 RefineNet [ reference ] 68.6 Ours , pre - trained 65.9 Ours , final network 66.3 * Result reported in [ reference ] no overlap between the Pascal Person - Part training set , and the VOC 2012 validation set . As Tab . 2 shows , our proposed approach outperforms previous state - of - the - art by a significant margin , particularly at high IoU thresholds . Our model receives extra supervision in its part labels , but the fact that our network can implicitly infer relationships between different parts whilst training may help it handle occluding instances better than other approaches , leading to better instance segmentation performance . The fact that our network is trained with part - level annotations may also help it identify small features of humans better , leading to more precise segmentations and thus improvements at high AP r thresholds . Our AP r at each IoU threshold for human instance segmentation is higher than that for part instance segmentation ( Tab . 1 ) . This is because parts are smaller than entire humans , and thus more difficult to localise accurately . An alternate method of performing instance - level part segmentation may be to first obtain an instance - level human segmentation using another method from Tab . 2 , and then partition it into the various body parts of a human . However , our approach , which groups parts into instances , is validated by the fact that it achieves state - of - the - art instance - level human segmentation performance . section : Results on Category - level Part Segmentation Finally , our model is also able to produce category - level segmentations ( as shown in Fig . 1b ) . This can be obtained from the output of the category - level segmentation module , or from our instance module as described in Sec . 3.4 . As shown in Tab . 3 , our semantic segmentation results are competitive with current state - of - the - art . By training our entire network consisting of the category - level and instance - level segmentation modules jointly , and then obtaining the semantic segmentation from the final instance segmentation output by our network , we are able to obtain a small improvement of 0.4 % in mean IoU over the output of the initial semantic segmentation module . section : Conclusion Our proposed , end - to - end trained network outputs instance - level body part and human segmentations , as well as category - level part segmentations in a single forward - pass . Moreover , section : Input Semantic Segmentation Instance Segmentation Ground Truth Figure 4 : Some results of our system . The first column shows the input image and the input detections we obtained from training the R - FCN detector [ reference ] . The second and third columns show our final semantic segmentation ( Sec . 3.4 ) and instance - level part segmentation . First row : our network can deal with poor bounding box localisation , as it manages to segment the third person from the left although the bounding box only partially covers her . Second row : our method is robust against false positive detections because of the box term . Observe that the bowl of the rightmost person in the bottom row is falsely detected as a person , but rejected in the final prediction . Following rows : we are able to handle overlapping bounding boxes by reasoning globally using the Instance CRF . we have shown how segmenting objects into their constituent parts helps us segment the object as a whole with our state - of - the - art results on instance - level segmentation of both body parts and entire humans . Furthermore , our category - level segmentations improve after training for instance - level segmentation . Our future work is to train the object detector end - to - end as well . Moreover , the improvement that we obtained in instance segmentation of humans as a result of first segmenting parts motivates us to explore weakly - supervised methods which do not require explicit object part annotations . In our main paper , we reported our AP r results averaged over all classes . Fig . 5 visualises the perclass results of our best model at different IoU thresholds . Fig . 6 displays the success cases of our method , while Fig . 7 shows examples of failure cases . Furthermore , we illustrate the strengths and weaknesses of our part instance segmentation method in comparison to MNC [ reference ] in Fig . 8 , and compare our instance - level human segmentation results , which we obtain by the simple mapping described in Sec . 3.4 of our main paper , to MNC in Fig . 9 . Finally , we attach an additional video . We run our system offline , on a frame - by - frame basis on the entire music video , and show how our method is able to accurately parse humans at both category and instance level on internet data outside the Pascal dataset . Instance - level segmentation of videos requires data association . We use a simple , greedy method which operates on a frame - by - frame basis . Segments from one frame are associated to segments in the next frame based on the IoU , using the same method we use for our loss function as described in Sec . 3.3 of the main paper . It shows that our method achieves best instance accuracy for the head category , and finds lower arms and lower legs most challenging to segment correctly . This is likely because of the thin shape of the lower limbs which is known to pose difficulty for semantic segmentation . section : Input Semantic Segmentation Instance Segmentation Ground Truth Figure 6 : Success cases of our method . The first column shows the input image and the input detections we obtained from training the R - FCN detector [ reference ] . The second column shows our final semantic segmentation ( as described in Sec . 3.4 of the main paper ) . Our proposed method is able to leverage an initial category - level segmentation network and human detections to produce accurate instance - level part segmentation as shown in the third column . First row : unlike MNC which predicts for each part instance independently , our method reasons globally and jointly . As a result , MNC predicts two instances of lower legs for the same lower leg of the second and third person from the left . Furthermore , with a dedicated category - level segmentation module , we are less prone to false negatives , whereas MNC misses the legs of the rightmost person , and the lower arm of the second person from the right . Second row : while we can handle poor bounding box localisation because of our global potential term , MNC is unable to segment regions outside the bounding boxes it generates . Consequently , only one lower arm of the person on the left is segmented as the other one is outside the bounding box . The square corners of the segmented lower arm correspond to the limits imposed by the bounding box which MNC internally uses ( box generation is the first stage of the cascade [ reference ] ) . Third row : By analysing an image globally and employing a differentiable CRF , our method can produce more precise boundaries . As MNC does not perform category - level segmentation over the entire image , it has no incentive to produce a coherent and continuous prediction . Visually , this is reflected in the gaps of \" background \" between body parts of the same person . Fourth row : MNC predicts two instances of lower leg for the second person from the right , and fails to segment any lower arms for all four people due to the aforementioned problems . [ reference ] using the default parameters and extract only its human instance predictions . In contrast with proposal - driven methods such as MNC , our approach assigns each pixel to only one instance , is robust against non - ideal bounding boxes , and often produces better boundaries due to the Instance CRF which is trained endto - end . First and second row : since MNC predicts instances independently , it is prone to predicting multiple instances for a single person . Third row : due to the global potential term , we can segment regions outside of a detection bounding box which fails to cover the entire person , whereas MNC is unable to recover from such imperfect bounding boxes , leading to its frequent occurrences of truncated instance predictions . Fourth row : a case where MNC and our method show different failure modes . MNC predicts three people where there are only two , and our method can only predict one instance due to a missing detection . MNC is unable to recover from a false positive detection and predicts two people . Second row : while both MNC and our method start off with poor bounding box localisation that does not cover the whole instance , we are able to segment the entire person , whereas MNC is bounded by its flawed region proposal . Third row : MNC performs better in this case as it is able to segment the infant , whereas we miss her completely due to a false negative person detection . section : B Additional information We detail our initial category - level segmentation module and compare it to DeepLab - v2 [ reference ] in Sec . B.1 , present our network training details in Sec . B.2 , and finally describe how we train the MNC model which serves as our baseline in Sec . B.3 . section : B.1 Details of the category - level segmentation module As shown in Fig 10b , the structure of our category - level segmentation module consists of a ResNet - 101 backbone , and a classifier that extracts multi - scale features from the ResNet - 101 output by using average pooling with different kernel sizes . While our category - level segmentation module and the Deeplab - v2 network ( Fig . 10a ) of Chen et al . [ reference ] both attempt to exploit multi - scale information in the image , the approach of [ reference ] entails executing three forward passes for each image , whereas we only need a single forward pass . In comparison to Deeplab - v2 , our network saves both memory and time , and achieves better performance . To carry out a single forward pass , our network uses 4.3 GB of memory while Deeplab - v2 [ reference ] needs 9.5 GB , 120 % more than ours . Speed - wise , our network runs forward passes at 0.255 seconds per image ( 3.9 fps ) , whereas Deeplab - v2 takes 55 % longer , at 0.396 seconds per image ( 2.5 fps ) on average . When Deeplab - v2 adds a CRF with 10 mean - field iterations to post - process the network output , it gains a small improvement in mean IoU by 0.54 % [ reference ] , but it requires 11.2 GB of memory to make a forward pass ( 140 % of the total amount used by our full network including the instance - level segmentation module ) , and takes 0.960 seconds per image ( 1.0 fps ) , almost a quater of our frame rate . Tests are done on a single GeForce GTX Titan X ( Maxwell ) card . Overall , we are able to achieve better segmentation accuracy ( as shown in Tab . 3 of our main paper ) and is more memory - and time - efficient than Deeplab - v2 . section : B.2 Training our proposed network B.2.1 Training the category - level segmentation module We initialise our semantic segmentation network with the COCO pre - trained ResNet - 101 weights provided by [ reference ] . Training is first performed on the Pascal VOC 2012 training set using the extra annotations from [ reference ] , which combine to a total of 9012 training images . Care is taken to ensure that all images from the Pascal Person - Parts test set is excluded from this training set . A polynomial learning rate policy is adopted such that the effective learning rate at iteration i is given by l i = l 0 ( 1 \u2212 i i max ) p , where the base learning rate , l 0 , is set to 6.25 \u00d7 10 \u22124 , the total number of iterations , i max , is set to 30k , and the power , p , is set to 0.9 . A batch size of 16 is used . However , due to memory constraints , we simulate this batch size by \" accumulating gradients \" : We carry out 16 forward and backward passes with one image per iteration , and only perform the weight update after completing all 16 passes . We use a momentum of 0.9 and weight decay of 1 \u00d7 10 \u22124 for these experiments . After 30k of iterations are completed , we take the best performing model and finetune on the Pascal Person - Parts training set using the same training scheme as described above . Note that the parameters of the batch normalisation modules are kept unchanged in the whole learning process . Online data - augmentation is performed during training to regularise the model . The training images are randomly mirrored , scaled by a ratio between 0.5 and 2 , rotated by an angle between - 10 and 10 degrees , translated by a random amount in the HSV colour space , and blurred with a randomly - sized Gaussian kernel , all on - the - fly . We observe that these techniques are effective at reducing the accuracy gap between training and testing , leading to overall higher test accuracies . [ reference ] and our network structure . The numbers following the layer type denote the kernel size and number of filters . For pooling layers , only their kernel sizes are shown as the number of filters is not applicable . The upsampling ratios can be inferred from the context . Fig . 10a : in the Deeplab - v2 architecture , a 513\u00d7513\u00d73 input image is downsampled by two different ratios ( 0.75 and 0.5 ) to produce multi - scale input at three different resolutions . The three resolutions are independently processed by a ResNet - 101 - based network using shared weights ( shown by the individually coloured paths ) . The output feature maps are then upsampled where appropriate , combined by taking the elementwise maximum , and finally upsampled back to 513\u00d7513 . Fig . 10b : the category - level segmentation module proposed in this paper forwards an input image of size 521\u00d7521\u00d73 through a ResNet - 101 - based CNN , producing a feature map of resolution 66\u00d766\u00d72048 . This feature map is average - pooled with four different kernel sizes , giving us four feature maps with spatial resolutions 1\u00d71 , 2\u00d72 , 3\u00d73 , and 6\u00d76 respectively . Each feature map undergoes convolution and upsampling , before being concatenated together with each other and the 66\u00d766\u00d72048 ResNet - 101 output . This is followed by a convolution layer that reduces the dimension of the concatenated features to 512 , and a convolutional classifier that maps the 512 channels to the size of label space in the dataset . Finally , the prediction is upsampled back to 521\u00d7521 . In both Fig . 10a and 10b , the ResNet - 101 backbone uses dilated convolution such that its output at res5c is at 1 / 8 of the input resolution , instead of 1 / 32 for the original ResNet - 101 [ reference ] . The convolutional classifiers ( coloured in purple ) output C channels , corresponding to the number of classes in the dataset including a background class . For the Pascal Person - Parts Dataset , C is 7 . Best viewed in colour . section : B.2.2 Training the instance - level segmentation module In our model , the pairwise term of the fully - connected CRF takes the following form : where \u00b5 ( \u00b7 , \u00b7 ) is a compatibility function , k ( \u00b7 , \u00b7 ) is a kernel function , and f i is a feature vector at spatial location i containing the 3 - dimensional colour vector I i and the 2 - dimensional position vector p i [ reference ] . We further define the kernel as follows : where w [ reference ] and w [ reference ] are the linear combination weights for the bilateral term and the Gaussian term respectively . In order to determine the initial values for the parameters in the Instance CRF to train from , we carry out a random search . According to the search results , the best prediction accuracy is obtained by initialising w ( 1 ) = 8 , w ( 2 ) = 2 , \u03b8 \u03b1 = 2 , \u03b8 \u03b2 = 8 , \u03b8 \u03b3 = 2 . Furthermore , we use a fixed learning rate of 1 \u00d7 10 \u22126 , momentum of 0.9 , and weight decay of 1 \u00d7 10 \u22124 for training both the instance - level and category - level segmentation modules jointly . Although we previously use the polynomial learning rate policy , we find that for training the instance - level segmentation module , a fixed learning rate leads to better results . Furthermore , our experiments show that a batch size of one works best at this training stage . Using this scheme , we train for 175k iterations , or approximately 100 epochs . section : B.3 Training Multi - task Network Cascades ( MNC ) We use the publicly available Multi - task Network Cascades ( MNC ) framework [ reference ] , and train a new model for instance - level part segmentation using the Pascal Person - Parts dataset . The weights are initialised with the officially released MNC model 1 which has been trained on Pascal VOC 2011 / SBD [ reference ] . The base learning rate is set to 1 \u00d7 10 \u22123 , which is reduced by 10 times after 20k iterations . A total of 25k training iterations are carried out . A batch size of 8 , momentum of 0.9 and weight decay of 5 \u00d7 10 \u22124 are used . These settings are identical to the ones used in training the original MNC and provided in their public source code . Using these settings , we are also able to reproduce the experimental results obtained in the original MNC paper [ reference ] , and hence we believe that the MNC model we have trained acts as a strong baseline for our proposed approach . section : section : section : section : Appendix In this appendix , we present additional results of our proposed approach in Sec . A , and provide additional training and implementation details in Sec . B ( both for our model , and the strong MNC baseline [ reference ] ) . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["PASCAL-Person-Part"]]], "Method": [[["Holistic_instance-level"]]], "Metric": [[["AP_0_5"]]], "Task": [[["Multi-Human_Parsing"]]]}]}
{"docid": "TST3-SREX-0060", "doctext": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training . In this paper we present an approach for training a convolutional neural network using only unlabeled data . We train the network to discriminate between a set of surrogate classes . Each surrogate class is formed by applying a variety of transformations to a randomly sampled \u2019 seed \u2019 image patch . We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition . The feature representation learned by our algorithm achieves classification results matching or outperforming the current state - of - the - art for unsupervised learning on several popular datasets ( STL - 10 , CIFAR - 10 , Caltech - 101 ) . 1 Introduction Convolutional neural networks ( CNNs ) trained via backpropagation were recently shown to perform well on image classification tasks with millions of training images and thousands of categories [ 1 , 2 ] . The feature representation learned by these networks achieves state - of - the - art performance not only on the classification task for which the network was trained , but also on various other visual recognition tasks , for example : classification on Caltech - 101 [ 2 , 3 ] , Caltech - 256 [ 2 ] and the CaltechUCSD birds dataset [ 3 ] ; scene recognition on the SUN - 397 database [ 3 ] ; detection on the PASCAL VOC dataset [ 4 ] . This capability to generalize to new datasets makes supervised CNN training an attractive approach for generic visual feature learning . The downside of supervised training is the need for expensive labeling , as the amount of required labeled samples grows quickly the larger the model gets . The large performance increase achieved by methods based on the work of Krizhevsky et al . [ 1 ] was , for example , only possible due to massive efforts on manually annotating millions of images . For this reason , unsupervised learning \u2013 although currently underperforming \u2013 remains an appealing paradigm , since it can make use of raw unlabeled images and videos . Furthermore , on vision tasks outside classification it is not even certain whether training based on object class labels is advantageous . For example , unsupervised feature learning is known to be beneficial for image restoration [ 5 ] and recent results show that it outperforms supervised feature learning also on descriptor matching [ 6 ] . In this work we combine the power of a discriminative objective with the major advantage of unsupervised feature learning : cheap data acquisition . We introduce a novel training procedure for convolutional neural networks that does not require any labeled data . It rather relies on an automatically generated surrogate task . The task is created by taking the idea of data augmentation \u2013 which is commonly used in supervised learning \u2013 to the extreme . Starting with trivial surrogate classes consisting of one random image patch each , we augment the data by applying a random set of transformations to each patch . Then we train a CNN to classify these surrogate classes . We refer to this method as exemplar training of convolutional neural networks ( Exemplar - CNN ) . The feature representation learned by Exemplar - CNN is , by construction , discriminative and invariant to typical transformations . We confirm this both theoretically and empirically , showing that this approach matches or outperforms all previous unsupervised feature learning methods on the standard image classification benchmarks STL - 10 , CIFAR - 10 , and Caltech - 101 . 1.1 Related Work Our approach is related to a large body of work on unsupervised learning of invariant features and training of convolutional neural networks . Convolutional training is commonly used in both supervised and unsupervised methods to utilize the invariance of image statistics to translations ( e.g. LeCun et al . [ 7 ] , Kavukcuoglu et al . [ 8 ] , Krizhevsky et al . [ 1 ] ) . Similar to our approach the current surge of successful methods employing convolutional neural networks for object recognition often rely on data augmentation to generate additional training samples for their classification objective ( e.g. Krizhevsky et al . [ 1 ] , Zeiler and Fergus [ 2 ] ) . While we share the architecture ( a convolutional neural network ) with these approaches , our method does not rely on any labeled training data . In unsupervised learning , several studies on learning invariant representations exist . Denoising autoencoders [ 9 ] , for example , learn features that are robust to noise by trying to reconstruct data from randomly perturbed input samples . Zou et al . [ 10 ] learn invariant features from video by enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder . Sohn and Lee [ 11 ] and Hui [ 12 ] learn features invariant to local image transformations . In contrast to our discriminative approach , all these methods rely on directly modeling the input distribution and are typically hard to use for jointly training multiple layers of a CNN . The idea of learning features that are invariant to transformations has also been explored for supervised training of neural networks . The research most similar to ours is early work on tangent propagation [ 13 ] ( and the related double backpropagation [ 14 ] ) which aims to learn invariance to small predefined transformations in a neural network by directly penalizing the derivative of the output with respect to the magnitude of the transformations . In contrast , our algorithm does not regularize the derivative explicitly . Thus it is less sensitive to the magnitude of the applied transformation . This work is also loosely related to the use of unlabeled data for regularizing supervised algorithms , for example self - training [ 15 ] or entropy regularization [ 16 ] . In contrast to these semi - supervised methods , Exemplar - CNN training does not require any labeled data . Finally , the idea of creating an auxiliary task in order to learn a good data representation was used by Ahmed et al . [ 17 ] , Collobert et al . [ 18 ] . 2 Creating Surrogate Training Data The input to the training procedure is a set of unlabeled images , which come from roughly the same distribution as the images to which we later aim to apply the learned features . We randomly sample N \u2208 [ 50 , 32000 ] patches of size 32\u00d732 pixels from different images at varying positions and scales forming the initial training set X = { x1 , . . .xN}. We are interested in patches containing objects or parts of objects , hence we sample only from regions containing considerable gradients . We define a family of transformations { T\u03b1|\u03b1 \u2208 A } parameterized by vectors \u03b1 \u2208 A , where A is the set of all possible parameter vectors . Each transformation T\u03b1 is a composition of elementary transformations from the following list : \u2022 translation : vertical or horizontal translation by a distance within 0.2 of the patch size ; \u2022 scaling : multiplication of the patch scale by a factor between 0.7 and 1.4 ; \u2022 rotation : rotation of the image by an angle up to 20 degrees ; \u2022 contrast 1 : multiply the projection of each patch pixel onto the principal components of the set of all pixels by a factor between 0.5 and 2 ( factors are independent for each principal component and the same for all pixels within a patch ) ; \u2022 contrast 2 : raise saturation and value ( S and V components of the HSV color representation ) of all pixels to a power between 0.25 and 4 ( same for all pixels within a patch ) , multiply these values by a factor between 0.7 and 1.4 , add to them a value between \u22120.1 and 0.1 ; \u2022 color : add a value between \u22120.1 and 0.1 to the hue ( H component of the HSV color representation ) of all pixels in the patch ( the same value is used for all pixels within a patch ) . All numerical parameters of elementary transformations , when concatenated together , form a single parameter vector \u03b1 . For each initial patch xi \u2208 X we sample K \u2208 [ 1 , 300 ] random parameter vectors { \u03b11i , . . . , \u03b1Ki } and apply the corresponding transformations Ti = { T\u03b11i , . . . , T\u03b1Ki } to the patch xi . This yields the set of its transformed versions Sxi = Tixi = { Txi|T \u2208 Ti}. Afterwards we subtract the mean of each pixel over the whole resulting dataset . We do not apply any other preprocessing . Exemplary patches sampled from the STL - 10 unlabeled dataset are shown in Fig . 1 . Examples of transformed versions of one patch are shown in Fig . 2 . 3 Learning Algorithm Given the sets of transformed image patches , we declare each of these sets to be a class by assigning label i to the class Sxi . We next train a CNN to discriminate between these surrogate classes . Formally , we minimize the following loss function : L ( X ) = \u2211 xi\u2208X \u2211 T\u2208Ti l ( i , Txi ) , ( 1 ) where l ( i , Txi ) is the loss on the transformed sample Txi with ( surrogate ) true label i. We use a CNN with a softmax output layer and optimize the multinomial negative log likelihood of the network output , hence in our case l ( i , Txi ) = M ( ei , f ( Txi ) ) , M ( y , f ) = \u2212\u3008y , log f \u3009 = \u2212 \u2211 k yk log fk , ( 2 ) where f ( \u00b7 ) denotes the function computing the values of the output layer of the CNN given the input data , and ei is the ith standard basis vector . We note that in the limit of an infinite number of transformations per surrogate class , the objective function ( 1 ) takes the form L\u0302 ( X ) = \u2211 xi\u2208X E\u03b1 [ l ( i , T\u03b1xi ) ] , ( 3 ) which we shall analyze in the next section . Intuitively , the classification problem described above serves to ensure that different input samples can be distinguished . At the same time , it enforces invariance to the specified transformations . In the following sections we provide a foundation for this intuition . We first present a formal analysis of the objective , separating it into a well defined classification problem and a regularizer that enforces invariance ( resembling the analysis in Wager et al . [ 19 ] ) . We then discuss the derived properties of this classification problem and compare it to common practices for unsupervised feature learning . 3.1 Formal Analysis We denote by \u03b1 \u2208 A the random vector of transformation parameters , by g ( x ) the vector of activations of the second - to - last layer of the network when presented the input patch x , by W the matrix of the weights of the last network layer , by h ( x ) = Wg ( x ) the last layer activations before applying the softmax , and by f ( x ) = softmax ( h ( x ) ) the output of the network . By plugging in the definition of the softmax activation function softmax ( z ) = exp ( z )/ \u2016 exp ( z ) \u20161 ( 4 ) the objective function ( 3 ) with loss ( 2 ) takes the form\u2211 xi\u2208X E\u03b1 [ \u2212\u3008ei , h ( T\u03b1xi ) \u3009 + log \u2016 exp ( h ( T\u03b1xi )) \u20161 ] . ( 5 ) With g\u0302i = E\u03b1 [ g ( T\u03b1xi ) ] being the average feature representation of transformed versions of the image patch xi we can rewrite Eq . ( 5 ) as\u2211 xi\u2208X [ \u2212\u3008ei , Wg\u0302i\u3009 + log \u2016 exp ( Wg\u0302i ) \u20161 ] + \u2211 xi\u2208X [ E\u03b1 [ log \u2016 exp ( h ( T\u03b1xi )) \u20161 ] \u2212 log \u2016 exp ( Wg\u0302i ) \u20161 ] . ( 6 ) The first sum is the objective function of a multinomial logistic regression problem with input - target pairs ( g\u0302i , ei ) . This objective falls back to the transformation - free instance classification problem L ( X ) = \u2211 xi\u2208X l ( i , xi ) if g ( xi ) = E\u03b1 [ g ( T\u03b1x )] . In general , this equality does not hold and thus the first sum enforces correct classification of the average representation E\u03b1 [ g ( T\u03b1xi ) ] for a given input sample . For a truly invariant representation , however , the equality is achieved . Similarly , if we suppose that T\u03b1x = x for \u03b1 = 0 , that for small values of \u03b1 the feature representation g ( T\u03b1xi ) is approximately linear with respect to \u03b1 and that the random variable \u03b1 is centered , i.e. E\u03b1 [ \u03b1 ] = 0 , then g\u0302i = E\u03b1 [ g ( T\u03b1xi ) ] \u2248 E\u03b1 [ g ( xi ) + \u2207\u03b1 ( g ( T\u03b1xi )) |\u03b1=0 \u03b1 ] = g ( xi ) . The second sum in Eq . ( 6 ) can be seen as a regularizer enforcing all h ( T\u03b1xi ) to be close to their average value , i.e. , the feature representation is sought to be approximately invariant to the transformations T\u03b1 . To show this we use the convexity of the function log \u2016 exp ( \u00b7 ) \u20161 and Jensen \u2019s inequality , which yields ( proof in supplementary material ) E\u03b1 [ log \u2016 exp ( h ( T\u03b1xi )) \u20161 ] \u2212 log \u2016 exp ( Wg\u0302i ) \u20161 \u2265 0 . ( 7 ) If the feature representation is perfectly invariant , then h ( T\u03b1xi ) = Wg\u0302i and inequality ( 7 ) turns to equality , meaning that the regularizer reaches its global minimum . 3.2 Conceptual Comparison to Previous Unsupervised Learning Methods Suppose we want to unsupervisedly learn a feature representation useful for a recognition task , for example classification . The mapping from input images x to a feature representation g ( x ) should then satisfy two requirements : ( 1 ) there must be at least one feature that is similar for images of the same category y ( invariance ) ; ( 2 ) there must be at least one feature that is sufficiently different for images of different categories ( ability to discriminate ) . Most unsupervised feature learning methods aim to learn such a representation by modeling the input distribution p ( x ) . This is based on the assumption that a good model of p ( x ) contains information about the category distribution p ( y|x ) . That is , if a representation is learned , from which a given sample can be reconstructed perfectly , then the representation is expected to also encode information about the category of the sample ( ability to discriminate ) . Additionally , the learned representation should be invariant to variations in the samples that are irrelevant for the classification task , i.e. , it should adhere to the manifold hypothesis ( see e.g. Rifai et al . [ 20 ] for a recent discussion ) . Invariance is classically achieved by regularization of the latent representation , e.g. , by enforcing sparsity [ 8 ] or robustness to noise [ 9 ] . In contrast , the discriminative objective in Eq . ( 1 ) does not directly model the input distribution p ( x ) but learns a representation that discriminates between input samples . The representation is not required to reconstruct the input , which is unnecessary in a recognition or matching task . This leaves more degrees of freedom to model the desired variability of a sample . As shown in our analysis ( see Eq . ( 7 ) ) , we achieve partial invariance to transformations applied during surrogate data creation by forcing the representation g ( T\u03b1xi ) of the transformed image patch to be predictive of the surrogate label assigned to the original image patch xi . It should be noted that this approach assumes that the transformations T\u03b1 do not change the identity of the image content . If we , for example , use a color transformation we will force the network to be invariant to this change and can not expect the extracted features to perform well in a task relying on color information ( such as differentiating black panthers from pumas ) 1 . 4 Experiments To compare our discriminative approach to previous unsupervised feature learning methods , we report classification results on the STL - 10 [ 21 ] , CIFAR - 10 [ 22 ] and Caltech - 101 [ 23 ] datasets . Moreover , we assess the influence of the augmentation parameters on the classification performance and study the invariance properties of the network . 4.1 Experimental Setup The datasets we test on differ in the number of classes ( 10 for CIFAR and STL , 101 for Caltech ) and the number of samples per class . STL is especially well suited for unsupervised learning as it contains a large set of 100 , 000 unlabeled samples . In all experiments ( except for the dataset transfer experiment in the supplementary material ) we extracted surrogate training data from the unlabeled subset of STL - 10 . When testing on CIFAR - 10 , we resized the images from 32\u00d732 pixels to 64\u00d764 pixels so that the scale of depicted objects roughly matches the two other datasets . We worked with two network architectures . A \u201c small \u201d network was used to evaluate the influence of different components of the augmentation procedure on classification performance . It consists of two convolutional layers with 64 filters each followed by a fully connected layer with 128 neurons . This last layer is succeeded by a softmax layer , which serves as the network output . A \u201c large \u201d network , consisting of three convolutional layers with 64 , 128 and 256 filters respectively followed by a fully connected layer with 512 neurons , was trained to compare our method to the state - of - theart . In both models all convolutional filters are connected to a 5\u00d75 region of their input . 2\u00d72 maxpooling was performed after the first and second convolutional layers . Dropout [ 24 ] was applied to the fully connected layers . We trained the networks using an implementation based on Caffe [ 25 ] . Details on the training , the hyperparameter settings , and an analysis of the performance depending on the network architecture is provided in the supplementary material . Our code and training data are available at http: // lmb.informatik.uni - freiburg.de / resources . We applied the feature representation to images of arbitrary size by convolutionally computing the responses of all the network layers except the top softmax . To each feature map , we applied the pooling method that is commonly used for the respective dataset : 1 ) 4 - quadrant max - pooling , resulting in 4 values per feature map , which is the standard procedure for STL - 10 and CIFAR - 10 [ 26 , 10 , 27 , 12 ] ; 2 ) 3 - layer spatial pyramid , i.e. max - pooling over the whole image as well as within 4 quadrants and within the cells of a 4 \u00d7 4 grid , resulting in 1 + 4 + 16 = 21 values per feature map , which is the standard for Caltech - 101 [ 28 , 10 , 29 ] . Finally , we trained a linear support vector machine ( SVM ) on the pooled features . On all datasets we used the standard training and test protocols . On STL - 10 the SVM was trained on 10 pre - defined folds of the training data . We report the mean and standard deviation achieved on the fixed test set . For CIFAR - 10 we report two results : ( 1 ) training the SVM on the whole CIFAR - 10 training set ( \u2019 CIFAR - 10 \u2019 ) ; ( 2 ) the average over 10 random selections of 400 training samples per class ( \u2019 CIFAR - 10 ( 400 ) \u2019 ) . For Caltech - 101 we followed the usual protocol of selecting 30 random samples per class for training and not more than 50 samples per class for testing . This was repeated 10 times . 4.2 Classification Results In Table 1 we compare Exemplar - CNN to several unsupervised feature learning methods , including the current state - of - the - art on each dataset . We also list the state - of - the - art for supervised learning ( which is not directly comparable ) . Additionally we show the dimensionality of the feature vectors 1Such cases could be covered either by careful selection of applied transformations or by combining features from multiple networks trained with different sets of transformations and letting the final classifier choose which features to use . produced by each method before final pooling . The small network was trained on 8000 surrogate classes containing 150 samples each and the large one on 16000 classes with 100 samples each . The features extracted from the larger network match or outperform the best prior result on all datasets . This is despite the fact that the dimensionality of the feature vector is smaller than that of most other approaches and that the networks are trained on the STL - 10 unlabeled dataset ( i.e. they are used in a transfer learning manner when applied to CIFAR - 10 and Caltech 101 ) . The increase in performance is especially pronounced when only few labeled samples are available for training the SVM ( as is the case for all the datasets except full CIFAR - 10 ) . This is in agreement with previous evidence that with increasing feature vector dimensionality and number of labeled samples , training an SVM becomes less dependent on the quality of the features [ 26 , 12 ] . Remarkably , on STL - 10 we achieve an accuracy of 72.8 % , which is a large improvement over all previously reported results . 4.3 Detailed Analysis We performed additional experiments ( using the \u201c small \u201d network ) to study the effect of three design choices in Exemplar - CNN training and validate the invariance properties of the learned features . Experiments on sampling \u2019 seed \u2019 patches from different datasets can be found in the supplementary . 4.3.1 Number of Surrogate Classes We varied the number N of surrogate classes between 50 and 32000 . As a sanity check , we also tried classification with random filters . The results are shown in Fig . 3 . Clearly , the classification accuracy increases with the number of surrogate classes until it reaches an optimum at about 8000 surrogate classes after which it did not change or even decreased . This is to be expected : the larger the number of surrogate classes , the more likely it is to draw very similar or even identical samples , which are hard or impossible to discriminate . Few such cases are not detrimental to the classification performance , but as soon as such collisions dominate the set of surrogate labels , the discriminative loss is no longer reasonable and training the network to the surrogate task no longer succeeds . To check the validity of this explanation we also plot in Fig . 3 the classification error on the validation set ( taken from the surrogate data ) computed after training the network . It rapidly grows as the number of surrogate classes increases . We also observed that the optimal number of surrogate classes increases with the size of the network ( not shown in the figure ) , but eventually saturates . This demonstrates the main limitation of our approach to randomly sample \u2019 seed \u2019 patches : it does not scale to arbitrarily large amounts of unlabeled data . However , we do not see this as a fundamental restriction and discuss possible solutions in Section 5 . 4.3.2 Number of Samples per Surrogate Class Fig . 4 shows the classification accuracy when the number K of training samples per surrogate class varies between 1 and 300 . The performance improves with more samples per surrogate class and 2 On Caltech - 101 one can either measure average accuracy over all samples ( average overall accuracy ) or calculate the accuracy for each class and then average these values ( average per - class accuracy ) . These differ , as some classes contain fewer than 50 test samples . Most researchers in ML use average overall accuracy . saturates at around 100 samples . This indicates that this amount is sufficient to approximate the formal objective from Eq . ( 3 ) , hence further increasing the number of samples does not significantly change the optimization problem . On the other hand , if the number of samples is too small , there is insufficient data to learn the desired invariance properties . 4.3.3 Types of Transformations We varied the transformations used for creating the surrogate data to analyze their influence on the final classification performance . The set of \u2019 seed \u2019 patches was fixed . The result is shown in Fig . 5 . The value \u2019 0 \u2019 corresponds to applying random compositions of all elementary transformations : scaling , rotation , translation , color variation , and contrast variation . Different columns of the plot show the difference in classification accuracy as we discarded some types of elementary transformations . Several tendencies can be observed . First , rotation and scaling have only a minor impact on the performance , while translations , color variations and contrast variations are significantly more important . Secondly , the results on STL - 10 and CIFAR - 10 consistently show that spatial invariance and color - contrast invariance are approximately of equal importance for the classification performance . This indicates that variations in color and contrast , though often neglected , may also improve performance in a supervised learning scenario . Thirdly , on Caltech - 101 color and contrast transformations are much more important compared to spatial transformations than on the two other datasets . This is not surprising , since Caltech - 101 images are often well aligned , and this dataset bias makes spatial invariance less useful . 4.3.4 Invariance Properties of the Learned Representation In a final experiment , we analyzed to which extent the representation learned by the network is invariant to the transformations applied during training . We randomly sampled 500 images from the STL - 10 test set and applied a range of transformations ( translation , rotation , contrast , color ) to each image . To avoid empty regions beyond the image boundaries when applying spatial transformations , we cropped the central 64\u00d764 pixel sub - patch from each 96\u00d796 pixel image . We then applied two measures of invariance to these patches . First , as an explicit measure of invariance , we calculated the normalized Euclidean distance between normalized feature vectors of the original image patch and the transformed one [ 10 ] ( see the supplementary material for details ) . The downside of this approach is that the distance between extracted features does not take into account how informative and discriminative they are . We there - fore evaluated a second measure \u2013 classification performance depending on the magnitude of the transformation applied to the classified patches \u2013 which does not come with this problem . To compute the classification accuracy , we trained an SVM on the central 64 \u00d7 64 pixel patches from one fold of the STL - 10 training set and measured classification performance on all transformed versions of 500 samples from the test set . The results of both experiments are shown in Fig . 6 . Due to space restrictions we show only few representative plots . Overall the experiment empirically confirms that the Exemplar - CNN objective leads to learning invariant features . Features in the third layer and the final pooled feature representation compare favorably to a HOG baseline ( Fig . 6 ( a ) ) . Furthermore , adding stronger transformations in the surrogate training data leads to more invariant classification with respect to these transformations ( Fig . 6 ( b )-( d ) ) . However , adding too much contrast variation may deteriorate classification performance ( Fig . 6 ( d ) ) . One possible reason is that level of contrast can be a useful feature : for example , strong edges in an image are usually more important than weak ones . 5 Discussion We have proposed a discriminative objective for unsupervised feature learning by training a CNN without class labels . The core idea is to generate a set of surrogate labels via data augmentation . The features learned by the network yield a large improvement in classification accuracy compared to features obtained with previous unsupervised methods . These results strongly indicate that a discriminative objective is superior to objectives previously used for unsupervised feature learning . One potential shortcoming of the proposed method is that in its current state it does not scale to arbitrarily large datasets . Two probable reasons for this are that ( 1 ) as the number of surrogate classes grows larger , many of them become similar , which contradicts the discriminative objective , and ( 2 ) the surrogate task we use is relatively simple and does not allow the network to learn invariance to complex variations , such as 3D viewpoint changes or inter - instance variation . We hypothesize that the presented approach could learn more powerful higher - level features , if the surrogate data were more diverse . This could be achieved by using additional weak supervision , for example , by means of video or a small number of labeled samples . Another possible way of obtaining richer surrogate training data and at the same time avoiding similar surrogate classes would be ( unsupervised ) merging of similar surrogate classes . We see these as interesting directions for future work . Acknowledgements We acknowledge funding by the ERC Starting Grant VideoLearn ( 279401 ) ; the work was also partly supported by the BrainLinks - BrainTools Cluster of Excellence funded by the German Research Foundation ( DFG , grant number EXC 1086 ) .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["CIFAR-10"]]], "Method": [[["Discriminative_Unsupervised_Feature_Learning_with_Convolutional_Neural_Networks"]]], "Metric": [[["Percentage_correct"]]], "Task": [[["Image_Classification"]]]}, {"incident_type": "SciREX_incident", "Material": [[["STL-10"]]], "Method": [[["Discriminative_Unsupervised_Feature_Learning_with_Convolutional_Neural_Networks"]]], "Metric": [[["Percentage_correct"]]], "Task": [[["Image_Classification"]]]}]}
{"docid": "TST3-SREX-0061", "doctext": "document : CoupleNet : Coupling Global Structure with Local Parts for Object Detection The region - based Convolutional Neural Network ( CNN ) detectors such as Faster R - CNN or R - FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together . Although R - FCN has achieved higher detection speed while keeping the detection performance , the global structure information is ignored by the position - sensitive score maps . To fully explore the local and global properties , in this paper , we propose a novel fully convolutional network , named as CoupleNet , to couple the global structure with local parts for object detection . Specifically , the object proposals obtained by the Region Proposal Network ( RPN ) are fed into the the coupling module which consists of two branches . One branch adopts the position - sensitive RoI ( PSRoI ) pooling to capture the local part information of the object , while the other employs the RoI pooling to encode the global and context information . Next , we design different coupling strategies and normalization ways to make full use of the complementary advantages between the global and local branches . Extensive experiments demonstrate the effectiveness of our approach . We achieve state - of - the - art results on all three challenging datasets , a mAP of on VOC07 , on VOC12 , and on COCO . Codes will be made publicly available . section : Introduction General object detection requires to accurately locate and classify all targets in the image or video . Compared to specific object detection , such as face , pedestrian and vehicle detection , general object detection often faces more challenges due to the large inter - class appearance differences . The variations arise not only from changes in a variety of non - rigid deformations , but also due to the truncations , occlusions and inter - class interference . However , no matter how complicated the objects are , when humans identify a target , the recognition of object categories is subserved by both a global process that retrieves structural information and a local process that is sensitive to individual parts . This motivates us to build a detection model that fused both global and local information . With the revival of Convolutional Neural Networks ( CNN ) , CNN - based object detection pipelines have been proposed consecutively and made impressive improvements in generic benchmarks , PASCAL VOC and MS COCO . As two representative region - based CNN approaches , Fast / Faster R - CNN uses a certain subnetwork to predict the category of each region proposal while R - FCN conducts the inference with the position - sensitive score maps . Through removing the RoI - wise subnetwork , R - FCN has achieved higher detection speed while keeping the detection performance . However , the global structure information is ignored by the PSRoI pooling . As shown in Figure [ reference ] , using PSRoI pooling to extract local part information for final object category prediction , R - FCN leads to a low confidence score of 0.08 for the sofa detection since the local responses of sofa are disturbed by a women and a dog ( they are also the categories that need to be detected ) . Conversely , the global structure of sofa could be extracted by the RoI pooling , but the confidence score is 0.45 , which is also very low for the incomplete structure of sofa . By coupling the global confidence with the local part confidence together , we can obtain a more reliable prediction with the confidence score of 0.78 . In fact , the idea of fusing global and local information together is widely used in lots of visual tasks . In fingerprint recognition , Gu combined the global orientation field and local minutiae cue to largely improve the performance . In clique - graph matching , Nie proposed a clique - graph matching method by preserving global clique - to - clique correspondence and local unary and pairwise correspondences . In scene parsing , Zhao designed a pyramid pooling module to effectively extract hierarchical global contextual prior , and then concatenated it with the local FCN feature to improve the performance . In traditional object detection , Felzenszwalb incorporated a global root model and several finer local part models to represent highly variable objects . All of which show that effective combination of the global structural properties and local fine - grained details can achieve complementary advantages . Therefore , to fully explore the global and local clues , in this paper , we propose a novel full convolutional network named as CoupleNet , to couple the global structure and local parts to boost the detection accuracy . Specifically , the object proposals obtained by the RPN are fed into the coupling module which consists of two branches . One branch adopts the PSRoI pooling to capture the local part information of the object , while the other employs the RoI pooling to encode the global and context information . Moreover , we design different coupling strategies and normalization ways to make full use of the complementary advantages between the global and local branches . With the coupling structure , our network can jointly learn the local , global and context expression of the objects , which makes the model have a more powerful representation capacity and generalization ability . Extensive experiments demonstrate that CoupleNet can significantly improve the detection performance . Our detector shows competitive results on PASCAL VOC 07 / 12 and MS COCO compared to other state - of - the - art detectors , even with model ensemble approaches . In summary , our main contributions are as follows : 1 . We propose a unified fully convolutional network to jointly learn the local , global and context information for object detection . 2 . We design different normalization methods and coupling strategies to mine the compatibility and complementarity between the global and local branches . 3 . We achieve the state - of - the - art results on all three challenging datasets , a mAP of on VOC07 , on VOC12 , and on MS COCO . section : Related work Before the arrival of CNN , visual tasks have been dominated by traditional paradigms . As one of an outstanding framework , DPM described the object system using mixtures of multi - scale deformable part models , including a coarse global root model and several finer local part models . The root model extracts structural information of the objects , while the part models capture local appearance properties of an object . The sum of root response and weighted average response of each part is used as the final confidence of an object . Although DPM provides an elegant framework for object detection , the hand - crafted features , improved HOG , are not discriminative enough to express the diversity of object categories . This is also the main reason that CNN completely surpassed the traditional methods in a short period time . In order to leverage the great success of deep neural networks for image classification , considerable object detection methods based on deep learning have been proposed . Although there are end - to - end detection frameworks , like SSD , YOLO and DenseBox , region - based systems ( Fast / Faster R - CNN and R - FCN ) still dominate the detection accuracy on generic benchmarks . Compared to the end - to - end framework , the region - based systems have several advantages . Firstly , by exploiting a divide - and - conquer strategy , the two - step framework is more stable and easier to converge . Secondly , without the complicated data augmentation and training skills , you can still easily achieve state - of - the - art performance . The main reason for these advantages is that there is a certain structure to encode translation variance features for each proposal , since in deep networks , higher - layers contain more semantic meaning and less location information . As a consequence , a RoI - wise subnetwork or a position - sensitive RoI pooling layer is used to achieve the translation variance in region - based systems . However , all the existing region - based systems utilize either the region - level or part - level features to learn the variations , where each one alone is not representative enough for a variety of challenging situations . Therefore , this motivates us to design a certain structure to take advantages of both the global and local features . In addition , context is known to play an important role in visual recognition . Considerable works have been proposed for exploting context in object detection . Bell explored the use of recurrent neural networks to model the contextual information . Gidaris proposed to utilize multiple contextual regions around the object . Cai collected the context by padding the proposals for pedestrian and car detection . Similar to these works , we also absorb the context prior to enhance the global feature representation . section : CoupleNet In this section , we first introduce the architecture of the proposed CoupleNet for object detection . Then we explain in detail how we incorporate local representations , global appearance and contextual information for robust object detection . subsection : Network architecture The architecture of our proposed CoupleNet is illustrated in Figure [ reference ] . Our CoupleNet includes two different branches : a ) a local part - sensitive fully convolutional network to learn the object - specific parts , denoted as local FCN ; b ) a global region - sensitive fully convolutional network to encode the whole appearance structure and context prior of the object , denoted as global FCN . We first use the ImageNet pre - trained ResNet - 101 released in to initialize our network . For our detection task , we remove the last average pooling layer and the fc layer . Given an input image , we extract candidate proposals by using the Region Proposal Network ( RPN ) , which also shares convolution features with CoupleNet following . Then each proposal flows to two different branches : the local FCN and the global FCN . Finally , the output of global and local FCN are coupled together as the final score of the object . We also perform class - agnostic bounding box regression in a similar way . subsection : Local FCN To effectively capture the specific fine - grained parts in local FCN , we construct a set of part - sensitive score maps by appending a 1x1 convolutional layer with channels , where means we divide the object into local parts ( here is set to the default value 7 ) and is the number of object categories plus background . For each category , there are totally channels and each channel is responsible for encoding a specific part of the object . The final score of a category is determined by voting the responses . Here we use position - sensitive RoI pooling layer in to extract object - specific parts and we simply perform average pooling for voting . Then , we obtain a - d vector which indicates the probability that the object belongs to each class . This procedure is equivalent to dividing a strong object category decision into the sum of multiple weak classifiers , which serves as the ensemble of several part models . Here we refer this part ensemble as local structure representation . As shown in Figure [ reference ] ( a ) , for the truncated person , one can hardly get a strong response from the global description of the person due to truncation , on the contrary , our local FCN can effectively capture several specific parts , such as human nose , mouth , , which correspond to the regions with large responses in the feature map . We argue that the local FCN is much concerned with the internal structure and components , which can effectively reflect the local properties of visual object , especially when the object is occluded or the whole boundary is incomplete . However , for those having simple spatial structure and encompassing considerable background in the bounding box , dining table , the local FCN alone is difficult to make robust predictions . Thus it is necessary to add the global structure information to enhance the discrimination . subsection : Global FCN For the global FCN , we aim to describe the object by using the whole region - level features . Firstly , we attach a 1024 - d 1x1 convolutional layer after the last convolutional block in ResNet - 101 for reducing the dimension . Due to the diverse size of the object , we insert a RoI pooling layer in to extract a fixed - length feature vector as the global structure description of the object . Secondly , we use two convolutional layers with kernal size and respectively ( is set to the default value 7 ) to further abstract the global representation of RoI. Finally , the output of 1x1 convolution is fed into the classifier whose output is also a - d vector . In addition , context prior is the most basic and important factor for visual recognition tasks . For example , the boat usually travels in the water while is unlikely to fly in the sky . Despite the higher layers in deep neural network can involve the spatial context information around the objects due to the large receptive field , Zhou have shown that the practical receptive field is actually much smaller than the theoretical one . Therefore , it is necessary to explicitly collect the surrounding information to reduce the chance of misclassification . To enhance the feature representation ability of the global FCN , here we introduce the contextual information as an effective supplement . Specifically , we extend the context region by 2 times larger than the size of original proposal . Then the features RoI pooled from the original region and context region are concatenated together and fed into the latter RoI - wise subnetwork . As shown in Figure [ reference ] , the context region is embedded into the global branch to extract a more complete appearance structure and discriminative prior representation , which will help the classifier to better identity the object categories . Due to the RoI pooling operation , the global FCN describes the proposal as a whole with CNN features , which can be seen as a global structure description of the object . Therefore , it can easily deal with the objects with intact structure and finer scale . As shown in Figure [ reference ] ( b ) , our global FCN shows a large confidence for the dining table . However , in most cases , natural scenes consist of considerable objects with occlusions or truncations , making the detection more difficult . Figure [ reference ] ( a ) shows that using the global structure information alone can hardly make a confident prediction for the truncated person . By adding local part structural supports , the detection performance can be significantly boosted . Therefore , it is essential to combine both local and global descriptions for a robust detection . subsection : Coupling structure To match the same order of magnitude , we apply a normalization operation to the output of local and global FCN before they are combined together . We explored two different methods to perform normalization : an L2 normalization layer or a 1x1 convolutional layer to model the scale . Meanwhile , how to couple the local and global output is also a problem that needs to be researched . Here , we investigated three different coupling methods : element - wise sum , element - wise product and element - wise maximum . Our experiments show that using 1x1 convolution along with element - wise sum achieves the best performance and we will discuss it in Section [ reference ] . With the coupling structure , CoupleNet simultaneously exploits the local parts , global structure and context prior for object detection . The whole network is fully convolutional and benefits from approximate joint training and multi - task learning . We also note that the global branch can be regarded as a lightweight Faster R - CNN , in which all learnable parameters are from convolutional layers and the depth of RoI - wise subnetwork is only two . Therefore , the computational complexity is far less than the subnetwork in ResNet - based Faster R - CNN system whose depth is ten . As a consequence , our CoupleNet can perform the inference efficiently , which runs slightly slower than R - FCN but much more faster than Faster R - CNN . section : Experiments We train and evaluate our method on three challenging object detection datasets : PASCAL VOC2007 , VOC2012 and MS COCO . Since all these three datasets contain a variety of circumstances , which can sufficiently verify the effectiveness of our method . We demonstrate state - of - the - art results on all three datasets without bells and whistles . subsection : Ablation studies on VOC2007 We first perform experiments on PASCAL VOC 2007 with 20 object categories for detailed analysis of our proposed CoupleNet detector . We train the models on the union set of VOC 2007 trainval and VOC 2012 trainval ( \u201c 07 + 12 \u201d ) following , and evaluate on VOC 2007 test set . Object detection accuracy is measured by mean Average Precision ( mAP ) , all the ablation experiments use single - scale training and testing , and we did not add the context prior . Normalization . Since features extracted form different layers of CNN show various of scales , it is essential to normalize different features before coupling them together . Bell proposed to use L2 normalization to each RoI - pooled feature and re - scale back up by a empirical scale , which shows a great gain on VOC dataset . In this paper , we also explore two different normalization ways to normalize the output of local and global FCN : an L2 normalization layer or a 1x1 convolutional layer to learn the scale . As shown in Table [ reference ] , we find that the use of L2 normalization decreases the performance greatly , even worse than the direct addition ( without any normalization ways ) . To explain such a phenomenon , we measured the outputs of two branches before and after L2 normalization . We further found that L2 normalization reduces the output gap between different categories , which results in a smaller score gap . As we know , a small score gap between different categories always means the classifier can not make a confident prediction . Therefore , we assume that this is the reason for the performance degradation . Moreover , we also exploit a 1x1 convolution to adaptively learn the scales between the global and local branches . Table [ reference ] shows that using 1x1 convolution increases by points compared to the direct addition and points over R - FCN . Therefore , we use 1x1 convolution to replace the L2 normalization in the following experiments . Coupling strategy . We explore three different response coupling strategies : element - wise sum , element - wise product and element - wise maximum . Table [ reference ] shows the comparison results for the above three different implementations . We can see that the element - wise sum always achieves the best performance even though in different normalization methods . Generally , current advanced residual networks also use element - wise sum as the effective way to integrate information from previous layers , which greatly facilitates the circulation of information and achieves the complementary advantages . For element - wise product , we argue that the system is relatively unstable and is susceptible to the weak side , which results in a large gradient to update the weak branch that makes it difficult to converge . For element - wise maximum , it equals to an ensemble model within the network to some extent , which losts the advantages of mutual support compared to element - wise sum when both two branches are failed to detect the object . Moreover , a better coupling strategy can be taken into consideration as the future work to further improve the accuracy , such as designing a more subtle nonlinear structure to learn the coupling relationship . Model ensemble . Model ensemble is commonly used to improve the final detection performance , since diverse initialization of parameters and the randomness of training samples both lead to different performance for the same model . Although the differences and complementarities will be more pronounced for different models , the promotion is often very limited . As shown in Table [ reference ] , we also compare our CoupleNet with the model ensemble . For a fair comparison , we first re - implemented Faster R - CNN using ResNet - 101 and online hard example mining ( OHEM ) , which achieves a mAP of on VOC07 ( in original paper without OHEM ) . We also re - implemented R - FCN with appropriate joint training using the public available code py - R - FCN , which achieves a slightly lower result compared to ( vs. ) . We use our reimplementation models to conduct the comparisons for consistency . We found that the promotion brought by model ensemble is less than 1 point . As shown in Table [ reference ] , it is far less than our method ( ) . On the one hand , we argue that the naive model ensemble just combines the results together and does not essentially guide the learning process of the network , while our CoupleNet can simultaneously utilize the global and local information to update the network and to infer the final results . On the other hand , our method enjoys end - to - end training and there is no need to train multiple models , thus greatly reducing the training time . Amount of parameters . Since our CoupleNet introduces a few more parameters compared with the single branch detectors , to further verify effectiveness of the coupling structure , here we increase the parameters of the prediction head for each single branch implementation to maintain the same amount of parameters with CoupleNet for comparison . In detail , we add a new residual variant block with three convolution layers , where the kernel size is 1x1x256 , 3x3x256 and 1x1x1024 respectively , to the prediction sub - network . We found that the standard R - FCN with one or two extra heads got a mAP of and respectively in VOC07 , which is slightly higher than our re - implemented version ( ) in as shown in Table [ reference ] . Meanwhile , our global FCN , which performs the ROI Pooling on top of conv5 , got a relative higher gain ( a mAP of for one head , for two heads ) . The results indicate that simply adding more prediction layers obtains a very limited performance gain , while our coupling structure shows more discriminative power with the same amount of parameters . subsection : Results on VOC2007 Using the public available ResNet - 101 as the initialization model , we note that our method is easy to follow and the hyper - parameters for training are the same as in . Similarly , we use the dilation strategy to reduce the effective stride of ResNet - 101 , just as shows , thus both the global and local branches have a stride of 16 . We also use a 1 - GPU implementation , and the effective mini - batch size is 2 images by setting the to 2 . The whole network is trained for 80k iterations with a learning rate of 0.001 and then for 30k iterations with 0.0001 . In addition , the context prior is proposed to further boost the performance while keeping the iterations unchanged . Finally , we also perform multi - scale training with the shorter sides of images are randomly resized from 480 to 864 . Table [ reference ] shows the detailed comparisons with Faster R - CNN and R - FCN . As we can see that our single model achieves a mAP of , which outperforms the R - FCN by 2.2 points . However , while embedding the context prior to the global branch , our mAP rises up to , which is the current best single model detector to our knowledge . Moreover , we also evaluate the inference time of our network using a NVIDIA TITAN X GPU ( pascal ) along with CUDA 8.0 and cuDNN - v5.1 . As shown in the last column of Table [ reference ] , our method is slightly slower than R - FCN , which also reaches a real - time speed ( 8.2 fps or 9.8 fps without context ) and achieves the best trade - off between accuracy and speed . We argue that the sharing process of feature extraction between two branches and the design of lightweight RoI - wise subnetwork after RoI pooling both greatly reduce the model complexity . As shown in Table [ reference ] , we also compared our method with other state - of - the - art single model . We found that our method outperforms the others with a large margin , including the advanced end - to - end SSD method , which requires complicated data augmentation and careful training skills . Just as discussed earlier , CoupleNet shows a large gain over the classes with occlusions , truncations and considerable background information , like sofa , person , table and chair , which verifies our analyses . We also observed a large improvement for airplane , bird , boat and pottedplant , which usually have class - specific backgrounds , the sky for airplane and bird , water for boat and so on . Therefore , the context surrounding the objects provides an extra auxiliary discrimination . subsection : Results on VOC2012 We also evaluate our method on the more challenging VOC2012 dataset by submitting results to the public evaluation server . We use VOC07 trainval , VOC07 test and VOC12 trainval as the training set , which consists of 21k images in total . We also follow the similar hyper - parameter settings in VOC07 but change the iterations , since there are more training images . We train our models with 4 GPUs , and the effective mini - batch size thus becomes 4 ( 1 per GPU ) . As a result , the network is trained for 60k iterations with a learning rate of 0.001 and 0.0001 for the following 20k iterations . Table [ reference ] shows the results on the VOC2012 test set . Our method obtains a top mAP of , which is 2.8 points higher than R - FCN . We note that without using the extra tricks in the testing phase , our detector is the first one with a mAP higher than . Similar promotions over the specific classes analysed in VOC07 are also observed , which once again validates the effectiveness of our method . Figure [ reference ] shows some detection examples on VOC 2012 test set . subsection : Results on MS COCO Next we present more results on the Microsoft COCO object detection dataset . The dataset consists of 80k training set , 40k validation set and 20k test - dev set , which involves 80 object categories . All our models are trained on the union set of 80k training set and 40k validation set , and evaluated on 20k test - dev set . The COCO standard metric denotes as AP , which is evaluated at . Following the VOC2012 , a 4 - GPU implementation is used to accelerate the training process . We use an initial learning rate of 0.001 for the first 510k iterations and 0.0001 for the next 70k iterations . In addition , we conduct multi - scale training with the scales are randomly sampled from while testing in a single scale . Table [ reference ] shows our results . Our single - scale trained detector has already achieved a result of , which outperforms the R - FCN by 3.9 points . In addition , the multi - scale training further improves the performance up to . Interestingly , we observed that the more challenging the dataset , the more the promotion ( , for VOC07 , for VOC12 and for COCO , all in multi - scale training ) , which directly proves that our approach can effectively cope with a variety of complex situations . section : Conclusion In this paper , we present the CoupleNet , a concise yet effective network that simultaneously couples global , local and context cues for accurate object detection . Our system naturally combines the advantages of different region - based approaches with the coupling structure . With the combination of local part representation , global structural information and the contextual assistance , our CoupleNet achieves state - of - the - art results on the challenging PASCAL VOC and COCO datasets without using any extra tricks in the testing phase , which validates the effectiveness of our method . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["CoupleNet"]]], "Metric": [[["MAP"]]], "Task": [[["Object_Detection"]]]}]}
{"docid": "TST3-SREX-0062", "doctext": "OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER section : ABSTRACT The capacity of a neural network to absorb information is limited by its number of parameters . Conditional computation , where parts of the network are active on a per - example basis , has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation . In practice , however , there are significant algorithmic and performance challenges . In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters . We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks . A trainable gating network determines a sparse combination of these experts to use for each example . We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora . We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers . On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost . section : INTRODUCTION AND RELATED WORK section : CONDITIONAL COMPUTATION Exploiting scale in both training data and model size has been central to the success of deep learning . When datasets are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy . This has been shown in domains such as text [ reference ][ reference ][ reference ][ reference ] , images [ reference ][ reference ] , and audio [ reference ][ reference ] . For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase . Unfortunately , the advances in computing power and distributed computation fall short of meeting such demand . Various forms of conditional computation have been proposed as a way to increase model capacity without a proportional increase in computational costs [ reference ][ reference ][ reference ][ reference ][ reference ][ reference ] . In these schemes , large parts of a network are active or inactive on a per - example basis . The gating decisions may be binary or sparse and continuous , stochastic or deterministic . Various forms of reinforcement learning and back - propagation are proposed for trarining the gating decisions . \u2022 Model capacity is most critical for very large data sets . The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600 , 000 images . It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters . In this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation . We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets . section : OUR APPROACH : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) . The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see Figure 1 ) . All parts of the network are trained jointly by back - propagation . While the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models . In particular , we apply a MoE convolutionally between stacked LSTM layers [ reference ] , as in Figure 1 . The MoE is called once for each position in the text , selecting a potentially different combination of experts at each position . The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E Table 9 ) . On both language modeling and machine translation benchmarks , we improve on best published results at a fraction of the computational cost . section : RELATED WORK ON MIXTURES OF EXPERTS Since its introduction more than two decades ago [ reference ][ reference ] , the mixture - of - experts approach has been the subject of much research . Different types of expert architectures hae been proposed such as SVMs [ reference ] , Gaussian Processes [ reference ][ reference ][ reference ] , Dirichlet Processes [ reference ] , and deep networks . Other work has focused on different expert configurations such as a hierarchical structure [ reference ] , infinite numbers of experts [ reference ] , and adding experts sequentially [ reference ] . [ reference ] suggest an ensemble model in the format of mixture of experts for machine translation . The gating network is trained on a pre - trained ensemble NMT model . The works above concern top - level mixtures of experts . The mixture of experts is the whole model . [ reference ] introduce the idea of using multiple MoEs with their own gating networks as parts of a deep model . It is intuitive that the latter approach is more powerful , since complex problems may contain many sub - problems each requiring different experts . They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation . Our work builds on this use of MoEs as a general purpose neural network component . While [ reference ] uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text . We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity . section : THE STRUCTURE OF THE MIXTURE - OF - EXPERTS LAYER The Mixture - of - Experts ( MoE ) layer consists of a set of n \" expert networks \" E 1 , \u00b7 \u00b7 \u00b7 , E n , and a \" gating network \" G whose output is a sparse n - dimensional vector . Figure 1 shows an overview of the MoE module . The experts are themselves neural networks , each with their own parameters . Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters . Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network for a given input x. The output y of the MoE module can be written as follows : We save computation based on the sparsity of the output of G ( x ) . Wherever G ( x ) i = 0 , we need not compute E i ( x ) . In our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example . If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of \" experts \" , each of which is itself a secondary mixture - of - experts with its own gating network . In the following we focus on ordinary MoEs . We provide more details on hierarchical MoEs in Appendix B. Our implementation is related to other models of conditional computation . A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in . A MoE whose experts have one hidden layer is similar to the block - wise dropout described in [ reference ] , where the dropped - out layer is sandwiched between fully - activated layers . section : GATING NETWORK Softmax Gating : A simple choice of non - sparse gating function [ reference ] is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function . Noisy Top - K Gating : We add two components to the Softmax gating network : sparsity and noise . Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to \u2212\u221e ( which causes the corresponding gate values to equal 0 ) . The sparsity serves to save computation , as described above . While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice . The noise term helps with load balancing , as will be discussed in Appendix A. The amount of noise per component is controlled by a second trainable weight matrix W noise . Training the Gating Network We train the gating network by simple back - propagation , along with the rest of the model . If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network . This type of occasionally - sensitive behavior is described in [ reference ] ) with respect to noisy rectifiers . Gradients also backpropagate through the gating network to its inputs . Our method differs here from [ reference ] who use boolean gates and a REINFORCE - style approach to train the gating network . section : ADDRESSING PERFORMANCE CHALLENGES section : THE SHRINKING BATCH PROBLEM On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates . If the gating network chooses k out of n experts for each example , then for a batch of b examples , each expert receives a much smaller batch of approximately kb n b examples . This causes a naive MoE implementation to become very inefficient as the number of experts increases . The solution to this shrinking batch problem is to make the original batch size as large as possible . However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes . We propose the following techniques for increasing the batch size : Mixing Data Parallelism and Model Parallelism : In a conventional distributed training setting , multiple copies of the model on different devices asynchronously process distinct batches of data , and parameters are synchronized through a set of parameter servers . In our technique , these different batches run synchronously so that they can be combined for the MoE layer . We distribute the standard layers of the model and the gating network according to conventional data - parallel schemes , but keep only one shared copy of each expert . Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batches . The same set of devices function as data - parallel replicas ( for the standard layers and the gating networks ) and as model - parallel shards ( each hosting a subset of the experts ) . If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples . Thus , we achieve a factor of d improvement in expert batch size . In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism . Each secondary MoE resides on one device . This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster . The total batch size increases , keeping the batch size per expert constant . The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model . It is our goal to train a trillionparameter model on a trillion - word corpus . We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware . Taking Advantage of Convolutionality : In our language models , we apply the same MoE to each time step of the previous layer . If we wait for the previous layer to finish , we can apply the MoE to all the time steps together as one big batch . Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps . Increasing Batch Size for a Recurrent MoE : We suspect that even more powerful models may involve applying a MoE recurrently . For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE at one timestep depends on the output of the MoE at the previous timestep . [ reference ] describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations . This would allow for a large increase in batch size . section : NETWORK BANDWIDTH Another major performance concern in distributed computing is network bandwidth . Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network . To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device . For GPUs , this may be thousands to one . In our experiments , we use experts with one hidden layer containing thousands of RELU - activated units . Since the weight matrices in the expert have sizes input_size\u00d7hidden_size and hidden_size \u00d7 output_size , the ratio of computation to input and output is equal to the size of the hidden layer . Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers . section : BALANCING EXPERT UTILIZATION We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts . This imbalance is self - reinforcing , as the favored experts are trained more rapidly and thus are selected even more by the gating network . [ reference ] describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum . [ reference ] include a soft constraint on the batch - wise average of each gate . [ reference ] We take a soft constraint approach . We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert . We define an additional loss L importance , which is added to the overall loss function for the model . This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance . This additional loss encourages all experts to have equal importance . 1 [ reference ] also include two additional losses . One controls per - example sparsity , which we do not need since it is enforced by the fixed value of k. A third loss encourages diversity of gate values . In our experiments , we find that the gate values naturally diversify as the experts specialize ( in a virtuous cycle ) , and we do not need to enforce diversity of gate values . While this loss function can ensure equal importance , experts may still receive very different numbers of examples . For example , one expert may receive a few examples with large weights , and another may receive many examples with small weights . This can cause memory and performance problems on distributed hardware . To solve this problem , we introduce a second loss function , L load , which ensures balanced loads . Appendix A contains the definition of this function , along with experimental results . section : EXPERIMENTS section : 1 BILLION WORD LANGUAGE MODELING BENCHMARK Dataset : This dataset , introduced by [ reference ] consists of shuffled unique sentences from news articles , totaling approximately 829 million words , with a vocabulary of 793 , 471 words . Previous State - of - the - Art : The best previously published results [ reference ] use models consisting of one or more stacked Long Short - Term Memory ( LSTM ) layers [ reference ][ reference ] . The number of parameters in the LSTM layers of these models vary from 2 million to 151 million . Quality increases greatly with parameter count , as do computational costs . Results for these models form the top line of Figure 2 - right . MoE Models : Our models consist of two stacked LSTM layers with a MoE layer between them ( see Figure 1 ) . We vary the sizes of the layers and the number of experts . For full details on model architecture , training regimen , additional baselines and results , see Appendix C. Low Computation , Varied Capacity : To investigate the effects of adding capacity , we trained a series of MoE models all with roughly equal computational costs : about 8 million multiply - andadds per training example per timestep in the forwards pass , excluding the softmax layer . We call this metric ( ops / timestep ) . We trained models with flat MoEs containing 4 , 32 , and 256 experts , and models with hierarchical MoEs containing 256 , 1024 , and 4096 experts . Each expert had about 1 million parameters . For all the MoE layers , 4 experts were active per input . The results of these models are shown in Figure 2 - left . The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set . Varied Computation , High Capacity : In addition to the largest model from the previous section , we trained two more MoE models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets . These models had larger LSTMs , and fewer but larger and experts . Details can be found in Appendix C.2 . Results of these three models form the bottom line of Figure 2 - right . Table 1 compares the results of these models to the best previously - published result on this dataset . Even the fastest of these models beats the best published result ( when controlling for the number of training epochs ) , despite requiring only 6 % of the computation . Computational Efficiency : We trained our models using TensorFlow [ reference ] on clusters containing 16 - 32 Tesla K40 GPUs . For each of our models , we determine computational efficiency in TFLOPS / GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster . The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations . For all of our MoE models , the floating point operations involved in the experts represent between 37 % and 46 % of the total . For our baseline models wtih no MoE , observed computational efficiency ranged from 1.07 - 1.29 TFLOPS / GPU . For our low - computation MoE models , computation efficiency ranged from 0.74 - 0.90 TFLOPS / GPU , except for the 4 - expert model which did not make full use of the available parallelism . Our highest - computation MoE model was more efficient at 1.56 TFLOPS / GPU , likely due to the larger matrices . These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA . Detailed results are in Appendix C , Table 7 . section : 100 BILLION WORD GOOGLE NEWS CORPUS Figure 3 : Language modeling on a 100 billion word corpus . Models have similar computational budgets ( 8 million ops / timestep ) . On the 1 - billion - word corpus , adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion , as can be seen in Figure 2 - left . We hypothesized that for a larger training set , even higher capacities would produce significant quality improvements . We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words . Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep . In addition to a baseline LSTM model , we trained models augmented with MoE layers containing [ reference ] experts . This corresponds to up to 137 billion parameters in the MoE layer . Details on architecture , training , and results are given in Appendix D. Results : Figure 3 shows test perplexity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) . When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity . The widening gap between the two lines demonstrates ( unsurprisingly ) that increased model capacity helps more on larger training sets . Even at 65536 experts ( 99.994 % layer sparsity ) , computational efficiency for the model stays at a respectable 0.72 TFLOPS / GPU . section : MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ) Model Architecture : Our model was a modified version of the GNMT model described in [ reference ] . To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively . We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) . Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models . Further details on model architecture , testing procedure and results can be found in Appendix E. Datasets : We benchmarked our method on the WMT'14 En\u2192Fr and En\u2192De corpora , whose training sets have 36 M sentence pairs and 5 M sentence pairs , respectively . The experimental protocols were also similar to those in [ reference ] : newstest2014 was used as the test set to compare against previous work [ reference ][ reference ][ reference ] , while the combination of newstest2012 and newstest2013 was used as the development set . We also tested the same model on a Google 's Production English to French data . [ reference ] 2.79 39.22 214 M 278 M 6 days / 96 k80s GNMT + RL [ reference ] 2.96 39.92 214 M 278 M 6 days / 96 k80s PBMT [ reference ] 37.0 LSTM ( 6 - layer ) [ reference ] 31.5 LSTM ( 6 - layer + PosUnk ) [ reference ] 33.1 DeepAtt [ reference ] 37.7 DeepAtt + PosUnk [ reference ] 39.2 [ reference ] 5.25 24.91 214 M 278 M 1 day / 96 k80s GNMT + RL [ reference ] 8.08 24.66 214 M 278 M 1 day / 96 k80s PBMT [ reference ] 20.7 DeepAtt [ reference ] 20.6 Results : Tables 2 , 3 , and 4 show the results of our largest models , compared with published results . Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT'14 En\u2192Fr and En\u2192De benchmarks . As our models did not use RL refinement , these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in [ reference ] . The perplexity scores are also better . 2 On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time . section : MULTILINGUAL MACHINE TRANSLATION Dataset : ( Johnson et al . , 2016 ) train a single GNMT [ reference ] ) model on a very large combined dataset of twelve language pairs . Results are somewhat worse than those for 12 separately trained single - pair GNMT models . This is not surprising , given that the twelve models have 12 times the capacity and twelve times the aggregate training of the one model . We repeat this experiment with a single MoE - augmented model . See Appendix E for details on model architecture . We train our model on the same dataset as [ reference ] and process the same number of training examples ( about 3 billion sentence pairs ) . Our training time was shorter due to the lower computational budget of our model . section : Results : Results for the single - pair GNMT models , the multilingual GNMT model and the multilingual MoE model are given in Table 5 . The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model . On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs . The poor performance on English \u2192 Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus . section : CONCLUSION This work is the first to demonstrate major wins from conditional computation in deep networks . We carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions . While we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets . We look forward to seeing many novel implementations and applications of conditional computation in the years to come . section : ACKNOWLEDGMENTS We would like to thank all of the members of the Google Brain and Google Translate teams who helped us with this project , in particular Zhifeng Chen , Yonghui Wu , and Melvin Johnson . Thanks also to our anonymous ICLR reviewers for the helpful suggestions on making this paper better . section : APPENDICES A LOAD - BALANCING LOSS As discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples . Unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in backpropagation . Instead , we define a smooth estimator Load ( X ) of the number of examples assigned to each expert for a batch X of inputs . The smoothness allows us to back - propagate gradients through the estimator . This is the purpose of the noise term in the gating function . We define P ( x , i ) as the probability that G ( x ) i is nonzero , given a new random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements . To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself . The probability works out to be : Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i. Simplifying , we get : Where \u03a6 is the CDF of the standard normal distribution . We can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load . Initial Load Imbalance : To avoid out - of - memory errors , we need to initialize the network in a state of approximately equal expert load ( since the soft constraints need some time to work ) . To accomplish this , we initialize the matrices W g and W noise to all zeros , which yields no signal and some noise . section : Experiments : We trained a set of models with identical architecture ( the MoE - 256 model described in Appendix C ) , using different values of w importance and w load . We trained each model for 10 epochs , then measured perplexity on the test set . We also measured the coefficients of variation in Importance and Load , as well as ratio of the load on the most overloaded expert to the average load . This last value is significant for load balancing purposes on distributed hardware . All of these metrics were averaged over several training batches . Results : Results are reported in Table 6 . All the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse . Models with higher values of w load had lower loads on the most overloaded expert . section : B HIERACHICAL MIXTURE OF EXPERTS If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of \" experts \" , each of which is itself a secondary mixture - of - experts with its own gating network . 3 If the hierarchical MoE consists of a groups of b experts each , we denote the primary gating network by G primary , the secondary gating networks by ( G 1 , G 2 .. G a ) , and the expert networks by ( E 0 , 0 , E 0 , 1 .. E a , b ) . The output of the MoE is given by : Our metrics of expert utilization change to the following : Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively . X ( i ) denotes the subset of X for which G primary ( x ) i > 0 . It would seem simpler to let Load H ( X ) i , j = Load i ( X i ) j , but this would not have a gradient with respect to the primary gating network , so we use the formulation above . section : C 1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS C.1 8 - MILLION - OPERATIONS - PER - TIMESTEP MODELS Model Architecture : Our model consists of five layers : a word embedding layer , a recurrent Long Short - Term Memory ( LSTM ) layer [ reference ][ reference ] , a MoE layer , a second LSTM layer , and a softmax layer . The dimensionality of the embedding layer , the number of units in each LSTM layer , and the input and output dimensionality of the MoE layer are all equal to 512 . For every layer other than the softmax , we apply drouput [ reference ] to the layer output , dropping each activation with probability DropP rob , otherwise dividing by ( 1 \u2212 DropP rob ) . After dropout , the output of the previous layer is added to the layer output . This residual connection encourages gradient flow [ reference ] . For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster . We use Noisy - Top - K Gating ( see Section 2.1 ) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers . Thus , each example is processed by exactly 4 experts for a total of 4 M ops / timestep . The two LSTM layers contribute 2 M ops / timestep each for the desired total of 8M. section : Computationally - Matched Baselines : The MoE - 4 model does not employ sparsity , since all 4 experts are always used . In addition , we trained four more computationally - matched baseline models with no sparsity : \u2022 MoE - 1 - Wide : The MoE layer consists of a single \" expert \" containing one ReLU - activated hidden layer of size 4096 . \u2022 MoE - 1 - Deep : The MoE layer consists of a single \" expert \" containing four ReLU - activated hidden layers , each with size 1024 . \u2022 4xLSTM - 512 : We replace the MoE layer with two additional 512 - unit LSTM layers . \u2022 LSTM - 2048 - 512 : The model contains one 2048 - unit LSTM layer ( and no MoE ) . The output of the LSTM is projected down to 512 dimensions [ reference ] . The next timestep of the LSTM receives the projected output . This is identical to one of the models published in [ reference ] . We re - ran it to account for differences in training regimen , and obtained results very similar to the published ones . Training : The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3 . Each batch consisted of a set of sentences totaling roughly 300 , 000 words . In the interest of time , we limited training to 10 epochs , ( 27 , 000 steps ) . Training took 12 - 16 hours for all models , except for MoE - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 GPUs ) . We used the Adam optimizer [ reference ] . The base learning rate was increased linearly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number . The Softmax output layer was trained efficiently using importance sampling similarly to the models in [ reference ] . For each model , we performed a hyper - parmeter search to find the best dropout probability , in increments of 0.1 . To ensure balanced expert utilization we set w importance = 0.1 and w load = 0.1 , as described in Section 4 and Appendix A. Results : We evaluate our model using perplexity on the holdout dataset , used by [ reference ][ reference ] . We follow the standard procedure and sum over all the words including the end of sentence symbol . Results are reported in Table 7 . For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency . section : C.2 MORE EXPENSIVE MODELS We ran two additional models ( MoE - 34 M and MoE - 143 M ) to investigate the effects of adding more computation in the presence of a large MoE layer . These models have computation budgets of 34 M and 143 M ops / timestep . Similar to the models above , these models use a MoE layer between two LSTM layers . The dimensionality of the embedding layer , and the input and output dimensionality of the MoE layer are set to 1024 instead of 512 . For MoE - 34 M , the LSTM layers have 1024 units . For MoE - 143 M , the LSTM layers have 4096 units and an output projection of size 1024 [ reference ] . MoE - 34 M uses a hierarchical MoE layer with 1024 experts , each with a hidden layer of size 2048 . MoE - 143 M uses a hierarchical MoE layer with 256 experts , each with a hidden layer of size 8192 . Both models have 4B parameters in the MoE layers . We searched for the best DropP rob for each model , and trained each model for 10 epochs . The two models achieved test perplexity of 31.3 and 28.0 respectively , showing that even in the presence of a large MoE , more computation is still useful . Results are reported at the bottom of Table 7 . The larger of the two models has a similar computational budget to the best published model from the literature , and training times are similar . Comparing after 10 epochs , our model has a lower test perplexity by 18 % . section : D 100 BILLION WORD GOOGLE NEWS CORPUS - EXPERIMENTAL DETAILS Model Architecture : The models are similar in structure to the 8 - million - operations - per - timestep models described in the previous section . We vary the number of experts between models , using an ordinary MoE layer with 32 experts and hierarchical MoE layers with 256 , 1024 , 4096 , 16384 , 65536 and 131072 experts . For the hierarchical MoE layers , the first level branching factors are [ reference ] Training : Models are trained on a cluster of 32 Tesla K40 GPUs , except for the last two models , which are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the parameters . For all models , training batch sizes are approximately 2.5 million words . Models are trained once - through over about 100 billion words . We implement several memory optimizations in order to fit up to 1 billion parameters per GPU . First , we do not store the activations of the hidden layers of the experts , but instead recompute them on the backwards pass . Secondly , we modify the optimizer on the expert parameters to require less auxiliary storage : The Adam optimizer [ reference ] keeps first and second moment estimates of the perparameter gradients . This triples the required memory . To avoid keeping a first - moment estimator , we set \u03b2 1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation . For a matrix of parameters , instead of maintaining a full matrix of second - moment estimators , we maintain vectors of row - wise and column - wise averages of that matrix . At each step , the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one . This technique could similarly be applied to Adagrad [ reference ] . Results : We evaluate our model using perplexity on a holdout dataset . Results are reported in Table 8 . Perplexity after 100 billion training words is 39 % lower for the 68 - billion - parameter MoE model than for the baseline model . It is notable that the measured computational efficiency of the largest model ( 0.30 TFLOPS / GPU ) is very low compared to the other models . This is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportionally to the number of GPUs . For comparison , we include results for a computationally matched baseline model consisting of 4 LSTMs , and for an unpruned 5 - gram model with Kneser - Ney smoothing [ reference ] . section : E MACHINE TRANSLATION - EXPERIMENTAL DETAILS Model Architecture for Single Language Pair MoE Models : Our model is a modified version of the GNMT model described in [ reference ] . To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively . We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) . We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 . All of the layers in our model have input and output dimensionality of 512 . Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection . We add residual connections around all LSTM and MoE layers to encourage gradient flow [ reference ] . Similar to GNMT , to effectively deal with rare words , we used subword units ( also known as \" wordpieces \" ) ( Schuster & Nakajima , 2012 ) for inputs and outputs in our system . We use a shared source and target vocabulary of 32 K wordpieces . We also used the same beam search technique as proposed in [ reference ] . We train models with different numbers of experts in the MoE layers . In addition to a baseline model with no MoE layers , we train models with flat MoE layers containing 32 experts , and models with hierarchical MoE layers containing 512 and 2048 experts . The flat MoE layers use k = 4 and the hierarchical MoE models use k = 2 at each level of the gating network . Thus , each input is processed by exactly 4 experts in each MoE layer . Each expert in the MoE layer is a feed forward network with one hidden layer of size 2048 and ReLU activation . Thus , each expert contains [ 512 * 2048 ] + [ 2048 * 512 ] = 2 M parameters . The output of the MoE layer is passed through a sigmoid function . We use the strictly - balanced gating function described in Appendix F. section : Model Architecture for Multilingual MoE Model : We used the same model architecture as for the single - language - pair models , with the following exceptions : We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non - hierarchical MoEs with n = 512 experts , and k = 2 . Each expert has a larger hidden layer of size 8192 . This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102 M ops / timestep . Training : We trained our networks using the Adam optimizer [ reference ] . The base learning rate was increased linearly for the first 2000 training steps , held constant for an additional 8000 steps , and decreased after that so as to be proportional to the inverse square root of the step number . For the single - language - pair models , similarly to [ reference ] , we applied dropout [ reference ] to the output of all embedding , LSTM and MoE layers , using DropP rob = 0.4 . Training was done synchronously on a cluster of up to 64 GPUs as described in section 3 . Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU . To ensure balanced expert utilization we set w importance = 0.01 and w load = 0.01 , as described in Section 4 and Appendix A. section : Metrics : We evaluated our models using the perplexity and the standard BLEU score metric . We reported tokenized BLEU score as computed by the multi - bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in [ reference ] . Tables 2 , 3 and 4 in Section 5.3 show comparisons of our results to other published methods . Figure 4 shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts . As can be seen from the Figure , as we increased the number of experts to approach 2048 , the test perplexity of our model continued to improve . Figure 4 : Perplexity on WMT'14 En\u2192 Fr ( left ) and Google Production En\u2192 Fr ( right ) datasets as a function of number of words processed . The large differences between models at the beginning of training are due to different batch sizes . All models incur the same computational budget ( 85 M ops / timestep ) except the one with no experts . section : Results : We found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in Table 9 . For example , one expert is used when the indefinite article \" a \" introduces the direct object in a verb phrase indicating importance or leadership . section : F STRICTLY BALANCED GATING Due to some peculiarities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size . To accommodate this , we used a different gating function which we describe below . Recall that we define the softmax gating function to be : Sparse Gating ( alternate formulation ) : To obtain a sparse gating vector , we multiply G \u03c3 ( x ) component - wise with a sparse mask M ( G \u03c3 ( x ) ) and normalize the output . The mask itself is a function of G \u03c3 ( x ) and specifies which experts are assigned to each input example : As our experiments suggest and also observed in [ reference ] , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples . Our solution to this is to train a vector T of per - expert threshold values to approximate the effects of the batchwise mask . We use the following mask at inference time : To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical . G ATTENTION FUNCTION The attention mechanism described in GNMT [ reference ] ) involves a learned \" Attention Function \" A ( x i , y j ) which takes a \" source vector \" x i and a \" target vector \" y j , and must be computed for every source time step i and target time step j. In GNMT , the attention function is implemented as a feed forward neural network with a hidden layer of size n. It can be expressed as : Where U and W are trainable weight matrices and V is a trainable weight vector . For performance reasons , in our models , we used a slightly different attention function : With our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications . We found little difference in quality between the two functions . section :", "templates": [{"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["High-Budget_MoE"]]], "Metric": [[["Number_of_params"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["Low-Budget_MoE"]]], "Metric": [[["Number_of_params"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["High-Budget_MoE"]]], "Metric": [[["PPL"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["One_Billion_Word"]]], "Method": [[["Low-Budget_MoE"]]], "Metric": [[["PPL"]]], "Task": [[["Language_Modelling"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WMT2014_English-French"]]], "Method": [[["MoE"]]], "Metric": [[["BLEU_score"]]], "Task": [[["Machine_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["WMT2014_English-German"]]], "Method": [[["MoE"]]], "Metric": [[["BLEU_score"]]], "Task": [[["Machine_Translation"]]]}]}
{"docid": "TST3-SREX-0063", "doctext": "document : Triplet Probabilistic Embedding for Face Verification and Clustering Despite significant progress made over the past twenty five years , unconstrained face verification remains a challenging problem . This paper proposes an approach that couples a deep CNN - based approach with a low - dimensional discriminative embedding step , learned using triplet probability constraints to address the unconstrained face verification problem . Aside from yielding performance improvements , this embedding provides significant advantages in terms of memory and for post - processing operations like subject specific clustering . Experiments on the challenging IJB - A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics , while requiring much less training data and training / test time . The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to large pose variation . Furthermore , we demonstrate the robustness of deep features to challenges including age , pose , blur and clutter by performing simple clustering experiments on both IJB - A and LFW datasets . section : Introduction Recently , with the advent of curated face datasets like Labeled faces in the Wild ( LFW ) and advances in learning algorithms like Deep neural nets , there is more hope that the unconstrained face verification problem can be solved . A face verification algorithm compares two given templates that are typically not seen during training . Research in face verification has progressed well over the past few years , resulting in the saturation of performance on the LFW dataset , yet the problem of unconstrained face verification remains a challenge . This is evident by the performance of traditional algorithms on the publicly available IJB - A dataset ( , ) that was released recently . Moreover , despite the superb performance of CNN - based approaches compared to traditional methods , a drawback of such methods is the long training time needed . In this work , we present a Deep CNN ( DCNN ) architecture that ensures faster training , and investigate how much the performance can be improved if we are provided domain specific data . Specifically , our contributions are as follows : We propose a deep network architecture and a training scheme that ensures faster training time . We formulate a triplet probability embedding learning method to improve the performance of deep features for face verification and subject clustering . During training , we use a publicly available face dataset to train our deep architecture . Each image is pre - processed and aligned to a canonical view before passing it to the deep network whose features are used to represent the image . In the case of IJB - A dataset , the data is divided into 10 splits , each split containing a training set and a test set . Hence , to further improve performance , we learn the proposed triplet probability embedding using the training set provided with each split over the features extracted from our DCNN model . During the deployment phase , given a face template , we extract the deep features using the raw CNN model after implementing automatic pre - processing steps such as face detection and fiducial extraction . The deep features are projected onto a low - dimensional space using the embedding matrix learned during training ( note that the projection involves only matrix multiplication ) . We use the 128 - dimensional feature as the final representation of the given face template . This paper is organized as follows : Section [ reference ] places our work among the recently proposed approaches for face verification . Section [ reference ] details the network architecture and the training scheme . The triplet probabilistic embedding learning method is described in Section [ reference ] followed by results on IJB - A and CFP datasets and a brief discussion in Section [ reference ] . In Section [ reference ] , we demonstrate the ability of the proposed method to cluster a media collection from LFW and IJB - A datasets . section : Related Work In the past few years , there have been numerous works in using deep features for tasks related to face verification . The DeepFace approach uses a carefully crafted 3D alignment procedure to preprocess face images and feeds them to a deep network that is trained using a large training set . More recently , Facenet uses a large private dataset to train several deep network models using a triplet distance loss function . The training time for this network is of the order of few weeks . Since the release of the IJB - A dataset , there have been several works that have published verification results for this dataset . Previous approaches presented in and train deep networks using the CASIA - WebFace dataset and the VGG - Face dataset respectively , requiring substantial training time . This paper proposes a network architecture and a training scheme that needs shorter training time and a small query time . The idea of learning a compact and discriminative representation has been around for decades . Weinberger et al . used a Semi Definite Programming ( SDP )- based formulation to learn a metric satisfying pairwise and triplet distance constraints in a large margin framework . More recently , this idea has been successfully applied to face verification by integrating the loss function within the deep network architecture ( , ) . Joint Bayesian metric learning is also another popular metric used for face verification ( , ) . These methods either require a large dataset for convergence or learn a metric directly and therefore are not amenable to subsequent operations like discriminative clustering or hashing . Classic methods like t - SNE , t - STE and Crowd Kernel Learning ( CKL ) perform extremely well when used to visualize or cluster a given data collection . They either operate on the data matrix directly or the distance matrix generated from data by generating a large set of pairwise or triplet constraints . While these methods perform very well on a given set of data points , they do not generalize to out - of - sample data . In the current work , we aim to generalize such formulations , to a more traditional classification setting , where domain specific training and testing data is provided . We formulate an optimization problem based on triplet probabilities that performs dimensionality reduction aside from improving the discriminative ability of the test data . The embedding scheme described in this work is a more general framework that can be applied to any setting where labeled training data is available . section : Network Architecture This section details the architecture and training algorithm for the deep network used in our work . Our architecture consists of 7 convolutional layers with varying kernel sizes . The initial layers have a larger size rapidly subsampling the image and reducing the parameters while subsequent layers consist of small filter sizes , which has proved to be very useful in face recognition tasks ( , ) . Furthermore , we use the Parametric Rectifier Linear units ( PReLUs ) instead of ReLUs , since they allow a negative value for the output based on a learned threshold and have been shown to improve the convergence rate . tableDeep Network architecture details The top three convolutional layers ( conv1 - conv3 ) are initialized with the weights from the AlexNet model trained on the ImageNet challenge dataset . Several recent works ( , ) have empirically shown that this transfer of knowledge across different networks , albeit for a different objective , improves performance and more significantly reduces the need to train over a large number of iterations . The compared methods either learn their deep models from scratch ( , ) or finetune only the last layer of fully pre - trained models . The former results in large training time and the latter does not generalize well to the task at hand ( face verification ) and hence resulting in sub optimal performance . In the current work , even though we use a pre - trained model ( AlexNet ) to initialize the proposed deep network , we do so only for the first three convolutional layers , since they retain more generic information ( ) . Subsequent layers learn representations which are more specific to the task at hand . Thus , to learn more task specific information , we add 4 convolutional layers each consisting of 512 kernels of size . The layers conv4 - conv7 do not downsample the input thereby learning more complex higher dimensional representations . This hybrid architecture proves to be extremely effective as our raw CNN representation outperforms some very deep CNN models on the IJB - A dataset ( Table 2 in Results ) . In addition , we achieve that performance by training the proposed deep network using the relatively smaller CASIA - WebFace dataset . The architecture of our network is shown in Table [ reference ] . Layers conv4 - conv7 and the fully connected layers fc6 - fc8 are initialized from scratch using random Gaussian distributions . PReLU activation functions are added between each layer . Since the network is used as a feature extractor , the last layer fc8 is removed during deployment , thus reducing the number of parameters to 29M. The inputs to the network are 227x227x3 RGB images . When the network is deployed , the features are extracted from the fc7 layer resulting in a dimensionality of 512 . The network is trained using the Softmax loss function for multiclass classification using the Caffe deep learning platform . section : Learning a Discriminative Embedding In this section , we describe our algorithm for learning a low - dimensional embedding such that the resulting projections are more discriminative . Aside from an improved performance , this embedding provides significant advantages in terms of memory and enables post - processing operations like visualization and clustering . Consider a triplet , where ( anchor ) and ( positive ) are from the same class , but ( negative ) belongs to a different class . Consider a function that is parameterized by the matrix , that measures the similarity between two vectors . Ideally , for all triplets that exist in the training set , we would like the following constraint to be satisfied : Thus , the probability of a given triplet satisfying ( [ reference ] ) can be written as : The specific form of the similarity function is given as : . In our case , and are deep features normalized to unit length . To learn the embedding from a given set of triplets , we solve the following optimization : ( [ reference ] ) can be interpreted as maximizing the likelihood ( [ reference ] ) or minimizing the negative log - likelihood ( NLL ) over the triplet set . In practice , the above problem is solved in a Large - Margin framework using Stochastic Gradient Descent ( SGD ) and the triplets are sampled online . The gradient update for is given as : where is the estimate at iteration , is the updated estimate , is the triplet sampled at the current iteration and is the learning rate . By choosing the dimension of as with , we achieve dimensionality reduction in addition to improved performance . For our work , we fix based on cross validation and is the dimensionality of our deep features . is initialized with the first principal components of the training data . At each iteration , a random anchor and a random positive data point are chosen . To choose the negative , we perform hard negative mining , ie . we choose the data point that has the least likelihood ( [ reference ] ) among the randomly chosen 2000 negative instances at each iteration . Since we compute the embedding matrix by optimizing over triplet probabilities , we call this method Triplet Probability Embedding ( TPE ) . The technique closest to the one presented in this section , which is used in recent works ( , ) computes the embedding based on satisfying a hinge loss constraint : acts a margin parameter for the loss function . To be consistent with the terminology used in this paper , we call it Triplet Distance Embedding ( TDE ) . To appreciate the difference between the two approaches , Figure [ reference ] shows the case where the gradient update for the TDE method ( [ reference ] ) occurs . If the value of is not appropriately chosen , a triplet is considered good even if the positive and negative are very close to one another . But under the proposed formulation , both cases referred to in Figure [ reference ] will update the gradient but their contribution to the gradient will be modulated by the probability with which they violate the constraint in ( [ reference ] ) . This modulation factor is specified by the term in the gradient update for TPE in ( [ reference ] ) implying that if the likelihood of a sampled triplet satisfying ( [ reference ] ) is high , then the gradient update is given a lower weight and vice - versa . Thus , in our method , the margin parameter ( ) is automatically set based on the likelihood . To compare the relative performances of the raw features before projection , with TDE and with TPE ( proposed method ) , we plot the traditional ROC curve ( TAR ( vs ) FAR ) for split 1 of the IJB - A verify protocol for the three methods in Figure [ reference ] . The Equal Error Rate ( EER ) metric is specified for each method . The performance improvement due to TPE is significant , especially at regions of FAR . We observed a similar behaviour for all the ten splits of the IJB - A dataset . section : Experimental setup and Results In this section we evaluate the proposed method on two challenging datasets : IARPA Janus Benchmark - A ( IJB - A ) : This dataset contains 500 subjects with a total of 25 , 813 images ( 5 , 399 still images and 20 , 414 video frames sampled at a rate of 1 in 60 ) . The faces in the IJB - A dataset contain extreme poses and illuminations , more challenging than LFW . Some sample images from the IJB - A dataset are shown in Figure [ reference ] . An additional challenge of the IJB - A verification protocol is that the template comparisons include image to image , image to set and set to set comparisons . In this work , for a given test template of the IJB - A data we perform two kinds of pooling to produce its final representation : Average pooling ( CNN ) : The deep features of the images and / or frames present in the template are combined by taking a componentwise average to produce one feature vector . Thus each feature equally contributes to the final representation . Media pooling ( CNN ) : The deep features are combined keeping in mind the media source they come from . The metadata provided with IJB - A gives us the media i d for each item of the template . Thus to get the final feature vector , we first take an intra - media average and then combine these by taking the inter - media average . Thus each feature \u2019s contribution to the final representation is weighted based on its source . Celebrities in Frontal - Profile ( CFP ) [ ] : This dataset contains 7000 images of 500 subjects . The dataset is used for evaluating how face verification approaches handle pose variation . Hence , it consists of 5000 images in frontal view and 2000 images in extreme profile . The data is organized into 10 splits , each containing equal number of frontal - frontal and frontal - profile comparisons . Sample comparison pairs of the CFP dataset are shown in Figure [ reference ] . .5 .25 .25 subsection : Pre - processing In the training phase , given an input image , we use the HyperFace method for face detection and fiducial point extraction . The HyperFace detector automatically extracts many faces from a given image . For the IJB - A dataset , since most images contain more than one face , we use the bounding boxes provided along with the dataset to select the person of interest from the list of automatic detections . We select the detection that has the maximum area overlap with the manually provided bounding box . In the IJB - A dataset , there are few images for which the HyperFace detector can not find the relevant face . For the missed cases , we crop the face using the bounding box information provided with the dataset and pass it to HyperFace to extract the fiducials . We use six fiducial points ( eyes and mouth corners ) to align the detected image to a canonical view using the similarity transform . For the CFP dataset , since the six keypoints can not be computed for profile faces we only use three keypoints on one side of the face for aligning them . tableIdentification and Verification results on the IJB - A dataset . For identification , the scores reported are TPIR values at the indicated points . The results are averages over 10 splits and the standard deviation is given in the brackets for methods which have reported them . implies that the result is not reported for that method . The best results are given in bold . tableResults on the CFP dataset . The numbers are averaged over ten test splits and the numbers in brackets indicate standard deviations of those runs . The best results are given in bold . subsection : Parameters and training times The training of the proposed deep architecture is done using SGD with momentum , which is set to 0.9 and the learning rate is set to 1e - 3 and decreased uniformly by a factor of 10 every 50 K iterations . The weight decay is set to 5e - 4 for all layers . The training batch size is set to 256 . The training time for our deep network is 24 hours on a single NVIDIA TitanX GPU . For the IJB - A dataset , we use the training data provided with each split to obtain the triplet embedding which takes 3 mins per split . This is the only additional splitwise processing that is done by the proposed approach . During deployment , the average enrollment time per image after pre - processing , including alignment and feature extraction is 8ms . subsection : Evaluation Pipeline Given an image , we pre - process it as described in Section 5.1 . The deep features are computed as an average of the image and its flip . Given two deep features to compare , we compute their cosine similarity score . More specifically , for the IJB - A dataset , given a template containing multiple faces , we flatten the template features by average pooling or media pooling to obtain a vector representation . For each split , we learn the TPE projection using the provided training data . Given two templates for comparison , we compute the cosine similarity score using the projected 128 - dimensional representations . matrix . subsection : Evaluation Metrics We report two types of results for the IJB - A dataset : Verification and Identification . For the verification protocol , we report the False Non - Match Rate ( FNMR ) values at several False Match Rates ( FMR ) . For the identification results , we report open set and closed set metrics . For the open set metrics , the True Positive Identification Rate quantifies the fraction of subjects that are classified correctly among the ones that exist in probe but not in gallery . For the closed set metrics , we report the CMC numbers at different values of False Positive Identification Rates ( FPIRs ) and Ranks . More details on the evaluation metrics for the IJB - A protocol can be found in . For the CFP dataset , following the protocol set in , we report the Area under the curve ( AUC ) and Equal Error Rate ( EER ) values as averages across splits , in addition to the classification accuracy . To obtain the accuracy for each split , we threshold our CNN similarity scores where the threshold is set to the value that provides the highest classification accuracy over the training data for each split . subsection : Discussion subsubsection : Performance on IJB - A Table [ reference ] presents the results for the proposed methods compared to existing results for the IJB - A Verification and Identification protocol . The compared methods are described below : Government - of - the - Shelf ( GOTS ) is the baseline performance provided along with the IJB - A dataset . Parkhi et al . train a very deep network ( 22 layers ) over the VGG - Face dataset which contains 2.6 M images from 2622 subjects . The Neural Aggregation network ( NAN ) is trained over large amount of videos from the CELEB - 1000 dataset starting from the GoogleNet architecture . Masi et al . use a deep CNN based approach that includes a combination of in - plane aligned images , 3D rendered images to augment their performance . The 3D rendered images are also generated during test time per template comparison . It should be noted that many test images of the IJB - A dataset contain extreme poses , harsh illumination conditions and significant blur . Crosswhite et al . use template adaptation to tune the performance of their raw features specifically to the IJB - A dataset . Compared to these methods , the proposed method trains a single CNN model on the CASIA - WebFace dataset which consists of about 500 K images and requires much shorter training time and has a very fast query time ( 0.08s after face detection per image pair ) . As shown in Table [ reference ] , our raw CNN features after media pooling perform better than most compared methods across both the verification and identification protocols of the IJB - A dataset , with the exception of the template adaptation method by Crosswhite et al . which is discussed below . The TPE method provides significant improvement for both identification and verification tasks as shown in Table [ reference ] . The method by Crosswhite et al . uses the VGG - Face network descriptors ( 4096 - d ) as the raw features . They use the concept of template adaptation to improve their performance as follows : when pooling multiple faces of a given template , they train a linear SVM with the features of this template as positive and a fixed set of negatives extracted from the training data of the IJB - A splits . Let \u2019s denote the pooled template feature and classifier pair as . Then , at query time when comparing two templates and , the similarity score is computed as : . Even when using a carefully engineered fast linear classifier training algorithm , this procedure increases the run time of the pooling procedure . The query time per template comparison is also higher due to the high dimensionality of the input features . In contrast , the proposed approach requires a matrix multiplication and a vector dot product per comparison . By using a simple neural network architecture , a relatively smaller training dataset and a fast embedding method we have realized a faster and more efficient end - to - end system . To improve our performance further , we are currently incorporating the use of video data into our approach . subsubsection : Performance on CFP On the CFP dataset , we achieve a new state - of - art on both Frontal - Frontal and Frontal - Profile comparisons , the latter by a large margin . More specifically , for the Frontal - Profile case , we manage to reduce the error rate by 40.8 % . It should be noted that for a fair comparison we have used our raw CNN features without performing TPE . This shows that the raw CNN features we learn are effective even at extreme pose variations . section : Clustering Faces .5 .5 .5 .5 This section illustrates how the proposed TPE method can be used to cluster a given data collection . We perform two clustering experiments : We perform clustering on the entire LFW dataset that consists of 13233 images of 5749 subjects . It should be noted that about 4169 subjects have only one image . We use the IJB - A dataset and cluster the templates corresponding to the query set for each split in the IJB - A verify protocol . For evaluating the clustering results , we use the metrics defined in . These are summarized below : Pairwise Precision ( P\u2062pair ) : The fraction of pairs of samples within a cluster among all possible pairs which are of the same class , over the total number of same cluster pairs . Pairwise Recall ( R\u2062pair ) : The fraction of pairs of samples within a class among all possible pairs which are placed in the same cluster , over the total number of same - class pairs . Using these metrics , the F - score is computed as : The simplest way we found to demonstrate the effectiveness of our deep features and the proposed TPE method , is to use the standard MATLAB implementation of the agglomerative clustering algorithm with the average linkage metric . We use the cosine similarity as our basic clustering metric . The simple clustering algorithm that we have used here has computational complexity of . In its current form , this does not scale to large datasets with millions of images . We are currently working on a more efficient and scalable ( yet approximate ) version of this algorithm . paragraph : Clustering LFW: - The images in the LFW dataset are pre - processed as described in Section 5.1 . For each image and its flip , the deep features are extracted using the proposed architecture , averaged and normalized to unit norm . We run the clustering algorithm over the entire data in a single shot . The clustering algorithm takes as input a cut - off parameter which acts as a distance threshold ( below which any two clusters will not be merged ) . In our experiments , we vary this cut - off parameter over a small range and evaluate the resulting clustering using the - score . We pick the result that yields the best - score . Table [ reference ] shows the result of our approach and compares it to a recently released clustering approach based on approximate Rank - order clustering . It should be noted that , in the case of , the clustering result is chosen by varying the number of clusters and picking the one with the best - score . In our approach , we vary the cut - off threshold which is the property of deep features and hence is a more intuitive parameter to tune . We see from Table [ reference ] that aside from better performance , our total cluster estimate is closer to the ground truth value of 5749 than . table - score for comparison of the two clustering schemes on the LFW dataset . The ground truth cluster number is 5749 . tableClustering metrics over the IJB - A 1:1 protocol . The standard deviation is indicated in brackets . The ground truth subjects per each split is 167 . paragraph : Clustering IJB - A: - The IJB - A dataset is processed as described in Section 5 . In this section , we aim to cluster the query templates provided with each split for the verify protocol . We report the results of two experiments : with the raw CNN features ( CNN in Table 2 ) and with the projected CNN features , where the projection matrix is learned through the proposed TPE method ( CNN + TPE in Table 2 ) . The cut - off threshold required for our clustering algorithm is learned automatically based on the training data , i.e. we choose the threshold that gives the maximum - score over the training data . The scores reported in Table [ reference ] are average values over ten splits . As expected , the TPE method improves the clustering performance of raw features . The subject estimate is the number of clusters produced as a direct result of our clustering algorithm . The pruned estimate is obtained by ignoring clusters that have fewer than 3 images . For a more complete evaluation of our performance over varying threshold values , we plot the Precision - Recall ( PR ) curve for the IJB - A clustering experiment in Figure [ reference ] . As can be observed , the PR curve for clustering the IJB - A data using embedded features exhibits a better performance at all operating points . This is a more transparent evaluation than reporting only the - score since the latter effectively fixes the operating point but the PR curve reveals the performance at all operating points . section : Conclusion and Future Work In this paper , we proposed a deep CNN - based approach coupled with a low - dimensional discriminative embedding learned using triplet probability constraints in a large margin fashion . The proposed pipeline enables a faster training time and improves face verification performance especially at low FMRs . We demonstrated the effectiveness of the proposed method on two challenging datasets : IJB - A and CFP and achieved performance close to the state of the art while using a deep model which is more compact and trained using a moderately sized dataset . We demonstrated the robustness of our features using a simple clustering algorithm on the LFW and IJB - A datasets . For future work , we plan to use videos directly during training and also embed our TPE approach into training the deep network . We intend to scale our clustering algorithm to handle large scale scenarios such as large impostor sets of the order of millions . section : Acknowledgement This research is based upon work supported by the Office of the Director of National Intelligence ( ODNI ) , Intelligence Advanced Research Projects Activity ( IARPA ), via IARPA R & D Contract No . 2014 - 14071600012 . The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied , of the ODNI , IARPA , or the U.S. Government . The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon . bibliography : References", "templates": [{"incident_type": "SciREX_incident", "Material": [[["IJB-A"]]], "Method": [[["Triplet_probabilistic_embedding"]]], "Metric": [[["TAR___FAR_0_01"]]], "Task": [[["Face_Verification"]]]}]}
{"docid": "TST3-SREX-0064", "doctext": "document : High - Resolution Image Synthesis and Semantic Manipulation with Conditional GANs We present a new method for synthesizing high - resolution photo - realistic images from semantic label maps using conditional generative adversarial networks ( conditional GANs ) . Conditional GANs have enabled a variety of applications , but the results are often limited to low - resolution and still far from realistic . In this work , we generate visually appealing results with a novel adversarial loss , as well as new multi - scale generator and discriminator architectures . Furthermore , we extend our framework to interactive visual manipulation with two additional features . First , we incorporate object instance segmentation information , which enables object manipulations such as removing / adding objects and changing the object category . Second , we propose a method to generate diverse results given the same input , allowing users to edit the object appearance interactively . Human opinion studies demonstrate that our method significantly outperforms existing methods , advancing both the quality and the resolution of deep image synthesis and editing . section : Introduction Photo - realistic image rendering using standard graphics techniques is involved , since geometry , materials , and light transport must be simulated explicitly . Although existing graphics algorithms excel at the task , building and editing virtual environments is expensive and time - consuming . That is because we have to model every aspect of the world explicitly . If we were able to render photo - realistic images using a model learned from data , we could turn the process of graphics rendering into a model learning and inference problem . Then , we could simplify the process of creating new virtual worlds by training models on new datasets . We could even make it easier to customize environments by allowing users to simply specify overall semantic structure rather than modeling geometry , materials , or lighting . In this paper , we discuss a new approach that produces high - resolution images from semantic label maps . This method has a wide range of applications . For example , we can use it to create synthetic training data for training visual recognition algorithms , since it is much easier to create semantic labels for desired scenarios than to generate training images . Using semantic segmentation methods , we can transform images into a semantic label domain , edit the objects in the label domain , and then transform them back to the image domain . This method also gives us new tools for higher - level image editing , e.g. , adding objects to images or changing the appearance of existing objects . To synthesize images from semantic labels , one can use the pix2pix method , an image - to - image translation framework which leverages generative adversarial networks ( GANs ) in a conditional setting . Recently , Chen and Koltun suggest that adversarial training might be unstable and prone to failure for high - resolution image generation tasks . Instead , they adopt a modified perceptual loss to synthesize images , which are high - resolution but often lack fine details and realistic textures . Here we address two main issues of the above state - of - the - art methods : ( 1 ) the difficulty of generating high - resolution images with GANs and ( 2 ) the lack of details and realistic textures in the previous high - resolution results . We show that through a new , robust adversarial learning objective together with new multi - scale generator and discriminator architectures , we can synthesize photo - realistic images at resolution , which are more visually appealing than those computed by previous methods . We first obtain our results with adversarial training only , without relying on any hand - crafted losses or pre - trained networks ( e.g. VGGNet ) for perceptual losses ( Figs . [ reference ] c , [ reference ] b ) . Then we show that adding perceptual losses from pre - trained networks can slightly improve the results in some circumstances ( Figs . [ reference ] d , [ reference ] c ) , if a pre - trained network is available . Both results outperform previous works substantially in terms of image quality . Furthermore , to support interactive semantic manipulation , we extend our method in two directions . First , we use instance - level object segmentation information , which can separate different object instances within the same category . This enables flexible object manipulations , such as adding / removing objects and changing object types . Second , we propose a method to generate diverse results given the same input label map , allowing the user to edit the appearance of the same object interactively . We compare against state - of - the - art visual synthesis systems , and show that our method outperforms these approaches regarding both quantitative evaluations and human perception studies . We also perform an ablation study regarding the training objectives and the importance of instance - level segmentation information . In addition to semantic manipulation , we test our method on edge2photo applications ( Figs . [ reference ] , [ reference ] ) , which shows the generalizability of our approach . Code and data are available at our . section : Related Work paragraph : Generative adversarial networks Generative adversarial networks ( GANs ) aim to model the natural image distribution by forcing the generated samples to be indistinguishable from natural images . GANs enable a wide variety of applications such as image generation , representation learning , image manipulation , object detection , and video applications . Various coarse - to - fine schemes have been proposed to synthesize larger images ( e.g. ) in an unconditional setting . Inspired by their successes , we propose a new coarse - to - fine generator and multi - scale discriminator architectures suitable for conditional image generation at a much higher resolution . paragraph : Image - to - image translation Many researchers have leveraged adversarial learning for image - to - image translation , whose goal is to translate an input image from one domain to another domain given input - output image pairs as training data . Compared to loss , which often leads to blurry images , the adversarial loss has become a popular choice for many image - to - image tasks . The reason is that the discriminator can learn a trainable loss function and automatically adapt to the differences between the generated and real images in the target domain . For example , the recent pix2pix framework used image - conditional GANs for different applications , such as transforming Google maps to satellite views and generating cats from user sketches . Various methods have also been proposed to learn an image - to - image translation in the absence of training pairs . Recently , Chen and Koltun suggest that it might be hard for conditional GANs to generate high - resolution images due to the training instability and optimization issues . To avoid this difficulty , they use a direct regression objective based on a perceptual loss and produce the first model that can synthesize images . The generated results are high - resolution but often lack fine details and realistic textures . Motivated by their success , we show that using our new objective function as well as novel multi - scale generators and discriminators , we not only largely stabilize the training of conditional GANs on high - resolution images , but also achieve significantly better results compared to Chen and Koltun . Side - by - side comparisons clearly show our advantage ( Figs . [ reference ] , [ reference ] , [ reference ] , [ reference ] ) . paragraph : Deep visual manipulation Recently , deep neural networks have obtained promising results in various image processing tasks , such as style transfer , inpainting , colorization , and restoration . However , most of these works lack an interface for users to adjust the current result or explore the output space . To address this issue , Zhu developed an optimization method for editing the object appearance based on the priors learned by GANs . Recent works also provide user interfaces for creating novel imagery from low - level cues such as color and sketch . All of the prior works report results on low - resolution images . Our system shares the same spirit as this past work , but we focus on object - level semantic editing , allowing users to interact with the entire scene and manipulate individual objects in the image . As a result , users can quickly create a new scene with minimal effort . Our interface is inspired by prior data - driven graphics systems . But our system allows more flexible manipulations and produces high - res results in real - time . section : Instance - Level Image Synthesis We propose a conditional adversarial framework for generating high - resolution photo - realistic images from semantic label maps . We first review our baseline model pix2pix ( Sec . [ reference ] ) . We then describe how we increase the photo - realism and resolution of the results with our improved objective function and network design ( Sec . [ reference ] ) . Next , we use additional instance - level object semantic information to further improve the image quality ( Sec . [ reference ] ) . Finally , we introduce an instance - level feature embedding scheme to better handle the multi - modal nature of image synthesis , which enables interactive object editing ( Sec . [ reference ] ) . subsection : The pix2pix Baseline The pix2pix method is a conditional GAN framework for image - to - image translation . It consists of a generator and a discriminator . For our task , the objective of the generator is to translate semantic label maps to realistic - looking images , while the discriminator aims to distinguish real images from the translated ones . The framework operates in a supervised setting . In other words , the training dataset is given as a set of pairs of corresponding images , where is a semantic label map and is a corresponding natural photo . Conditional GANs aim to model the conditional distribution of real images given the input semantic label maps via the following minimax game : where the objective function is given by The pix2pix method adopts U - Net as the generator and a patch - based fully convolutional network as the discriminator . The input to the discriminator is a channel - wise concatenation of the semantic label map and the corresponding image . However , the resolution of the generated images on Cityscapes is up to . We tested directly applying the pix2pix framework to generate high - resolution images but found the training unstable and the quality of generated images unsatisfactory . Therefore , we describe how we improve the pix2pix framework in the next subsection . subsection : Improving Photorealism and Resolution We improve the pix2pix framework by using a coarse - to - fine generator , a multi - scale discriminator architecture , and a robust adversarial learning objective function . Coarse - to - fine generator We decompose the generator into two sub - networks : and . We term as the global generator network and as the local enhancer network . The generator is then given by the tuple as visualized in Fig . [ reference ] . The global generator network operates at a resolution of , and the local enhancer network outputs an image with a resolution that is the output size of the previous one ( along each image dimension ) . For synthesizing images at an even higher resolution , additional local enhancer networks could be utilized . For example , the output image resolution of the generator is , and the output image resolution of is . Our global generator is built on the architecture proposed by Johnson , which has been proven successful for neural style transfer on images up to . It consists of components : a convolutional front - end , a set of residual blocks , and a transposed convolutional back - end . A semantic label map of resolution is passed through the 3 components sequentially to output an image of resolution . The local enhancer network also consists of 3 components : a convolutional front - end , a set of residual blocks , and a transposed convolutional back - end . The resolution of the input label map to is . Different from the global generator network , the input to the residual block is the element - wise sum of two feature maps : the output feature map of , and the last feature map of the back - end of the global generator network . This helps integrating the global information from to . During training , we first train the global generator and then train the local enhancer in the order of their resolutions . We then jointly fine - tune all the networks together . We use this generator design to effectively aggregate global and local information for the image synthesis task . We note that such a multi - resolution pipeline is a well - established practice in computer vision and two - scale is often enough . Similar ideas but different architectures could be found in recent unconditional GANs and conditional image generation . Multi - scale discriminators High - resolution image synthesis poses a significant challenge to the GAN discriminator design . To differentiate high - resolution real and synthesized images , the discriminator needs to have a large receptive field . This would require either a deeper network or larger convolutional kernels , both of which would increase the network capacity and potentially cause overfitting . Also , both choices demand a larger memory footprint for training , which is already a scarce resource for high - resolution image generation . To address the issue , we propose using multi - scale discriminators . We use discriminators that have an identical network structure but operate at different image scales . We will refer to the discriminators as , and . Specifically , we downsample the real and synthesized high - resolution images by a factor of and to create an image pyramid of 3 scales . The discriminators , and are then trained to differentiate real and synthesized images at the different scales , respectively . Although the discriminators have an identical architecture , the one that operates at the coarsest scale has the largest receptive field . It has a more global view of the image and can guide the generator to generate globally consistent images . On the other hand , the discriminator at the finest scale encourages the generator to produce finer details . This also makes training the coarse - to - fine generator easier , since extending a low - resolution model to a higher resolution only requires adding a discriminator at the finest level , rather than retraining from scratch . Without the multi - scale discriminators , we observe that many repeated patterns often appear in the generated images . With the discriminators , the learning problem in Eq . ( [ reference ] ) then becomes a multi - task learning problem of Using multiple GAN discriminators at the same image scale has been proposed in unconditional GANs . Iizuka et al . add a global image classifier to conditional GANs to synthesize globally coherent content for inpainting . Here we extend the design to multiple discriminators at different image scales for modeling high - resolution images . Improved adversarial loss We improve the GAN loss in Eq . ( [ reference ] ) by incorporating a feature matching loss based on the discriminator . This loss stabilizes the training as the generator has to produce natural statistics at multiple scales . Specifically , we extract features from multiple layers of the discriminator and learn to match these intermediate representations from the real and the synthesized image . For ease of presentation , we denote the th - layer feature extractor of discriminator as ( from input to the th layer of ) . The feature matching loss is then calculated as : where is the total number of layers and denotes the number of elements in each layer . Our GAN discriminator feature matching loss is related to the perceptual loss , which has been shown to be useful for image super - resolution and style transfer . In our experiments , we discuss how the discriminator feature matching loss and the perceptual loss can be jointly used for further improving the performance . We note that a similar loss is used in VAE - GANs . Our full objective combines both GAN loss and feature matching loss as : where controls the importance of the two terms . Note that for the feature matching loss , only serves as a feature extractor and does not maximize the loss . subsection : Using Instance Maps Existing image synthesis methods only utilize semantic label maps , an image where each pixel value represents the object class of the pixel . This map does not differentiate objects of the same category . On the other hand , an instance - level semantic label map contains a unique object ID for each individual object . To incorporate the instance map , one can directly pass it into the network , or encode it into a one - hot vector . However , both approaches are difficult to implement in practice , since different images may contain different numbers of objects of the same category . Alternatively , one can pre - allocate a fixed number of channels ( e.g. , ) for each class , but this method fails when the number is set too small , and wastes memory when the number is too large . Instead , we argue that the most critical information the instance map provides , which is not available in the semantic label map , is the object boundary . For example , when objects of the same class are next to one another , looking at the semantic label map alone can not tell them apart . This is especially true for the street scene since many parked cars or walking pedestrians are often next to one another , as shown in Fig . [ reference ] a. However , with the instance map , separating these objects becomes an easier task . Therefore , to extract this information , we first compute the instance boundary map ( Fig . [ reference ] b ) . In our implementation , a pixel in the instance boundary map is if its object ID is different from any of its - neighbors , and otherwise . The instance boundary map is then concatenated with the one - hot vector representation of the semantic label map , and fed into the generator network . Similarly , the input to the discriminator is the channel - wise concatenation of instance boundary map , semantic label map , and the real / synthesized image . Figure [ reference ] b shows an example demonstrating the improvement by using object boundaries . Our user study in Sec . [ reference ] also shows the model trained with instance boundary maps renders more photo - realistic object boundaries . subsection : Learning an Instance - level Feature Embedding Image synthesis from semantic label maps is a one - to - many mapping problem . An ideal image synthesis algorithm should be able to generate diverse , realistic images using the same semantic label map . Recently , several works learn to produce a fixed number of discrete outputs given the same input or synthesize diverse modes controlled by a latent code that encodes the entire image . Although these approaches tackle the multi - modal image synthesis problem , they are unsuitable for our image manipulation task mainly for two reasons . First , the user has no intuitive control over which kinds of images the model would produce . Second , these methods focus on global color and texture changes and allow no object - level control on the generated contents . To generate diverse images and allow instance - level control , we propose adding additional low - dimensional feature channels as the input to the generator network . We show that , by manipulating these features , we can have flexible control over the image synthesis process . Furthermore , note that since the feature channels are continuous quantities , our model is , in principle , capable of generating infinitely many images . To generate the low - dimensional features , we train an encoder network to find a low - dimensional feature vector that corresponds to the ground truth target for each instance in the image . Our feature encoder architecture is a standard encoder - decoder network . To ensure the features are consistent within each instance , we add an instance - wise average pooling layer to the output of the encoder to compute the average feature for the object instance . The average feature is then broadcast to all the pixel locations of the instance . Figure [ reference ] visualizes an example of the encoded features . We replace with in Eq . ( [ reference ] ) and train the encoder jointly with the generators and discriminators . After the encoder is trained , we run it on all instances in the training images and record the obtained features . Then we perform a - means clustering on these features for each semantic category . Each cluster thus encodes the features for a specific style , for example , the asphalt or cobblestone texture for a road . At inference time , we randomly pick one of the cluster centers and use it as the encoded features . These features are concatenated with the label map and used as the input to our generator . We tried to enforce the Kullback - Leibler loss on the feature space for better test - time sampling as used in the recent work but found it quite involved for users to adjust the latent vectors for each object directly . Instead , for each object instance , we present modes for users to choose from . section : Results We first provide a quantitative comparison against leading methods in Sec . [ reference ] . We then report a subjective human perceptual study in Sec . [ reference ] . Finally , we show a few examples of interactive object editing results in Sec . [ reference ] . Implementation details We use LSGANs for stable training . In all experiments , we set the weight ( Eq . ( [ reference ] ) ) and for K - means . We use - dimensional vectors to encode features for each object instance . We experimented with adding a perceptual loss to our objective ( Eq . ( [ reference ] ) ) , where and denotes the - th layer with elements of the VGG network . We observe that this loss slightly improves the results . We name these two variants as ours and ours ( w / o VGG loss ) . Please find more training and architecture details in the appendix . Datasets We conduct extensive comparisons and ablation studies on Cityscapes dataset and NYU Indoor RGBD dataset . We report additional qualitative results on ADE20 K dataset and Helen Face dataset . Baselines We compare our method with two state - of - the - art algorithms : pix2pix and CRN . We train pix2pix models on high - res images with the default setting . We produce the high - res CRN images via the authors \u2019 publicly available model . subsection : Quantitative Comparisons We adopt the same evaluation protocol from previous image - to - image translation works . To quantify the quality of our results , we perform semantic segmentation on the synthesized images and compare how well the predicted segments match the input . The intuition is that if we can produce realistic images that correspond to the input label map , an off - the - shelf semantic segmentation model ( e.g. , PSPNet that we use ) should be able to predict the ground truth label . Table [ reference ] reports the calculated segmentation accuracy . As can be seen , for both pixel - wise accuracy and mean intersection - over - union ( IoU ) , our method outperforms the other methods by a large margin . Moreover , our result is very close to the result of the original images , the theoretical \u201c upper bound \u201d of the realism we can achieve . This justifies the superiority of our algorithm . subsection : Human Perceptual Study We further evaluate our algorithm via a human subjective study . We perform pairwise A / B tests deployed on the Amazon Mechanical Turk ( MTurk ) platform on the Cityscapes dataset . We follow the same experimental procedure as described in Chen and Koltun . More specifically , two different kinds of experiments are conducted : unlimited time and limited time , as explained below . Unlimited time For this task , workers are given two images at once , each of which is synthesized by a different method for the same label map . We give them unlimited time to select which image looks more natural . The left - right order and the image order are randomized to ensure fair comparisons . All Cityscapes test images are compared times , resulting in human judgments for each method . In this experiment , we use the model trained on labels only ( without instance maps ) to ensure a fair comparison . Table [ reference ] shows that both variants of our method outperform the other methods significantly . Limited time Next , for the limited time experiment , we compare our result with CRN and the original image ( ground truth ) . In each comparison , we show results of two methods for a short period of time . We randomly select a duration between seconds and seconds , as adopted by prior work . This evaluates how quickly the difference between the images can be perceived . Fig . [ reference ] shows the comparison results at different time intervals . As the given time becomes longer and longer , the differences between these three types of images become more apparent and easier to observe . Figures [ reference ] and [ reference ] show some example results . Analysis of the loss function We also study the importance of each term in our objective function using the unlimited time experiment . Specifically , our final loss contains three components : GAN loss , discriminator - based feature matching loss , and VGG perceptual loss . We compare our final implementation to the results using ( ) only GAN loss , and ( ) GAN feature matching loss ( i.e. , without VGG loss ) . The obtained preference rates are and , respectively . As can be seen , adding the feature matching loss substantially improves the performance , while adding perceptual loss further enhances the results . However , note that using the perceptual loss is not critical , and we are still able to generate visually appealing results even without it ( e.g. , Figs . [ reference ] c , [ reference ] b ) . Using instance maps We compare the results using instance maps to results without using them . We highlight the car regions in the images and ask the participants to choose which region looks more realistic . We obtain a preference rate of , which indicates that using instance maps improves the realism of our results , especially around the object boundaries . Analysis of the generator We compare results of different generators with all the other components fixed . In particular , we compare our generator with two state - of - the - art generator architectures : U - Net and CRN . We evaluate the performance regarding both semantic segmentation scores and human perceptual study results . Table [ reference ] and Table [ reference ] show that our coarse - to - fine generator outperforms other networks by a large margin . Analysis of the discriminator Next , we also compare results using our multi - scale discriminators and results using only one discriminator while we keep the generator and the loss function fixed . The segmentation scores on Cityscapes ( Table [ reference ] ) demonstrate that using multi - scale discriminators helps produce higher quality results as well as stabilize the adversarial training . We also perform pairwise A / B tests on the Amazon Mechanical Turk platform . of the participants prefer our results with multi - scale discriminators over the results trained with a single - scale discriminator ( Chance is ) . Additional datasets To further evaluate our method , we perform unlimited time comparisons on the NYU dataset . We obtain and against pix2pix and CRN , respectively . Fig . [ reference ] show some example images . Finally , we show results on the ADE20 K dataset ( Fig . [ reference ] ) . subsection : Interactive Object Editing Our feature encoder allows us to perform interactive instance editing on the resulting images . For example , we can change the object labels in the image to quickly create novel scenes , such as replacing trees with buildings ( Fig . [ reference ] b ) . We can also change the colors of individual cars or the textures of the road ( Fig . [ reference ] c ) . Please check out our interactive demos on our website . Besides , we implement our interactive object editing feature on the Helen Face dataset where labels for different facial parts are available ( Fig . [ reference ] ) . This makes it easy to edit human portraits , e.g. , changing the face color to mimic different make - up effects or adding beard to a face . section : Discussion and Conclusion The results in this paper suggest that conditional GANs can synthesize high - resolution photo - realistic imagery without any hand - crafted losses or pre - trained networks . We have observed that incorporating a perceptual loss can slightly improve the results . Our method allows many applications and will be potentially useful for domains where high - resolution results are in demand but pre - trained networks are not available ( e.g. , medical imaging and biology ) . This paper also shows that an image - to - image synthesis pipeline can be extended to produce diverse outputs , and enable interactive image manipulation given appropriate training input - output pairs ( e.g. , instance maps in our case ) . Without ever been told what a \u201c texture \u201d is , our model learns to stylize different objects , which may be generalized to other datasets as well ( i.e. , using textures in one dataset to synthesize images in another dataset ) . We believe these extensions can be potentially applied to other image synthesis problems . Acknowledgements We thank Taesung Park , Phillip Isola , Tinghui Zhou , Richard Zhang , Rafael Valle and Alexei A. Efros for helpful comments . We also thank Chen and Koltun and Isola et al . for sharing their code . JYZ is supported by a Facebook graduate fellowship . bibliography : References appendix : Training Details All the networks were trained from scratch , using the Adam solver and a learning rate of . We keep the same learning rate for the first epochs and linearly decay the rate to zero over the next epochs . Weights were initialized from a Gaussian distribution with mean and standard deviation . We train all our models on an NVIDIA Quadro M6000 GPU with GB GPU memory . The inference time is between milliseconds per input image on an NVIDIA 1080Ti GPU with GB GPU memory . This real - time performance allows us to develop interactive image editing applications . Below we discuss the details of the datasets we used . Cityscapes dataset [ ] : training images from the Cityscapes training set with image size . We use the Cityscapes validation set for testing , which consists of 500 images . NYU Indoor RGBD dataset [ ] : training images and test images , all at resolution of . ADE20 K dataset [ ] : training images and test images with varying image sizes . We scale the width of all images to before training and inference . Helen Face dataset [ ] : training images and test images with varying image sizes . We resize all images to before training and inference . appendix : Generator Architectures Our generator consists of a global generator network and a local enhancer network . we follow the naming convention used in Johnson el al . and CycleGAN . Let c7s1 - k denote a Convolution - InstanceNorm - ReLU layer with filters and stride . dk denotes a Convolution - InstanceNorm - ReLU layer with filters , and stride . We use reflection padding to reduce boundary artifacts . Rk denotes a residual block that contains two convolutional layers with the same number of filters on both layers . uk denotes a fractional - strided - Convolution - InstanceNorm - ReLU layer with filters , and stride . Recall that we have two generators : the global generator and the local enhancer . Our global network : c7s1 - 64 , d128 , d256 , d512 , d1024 , R1024 , R1024 , R1024 , R1024 , R1024 , R1024 , R1024 , R1024 , R1024 , u512 , u256 , u128 , u64 , c7s1 - 3 Our local enhancer : c7s1 - 32 , d64We add the last feature map ( u64 ) in our global network to the output of this layer. , R64 , R64 , R64 , u32 , c7s1 - 3 appendix : Discriminator Architectures For discriminator networks , we use PatchGAN . Let Ck denote a Convolution - InstanceNorm - LeakyReLU layer with k filters and stride . After the last layer , we apply a convolution to produce a dimensional output . We do not use InstanceNorm for the first C64 layer . We use leaky ReLUs with slope . All our three discriminators have the identical architecture as follows : C64 - C128 - C256 - C512 appendix : Change log paragraph : v1 initial preprint release paragraph : v2 CVPR camera ready , adding more results for edge - to - photo examples .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["ADE20K_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["Accuracy"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ADE20K_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["FID"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ADE20K_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["mIoU"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ADE20K-Outdoor_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["Accuracy"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ADE20K-Outdoor_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["FID"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["ADE20K-Outdoor_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["mIoU"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["COCO-Stuff_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["Accuracy"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["COCO-Stuff_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["FID"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["COCO-Stuff_Labels-to-Photos"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["mIoU"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes_Labels-to-Photo"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["Class_IOU"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes_Labels-to-Photo"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["FID"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes_Labels-to-Photo"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["Per-class_Accuracy"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes_Labels-to-Photo"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["Per-pixel_Accuracy"]]], "Task": [[["Image-to-Image_Translation"]]]}, {"incident_type": "SciREX_incident", "Material": [[["Cityscapes_Labels-to-Photo"]]], "Method": [[["pix2pixHD"]]], "Metric": [[["mIoU"]]], "Task": [[["Image-to-Image_Translation"]]]}]}
{"docid": "TST3-SREX-0065", "doctext": "document : Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition Existing deep convolutional neural networks ( CNNs ) require a fixed - size ( , 224 224 ) input image . This requirement is \u201c artificial \u201d and may reduce the recognition accuracy for the images or sub - images of an arbitrary size / scale . In this work , we equip the networks with another pooling strategy , \u201c spatial pyramid pooling \u201d , to eliminate the above requirement . The new network structure , called SPP - net , can generate a fixed - length representation regardless of image size / scale . Pyramid pooling is also robust to object deformations . With these advantages , SPP - net should in general improve all CNN - based image classification methods . On the ImageNet 2012 dataset , we demonstrate that SPP - net boosts the accuracy of a variety of CNN architectures despite their different designs . On the Pascal VOC 2007 and Caltech101 datasets , SPP - net achieves state - of - the - art classification results using a single full - image representation and no fine - tuning . The power of SPP - net is also significant in object detection . Using SPP - net , we compute the feature maps from the entire image only once , and then pool features in arbitrary regions ( sub - images ) to generate fixed - length representations for training the detectors . This method avoids repeatedly computing the convolutional features . In processing test images , our method is 24 - 102 faster than the R - CNN method , while achieving better or comparable accuracy on Pascal VOC 2007 . In ImageNet Large Scale Visual Recognition Challenge ( ILSVRC ) 2014 , our methods rank # 2 in object detection and # 3 in image classification among all 38 teams . This manuscript also introduces the improvement made for this competition . onvolutional Neural Networks , Spatial Pyramid Pooling , Image Classification , Object Detection section : Introduction We are witnessing a rapid , revolutionary change in our vision community , mainly caused by deep convolutional neural networks ( CNNs ) and the availability of large scale training data . Deep - networks - based approaches have recently been substantially improving upon the state of the art in image classification , object detection , many other recognition tasks , and even non - recognition tasks . However , there is a technical issue in the training and testing of the CNNs : the prevalent CNNs require a fixed input image size ( , 224 224 ) , which limits both the aspect ratio and the scale of the input image . When applied to images of arbitrary sizes , current methods mostly fit the input image to the fixed size , either via cropping or via warping , as shown in Figure [ reference ] ( top ) . But the cropped region may not contain the entire object , while the warped content may result in unwanted geometric distortion . Recognition accuracy can be compromised due to the content loss or distortion . Besides , a pre - defined scale may not be suitable when object scales vary . Fixing input sizes overlooks the issues involving scales . So why do CNNs require a fixed input size ? A CNN mainly consists of two parts : convolutional layers , and fully - connected layers that follow . The convolutional layers operate in a sliding - window manner and output feature maps which represent the spatial arrangement of the activations ( Figure [ reference ] ) . In fact , convolutional layers do not require a fixed image size and can generate feature maps of any sizes . On the other hand , the fully - connected layers need to have fixed - size / length input by their definition . Hence , the fixed - size constraint comes only from the fully - connected layers , which exist at a deeper stage of the network . In this paper , we introduce a spatial pyramid pooling ( SPP ) layer to remove the fixed - size constraint of the network . Specifically , we add an SPP layer on top of the last convolutional layer . The SPP layer pools the features and generates fixed - length outputs , which are then fed into the fully - connected layers ( or other classifiers ) . In other words , we perform some information \u201c aggregation \u201d at a deeper stage of the network hierarchy ( between convolutional layers and fully - connected layers ) to avoid the need for cropping or warping at the beginning . Figure [ reference ] ( bottom ) shows the change of the network architecture by introducing the SPP layer . We call the new network structure SPP - net . Spatial pyramid pooling ( popularly known as spatial pyramid matching or SPM ) , as an extension of the Bag - of - Words ( BoW ) model , is one of the most successful methods in computer vision . It partitions the image into divisions from finer to coarser levels , and aggregates local features in them . SPP has long been a key component in the leading and competition - winning systems for classification ( , ) and detection ( , ) before the recent prevalence of CNNs . Nevertheless , SPP has not been considered in the context of CNNs . We note that SPP has several remarkable properties for deep CNNs : 1 ) SPP is able to generate a fixed - length output regardless of the input size , while the sliding window pooling used in the previous deep networks can not ; 2 ) SPP uses multi - level spatial bins , while the sliding window pooling uses only a single window size . Multi - level pooling has been shown to be robust to object deformations ; 3 ) SPP can pool features extracted at variable scales thanks to the flexibility of input scales . Through experiments we show that all these factors elevate the recognition accuracy of deep networks . SPP - net not only makes it possible to generate representations from arbitrarily sized images / windows for testing , but also allows us to feed images with varying sizes or scales during training . Training with variable - size images increases scale - invariance and reduces over - fitting . We develop a simple multi - size training method . For a single network to accept variable input sizes , we approximate it by multiple networks that share all parameters , while each of these networks is trained using a fixed input size . In each epoch we train the network with a given input size , and switch to another input size for the next epoch . Experiments show that this multi - size training converges just as the traditional single - size training , and leads to better testing accuracy . The advantages of SPP are orthogonal to the specific CNN designs . In a series of controlled experiments on the ImageNet 2012 dataset , we demonstrate that SPP improves four different CNN architectures in existing publications ( or their modifications ) , over the no - SPP counterparts . These architectures have various filter numbers / sizes , strides , depths , or other designs . It is thus reasonable for us to conjecture that SPP should improve more sophisticated ( deeper and larger ) convolutional architectures . SPP - net also shows state - of - the - art classification results on Caltech101 and Pascal VOC 2007 using only a single full - image representation and no fine - tuning . SPP - net also shows great strength in object detection . In the leading object detection method R - CNN , the features from candidate windows are extracted via deep convolutional networks . This method shows remarkable detection accuracy on both the VOC and ImageNet datasets . But the feature computation in R - CNN is time - consuming , because it repeatedly applies the deep convolutional networks to the raw pixels of thousands of warped regions per image . In this paper , we show that we can run the convolutional layers only once on the entire image ( regardless of the number of windows ) , and then extract features by SPP - net on the feature maps . This method yields a speedup of over one hundred times over R - CNN . Note that training / running a detector on the feature maps ( rather than image regions ) is actually a more popular idea . But SPP - net inherits the power of the deep CNN feature maps and also the flexibility of SPP on arbitrary window sizes , which leads to outstanding accuracy and efficiency . In our experiment , the SPP - net - based system ( built upon the R - CNN pipeline ) computes features 24 - 102 faster than R - CNN , while has better or comparable accuracy . With the recent fast proposal method of EdgeBoxes , our system takes 0.5 seconds processing an image ( including all steps ) . This makes our method practical for real - world applications . A preliminary version of this manuscript has been published in ECCV 2014 . Based on this work , we attended the competition of ILSVRC 2014 , and ranked # 2 in object detection and # 3 in image classification ( both are provided - data - only tracks ) among all 38 teams . There are a few modifications made for ILSVRC 2014 . We show that the SPP - nets can boost various networks that are deeper and larger ( Sec . [ reference ] - [ reference ] ) over the no - SPP counterparts . Further , driven by our detection framework , we find that multi - view testing on feature maps with flexibly located / sized windows ( Sec . [ reference ] ) can increase the classification accuracy . This manuscript also provides the details of these modifications . We have released the code to facilitate future research ( http: // research.microsoft.com / en - us / um / people / kahe / ) . section : Deep Networks with Spatial Pyramid Pooling subsection : Convolutional Layers and Feature Maps Consider the popular seven - layer architectures . The first five layers are convolutional , some of which are followed by pooling layers . These pooling layers can also be considered as \u201c convolutional \u201d , in the sense that they are using sliding windows . The last two layers are fully connected , with an N - way softmax as the output , where N is the number of categories . The deep network described above needs a fixed image size . However , we notice that the requirement of fixed sizes is only due to the fully - connected layers that demand fixed - length vectors as inputs . On the other hand , the convolutional layers accept inputs of arbitrary sizes . The convolutional layers use sliding filters , and their outputs have roughly the same aspect ratio as the inputs . These outputs are known as feature maps - they involve not only the strength of the responses , but also their spatial positions . In Figure [ reference ] , we visualize some feature maps . They are generated by some filters of the conv layer . Figure [ reference ] ( c ) shows the strongest activated images of these filters in the ImageNet dataset . We see a filter can be activated by some semantic content . For example , the 55 - th filter ( Figure [ reference ] , bottom left ) is most activated by a circle shape ; the 66 - th filter ( Figure [ reference ] , top right ) is most activated by a - shape ; and the 118 - th filter ( Figure [ reference ] , bottom right ) is most activated by a - shape . These shapes in the input images ( Figure [ reference ] ( a ) ) activate the feature maps at the corresponding positions ( the arrows in Figure [ reference ] ) . It is worth noticing that we generate the feature maps in Figure [ reference ] without fixing the input size . These feature maps generated by deep convolutional layers are analogous to the feature maps in traditional methods . In those methods , SIFT vectors or image patches are densely extracted and then encoded , , by vector quantization , sparse coding , or Fisher kernels . These encoded features consist of the feature maps , and are then pooled by Bag - of - Words ( BoW ) or spatial pyramids . Analogously , the deep convolutional features can be pooled in a similar way . subsection : The Spatial Pyramid Pooling Layer The convolutional layers accept arbitrary input sizes , but they produce outputs of variable sizes . The classifiers ( SVM / softmax ) or fully - connected layers require fixed - length vectors . Such vectors can be generated by the Bag - of - Words ( BoW ) approach that pools the features together . Spatial pyramid pooling improves BoW in that it can maintain spatial information by pooling in local spatial bins . These spatial bins have sizes proportional to the image size , so the number of bins is fixed regardless of the image size . This is in contrast to the sliding window pooling of the previous deep networks , where the number of sliding windows depends on the input size . To adopt the deep network for images of arbitrary sizes , we replace the last pooling layer ( , pool , after the last convolutional layer ) with a spatial pyramid pooling layer . Figure [ reference ] illustrates our method . In each spatial bin , we pool the responses of each filter ( throughout this paper we use max pooling ) . The outputs of the spatial pyramid pooling are - dimensional vectors with the number of bins denoted as ( is the number of filters in the last convolutional layer ) . The fixed - dimensional vectors are the input to the fully - connected layer . With spatial pyramid pooling , the input image can be of any sizes . This not only allows arbitrary aspect ratios , but also allows arbitrary scales . We can resize the input image to any scale ( , = 180 , 224 , \u2026 ) and apply the same deep network . When the input image is at different scales , the network ( with the same filter sizes ) will extract features at different scales . The scales play important roles in traditional methods , , the SIFT vectors are often extracted at multiple scales ( determined by the sizes of the patches and Gaussian filters ) . We will show that the scales are also important for the accuracy of deep networks . Interestingly , the coarsest pyramid level has a single bin that covers the entire image . This is in fact a \u201c global pooling \u201d operation , which is also investigated in several concurrent works . In a global average pooling is used to reduce the model size and also reduce overfitting ; in , a global average pooling is used on the testing stage after all fc layers to improve accuracy ; in , a global max pooling is used for weakly supervised object recognition . The global pooling operation corresponds to the traditional Bag - of - Words method . subsection : Training the Network Theoretically , the above network structure can be trained with standard back - propagation , regardless of the input image size . But in practice the GPU implementations ( such as cuda - convnet and Caffe ) are preferably run on fixed input images . Next we describe our training solution that takes advantage of these GPU implementations while still preserving the spatial pyramid pooling behaviors . subsubsection : Single - size training As in previous works , we first consider a network taking a fixed - size input ( 224 224 ) cropped from images . The cropping is for the purpose of data augmentation . For an image with a given size , we can pre - compute the bin sizes needed for spatial pyramid pooling . Consider the feature maps after conv that have a size of ( , 13 13 ) . With a pyramid level of bins , we implement this pooling level as a sliding window pooling , where the window size and stride with and denoting ceiling and floor operations . With an - level pyramid , we implement such layers . The next fully - connected layer ( fc ) will concatenate the outputs . Figure [ reference ] shows an example configuration of 3 - level pyramid pooling ( 3 3 , 2 2 , 1 1 ) in the cuda - convnet style . The main purpose of our single - size training is to enable the multi - level pooling behavior . Experiments show that this is one reason for the gain of accuracy . subsubsection : Multi - size training Our network with SPP is expected to be applied on images of any sizes . To address the issue of varying image sizes in training , we consider a set of pre - defined sizes . We consider two sizes : 180 180 in addition to 224 224 . Rather than crop a smaller 180 180 region , we resize the aforementioned 224 224 region to 180 180 . So the regions at both scales differ only in resolution but not in content / layout . For the network to accept 180 180 inputs , we implement another fixed - size - input ( 180 180 ) network . The feature map size after conv is 10 in this case . Then we still use and to implement each pyramid pooling level . The output of the spatial pyramid pooling layer of this 180 - network has the same fixed length as the 224 - network . As such , this 180 - network has exactly the same parameters as the 224 - network in each layer . In other words , during training we implement the varying - input - size SPP - net by two fixed - size networks that share parameters . To reduce the overhead to switch from one network ( , 224 ) to the other ( , 180 ) , we train each full epoch on one network , and then switch to the other one ( keeping all weights ) for the next full epoch . This is iterated . In experiments , we find the convergence rate of this multi - size training to be similar to the above single - size training . The main purpose of our multi - size training is to simulate the varying input sizes while still leveraging the existing well - optimized fixed - size implementations . Besides the above two - scale implementation , we have also tested a variant using as input where is randomly and uniformly sampled from at each epoch . We report the results of both variants in the experiment section . Note that the above single / multi - size solutions are for training only . At the testing stage , it is straightforward to apply SPP - net on images of any sizes . section : SPP - net for Image Classification subsection : Experiments on ImageNet 2012 Classification We train the networks on the 1000 - category training set of ImageNet 2012 . Our training algorithm follows the practices of previous work . The images are resized so that the smaller dimension is 256 , and a 224 224 crop is picked from the center or the four corners from the entire image . The data are augmented by horizontal flipping and color altering . Dropout is used on the two fully - connected layers . The learning rate starts from 0.01 , and is divided by 10 ( twice ) when the error plateaus . Our implementation is based on the publicly available code of cuda - convnet and Caffe . All networks in this paper can be trained on a single GeForce GTX Titan GPU ( 6 GB memory ) within two to four weeks . subsubsection : Baseline Network Architectures The advantages of SPP are independent of the convolutional network architectures used . We investigate four different network architectures in existing publications ( or their modifications ) , and we show SPP improves the accuracy of all these architectures . These baseline architectures are in Table [ reference ] and briefly introduced below : ZF - 5 : this architecture is based on Zeiler and Fergus \u2019s ( ZF ) \u201c fast \u201d ( smaller ) model . The number indicates five convolutional layers . Convnet* - 5 : this is a modification on Krizhevsky \u2019s network . We put the two pooling layers after conv and conv ( instead of after conv and conv ) . As a result , the feature maps after each layer have the same size as ZF - 5 . Overfeat - 5 / 7 : this architecture is based on the Overfeat paper , with some modifications as in . In contrast to ZF - 5 / Convnet* - 5 , this architecture produces a larger feature map ( instead of ) before the last pooling layer . A larger filter number ( 512 ) is used in conv and the following convolutional layers . We also investigate a deeper architecture with 7 convolutional layers , where conv to conv have the same structures . In the baseline models , the pooling layer after the last convolutional layer generates feature maps , with two 4096 - d fc layers and a 1000 - way softmax layer following . Our replications of these baseline networks are in Table [ reference ] ( a ) . We train 70 epochs for ZF - 5 and 90 epochs for the others . Our replication of ZF - 5 is better than the one reported in . This gain is because the corner crops are from the entire image , as is also reported in . subsubsection : Multi - level Pooling Improves Accuracy In Table [ reference ] ( b ) we show the results using single - size training . The training and testing sizes are both 224 224 . In these networks , the convolutional layers have the same structures as the corresponding baseline models , whereas the pooling layer after the final convolutional layer is replaced with the SPP layer . For the results in Table [ reference ] , we use a 4 - level pyramid . The pyramid is { 6 6 , 3 3 , 2 2 , 1 1 } ( totally 50 bins ) . For fair comparison , we still use the standard 10 - view prediction with each view a 224 224 crop . Our results in Table [ reference ] ( b ) show considerable improvement over the no - SPP baselines in Table [ reference ] ( a ) . Interestingly , the largest gain of top - 1 error ( 1.65 % ) is given by the most accurate architecture . Since we are still using the same 10 cropped views as in ( a ) , these gains are solely because of multi - level pooling . It is worth noticing that the gain of multi - level pooling is not simply due to more parameters ; rather , it is because the multi - level pooling is robust to the variance in object deformations and spatial layout . To show this , we train another ZF - 5 network with a different 4 - level pyramid : { 4 4 , 3 3 , 2 2 , 1 1 } ( totally 30 bins ) . This network has fewer parameters than its no - SPP counterpart , because its fc layer has 30 256 - d inputs instead of 36 256 - d . The top - 1 / top - 5 errors of this network are 35.06 / 14.04 . This result is similar to the 50 - bin pyramid above ( 34.98 / 14.14 ) , but considerably better than the no - SPP counterpart ( 35.99 / 14.76 ) . subsubsection : Multi - size Training Improves Accuracy Table [ reference ] ( c ) shows our results using multi - size training . The training sizes are 224 and 180 , while the testing size is still 224 . We still use the standard 10 - view prediction . The top - 1 / top - 5 errors of all architectures further drop . The top - 1 error of SPP - net ( Overfeat - 7 ) drops to 29.68 % , which is 2.33 % better than its no - SPP counterpart and 0.68 % better than its single - size trained counterpart . Besides using the two discrete sizes of 180 and 224 , we have also evaluated using a random size uniformly sampled from . The top - 1 / 5 error of SPP - net ( Overfeat - 7 ) is 30.06% / 10.96 % . The top - 1 error is slightly worse than the two - size version , possibly because the size of 224 ( which is used for testing ) is visited less . But the results are still better the single - size version . There are previous CNN solutions that deal with various scales / sizes , but they are mostly based on testing . In Overfeat and Howard \u2019s method , the single network is applied at multiple scales in the testing stage , and the scores are averaged . Howard further trains two different networks on low / high - resolution image regions and averages the scores . To our knowledge , our method is the first one that trains a single network with input images of multiple sizes . subsubsection : Full - image Representations Improve Accuracy Next we investigate the accuracy of the full - image views . We resize the image so that = 256 while maintaining its aspect ratio . The SPP - net is applied on this full image to compute the scores of the full view . For fair comparison , we also evaluate the accuracy of the single view in the center 224 224 crop ( which is used in the above evaluations ) . The comparisons of single - view testing accuracy are in Table [ reference ] . Here we evaluate ZF - 5 / Overfeat - 7 . The top - 1 error rates are all reduced by the full - view representation . This shows the importance of maintaining the complete content . Even though our network is trained using square images only , it generalizes well to other aspect ratios . Comparing Table [ reference ] and Table [ reference ] , we find that the combination of multiple views is substantially better than the single full - image view . However , the full - image representations are still of good merits . First , we empirically find that ( discussed in the next subsection ) even for the combination of dozens of views , the additional two full - image views ( with flipping ) can still boost the accuracy by about 0.2 % . Second , the full - image view is methodologically consistent with the traditional methods where the encoded SIFT vectors of the entire image are pooled together . Third , in other applications such as image retrieval , an image representation , rather than a classification score , is required for similarity ranking . A full - image representation can be preferred . subsubsection : Multi - view Testing on Feature Maps Inspired by our detection algorithm ( described in the next section ) , we further propose a multi - view testing method on the feature maps . Thanks to the flexibility of SPP , we can easily extract the features from windows ( views ) of arbitrary sizes from the convolutional feature maps . On the testing stage , we resize an image so where represents a predefined scale ( like 256 ) . Then we compute the convolutional feature maps from the entire image . For the usage of flipped views , we also compute the feature maps of the flipped image . Given any view ( window ) in the image , we map this window to the feature maps ( the way of mapping is in Appendix ) , and then use SPP to pool the features from this window ( see Figure [ reference ] ) . The pooled features are then fed into the fc layers to compute the softmax score of this window . These scores are averaged for the final prediction . For the standard 10 - view , we use and the views are 224 224 windows on the corners or center . Experiments show that the top - 5 error of the 10 - view prediction on feature maps is within 0.1 % around the original 10 - view prediction on image crops . We further apply this method to extract multiple views from multiple scales . We resize the image to six scales and compute the feature maps on the entire image for each scale . We use as the view size for any scale , so these views have different relative sizes on the original image for different scales . We use 18 views for each scale : one at the center , four at the corners , and four on the middle of each side , with / without flipping ( when = 224 there are 6 different views ) . The combination of these 96 views reduces the top - 5 error from 10.95 % to 9.36 % . Combining the two full - image views ( with flipping ) further reduces the top - 5 error to 9.14 % . In the Overfeat paper , the views are also extracted from the convolutional feature maps instead of image crops . However , their views can not have arbitrary sizes ; rather , the windows are those where the pooled features match the desired dimensionality . We empirically find that these restricted windows are less beneficial than our flexibly located / sized windows . subsubsection : Summary and Results for ILSVRC 2014 In Table [ reference ] we compare with previous state - of - the - art methods . Krizhevsky \u2019s is the winning method in ILSVRC 2012 ; Overfeat , Howard \u2019s , and Zeiler and Fergus \u2019s are the leading methods in ILSVRC 2013 . We only consider single - network performance for manageable comparisons . Our best single network achieves 9.14 % top - 5 error on the validation set . This is exactly the single - model entry we submitted to ILSVRC 2014 . The top - 5 error is 9.08 % on the testing set ( ILSVRC 2014 has the same training / validation / testing data as ILSVRC 2012 ) . After combining eleven models , our team \u2019s result ( 8.06 % ) is ranked # 3 among all 38 teams attending ILSVRC 2014 ( Table [ reference ] ) . Since the advantages of SPP - net should be in general independent of architectures , we expect that it will further improve the deeper and larger convolutional architectures . subsection : Experiments on VOC 2007 Classification Our method can generate a full - view image representation . With the above networks pre - trained on ImageNet , we extract these representations from the images in the target datasets and re - train SVM classifiers . In the SVM training , we intentionally do not use any data augmentation ( flip / multi - view ) . We l - normalize the features for SVM training . The classification task in Pascal VOC 2007 involves 9 , 963 images in 20 categories . 5 , 011 images are for training , and the rest are for testing . The performance is evaluated by mean Average Precision ( mAP ) . Table [ reference ] summarizes the results . We start from a baseline in Table [ reference ] ( a ) . The model is ZF - 5 without SPP . To apply this model , we resize the image so that its smaller dimension is 224 , and crop the center 224 224 region . The SVM is trained via the features of a layer . On this dataset , the deeper the layer is , the better the result is . In Table [ reference ] ( b ) , we replace the no - SPP net with our SPP - net . As a first - step comparison , we still apply the SPP - net on the center 224 224 crop . The results of the fc layers improve . This gain is mainly due to multi - level pooling . Table [ reference ] ( c ) shows our results on full images , where the images are resized so that the shorter side is 224 . We find that the results are considerably improved ( 78.39 % 76.45 % ) . This is due to the full - image representation that maintains the complete content . Because the usage of our network does not depend on scale , we resize the images so that the smaller dimension is and use the same network to extract features . We find that gives the best results ( Table [ reference ] ( d ) ) based on the validation set . This is mainly because the objects occupy smaller regions in VOC 2007 but larger regions in ImageNet , so the relative object scales are different between the two sets . These results indicate scale matters in the classification tasks , and SPP - net can partially address this \u201c scale mismatch \u201d issue . In Table [ reference ] ( e ) the network architecture is replaced with our best model ( Overfeat - 7 , multi - size trained ) , and the mAP increases to 82.44 % . Table [ reference ] summarizes our results and the comparisons with the state - of - the - art methods . Among these methods , VQ , LCC , and FK are all based on spatial pyramids matching , and are based on deep networks . In these results , Oquab \u2019s ( 77.7 % ) and Chatfield \u2019s ( 82.42 % ) are obtained by network fine - tuning and multi - view testing . Our result is comparable with the state of the art , using only a single full - image representation and without fine - tuning . subsection : Experiments on Caltech101 The Caltech101 dataset contains 9 , 144 images in 102 categories ( one background ) . We randomly sample 30 images per category for training and up to 50 images per category for testing . We repeat 10 random splits and average the accuracy . Table [ reference ] summarizes our results . There are some common observations in the Pascal VOC 2007 and Caltech101 results : SPP - net is better than the no - SPP net ( Table [ reference ] ( b ) ( a ) ) , and the full - view representation is better than the crop ( ( c ) ( b ) ) . But the results in Caltech101 have some differences with Pascal VOC . The fully - connected layers are less accurate , and the SPP layers are better . This is possibly because the object categories in Caltech101 are less related to those in ImageNet , and the deeper layers are more category - specialized . Further , we find that the scale 224 has the best performance among the scales we tested on this dataset . This is mainly because the objects in Caltech101 also occupy large regions of the images , as is the case of ImageNet . Besides cropping , we also evaluate warping the image to fit the 224 224 size . This solution maintains the complete content , but introduces distortion . On the SPP ( ZF - 5 ) model , the accuracy is 89.91 % using the SPP layer as features - lower than 91.44 % which uses the same model on the undistorted full image . Table [ reference ] summarizes our results compared with the state - of - the - art methods on Caltech101 . Our result ( 93.42 % ) exceeds the previous record ( 88.54 % ) by a substantial margin ( 4.88 % ) . section : SPP - net for Object Detection Deep networks have been used for object detection . We briefly review the recent state - of - the - art R - CNN method . R - CNN first extracts about 2 , 000 candidate windows from each image via selective search . Then the image region in each window is warped to a fixed size ( 227 227 ) . A pre - trained deep network is used to extract the feature of each window . A binary SVM classifier is then trained on these features for detection . R - CNN generates results of compelling quality and substantially outperforms previous methods . However , because R - CNN repeatedly applies the deep convolutional network to about 2 , 000 windows per image , it is time - consuming . Feature extraction is the major timing bottleneck in testing . Our SPP - net can also be used for object detection . We extract the feature maps from the entire image only once ( possibly at multiple scales ) . Then we apply the spatial pyramid pooling on each candidate window of the feature maps to pool a fixed - length representation of this window ( see Figure [ reference ] ) . Because the time - consuming convolutions are only applied once , our method can run orders of magnitude faster . Our method extracts window - wise features from regions of the feature maps , while R - CNN extracts directly from image regions . In previous works , the Deformable Part Model ( DPM ) extracts features from windows in HOG feature maps , and the Selective Search ( SS ) method extracts from windows in encoded SIFT feature maps . The Overfeat detection method also extracts from windows of deep convolutional feature maps , but needs to pre - define the window size . On the contrary , our method enables feature extraction in arbitrary windows from the deep convolutional feature maps . subsection : Detection Algorithm We use the \u201c fast \u201d mode of selective search to generate about 2 , 000 candidate windows per image . Then we resize the image such that , and extract the feature maps from the entire image . We use the SPP - net model of ZF - 5 ( single - size trained ) for the time being . In each candidate window , we use a 4 - level spatial pyramid ( 1 1 , 2 2 , 3 3 , 6 6 , totally 50 bins ) to pool the features . This generates a 12 , 800 - d ( 256 50 ) representation for each window . These representations are provided to the fully - connected layers of the network . Then we train a binary linear SVM classifier for each category on these features . Our implementation of the SVM training follows . We use the ground - truth windows to generate the positive samples . The negative samples are those overlapping a positive window by at most 30 % ( measured by the intersection - over - union ( IoU ) ratio ) . Any negative sample is removed if it overlaps another negative sample by more than 70 % . We apply the standard hard negative mining to train the SVM . This step is iterated once . It takes less than 1 hour to train SVMs for all 20 categories . In testing , the classifier is used to score the candidate windows . Then we use non - maximum suppression ( threshold of 30 % ) on the scored windows . Our method can be improved by multi - scale feature extraction . We resize the image such that , and compute the feature maps of conv for each scale . One strategy of combining the features from these scales is to pool them channel - by - channel . But we empirically find that another strategy provides better results . For each candidate window , we choose a single scale such that the scaled candidate window has a number of pixels closest to 224 224 . Then we only use the feature maps extracted from this scale to compute the feature of this window . If the pre - defined scales are dense enough and the window is approximately square , our method is roughly equivalent to resizing the window to 224 224 and then extracting features from it . Nevertheless , our method only requires computing the feature maps once ( at each scale ) from the entire image , regardless of the number of candidate windows . We also fine - tune our pre - trained network , following . Since our features are pooled from the conv feature maps from windows of any sizes , for simplicity we only fine - tune the fully - connected layers . In this case , the data layer accepts the fixed - length pooled features after conv , and the fc layers and a new 21 - way ( one extra negative category ) fc layer follow . The fc weights are initialized with a Gaussian distribution of = 0.01 . We fix all the learning rates to 1e - 4 and then adjust to 1e - 5 for all three layers . During fine - tuning , the positive samples are those overlapping with a ground - truth window by , and the negative samples by . In each mini - batch , 25 % of the samples are positive . We train 250k mini - batches using the learning rate 1e - 4 , and then 50k mini - batches using 1e - 5 . Because we only fine - tune the fc layers , the training is very fast and takes about 2 hours on the GPU ( excluding pre - caching feature maps which takes about 1 hour ) . Also following , we use bounding box regression to post - process the prediction windows . The features used for regression are the pooled features from conv ( as a counterpart of the pool features used in ) . The windows used for the regression training are those overlapping with a ground - truth window by at least 50 % . subsection : Detection Results We evaluate our method on the detection task of the Pascal VOC 2007 dataset . Table [ reference ] shows our results on various layers , by using 1 - scale ( = 688 ) or 5 - scale . Here the R - CNN results are as reported in using the AlexNet with 5 conv layers . Using the pool layers ( in our case the pooled features ) , our result ( 44.9 % ) is comparable with R - CNN \u2019s result ( 44.2 % ) . But using the non - fine - tuned fc layers , our results are inferior . An explanation is that our fc layers are pre - trained using image regions , while in the detection case they are used on the feature map regions . The feature map regions can have strong activations near the window boundaries , while the image regions may not . This difference of usages can be addressed by fine - tuning . Using the fine - tuned fc layers ( ftfc ) , our results are comparable with or slightly better than the fine - tuned results of R - CNN . After bounding box regression , our 5 - scale result ( 59.2 % ) is 0.7 % better than R - CNN ( 58.5 % ) , and our 1 - scale result ( 58.0 % ) is 0.5 % worse . In Table [ reference ] we further compare with R - CNN using the same pre - trained model of SPPnet ( ZF - 5 ) . In this case , our method and R - CNN have comparable averaged scores . The R - CNN result is boosted by this pre - trained model . This is because of the better architecture of ZF - 5 than AlexNet , and also because of the multi - level pooling of SPPnet ( if using the no - SPP ZF - 5 , the R - CNN result drops ) . Table [ reference ] shows the results for each category . Table [ reference ] also includes additional methods . Selective Search ( SS ) applies spatial pyramid matching on SIFT feature maps . DPM and Regionlet are based on HOG features . The Regionlet method improves to 46.1 % by combining various features including conv . DetectorNet trains a deep network that outputs pixel - wise object masks . This method only needs to apply the deep network once to the entire image , as is the case for our method . But this method has lower mAP ( 30.5 % ) . subsection : Complexity and Running Time Despite having comparable accuracy , our method is much faster than R - CNN . The complexity of the convolutional feature computation in R - CNN is with the window number ( 2000 ) . This complexity of our method is at a scale , where is the aspect ratio . Assume is about 4 / 3 . In the single - scale version when , this complexity is about 1 / 160 of R - CNN \u2019s ; in the 5 - scale version , this complexity is about 1 / 24 of R - CNN \u2019s . In Table [ reference ] , we provide a fair comparison on the running time of the feature computation using the same SPP ( ZF - 5 ) model . The implementation of R - CNN is from the code published by the authors implemented in Caffe . We also implement our feature computation in Caffe . In Table [ reference ] we evaluate the average time of 100 random VOC images using GPU . R - CNN takes 14.37s per image for convolutions , while our 1 - scale version takes only 0.053s per image . So ours is 270 faster than R - CNN . Our 5 - scale version takes 0.293s per image for convolutions , so is 49 faster than R - CNN . Our convolutional feature computation is so fast that the computational time of fc layers takes a considerable portion . Table [ reference ] shows that the GPU time of computing the 4 , 096 - d fc features is 0.089s per image . Considering both convolutional and fully - connected features , our 1 - scale version is 102\u00d7 faster than R - CNN and is 1.2 % inferior ; our 5 - scale version is 38\u00d7 faster and has comparable results . We also compares the running time in Table [ reference ] where R - CNN uses AlexNet as is in the original paper . Our method is 24 to 64 faster . Note that the AlexNet has the same number of filters as our ZF - 5 on each conv layer . The AlexNet is faster because it uses splitting on some layers , which was designed for two GPUs in . We further achieve an efficient full system with the help of the recent window proposal method . The Selective Search ( SS ) proposal takes about 1 - 2 seconds per image on a CPU . The method of EdgeBoxes only takes 0.2s . Note that it is sufficient to use a fast proposal method during testing only . Using the same model trained as above ( using SS ) , we test proposals generated by EdgeBoxes only . The mAP is 52.8 without bounding box regression . This is reasonable considering that EdgeBoxes are not used for training . Then we use both SS and EdgeBox as proposals in the training stage , and adopt only EdgeBoxes in the testing stage . The mAP is 56.3 without bounding box regression , which is better than 55.2 ( Table [ reference ] ) due to additional training samples . In this case , the overall testing time is 0.5s per image including all steps ( proposal and recognition ) . This makes our method practical for real - world applications . subsection : Model Combination for Detection Model combination is an important strategy for boosting CNN - based classification accuracy . We propose a simple combination method for detection . We pre - train another network in ImageNet , using the same structure but different random initializations . Then we repeat the above detection algorithm . Table [ reference ] ( SPP - net ( 2 ) ) shows the results of this network . Its mAP is comparable with the first network ( 59.1 % 59.2 % ) , and outperforms the first network in 11 categories . Given the two models , we first use either model to score all candidate windows on the test image . Then we perform non - maximum suppression on the union of the two sets of candidate windows ( with their scores ) . A more confident window given by one method can suppress those less confident given by the other method . After combination , the mAP is boosted to 60.9 % ( Table [ reference ] ) . In 17 out of all 20 categories the combination performs better than either individual model . This indicates that the two models are complementary . We further find that the complementarity is mainly because of the convolutional layers . We have tried to combine two randomly initialized fine - tuned results of the same convolutional model , and found no gain . subsection : ILSVRC 2014 Detection The ILSVRC 2014 detection task involves 200 categories . There are 450k / 20k / 40k images in the training / validation / testing sets . We focus on the task of the provided - data - only track ( the 1000 - category CLS training data is not allowed to use ) . There are three major differences between the detection ( DET ) and classification ( CLS ) training datasets , which greatly impacts the pre - training quality . First , the DET training data is merely 1 / 3 of the CLS training data . This seems to be a fundamental challenge of the provided - data - only DET task . Second , the category number of DET is 1 / 5 of CLS . To overcome this problem , we harness the provided subcategory labels for pre - training . There are totally 499 non - overlapping subcategories ( , the leaf nodes in the provided category hierarchy ) . So we pre - train a 499 - category network on the DET training set . Third , the distributions of object scales are different between DET / CLS training sets . The dominant object scale in CLS is about 0.8 of the image length , but in DET is about 0.5 . To address the scale difference , we resize each training image to ( instead of ) , and randomly crop views for training . A crop is only used when it overlaps with a ground truth object by at least 50 % . We verify the effect of pre - training on Pascal VOC 2007 . For a CLS - pre - training baseline , we consider the pool features ( mAP 43.0 % in Table [ reference ] ) . Replaced with a 200 - category network pre - trained on DET , the mAP significantly drops to 32.7 % . A 499 - category pre - trained network improves the result to 35.9 % . Interestingly , even if the amount of training data do not increase , training a network of more categories boosts the feature quality . Finally , training with instead of further improves the mAP to 37.8 % . Even so , we see that there is still a considerable gap to the CLS - pre - training result . This indicates the importance of big data to deep learning . For ILSVRC 2014 , we train a 499 - category Overfeat - 7 SPP - net . The remaining steps are similar to the VOC 2007 case . Following , we use the validation set to generate the positive / negative samples , with windows proposed by the selective search fast mode . The training set only contributes positive samples using the ground truth windows . We fine - tune the fc layers and then train the SVMs using the samples in both validation and training sets . The bounding box regression is trained on the validation set . Our single model leads to 31.84 % mAP in the ILSVRC 2014 testing set . We combine six similar models using the strategy introduced in this paper . The mAP is 35.11 % in the testing set . This result ranks # 2 in the provided - data - only track of ILSVRC 2014 ( Table [ reference ] ) . The winning result is 37.21 % from NUS , which uses contextual information . Our system still shows great advantages on speed for this dataset . It takes our single model 0.6 seconds ( 0.5 for conv , 0.1 for fc , excluding proposals ) per testing image on a GPU extracting convolutional features from all 5 scales . Using the same model , it takes 32 seconds per image in the way of RCNN . For the 40k testing images , our method requires 8 GPU hours to compute convolutional features , while RCNN would require 15 GPU days . section : Conclusion SPP is a flexible solution for handling different scales , sizes , and aspect ratios . These issues are important in visual recognition , but received little consideration in the context of deep networks . We have suggested a solution to train a deep network with a spatial pyramid pooling layer . The resulting SPP - net shows outstanding accuracy in classification / detection tasks and greatly accelerates DNN - based detection . Our studies also show that many time - proven techniques / insights in computer vision can still play important roles in deep - networks - based recognition . section : In the appendix , we describe some implementation details : Mean Subtraction . The 224 224 cropped training / testing images are often pre - processed by subtracting the per - pixel mean . When input images are in any sizes , the fixed - size mean image is not directly applicable . In the ImageNet dataset , we warp the 224 224 mean image to the desired size and then subtract it . In Pascal VOC 2007 and Caltech101 , we use the constant mean ( 128 ) in all the experiments . Implementation of Pooling Bins . We use the following implementation to handle all bins when applying the network . Denote the width and height of the conv feature maps ( can be the full image or a window ) as and . For a pyramid level with bins , the - th bin is in the range of . Intuitively , if rounding is needed , we take the floor operation on the left / top boundary and ceiling on the right / bottom boundary . Mapping a Window to Feature Maps . In the detection algorithm ( and multi - view testing on feature maps ) , a window is given in the image domain , and we use it to crop the convolutional feature maps ( , conv ) which have been sub - sampled several times . So we need to align the window on the feature maps . In our implementation , we project the corner point of a window onto a pixel in the feature maps , such that this corner point in the image domain is closest to the center of the receptive field of that feature map pixel . The mapping is complicated by the padding of all convolutional and pooling layers . To simplify the implementation , during deployment we pad pixels for a layer with a filter size of . As such , for a response centered at , its effective receptive field in the image domain is centered at where is the product of all previous strides . In our models , for ZF - 5 on conv , and for Overfeat - 5 / 7 on conv . Given a window in the image domain , we project the left ( top ) boundary by : and the right ( bottom ) boundary . If the padding is not , we need to add a proper offset to . bibliography : References section : Changelog arXiv v1 . Initial technical report for ECCV 2014 paper . arXiv v2 . Submitted version for TPAMI . Includes extra experiments of SPP on various architectures . Includes details for ILSVRC 2014 . arXiv v3 . Accepted version for TPAMI . Includes comparisons with R - CNN using the same architecture . Includes detection experiments using EdgeBoxes . arXiv v4 . Revised \u201c Mapping a Window to Feature Maps \u201d in Appendix for easier implementation .", "templates": [{"incident_type": "SciREX_incident", "Material": [[["PASCAL_VOC_2007"]]], "Method": [[["SPP__Overfeat-7_"]]], "Metric": [[["MAP"]]], "Task": [[["Object_Detection"]]]}]}
